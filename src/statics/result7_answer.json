[
  [
    "The psychological framework of implicit bias can address the unconscious racial bias exhibited by police officers by providing training and education to increase awareness of these biases and implementing strategies to mitigate their impact on behavior towards certain individuals.",
    [
      "Racial bias in criminal news in the United States Racial biases are a form of implicit bias, which refers to the attitudes or stereotypes that affect an individual's understanding, actions, and decisions in an unconscious manner. These biases, which encompass unfavorable assessments, are often activated involuntarily and without the awareness or intentional control of the individual. Residing deep in the subconscious, these biases are different from known biases that individuals may choose to conceal for the purposes of social and/or political correctness. Police officers come from all walks of life and they too have implicit bias, regardless of their ethnicity.",
      "Implicit stereotype An implicit bias, or implicit stereotype, is the unconscious attribution of particular qualities to a member of a certain social group. Implicit stereotypes are shaped by experience and based on learned associations between particular qualities and social categories, including race and/or gender. Individuals' perceptions and behaviors can be influenced by the implicit stereotypes they hold, even if they are unaware/unintentionally hold such stereotypes. Implicit bias is an aspect of implicit social cognition: the phenomenon that perceptions, attitudes, and stereotypes operate without conscious intention. The existence of implicit bias is supported by a variety of scientific articles in psychological",
      "concerning the Michael Brown shooting in Ferguson, Missouri, the Justice Department admits to the pattern of racial bias of the Police Department in Ferguson. The department argues that it is typically an effort to ticket as many low-income Black residents as possible in an attempt to raise local budget revenue through fines and court fees. The Justice Department explains, police encounters could get downright abusive when the person being questioned by the police officers gets disrespectful or challenges their authority. In addition, The Department of Justice released a statement that confronts police officers\u2019 susceptibility to implicit bias: One of the"
    ]
  ],
  [
    "Hicham El Guerrouj currently holds the men's world record for the mile race with a time of 3:43.13.",
    [
      "Mile run world record progression The world record in the mile run is the best mark set by a male or female runner in the middle-distance track and field event. The IAAF is the official body which oversees the records. Hicham El Guerrouj is the current men's record holder with his time of 3:43.13, while Svetlana Masterkova has the women's record of 4:12.56. Since 1976, the mile has been the only non-metric distance recognized by the IAAF for record purposes. Accurate times for the mile run (1.609344 km) were not recorded until after 1850, when the first precisely measured running",
      "Decker ran 4:17.55 in Houston on 16 February 1980, and Natalya Artyomova (Soviet Union) ran 4:15.8 in Leningrad on 6 August 1984, but neither time was ratified by the IAAF. Mile run world record progression The world record in the mile run is the best mark set by a male or female runner in the middle-distance track and field event. The IAAF is the official body which oversees the records. Hicham El Guerrouj is the current men's record holder with his time of 3:43.13, while Svetlana Masterkova has the women's record of 4:12.56. Since 1976, the mile has been the",
      "relatively flat courses near sea level, during good weather conditions and with the assistance of pacesetters. The current world record time for men over the distance is 2 hours 1 minute and 39 seconds, set in the Berlin Marathon by Eliud Kipchoge of Kenya on 16 September 2018, an improvement of 1 minute 18 seconds over the previous record also set in the Berlin Marathon by Dennis Kipruto Kimetto, also of Kenya on 28 September 2014. The world record for women was set by Paula Radcliffe of Great Britain in the London Marathon on 13 April 2003, in 2 hours"
    ]
  ],
  [
    "The chemically based sensory system is the olfactory system, responsible for the sense of smell and detecting airborne substances through transduction (Olfactory system).",
    [
      "perceptual counterpart (for example, testing how much darker a computer screen can get before the viewer actually notices). The study of perception gave rise to the Gestalt school of psychology, with its emphasis on holistic approach. A \"sensory system\" is a part of the nervous system responsible for processing sensory information. A sensory system consists of sensory receptors, neural pathways, and parts of the brain involved in sensory perception. Commonly recognized sensory systems are those for vision, hearing, somatic sensation (touch), taste and olfaction (smell). It has been suggested that the immune system is an overlooked sensory modality. In short,",
      "or Medicine for their work on the olfactory system. Olfactory system The olfactory system, or sense of smell, is the part of the sensory system used for smelling (olfaction). Most mammals and reptiles have a main olfactory system and an accessory olfactory system. The main olfactory system detects airborne substances, while the accessory system senses fluid-phase stimuli. The senses of smell and taste (gustatory system) are often referred to together as the chemosensory system, because they both give the brain information about the chemical composition of objects through a process called transduction. The peripheral olfactory system consists mainly of the",
      "Sensory nervous system The sensory nervous system is a part of the nervous system responsible for processing sensory information. A sensory system consists of sensory neurons (including the sensory receptor cells), neural pathways, and parts of the brain involved in sensory perception. Commonly recognized sensory systems are those for vision, hearing, touch, taste, smell, and balance. In short, senses are transducers from the physical world to the realm of the mind where we interpret the information, creating our perception of the world around us. The receptive field is the area of the body or environment to which a receptor organ"
    ]
  ],
  [
    "The complete resynthesis of phosphocreatine after very high intensity exercise can take approximately 3 to 5 minutes.",
    [
      "that combines phosphocreatine and adenosine diphosphate (ADP) into ATP and creatine. This resource is short lasting because oxygen is required for the resynthesis of phosphocreatine via mitochondrial creatine kinase. Therefore, under anaerobic conditions, this substrate is finite and only lasts between approximately 10 to 30 seconds of high intensity work. Fast glycolysis, however, can function for approximately 2 minutes prior to fatigue, and predominately uses intracellular glycogen as a substrate. Glycogen is broken down rapidly via glycogen phosphorylase into individual glucose units during intense exercise. Glucose is then oxidized to pyruvate and under anaerobic conditions is reduced to lactic acid.",
      "inorganic phosphate for ATP formation. At the onset of exercise phosphocreatine is broken down to provide ATP for muscle contraction. ATP hydrolysis results in products of ADP and inorganic phosphate. The inorganic phosphate will be transported into the mitochondrial matrix, while the free creatine passes through the outer membrane where it will be resynthesised into PCr. The antiporter transports the ADP into the matrix, while transporting ATP out. Due to the high concentration of ATP around the mitrochondrial creatine kinase, it will convert ATP into PCr which will then move back out into the cells cytoplasm to be converted into",
      "and 85 W. Energy needed to perform short lasting, high intensity bursts of activity is derived from anaerobic metabolism within the cytosol of muscle cells, as opposed to aerobic respiration which utilizes oxygen, is sustainable, and occurs in the mitochondria. The quick energy sources consist of the phosphocreatine (PCr) system, fast glycolysis, and adenylate kinase. All of these systems re-synthesize adenosine triphosphate (ATP), which is the universal energy source in all cells. The most rapid source, but the most readily depleted of the above sources is the PCr system which utilizes the enzyme creatine kinase. This enzyme catalyzes a reaction"
    ]
  ],
  [
    "The car must accelerate at approximately 4 m/s^2 to reach a velocity of 130 km/h after traveling a distance of 50m.",
    [
      "0 to 60 mph The time it takes to accelerate from 0 to 60 mph (0 to 97 km/h or 0 to 27 m/s) is a commonly used performance measure for automotive acceleration in the United States and the United Kingdom. In the rest of the world, 0 to 100 km/h (0 to 62.1 mph) is used. Present performance cars are capable of going from 0 to 60 mph in under 6 seconds, while exotic cars can do 0 to 60 mph in between 3 and 4 seconds, whereas motorcycles have been able to achieve these figures with sub-500cc since",
      "to 50 km/h in 8 s (manual: 4 s in the 5th and 4 s in the 4th gear [sic]) and cruises for 69 s, then slowly accelerates to 70 km/h in 13 s . At 201 s, the car cruises at 70 km/h for 50 s (manual: in the 5th gear), then slowly accelerates to 100 km/h in 35 s and cruises for 30 s (manual: in the 5th or 6th gear). Finally, at 316 s the car slowly accelerates to 120 km/h in 20 s, cruises for 10 s, then slowly brakes to a full stop in 34",
      "short period of time, is called \"instantaneous speed\". By looking at a speedometer, one can read the instantaneous speed of a car at any instant. A car travelling at 50 km/h generally goes for less than one hour at a constant speed, but if it did go at that speed for a full hour, it would travel 50 km. If the vehicle continued at that speed for half an hour, it would cover half that distance (25 km). If it continued for only one minute, it would cover about 833 m. In mathematical terms, the instantaneous speed formula_2 is defined"
    ]
  ],
  [
    "Once inside the cytosol, fatty acids are transported into the mitochondrial matrix for beta-oxidation through specific transport proteins, such as the SLC27 family fatty acid transport protein.",
    [
      "activated, the acyl CoA is transported into the mitochondrial matrix. This occurs via a series of similar steps: It is important to note that \"carnitine acyltransferase I\" undergoes allosteric inhibition as a result of malonyl-CoA, an intermediate in fatty acid biosynthesis, in order to prevent futile cycling between beta-oxidation and fatty acid synthesis. The mitochondrial oxidation of fatty acids takes place in three major steps: 1.\u03b2-oxidation: conversion of fatty acids into 2-carbon acetyl Co-A units. 2.entry of acetyl Co-A into TCA cycle to yield energy. 3.finally, the electron transport chain in the mitochondria.though in this step, no direct participation of",
      "overall reaction for one cycle of beta oxidation is: Fatty acid catabolism consists of: Free fatty acids cannot penetrate any biological membrane due to their negative charge. Free fatty acids must cross the cell membrane through specific transport proteins, such as the SLC27 family fatty acid transport protein. Once in the cytosol, the following processes bring fatty acids into the mitochondrial matrix so that beta-oxidation can take place. Once the fatty acid is inside the mitochondrial matrix, beta-oxidation occurs by cleaving two carbons every cycle to form acetyl-CoA. The process consists of 4 steps. Fatty acids are oxidized by most",
      "fatty acids, the reaction continues. Once inside the mitochondria, the \u03b2-oxidation of fatty acids occurs via five recurring steps: Fatty acid degradation Fatty acid degradation is the process in which fatty acids are broken down into their metabolites, in the end generating acetyl-CoA, the entry molecule for the citric acid cycle, the main energy supply of animals. It includes three major steps: Initially in the process of degradation, fatty acids are stored in fat cells (adipocytes). The breakdown of this fat is known as lipolysis. The products of lipolysis, free fatty acids, are released into the bloodstream and circulate throughout"
    ]
  ],
  [
    "The KIHD study found that sauna use is associated with a 37 percent reduction in coronary mortality, suggesting a reduced risk of cardiovascular-related death and disease.",
    [
      "intake. However, it is usually used in combination with a sodium-controlled diet. Most research has shown that reducing sodium intake reduces the risk of cardiovascular disease(CVD) and all-cause mortality rate. A study by Langford in 1983 observed that populations that consume less sodium also tend to have relatively high potassium intake, and a lower rate of CVD. Within the USA, he also noticed racial and class differences in CVD, which he suggests may be due to sodium being cheaper to acquire in the diet than potassium, since many products contain added salt. Reddy and Katan recommend a salt intake below",
      "siesta habit has been associated with a 37 percent reduction in coronary mortality, possibly due to reduced cardiovascular stress mediated by daytime sleep. Epidemiological studies on the relations between cardiovascular health and siesta have led to conflicting conclusions, possibly because of poor control of confounding variables, such as physical activity. It is possible that people who take a siesta have different physical activity habits, for example, waking earlier and scheduling more activity during the morning. Such differences in physical activity may lead to different 24-hour profiles in cardiovascular function. Even if such effects of physical activity can be discounted in",
      "from wearing a sauna suit gives the impression of getting \"a good workout\". Sauna suits are also worn for body wrapping in some health spas. The wearer is first wrapped in bandages saturated with mineral and/or herbal preparations and then covered with a sauna suit. The objective of this process is to enable the wearer to \"sweat out toxins\", however there is little scientific evidence to support the effectiveness of this practice. Sauna suits are often worn specifically for such sweat excretion rather for exercise or weight loss. A medical use for a sauna suit is in the treatment of"
    ]
  ],
  [
    "DFP interacts with acetylcholinesterase in the synapses of neurons by irreversibly phosphorylating the enzyme, inhibiting the binding of acetylcholine to the esteratic site and leading to the accumulation of acetylcholine in the synapses, causing continuous activation of acetylcholine receptors and resulting in paralysis and potential death.",
    [
      "enzyme that deactivates the neurotransmitter acetylcholine. Neurotransmitters are needed to continue the passage of nerve impulses from one neuron to another across the synapse. Once the impulse has been transmitted, acetylcholinesterase functions to deactivate the acetylcholine almost immediately by breaking it down. If the enzyme is inhibited, acetylcholine accumulates and nerve impulses cannot be stopped, causing prolonged muscle contraction. Paralysis occurs and death may result since the respiratory muscles are affected. DFP also inhibits some proteases. It is a useful additive for protein or cell isolation procedure. Isoflurophate, the diisopropyl ester of fluorophosphoric acid, is made by reacting isopropyl alcohol",
      "a single phosphate molecule. In this way the cholinesterase is being phosphorylated. This phosphorylation inhibits the binding of the acetyl group of the acetylcholine to the esteratic site of the cholinesterase. Because the acetyl group can\u2019t bind the cholinesterase, the acetylcholine can\u2019t be cleaved. Therefore the acetylcholine will remain intact and will accumulate in the synapses. This results in continuous activation of acetylcholine receptors, which leads to the acute symptoms of TEPP poisoning. The phosphorylation of cholinesterase by TEPP (or any other organophosphate) is irreversible. This makes the inhibition of the cholinesterase permanent. The cholinesterase gets irreversible phosphorylated according to",
      "systems to muscle fibres. Normally, acetylcholine is released from the neuron to stimulate the muscle, after which it is degraded by acetylcholinesterase, allowing the muscle to relax. A build-up of acetylcholine in the synaptic cleft, due to the inhibition of cholinesterase, means the neurotransmitter continues to act on the muscle fibre, so that any nerve impulses are effectively continually transmitted. Sarin acts on cholinesterase by forming a covalent bond with the particular serine residue at the active site. Fluoride is the leaving group, and the resulting phosphoester is robust and biologically inactive. Its mechanism of action resembles that of some"
    ]
  ],
  [
    "An essential component in the process of drafting and adopting legal acts within the European Institutions is the adoption of regulations, which are legal acts that become immediately enforceable as law in all member states simultaneously (Regulation (European Union)).",
    [
      "legislative acts. Their function is to fill in the detail omitted by legislative acts. Legal Acts of the European Union Legal Acts of the European Union are laws which are adopted by the Institutions of the European Union in order to exercise the powers given to them by the EU Treaties. They come in five forms: regulations, directives, decisions, recommendations and opinions. Regulations and directives can be either legislative or non-legislative acts. Legislative acts are normally adopted by the Council of the European Union and the European Parliament acting together, and have their legal basis in the treaties. Non-legislative acts",
      "Legal Acts of the European Union Legal Acts of the European Union are laws which are adopted by the Institutions of the European Union in order to exercise the powers given to them by the EU Treaties. They come in five forms: regulations, directives, decisions, recommendations and opinions. Regulations and directives can be either legislative or non-legislative acts. Legislative acts are normally adopted by the Council of the European Union and the European Parliament acting together, and have their legal basis in the treaties. Non-legislative acts are adopted by the European Commission in pursuance with powers given to it by",
      "Regulation (European Union) A regulation is a legal act of the European Union that becomes immediately enforceable as law in all member states simultaneously. Regulations can be distinguished from directives which, at least in principle, need to be transposed into national law. Regulations can be adopted by means of a variety of legislative procedures depending on their subject matter. The description of regulations can be found in Article 288 of the Treaty on the Functioning of the European Union (formerly Article 249 TEC). Article 288 To exercise the Union's competences, the institutions shall adopt regulations, directives, decisions, recommendations and opinions."
    ]
  ],
  [
    "Performance enhancing synthetic steroids are structurally related to the hormone testosterone, as they are classified as anabolic agents that directly increase muscle protein synthesis, leading to muscle fiber growth and improved performance (Testosterone (medication); Anabolic\u2013androgenic steroids (AAS)).",
    [
      "form of doping among athletes in order to improve performance. Testosterone is classified as an anabolic agent and is on the World Anti-Doping Agency (WADA) List of Prohibited Substances and Methods. Hormone supplements cause the endocrine system to adjust its production and lower the natural production of the hormone, so when supplements are discontinued, natural hormone production is lower than it was originally. Anabolic\u2013androgenic steroids (AAS), including testosterone and its esters, have also been taken to enhance muscle development, strength, or endurance. They do so directly by increasing the muscles' protein synthesis. As a result, muscle fibers become larger and",
      "however, suggests a curvilinear or even quadratic relationship between spatial performance and circulating testosterone, where both hypo- and hypersecretion (deficient- and excessive-secretion) of circulating androgens have negative effects on cognition. The effects of testosterone in humans and other vertebrates occur by way of multiple mechanisms: by activation of the androgen receptor (directly or as DHT), and by conversion to estradiol and activation of certain estrogen receptors. Androgens such as testosterone have also been found to bind to and activate membrane androgen receptors. Free testosterone (T) is transported into the cytoplasm of target tissue cells, where it can bind to the",
      "Testosterone-cortisol ratio In human biology, the testosterone-cortisol ratio describes the ratio between testosterone, the primary male sex hormone and an anabolic steroid, and cortisol, another steroid hormone, in the human body. The ratio is often used as a biomarker of physiological stress in athletes during training, during athletic performance, and during recovery, and has been explored as a predictor of performance. At least among weight-lifters, the ratio tracks linearly with increases in training volume over the first year of training but the relationship breaks down after that. A lower ratio in weight-lifters just prior to performance appears to predict better"
    ]
  ],
  [
    "The key factors that contribute to the ease of deprotonation in strong acids include the presence of resonance stabilization, the electron-rich nature of the conjugate base formed, and the ability of the base to remove a proton from the acid molecule.",
    [
      "this ability as well. This occurs typically in compounds such as butyl lithium, alkoxides, and metal amides such as sodium amide. Bases of carbon, nitrogen and oxygen without resonance stabilization are usually very strong, or superbases, which cannot exist in a water solution due to the acidity of water. Resonance stabilization, however, enables weaker bases such as carboxylates; for example, sodium acetate is a weak base. A strong base is a basic chemical compound that can remove a proton (H) from (or \"deprotonate\") a molecule of even a very weak acid (such as water) in an acid-base reaction. Common examples",
      "hydride forms hydrogen gas with the liberated proton from the other molecule. The hydrogen is dangerous and could ignite with the oxygen in the air, so the chemical procedure should be done in an inert atmosphere (e.g., nitrogen). Deprotonation can be an important step in a chemical reaction. Acid/base reactions typically occur faster than any other step which may determine the product of a reaction. The conjugate base is more electron-rich than the molecule which can alter the reactivity of the molecule. For example, deprotonation of an alcohol forms the negatively charged alkoxide, which is a much stronger nucleophile. Deprotonation",
      "Deprotonation Deprotonation is the removal (transfer) of a proton (a hydrogen cation, H) from a Br\u00f8nsted\u2013Lowry acid in an acid-base reaction. The species formed is the conjugate base of that acid. The complementary process, when a proton is added (transferred) to a Br\u00f8nsted\u2013Lowry base, is protonation. The species formed is the conjugate acid of that base. A species that can either accept or donate a proton is referred to as amphiprotic. An example is the HO (water) molecule, which can gain a proton to form the hydronium ion, HO, or lose a proton, leaving the hydroxide ion, OH. The relative"
    ]
  ],
  [
    "The major product in the reaction involving 2-chlorobutane in a strong solution of ethanol is 2-butene due to allylic rearrangement and the use of a strong nucleophile.",
    [
      "Hydration reaction In chemistry, a hydration reaction is a chemical reaction in which a substance combines with water. In organic chemistry, water is added to an unsaturated substrate, which is usually an alkene or an alkyne. This type of reaction is employed industrially to produce ethanol, isopropanol, and 2-butanol. Several billion kilograms of ethylene glycol are produced annually by the hydration of oxirane, a cyclic compound also known as ethylene oxide: Acid catalysts are typically used. For the hydration of alkenes, the general chemical equation of the reaction is the following: A hydroxyl group (OH) attaches to one carbon of",
      "process referred to as S2' substitution. This is likely in cases when the allyl compound is unhindered, and a strong nucleophile is used. The products will be similar to those seen with S1' substitution. Thus reaction of 1-chloro-2-butene with sodium hydroxide gives a mixture of 2-buten-1-ol and 3-buten-2-ol: Nevertheless, the product in which the OH group is on the primary atom is minor. In the substitution of 1-chloro-3-methyl-2-butene, the tertiary 2-methyl-3-buten-2-ol is produced in a yield of 85%, while that for the primary 3-methyl-2-buten-1-ol is 15%. In one reaction mechanism the nucleophile attacks not directly at the electrophilic site but",
      "oxygen. Hexachlorobutadiene occurs as a by-product during the chlorinolysis of butane derivatives in the production of both carbon tetrachloride and tetrachloroethene. These two commodities are manufactured on such a large scale, that enough HCBD can generally be obtained to meet the industrial demand. Alternatively, hexachlorobutadiene can be directly synthesized via the chlorination of butane or butadiene. The products of chlorinolysis reactions heavily depend upon both the temperature and pressure under which the reaction occurs. Thus, by adjusting these reaction conditions in the presence of chlorine gas, hexachlorobutadiene can be even further chlorinated to give tetrachloroethylene, hexachloroethane, octachlorobutene, and even decachlorobutane."
    ]
  ],
  [
    "The teenager in the group who has violated societal norms to the greatest degree is likely engaging in anti-social behaviors such as stealing, vandalism, sexual promiscuity, excessive smoking, heavy drinking, confrontations with parents, and gambling (Schraffenberger).",
    [
      "pertain to the norms and values of their current society. One example of this is gender roles; from a young age, schools teach children to act in particular manners based on their gender. A peer group can be identified as a group of individuals who are similar in age and social class. By joining peer groups, children begin to detach from the authority the family has imposed in them, and start making choices of their own. Negative influences from peer groups can also lead to deviant behavior due to peer pressure. These groups in an individual's life have significant effects",
      "They are more likely to overestimate the risks, in fact. Teens also, however, will take risks because they find the reward (such as instant gratification or peer acceptance) more valuable. Not all teenage rebellion takes the form of violation of rules (i.e. illegal activity such as drug and alcohol abuse, vandalism, theft and other delinquency). Often teenage rebellion takes form in the violation of societal norms. And as these norms are set in place as much by teens themselves as by their adult caretakers, teenage rebellion within teenage culture is also commonplace. Rebecca Schraffenberger comments in her article \"This Modern",
      "a British criminologist and forensic psychologist, stated that teenagers can exhibit anti-social behaviour by engaging in various amounts of wrongdoings such as stealing, vandalism, sexual promiscuity, excessive smoking, heavy drinking, confrontations with parents, and gambling. \"Anti-social\" is frequently used, incorrectly, to mean either \"nonsocial\" or \"unsociable\". The words are not synonyms. Anti-social behaviour is typically associated with other behavioural and developmental issues such as hyperactivity, depression, learning disabilities and impulsivity. Alongside these issues one can be predisposed or more inclined to develop such behaviour due to one's genetics, neurobiological and environmental stressors in the prenatal stage of one's life, through"
    ]
  ],
  [
    "The concept of maximum power, defined as the maximum rate of useful energy transformation, is a guiding principle in ecological systems that aims to unify theories of electronic, thermodynamic, and biological systems to optimize energy efficiency and sustainability.",
    [
      "\u2013 to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine. Odum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example,",
      "Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power. The mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)",
      "Odum saw it in open systems operating on solar energy, like both photovoltaics and photosynthesis (1963, p. 438). Like the maximum power theorem, Odum's statement of the maximum power principle relies on the notion of 'matching', such that high-quality energy maximizes power by matching and amplifying energy (1994, pp. 262, 541): \"in surviving designs a matching of high-quality energy with larger amounts of low-quality energy is likely to occur\" (1994, p. 260). As with electronic circuits, the resultant rate of energy transformation will be at a maximum at an intermediate power efficiency. In 2006, T.T. Cai, C.L. Montague and J.S."
    ]
  ],
  [
    "Patients with disturbances in their dopamine system may exhibit symptoms such as pathological addiction-like patterns of medication intake, mood disorders, behavioral disorders, altered perceptions, tremors, slowness, stiffness, impaired balance, rigidity of muscles, fatigue, depression, difficulty swallowing, sexual problems, cognitive changes, and memory dysfunction.",
    [
      "has to increase medication intake beyond dosage needed to relieve their parkinsonian symptoms in a pathological addiction-like pattern. A current mood disorder (depression, anxiety, hypomanic state or euphoria), behavioral disorder (excessive gambling, shopping or sexual tendency, aggression, or social isolation) or an altered perception about the effect of medication also have to be present. A questionnaire on the typical symptoms of DDS has also been developed and can help in the diagnosis process. The main prevention measure proposed is the prescription of the lowest possible dose of dopamine replacement therapy to individuals at risk. The minimization of the use of",
      "a lot of the same neuropathologic and behavioral features. Movement is normally controlled by dopamine; a chemical that carries signals between the nerves in the brain. When cells that normally produce dopamine die off, the symptoms of Parkinson's appear. This degeneration also occurs in normal aging but is a much slower process. The most common symptoms include: tremors, slowness, stiffness, impaired balance, rigidity of the muscles, and fatigue. As the disease progresses, non-motor symptoms may also appear, such as depression, difficulty swallowing, sexual problems or cognitive changes. Another symptom associated with PD is memory dysfunction. This can be attributed to",
      "sample. The results show that some dysexecutive behaviours are part of everyday life, and the symptoms exist to varying degrees in everyone. For example, absent-mindedness and lapses in attention are common everyday occurrences for most people. However, for the majority of the population such inattentiveness is manageable, whereas patients with DES experience it to such a degree that daily tasks become difficult. Dysexecutive syndrome Dysexecutive syndrome (DES) consists of a group of symptoms, usually resulting from brain damage, that fall into cognitive, behavioural and emotional categories and tend to occur together. The term was introduced by Alan Baddeley to describe"
    ]
  ],
  [
    "The absolute threshold for hearing the decibels produced by the bell corresponds to a power level of 0 dBm, which is one milliwatt.",
    [
      "a 20 dB change in level. The extra factor of two is due to the logarithm of the quadratic relationship between power and amplitude. The decibel scales differ so that direct comparisons can be made between related power and field quantities when they are expressed in decibels. The definition of the decibel is based on the measurement of power in telephony of the early 20th century in the Bell System in the United States. One decibel is one tenth (deci-) of one bel, named in honor of Alexander Graham Bell; however, the bel is seldom used. Today, the decibel is",
      "as \"did not see anything.\" The absolute threshold of hearing is the minimum sound level of a pure tone that an average ear with normal hearing can hear with no other sound present. The absolute threshold relates to the sound that can just be heard by the organism.The threshold of hearing is generally reported as the RMS sound pressure of 20 \u00b5Pa (micropascals) = 2\u00d710 pascal (Pa). It is approximately the quietest sound a young human with undamaged hearing can detect at 1,000 Hz. The threshold of hearing is frequency dependent and it has been shown that the ear's sensitivity",
      "decibel unit can also be combined with a suffix to create an absolute unit of electric power. For example, it can be combined with \"m\" for \"milliwatt\" to produce the \"dBm\". A power level of 0 dBm corresponds to one milliwatt, and 1 dBm is one decibel greater (about 1.259 mW). In professional audio specifications, a popular unit is the dBu. This is relative to the root mean square voltage which delivers 1 mW (0 dBm) into a 600-ohm resistor, or \u2248 0.775 V. When used in a 600-ohm circuit (historically, the standard reference impedance in telephone circuits), dBu and"
    ]
  ],
  [
    "During transcription, RNA polymerase makes a copy of a gene from the DNA to mRNA as needed, creating a molecule of messenger RNA from a segment of DNA.",
    [
      "begins with transcription, and ultimately ends in degradation. During its life, an mRNA molecule may also be processed, edited, and transported prior to translation. Eukaryotic mRNA molecules often require extensive processing and transport, while prokaryotic mRNA molecules do not. A molecule of eukaryotic mRNA and the proteins surrounding it are together called a messenger RNP. Transcription is when RNA is made from DNA. During transcription, RNA polymerase makes a copy of a gene from the DNA to mRNA as needed. This process is similar in eukaryotes and prokaryotes. One notable difference, however, is that eukaryotic RNA polymerase associates with mRNA-processing",
      "of two antiparallel strands which are oriented in opposite directions. DNA is composed of base pairs in which adenine pairs with thymine and guanine pairs with cytosine. While DNA serves as template for production of ribonucleic acid (RNA), RNA is usually responsible for making protein. The process of making RNA from DNA is called transcription. RNA uses a similar set of bases except that thymine is replaced with uracil. A group of enzymes called RNA polymerases (isolated by biochemists Jerard Hurwitz and Samuel B. Weiss) function in the presence of DNA. These enzymes produce RNA using segments of chromosomal DNA",
      "place during S phase of the cell cycle. Transcription is the process by which the information contained in a section of DNA is replicated in the form of a newly assembled piece of messenger RNA (mRNA). Enzymes facilitating the process include RNA polymerase and transcription factors. In eukaryotic cells the primary transcript is pre-mRNA. Pre-mRNA must be processed for translation to proceed. Processing includes the addition of a 5' cap and a poly-A tail to the pre-mRNA chain, followed by splicing. Alternative splicing occurs when appropriate, increasing the diversity of the proteins that any single mRNA can produce. The product"
    ]
  ],
  [
    "The new enzyme found in transgenic mice that participates in the synthesis of an unknown product using two reactants by switching a nitrogen group would fall under the category of EC 2.6 (transferase enzymes that transfer nitrogenous groups).",
    [
      "which stands for \"Enzyme Commission\". The first number broadly classifies the enzyme based on its mechanism. The top-level classification is: These sections are subdivided by other features such as the substrate, products, and chemical mechanism. An enzyme is fully specified by four numerical designations. For example, hexokinase (EC 2.7.1.1) is a transferase (EC 2) that adds a phosphate group (EC 2.7) to a hexose sugar, a molecule containing an alcohol group (EC 2.7.1). Enzymes are generally globular proteins, acting alone or in larger complexes. The sequence of the amino acids specifies the structure which in turn determines the catalytic activity",
      "alkyl groups when transferred, as those are included in EC 2.3. EC 2.5 currently only possesses one sub-class: Alkyl and aryl transferases. Cysteine synthase, for example, catalyzes the formation of acetic acids and cysteine from O-acetyl-L-serine and hydrogen sulfide: O-acetyl-L-serine + HS formula_2 L-cysteine + acetate. The grouping consistent with transfer of nitrogenous groups is EC 2.6. This includes enzymes like transaminase (also known as \"aminotransferase\"), and a very small number of oximinotransferases and other nitrogen group transferring enzymes. EC 2.6 previously included amidinotransferase but it has since been reclassified as a subcategory of EC 2.1 (single-carbon transferring enzymes). In",
      "methyltransferase is the EC category grouping. This same action by the transferase can be illustrated as follows: However, other accepted names are more frequently used for transferases, and are often formed as \"acceptor grouptransferase\" or \"donor grouptransferase.\" For example, a DNA methyltransferase is a transferase that catalyzes the transfer of a methyl group to a DNA acceptor. In practice, many molecules are not referred to using this terminology due to more prevalent common names. For example, RNA Polymerase is the modern common name for what was formerly known as RNA nucleotidyltransferase, a kind of nucleotidyl transferase that transfers nucleotides to"
    ]
  ],
  [
    "The pH of a .20 M aqueous solution of pyridine, CHN, can be calculated using the formula pH = -log[H+], where [H+] is the concentration of hydrogen ions in the solution, and the percentage protonation can be calculated using the equation [protonated pyridine] / [total pyridine] x 100%.",
    [
      "than three reagents. The calculation of hydrogen ion concentrations, using this formalism, is a key element in the determination of equilibrium constants by potentiometric titration. PH In chemistry, pH () is a logarithmic scale used to specify the acidity or basicity of an aqueous solution. It is approximately the negative of the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions. More precisely it is the negative of the base 10 logarithm of the activity of the hydrogen ion. At 25 \u00b0C, solutions with a pH less than 7 are acidic and",
      "the concentration of the acid, so [A] = [H]. After some further algebraic manipulation an equation in the hydrogen ion concentration may be obtained. Solution of this quadratic equation gives the hydrogen ion concentration and hence p[H] or, more loosely, pH. This procedure is illustrated in an ICE table which can also be used to calculate the pH when some additional (strong) acid or alkaline has been added to the system, that is, when C \u2260 C. For example, what is the pH of a 0.01M solution of benzoic acid, pK = 4.19? For alkaline solutions an additional term is",
      "will ionize and protonate a water molecule to form hydronium. The acidity of hydronium is the implicit standard used to judge the strength of an acid in water: strong acids must be better proton donors than hydronium, otherwise a significant portion of acid will exist in a non-ionized state (i.e.: a weak acid). Unlike hydronium in neutral solutions that result from water's autodissociation, hydronium ions in acidic solutions are long-lasting and concentrated, in proportion to the strength of the dissolved acid. pH was originally conceived to be a measure of the hydrogen ion concentration of aqueous solution. We now know"
    ]
  ],
  [
    "The genetic material of an organism consists of DNA (or RNA in RNA viruses), which includes both genes and noncoding DNA, as well as mitochondrial DNA and chloroplast DNA.",
    [
      "Genome In the fields of molecular biology and genetics, a genome is the genetic material of an organism. It consists of DNA (or RNA in RNA viruses). The genome includes both the genes (the coding regions) and the noncoding DNA, as well as mitochondrial DNA and chloroplast DNA. The study of the genome is called genomics. The term \"genome\" was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name is a blend of the words \"gene\" and \"chromosome\". However, see omics for a more thorough discussion. A few",
      "against a future where genomic information fuels prejudice and extreme class differences between those who can and can't afford genetically engineered children. Genome In the fields of molecular biology and genetics, a genome is the genetic material of an organism. It consists of DNA (or RNA in RNA viruses). The genome includes both the genes (the coding regions) and the noncoding DNA, as well as mitochondrial DNA and chloroplast DNA. The study of the genome is called genomics. The term \"genome\" was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary",
      "living organism, it utilizes polymerases, ribosomes, and other biomolecules to replicate its own genetic material and to produce more virus genetic material similar to the original virus. Thus, gene transfer may occur through many varying means. Thus, the study of this gene transfer throughout each ecosystem, whether it be through a bacterial ecosystem or through the ecosystem of an organism, genetic ecology is the study of this gene transfer and its causes. Genetic ecology Genetic ecology is the study of the stability and expression of varying genetic material within abiotic mediums. Typically, genetic data is not thought of outside of"
    ]
  ],
  [
    "None of the molecules mentioned in the provided text are steroid-based.",
    [
      "Steroid A steroid is a biologically active organic compound with four rings arranged in a specific molecular configuration. Steroids have two principal biological functions: as important components of cell membranes which alter membrane fluidity; and as signaling molecules. Hundreds of steroids are found in plants, animals and fungi. All steroids are manufactured in cells from the sterols lanosterol (opisthokonts) or cycloartenol (plants). Lanosterol and cycloartenol are derived from the cyclization of the triterpene squalene. The steroid core structure is composed of seventeen carbon atoms, bonded in four \"fused\" rings: three six-member cyclohexane rings (rings A, B and C in the",
      "steroids). For example, the pharmaceutical Norgestrel begins from Methoxy-1-tetralone, a petrochemical derived from phenol. A number of Nobel Prizes have been awarded for steroid research, including: Steroid A steroid is a biologically active organic compound with four rings arranged in a specific molecular configuration. Steroids have two principal biological functions: as important components of cell membranes which alter membrane fluidity; and as signaling molecules. Hundreds of steroids are found in plants, animals and fungi. All steroids are manufactured in cells from the sterols lanosterol (opisthokonts) or cycloartenol (plants). Lanosterol and cycloartenol are derived from the cyclization of the triterpene squalene.",
      "consists of a steroid molecule attached to a sugar (glycoside) and an R group. The steroid nucleus consists of five fused rings to which other functional groups such as methyl, hydroxyl, and aldehyde groups can be attached to influence the overall molecule's biological activity. Cardiac glycosides also vary in the groups attached at either end of the steroid. Specifically, different sugar groups attached at the sugar end of the steroid can alter the molecule's solubility and kinetics; however, the lactone moiety at the R group end only serves a structural function. In particular, the structure of the ring attached at"
    ]
  ],
  [
    "The major carrier of free fatty acids in the blood is plasma albumin, as free fatty acids are transported bound to albumin due to their insolubility in water.",
    [
      "to release stored fatty acids into the blood as free fatty acids, in order to supply e.g. muscle cells with energy. In any case, also the fatty acids secreted from cells are anew taken up by other cells in the body, until entering fatty acid metabolism. The fate of cholesterol in the blood is highly determined by its constitution of lipoproteins, where some types favour transport towards body tissues and others towards the liver for excretion into the intestines. The 1987 report of National Cholesterol Education Program, Adult Treatment Panels suggest the total blood cholesterol level should be: <200 mg/dl",
      "the mitochondria, endoplasmic reticulum, and the Golgi apparatus). The \"uncombined fatty acids\" or \"free fatty acids\" found in the circulation of animals come from the breakdown (or lipolysis) of stored triglycerides. Because they are insoluble in water, these fatty acids are transported bound to plasma albumin. The levels of \"free fatty acids\" in the blood are limited by the availability of albumin binding sites. They can be taken up from the blood by all cells that have mitochondria (with the exception of the cells of the central nervous system). Fatty acids can only be broken down in mitochondria, by means",
      "and neurogenic properties. Blood fatty acids are in different forms in different stages in the blood circulation. They are taken in through the intestine in chylomicrons, but also exist in very low density lipoproteins (VLDL) and low density lipoproteins (LDL) after processing in the liver. In addition, when released from adipocytes, fatty acids exist in the blood as free fatty acids. It is proposed that the blend of fatty acids exuded by mammalian skin, together with lactic acid and pyruvic acid, is distinctive and enables animals with a keen sense of smell to differentiate individuals. Fatty acids are mainly used"
    ]
  ],
  [
    "Enzyme activity can be affected by inhibitors, activators, changes in temperature, pH, and denaturation, as well as by the unbinding of enzymes from substrates, leading to complex effects on reaction rates at the single-molecule level.",
    [
      "single-molecule experiments derives from their capacity to provide fundamental insights that are not attainable by conventional experimentation. For example: (a) Enzyme turnover can be measured directly from the number of fluorescent molecules of product produced by an individual enzyme molecule. (b) Studying heat inactivation of an enzyme at the single molecule level uncovered unprecedented mechanisms. Namely, heating causes an all-or-none distribution of enzymatic activity, i. e., the molecular population consists of either fully active or completely inactive enzyme molecules. This experiment rules out an alternative plausible mechanism postulating that partial heat inactivation produces enzyme molecules with partial activity; (c) In",
      "and are not consumed in chemical reactions, nor do they alter the equilibrium of a reaction. Enzymes differ from most other catalysts by being much more specific. Enzyme activity can be affected by other molecules: inhibitors are molecules that decrease enzyme activity, and activators are molecules that increase activity. Many therapeutic drugs and poisons are enzyme inhibitors. An enzyme's activity decreases markedly outside its optimal temperature and pH, and many enzymes are (permanently) denatured when exposed to excessive heat, losing their structure and catalytic properties. Some enzymes are used commercially, for example, in the synthesis of antibiotics. Some household products",
      "While the first prediction is well established, the second is more elusive. Mathematical analysis of the effect of enzyme-substrate unbinding on enzymatic reactions at the single-molecule level has shown that unbinding of an enzyme from a substrate can reduce the rate of product formation under some conditions, but may also have the opposite effect. As substrate concentrations increase, a tipping point can be reached where an increase in the unbinding rate results in an increase, rather than a decrease, of the reaction rate. The results indicate that enzymatic reactions can behave in ways that violate the classical Michaelis-Menten equation, and"
    ]
  ],
  [
    "Osmosis is the process that uses only the gradient of material to control the direction in which substances move across the cell membrane.",
    [
      "as a selective filter that allows only certain things to come inside or go outside the cell. The cell employs a number of transport mechanisms that involve biological membranes: 1. Passive osmosis and diffusion: Some substances (small molecules, ions) such as carbon dioxide (CO) and oxygen (O), can move across the plasma membrane by diffusion, which is a passive transport process. Because the membrane acts as a barrier for certain molecules and ions, they can occur in different concentrations on the two sides of the membrane. Diffusion occurs when small molecules and ions move freely from high concentration to low",
      "the cell membrane from an area of low solute concentration to high solute concentration. For example, if the cell is submerged in saltwater, water molecules move out of the cell. If a cell is submerged in freshwater, water molecules move into the cell. When the membrane has a volume of pure water on both sides, water molecules pass in and out in each direction at exactly the same rate. There is no net flow of water through the membrane. The mechanism responsible for driving osmosis has commonly been represented in biology and chemistry texts as either the dilution of water",
      "concentration in order to equilibrate the membrane. It is considered a passive transport process because it does not require energy and is propelled by the concentration gradient created by each side of the membrane. Such a concentration gradient across a semipermeable membrane sets up an osmotic flow for the water. Osmosis, in biological systems involves a solvent, moving through a semipermeable membrane similarly to passive diffusion as the solvent still moves with the concentration gradient and requires no energy. While water is the most common solvent in cell, it can also be other liquids as well as supercritical liquids and"
    ]
  ],
  [
    "Specialized features of the inner mitochondrial membrane, such as compartmentalization, ion transporters, antiport systems, and cristae invaginations, make it essential for metabolism by allowing for the efficient exchange of molecules and generation of a membrane potential.",
    [
      "by separating the matrix from the cytosolic environment. This compartmentalization is a necessary feature for metabolism. The inner mitochondrial membrane is both an electrical insulator and chemical barrier. Sophisticated ion transporters exist to allow specific molecules to cross this barrier. There are several antiport systems embedded in the inner membrane, allowing exchange of anions between the cytosol and the mitochondrial matrix. Inner mitochondrial membrane The inner mitochondrial membrane (IMM) is the mitochondrial membrane which separates the mitochondrial matrix from the intermembrane space. The structure of the inner mitochondrial membrane is extensively folded and compartmentalized. The numerous invaginations of the membrane",
      "and molecules require special membrane transporters to enter or exit the matrix. Proteins are ferried into the matrix via the translocase of the inner membrane (TIM) complex or via Oxa1. In addition, there is a membrane potential across the inner membrane, formed by the action of the enzymes of the electron transport chain. The inner mitochondrial membrane is compartmentalized into numerous cristae, which expand the surface area of the inner mitochondrial membrane, enhancing its ability to produce ATP. For typical liver mitochondria, the area of the inner membrane is about five times as large as the outer membrane. This ratio",
      "Inner mitochondrial membrane The inner mitochondrial membrane (IMM) is the mitochondrial membrane which separates the mitochondrial matrix from the intermembrane space. The structure of the inner mitochondrial membrane is extensively folded and compartmentalized. The numerous invaginations of the membrane are called cristae, separated by crista junctions from the inner boundary membrane juxtaposed to the outer membrane. Cristae significantly increases the total membrane surface area compared to a smooth inner membrane and thereby the available working space. The inner membrane creates two compartments. The region between the inner and outer membrane, called the intermembrane space which is largely continuous with the"
    ]
  ],
  [
    "The result of a DNA sequence experiencing both a point mutation and a deletion would be a genetic aberration where a part of the chromosome or DNA sequence is lost, along with a single nucleotide replacement.",
    [
      "the size of chromosomal deletion detected could as small as 5\u201320 kb in length. Other computation methods were selected to discover DNA sequencing deletion errors such as end-sequence profiling. Deletion (genetics) In genetics, a deletion (also called gene deletion, deficiency, or deletion mutation) (sign: \u0394) is a mutation (a genetic aberration) in which a part of a chromosome or a sequence of DNA is lost during DNA replication. Any number of nucleotides can be deleted, from a single base to an entire piece of chromosome. The smallest single base deletion mutations are believed to occur by a single base flipping",
      "Deletion (genetics) In genetics, a deletion (also called gene deletion, deficiency, or deletion mutation) (sign: \u0394) is a mutation (a genetic aberration) in which a part of a chromosome or a sequence of DNA is lost during DNA replication. Any number of nucleotides can be deleted, from a single base to an entire piece of chromosome. The smallest single base deletion mutations are believed to occur by a single base flipping in the template DNA, followed by template DNA strand slippage, within the DNA polymerase active site. Deletions can be caused by errors in chromosomal crossover during meiosis, which causes",
      "Throughout the cell's lifetime, this information is transcribed and replicated by cellular mechanisms to produce proteins or to provide instructions for daughter cells during cell division, and the possibility exists that the DNA may be altered during these processes. This is known as a mutation. At the molecular level, there are regulatory systems that correct most \u2014 but not all \u2014 of these changes to the DNA before it is replicated. One of the possible mutations that occurs is the replacement of a single nucleotide, known as a point mutation. If a point mutation occurs within an expressed region of"
    ]
  ],
  [
    "The perceived frequency at which the camera picks up the gunshot is 6 Hz.",
    [
      "hour. The driver focuses solely on driving straight and fast with professional quick gear shifting. The car is timed and recorded going in two separate and opposite directions. This practice eliminates variables such as wind, directional traction of the track and driver performance. The two times are averaged together to achieve the commonly accepted 0 to 60 time. Jalopnik has said that launch control systems appearing on production exotic cars in the 2010s have made published 0 to 60 times invalid, since these cars have slower times from 5 mph to 60 mph. Some car manufacturers and magazines in the",
      "fixes the \"natural\" frame rate of the image sequence. Moving image sequence can be captured at the rate which is different from presentation rate, however this is usually only done for the sake of artistic effect, or for studying fast-pace or slow processes. In order to faithfully reproduce familiar movements of persons, animals, or natural processes, and to faithfully reproduce accompanying sound, the capture rate must be equal to, or at least very close to the presentation rate. All modern moving image capture systems either use a mechanical or an electronic shutter. Shutter allows the image for a single frame",
      "Frame rate Frame rate (expressed in ' or fps) is the frequency (rate) at which consecutive images called frames appear on a display. The term applies equally to film and video cameras, computer graphics, and motion capture systems. Frame rate may also be called the ', and be expressed in \"hertz\". The temporal sensitivity and resolution of human vision varies depending on the type and characteristics of visual stimulus, and it differs between individuals. The human visual system can process 10 to 12 images per second and perceive them individually, while higher rates are perceived as motion. Modulated light (such"
    ]
  ],
  [
    "The formula for calculating the acceleration of an object on an inclined plane with a coefficient of friction is \\(a = g \\times \\sin(\\theta) - \\mu \\times g \\times \\cos(\\theta)\\), where \\(a\\) is the acceleration, \\(g\\) is the acceleration due to gravity, \\(\\theta\\) is the angle of the incline, and \\(\\mu\\) is the coefficient of friction.",
    [
      "formula_34 where, in this case, K depends on the elastic properties of the materials. Also for the elastic bodies the tangential force depends on the coefficient c seen above, and it will be formula_35 and therefore a fairly exhaustive description of the friction coefficient can be obtained formula_36. The simplest and most immediate method for evaluating the friction coefficient of two surfaces is the use of an inclined plane on which a block of material is made to slide. As can be seen in the figure, the normal force of the plane is given by formula_37, while the frictional force",
      "horizontal, the normal force formula_8 between the object and the surface is just its weight, which is equal to its mass multiplied by the acceleration due to earth's gravity, \"g\". If the object is on a tilted surface such as an inclined plane, the normal force is less, because less of the force of gravity is perpendicular to the face of the plane. Therefore, the normal force, and ultimately the frictional force, is determined using vector analysis, usually via a free body diagram. Depending on the situation, the calculation of the normal force may include forces other than gravity. The",
      "However, \"u\" = 0 and \"a\" = \"g\". So, But, formula_22 Consider a rolling body down an inclined plane and the following- Mass of body = \"m\", acceleration = \"a\", initial velocity = \"u\", final velocity = \"v\", time to reach bottom = \"t\", force imparted by body = \"F\" = \"mg\", frictional force = \"F\" = \"\u00b5F\" cos \"\u03b8\", net force = \"F\". However, formula_25. So, Suppose that a rolling body is moving uphill, i.e., instead of moving down, a body strikes the base of the inclined plane, naturally, the energy carried by the body shall make it rise"
    ]
  ],
  [
    "David's perfectionist nature and constant self-comparison to better players may hinder his ability to master the sheet music given to him for the fall semester, as he may become overly focused on achieving perfection rather than allowing his brain to dictate to his muscles how to play the composition effectively.",
    [
      "link below for full answer to question)\" \"...if you know which kind of sound you must produce for this composition, your muscles automatically play what is needed for that. Because your brain, dictates to your muscles much better than your teacher dictates to you. Sometimes of course I must make something technically more precise, but most important, your idea, how you must play in your brain.\" Alexander Markov is quoted as saying \"See I always felt about the music and the technical aspect of it, to me it's very much together, because I know some musicians, some violinists they isolate",
      "master will give them advice on how to play it, often including anecdotes about the composer, demonstrations of how to play certain passages, and admonitions of common technical errors. The student is then usually expected to play the piece again, in light of the master's comments, and the student may be asked to play a passage repeatedly to attain perfection. Master classes for musical instruments tend to focus on the finer details of attack, tone, phrasing, and overall shape, and the student is expected to have complete control of more basic elements such as rhythm and pitch. The value of",
      "for oneself. Perfectionism can drive people to accomplishments and provide the motivation to persevere in the face of discouragement and obstacles. Roedell (1984) argues: In a positive form, perfectionism can provide the driving energy which leads to great achievement. The meticulous attention to detail, necessary for scientific investigation, the commitment which pushes composers to keep working until the music realises the glorious sounds playing in the imagination, and the persistence which keeps great artists at their easels until their creation matches their conception all result from perfectionism. Slaney and his colleagues found that adaptive perfectionists had lower levels of procrastination"
    ]
  ],
  [
    "Trabecular VSDs are considered true heart defects, as they are defects in the ventricular septum that may close spontaneously shortly after birth in most cases (Ventricular septal defect).",
    [
      "left within the ventricular septum. It is debatable whether all those defects are true heart defects, or if some of them are normal phenomena, since most of the trabecular VSDs close spontaneously. Prospective studies give a prevalence of 2-5 per 100 births of trabecular VSDs that close shortly after birth in 80-90% of the cases. Ventricular septal defect A ventricular septal defect (VSD) is a defect in the ventricular septum, the wall dividing the left and right ventricles of the heart. The extent of the opening may vary from pin size to complete absence of the ventricular septum, creating one",
      "intervention, comprising over 80% of cases. Membranous ventricular septal defects are more common than muscular ventricular septal defects, and are the most common congenital cardiac anomaly. Ventricular septal defect is usually symptomless at birth. It usually manifests a few weeks after birth. VSD is an acyanotic congenital heart defect, aka a left-to-right shunt, so there are no signs of cyanosis in the early stage. However, uncorrected VSD can increase pulmonary resistance leading to the reversal of the shunt and corresponding cyanosis. The restrictive VSDs (smaller defects) are associated with a louder murmur and more palpable thrill (grade IV murmur). Larger",
      "be asymptomatic. Four different septal defects exist, with perimembranous most common, outlet, atrioventricular, and muscular less commonly. A VSD can be detected by cardiac auscultation. Classically, a VSD causes a pathognomonic holo- or pansystolic murmur. Auscultation is generally considered sufficient for detecting a significant VSD. The murmur depends on the abnormal flow of blood from the left ventricle, through the VSD, to the right ventricle. If there is not much difference in pressure between the left and right ventricles, then the flow of blood through the VSD will not be very great and the VSD may be silent. This situation"
    ]
  ],
  [
    " is a device that converts chemical energy from a fuel into electricity through an electrochemical reaction, requiring a continuous source of fuel and oxygen to sustain the reaction.",
    [
      "Electrochemical cell An electrochemical cell is a device capable of either generating electrical energy from chemical reactions or using electrical energy to cause chemical reactions. The electrochemical cells which generate an electric current are called voltaic cells or galvanic cells and the other ones are called electrolytic cells which are used to drive chemical reactions like electrolysis. A common example of a galvanic cell is a standard 1.5 volt cell meant for consumer use. A \"battery\" consists of one or more cells, connected either in parallel, series or series-and-parallel pattern.<r: A cell is a device which can convert chemical energy",
      "cell is discharged, the concentration of the reactants decreases and the cell potential also decreases. Electrochemical cell An electrochemical cell is a device capable of either generating electrical energy from chemical reactions or using electrical energy to cause chemical reactions. The electrochemical cells which generate an electric current are called voltaic cells or galvanic cells and the other ones are called electrolytic cells which are used to drive chemical reactions like electrolysis. A common example of a galvanic cell is a standard 1.5 volt cell meant for consumer use. A \"battery\" consists of one or more cells, connected either in",
      "Fuel cell A fuel cell is an electrochemical cell that converts the chemical energy from a fuel into electricity through an electrochemical reaction of hydrogen fuel with oxygen or another oxidizing agent. Fuel cells are different from batteries in requiring a continuous source of fuel and oxygen (usually from air) to sustain the chemical reaction, whereas in a battery the chemical energy comes from chemicals already present in the battery. Fuel cells can produce electricity continuously for as long as fuel and oxygen are supplied. The first fuel cells were invented in 1838. The first commercial use of fuel cells"
    ]
  ],
  [
    "The theory of evolution accounts for the complexity of life through the process of random mutation and natural selection, which allows for organisms to become more complex over time.",
    [
      "claims for the ability of random mutation and natural selection to account for the complexity of life. Careful examination of the evidence for Darwinian theory should be encouraged.\" The two-sentence statement has been widely used by its sponsor, the Discovery Institute, and some of their supporters in a national campaign to discredit evolution and to promote intelligent design. The New York Times article described Tour as saying that the explanations offered by evolution are incomplete, and he found it hard to believe that nature can produce the machinery of cells through random processes. Despite this, he said he remained open-minded",
      "that organisms become more complex. Although the history of life shows an apparent trend towards the evolution of biological complexity; there is a question if this appearance of increased complexity is real, or if it comes from neglecting the fact that the majority of life on Earth has always consisted of prokaryotes. In this view, complexity is not a necessary consequence of evolution, but that specific circumstances of evolution on Earth frequently made greater complexity advantageous and thus naturally selected for. Depending on the situation, organisms' complexity can either increase, decrease, or stay the same, and all three of these",
      "an evolutionary system in which there is variation and heredity, in the absence of natural selection, other forces, or constraints on diversity or complexity, diversity and complexity will increase on average\" while the general formulation states that \"in an evolutionary system in which there is variation and heredity, there is a tendency for diversity and complexity to increase, one that is always present but may be opposed or augmented by natural selection, other forces, or constraints acting on diversity or complexity\". The rationale for the claim is that as replicators such as genes replicate, errors will accumulate. If not eliminated"
    ]
  ],
  [
    "When a sound wave enters a denser medium, its velocity increases, intensity decreases, frequency remains the same, and wavelength decreases (Speed of sound).",
    [
      "can be calculated as the relation between a wave's speed and ordinary frequency. For sound waves, the amplitude of the wave is the difference between the pressure of the undisturbed air and the maximum pressure caused by the wave. Sound's propagation speed depends on the type, temperature, and composition of the medium through which it propagates. In an elastic medium with rigidity, a harmonic pressure wave oscillation has the form, where: The restoring force, which acts to return the medium to its original position, is provided by the medium's bulk modulus. Maxwell's equations lead to the prediction of electromagnetic waves",
      "with the same frequency formula_2 as the light intensity wave is induced. Otherwise, there is a frequency shift in the induced acoustic wave. The magnitude of the frequency shift depends on the relative velocity formula_1, the angle formula_7 between the velocity and the photon density wave propagation direction, and the angle formula_8 between the velocity and the ultrasonic wave propagation direction. The frequency shift is given by: formula_9 Where formula_10 is the speed of light in the medium and formula_11 is the speed of sound. The first term on the right side of the expression represents the frequency shift in",
      "waves; see optical dispersion for a description. The speed of sound is variable and depends on the properties of the substance through which the wave is travelling. In solids, the speed of transverse (or shear) waves depends on the shear deformation under shear stress (called the shear modulus), and the density of the medium. Longitudinal (or compression) waves in solids depend on the same two factors with the addition of a dependence on compressibility. In fluids, only the medium's compressibility and density are the important factors, since fluids do not transmit shear stresses. In heterogeneous fluids, such as a liquid"
    ]
  ],
  [
    "The adenylate energy charge is calculated as ([ATP] + 0.5[ADP]) / ([ATP] + [ADP] + [AMP]) and is used to measure the energy status of biological cells.",
    [
      "equilibrium (<chem>ATP + AMP <=> 2 ADP</chem>). The energy charge is related to ATP, ADP and AMP concentrations. It was first defined by Atkinson and Walton who found that it was necessary to take into account the concentration of all three nucleotides, rather than just ATP and ADP, to account for the energy status in metabolism. Since the adenylate kinase maintains two ADP molecules in equilibrium with one ATP (<chem>2 ADP <=> ATP + AMP</chem>), Atkinson defined the adenylate energy charge as : The energy charge of most cells varies between 0.7 and 0.95 - oscillations in this range are",
      "the adenylate energy charge by decreasing the total {ATP+ADP+AMP} concentration. Energy charge The adenylate energy charge is an index used to measure the energy status of biological cells. ATP or Mg-ATP is the principal molecule for storing and transferring energy in the cell : it is used for biosynthetic pathways, maintenance of transmembrane gradients, movement, cell division, etc... More than 90% of the ATP is produced by phosphorylation of ADP by the ATP synthase. ATP can also be produced by \u201csubstrate level phosphorylation\u201d reactions (ADP phosphorylation by (1,3)-bisphosphoglycerate, phosphoenolpyruvate, phosphocreatine), by the succinate-CoA ligase and phosphoenolpyruvate carboxylkinase, and by adenylate",
      "Energy charge The adenylate energy charge is an index used to measure the energy status of biological cells. ATP or Mg-ATP is the principal molecule for storing and transferring energy in the cell : it is used for biosynthetic pathways, maintenance of transmembrane gradients, movement, cell division, etc... More than 90% of the ATP is produced by phosphorylation of ADP by the ATP synthase. ATP can also be produced by \u201csubstrate level phosphorylation\u201d reactions (ADP phosphorylation by (1,3)-bisphosphoglycerate, phosphoenolpyruvate, phosphocreatine), by the succinate-CoA ligase and phosphoenolpyruvate carboxylkinase, and by adenylate kinase, an enzyme that maintains the three adenine nucleotides in"
    ]
  ],
  [
    "The body compensates for increased environmental temperature by increasing blood flow to the skin and promoting sweating to dissipate heat through evaporation.",
    [
      "and chemical impulses. In addition, the body is able to efficiently use the three energy systems which include the phosphagen system, the glycolytic system, and the oxidative system. In most cases, as the body is exposed to physical activity, the core temperature of the body tends to rise as heat gain becomes larger than the amount of heat lost. \u201cThe factors that contribute to heat gain during exercise include anything that stimulate metabolic rate, anything from the external environment that causes heat gain, and the ability of the body to dissipate heat under any given set of circumstances\u201d. In response",
      "dermis, and detects changes in surrounding blood to make decisions of whether to stimulate internal heat production, or to stimulate evaporation. There are two main types of stresses that can be experienced due to extreme environmental temperatures: heat stress and cold stress. Heat stress is physiologically combated in four ways: radiation, conduction, convection, and evaporation. Cold stress is physiologically combated by shivering, accumulation of body fat, circulatory adaptations (that provide an efficient transfer of heat to the epidermis), and increased blood flow to the extremities. There is one part of the body fully equipped to deal with cold stress. The",
      "between the periphery and the core to control heat loss from the surface of the body. Lastly, the organism can show insulation adjustments; a common example being \u201cgoosebumps\u201d in humans where hair follicles are raised by pilomotor muscles, also shown in animals\u2019 pelage and plumage. The thermoneutral zone describes a range of temperatures of the immediate environment in which a standard healthy adult can maintain normal body temperature without needing to use energy above and beyond normal basal metabolic rate. Its value is 25 - 30 degrees Celsius (77 - 86 degrees Fahrenheit) for a naked man, standing upright, in"
    ]
  ],
  [
    "The electron transport system releases the most energy when completely oxidised in the body.",
    [
      "electron transport system transferring the electrons ultimately to oxygen and conserving the released energy in the form of a proton gradient over a membrane (inner mitochondrial membrane in eukaryotes). Thus, oxygen is reduced to water and the original electron acceptors NAD and quinone are regenerated. This is why humans breathe in oxygen and breathe out carbon dioxide. The energy released from transferring the electrons from high-energy states in NADH and quinol is conserved first as proton gradient and converted to ATP via ATP synthase. This generates an additional \"28\" molecules of ATP (24 from the 8 NADH + 4 from",
      "Energy (\u0394G). Redox zonation refers to how the processes that transfer terminal electrons as a result of organic matter degradation vary depending on time and space. Certain reactions will be favored over others due to their energy yield as detailed in the energy acceptor cascade detailed above. In oxic conditions, in which oxygen is readily available, aerobic respiration will be favored due to its high energy yield. Once the use of oxygen through respiration exceeds the input of oxygen due to bioturbation and diffusion, the environment will become anoxic and organic matter will be broken down via other means, such",
      "total energy requirement during a two-hour aerobic training session. This process could severely degrade the protein structures needed to maintain survival such as contractile properties of proteins in the heart, cellular mitochondria, myoglobin storage, and metabolic enzymes within muscles. The oxidative system (aerobic) is the primary source of ATP supplied to the body at rest and during low intensity activities and uses primarily carbohydrates and fats as substrates. Protein is not normally metabolized significantly, except during long term starvation and long bouts of exercise (greater than 90 minutes.) At rest approximately 70% of the ATP produced is derived from fats"
    ]
  ],
  [
    "Individuals with hereditary persistence of fetal hemoglobin have continued production of significant amounts of fetal hemoglobin into adulthood, which can inhibit the polymerization of abnormal hemoglobin variants such as HbS in conditions like sickle cell disease, providing a protective effect compared to individuals with normal hemoglobin variants.",
    [
      "inhibits polymerization of HbS. A similar mechanism occurs with persons who have sickle cell \"trait\". Approximately 40% of the hemoglobin is in the HbS form while the rest is in normal HbA form. The HbA form interferes with HbS polymerization. Hereditary persistence of fetal hemoglobin Hereditary persistence of fetal hemoglobin (HPFH, BrE: \"Hereditary persistence of foetal haemoglobin\") is a benign condition in which significant fetal hemoglobin (hemoglobin F) production continues well into adulthood, disregarding the normal shutoff point after which only adult-type hemoglobin should be produced. The condition is usually asymptomatic, and is only noticed when screening for other hemoglobin",
      "fetus during pregnancy. Hemoglobin variants occur when there are genetic changes in specific genes, or globins, that cause changes or alterations in the amino acid. They could affect the structure, behavior, the production rate, and/or the stability of that specific gene. Usually there are four genes that code for alpha globin and two genes that code for beta globin. If the genes for alpha chains is mutated, the most common condition that occurs is alpha thalassemia, which causes a decrease in production of that gene. The level of severity of alpha thalassemia is determined by the number of genes that",
      "are affected. Hemoglobin variants are most often inherited characteristics. First, abnormal beta gene can be inherited in an autosomal recessive fashion. This means that the person who inherits this will have two copies of the altered gene. Both of these genes can be passed to offspring. The next way they can be inherited is in a heterozygous fashion. This means that the person has one normal beta gene and one abnormal beta gene. This person is considered to be a carrier of whichever hemoglobin variant is inherited. Only the abnormal gene can be passed on to offspring in this case."
    ]
  ],
  [
    "Women's world record performances in Athletics have improved rapidly in recent years due to advancements in training methods, technology, and the increase in all-women competitions as required by the IAAF.",
    [
      "International Association of Athletics Federations in 1981. As of June 21, 2009, eight women's world records have been ratified by the IAAF in the event. Before the event was recognised by the IAAF as an official world record event the 3000 metres was the most common international women's long-distance track event, although women did sometimes compete over 10,000 m before its addition to the World Championships and Olympic programme in 1987 and 1988, respectively. Auto times to the hundredth of a second were required by the IAAF for events up to and including 10,000 m from 1981. However, Henry Rono's",
      "the most prestigious competition for the distance and it attracts elite level, international competitors. The winner of the race is occasionally referred to as \"the world's fastest\" man or woman, reflecting the high level of the competition and the quality of performances. , the current Olympic records of 9.63 for men and 10.62 seconds for women rank as the second and third fastest times in history, for men and women respectively. The standard of performances at the Olympics has progressed in line with the discipline as a whole and the times in the final often rank highly in the end-of-season",
      "Statisticians, an independent organization that compiles data from road running events, also maintains an alternate marathon world best progression but with standards they consider to be more stringent. Performances claiming world best or world record status on \"point-to-point\" courses such as the Boston Marathon have historically been rejected by USA Track & Field. Performances on these courses could be aided by slope and/or tailwinds. The IAAF Congress at 2011 World Championships in Athletics passed a motion changing the record eligibility criteria effective January 2012, so that women's world records must be set in all-women competitions. The result of the change"
    ]
  ],
  [
    "The process of muscle contraction that requires a large amount of ATP besides the power stroke is the attachment and release of every myosin head during the sliding filament model.",
    [
      "can require a breakdown, and also to build in the rest period, which occurs during the strengthening phase associated with muscular contraction. ATP is composed of adenine, a nitrogen containing base, ribose, a five carbon sugar (collectively called adenosine), and three phosphate groups. ATP is a high energy molecule because it stores large amounts of energy in the chemical bonds of the two terminal phosphate groups. The breaking of these chemical bonds in the Krebs Cycle provides the energy needed for muscular contraction. Because the ratio of hydrogen to oxygen atoms in all carbohydrates is always the same as that",
      "They include molecules such as adenosine triphosphate (ATP), glycogen and creatine phosphate. ATP binds to the myosin head and causes the \u2018ratchetting\u2019 that results in contraction according to the sliding filament model. Creatine phosphate stores energy so ATP can be rapidly regenerated within the muscle cells from adenosine diphosphate (ADP) and inorganic phosphate ions, allowing for sustained powerful contractions that last between 5\u20137 seconds. Glycogen is the intramuscular storage form of glucose, used to generate energy quickly once intramuscular creatine stores are exhausted, producing lactic acid as a metabolic byproduct. Substrate shortage is one of the causes of metabolic fatigue.",
      "the filaments to start sliding and the sarcomeres to become shorter. This requires a large amount of ATP, as it is used in both the attachment and release of every myosin head. Very quickly Ca is actively transported back into the sarcoplasmic reticulum, which blocks the interaction between the thin and thick filament. This in turn causes the muscle cell to relax. There are four main different types of muscle contraction: twitch, treppe, tetanus and isometric/isotonic. Twitch contraction is the process in which a single stimulus signals for a single contraction. In twitch contraction the length of the contraction may"
    ]
  ],
  [
    "The presence of creatine kinase (CK-MB) in blood plasma is indicative of tissue damage and is used in the diagnosis of myocardial infarction.",
    [
      "be elevated in a wide range of clinical conditions including the use of medication such as statins; endocrine disorders such as hypothyroidism; and skeletal muscle diseases and disorders including malignant hyperthermia, and neuroleptic malignant syndrome. Furthermore, the isoenzyme determination has been used extensively as an indication for myocardial damage in heart attacks. Troponin measurement has largely replaced this in many hospitals, although some centers still rely on CK-MB. Creatine kinase Creatine kinase (CK), also known as creatine phosphokinase (CPK) or phosphocreatine kinase, is an enzyme () expressed by various tissues and cell types. CK catalyses the conversion of creatine and",
      "regeneration of ATP \"in situ\", as well as for intracellular energy transport by the PCr shuttle or circuit. Thus creatine kinase is an important enzyme in such tissues. Clinically, creatine kinase is assayed in blood tests as a marker of damage of CK-rich tissue such as in myocardial infarction (heart attack), rhabdomyolysis (severe muscle breakdown), muscular dystrophy, autoimmune myositides, and acute kidney injury. In the cells, the \"cytosolic\" CK enzymes consist of two subunits, which can be either \"B\" (brain type) or \"M\" (muscle type). There are, therefore, three different isoenzymes: CK-MM, CK-BB and CK-MB. The genes for these subunits",
      "an intense muscular or neuronal effort. Conversely, excess ATP can be used during a period of low effort to convert creatine to phosphocreatine. The reversible phosphorylation of creatine (i.e., both the forward and backward reaction) is catalyzed by several creatine kinases. The presence of creatine kinase (CK-MB, MB for muscle/brain) in blood plasma is indicative of tissue damage and is used in the diagnosis of myocardial infarction. The cell's ability to generate phosphocreatine from excess ATP during rest, as well as its use of phosphocreatine for quick regeneration of ATP during intense activity, provides a spatial and temporal buffer of"
    ]
  ],
  [
    "Incentives and rewards for students can positively impact their academic performance by motivating them to exhibit commendable behavior and meritorious acts, ultimately improving their overall conduct grade. (Madrasah Irsyad Zuhri Al-Islamiah)",
    [
      "Incentive Points System. In most national schools, a student's semestral grade for the category of conduct is determined in a subjective manner by the student's form teacher. At Irsyad, that grade is assessed based on Incentive and Penalty points awarded by all teachers concerned over a period of one year. At the beginning of the year, each student is given 75 TIPS points (equivalent to a \"Good\" grade). When a student does meritorious acts and displays commendable behaviour, incentive points are awarded to his individual TIPS account, as well as to the class and house accounts respectively. Conversely, when a",
      "positive and negative impacts. Some of the positive factors will include a teacher\u2019s dedication towards their work to improve the overall level of education of all students. These methods also help filter out teachers who are not the best teachers. If a teacher\u2019s performance is not meeting the set standards then it is a sign that something is wrong and a change is required. If incentives involve rewards then high scores among a few students can boost the overall average. In such cases a teacher might focus the bulk of their attention towards the brighter students and neglecting the other",
      "benefits certain groups of students, such as low-income students and others who have little opportunity for learning outside of school. Researchers caution that not all time in school is equal. The correlation between time and achievement increases exists only when students are given more instructional time and academic learning time. Other researchers conclude that more time spent in the classroom will have little impact on academic achievement if it isn't accompanied by other education reforms. They show that the key to student learning gains is the quality of the teacher, not the amount of time spent in school. Some homeschooler"
    ]
  ],
  [
    "In glycolysis, 2 ATP molecules are produced via substrate-level phosphorylation.",
    [
      "substrate-level phosphorylation. The second substrate-level phosphorylation occurs by dephosphorylating phosphoenolpyruvate, catalyzed by pyruvate kinase, producing pyruvate and ATP. During the preparatory phase, each 6-carbon glucose molecule is broken into two 3-carbon molecules. Thus, in glycolysis dephosphorylation results in the production of 4 ATP. However, the prior preparatory phase consumes 2 ATP, so the net yield in glycolysis is 2 ATP. 2 molecules of NADH are also produced and can be used in oxidative phosphorylation to generate more ATP. ATP can be generated by substrate-level phosphorylation in mitochondria in a pathway that is independent from the proton motive force. In the",
      "The amount of energy released by oxidative phosphorylation is high, compared with the amount produced by anaerobic fermentation. Glycolysis produces only 2 ATP molecules, but somewhere between 30 and 36 ATPs are produced by the oxidative phosphorylation of the 10 NADH and 2 succinate molecules made by converting one molecule of glucose to carbon dioxide and water, while each cycle of beta oxidation of a fatty acid yields about 14 ATPs. These ATP yields are theoretical maximum values; in practice, some protons leak across the membrane, lowering the yield of ATP. The electron transport chain carries both protons and electrons,",
      "are: two GTP, six NADH, two QH, and four CO. The above reactions are balanced if P represents the HPO ion, ADP and GDP the ADP and GDP ions, respectively, and ATP and GTP the ATP and GTP ions, respectively. The total number of ATP molecules obtained after complete oxidation of one glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is estimated to be between 30 and 38. The theoretical maximum yield of ATP through oxidation of one molecule of glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is 38 (assuming 3 molar equivalents of ATP per equivalent"
    ]
  ],
  [
    "Human chorionic gonadotropin (hCG) is secreted by the trophoblast tissue during pregnancy and its main function is to sustain the ovarian corpus luteum, which secretes progesterone necessary to maintain the viability of the endometrium for pregnancy.",
    [
      "outcome has focused on human chorionic gonadotropin (hCG). hCG is not necessary for fertilization, but is secreted by embryos shortly thereafter. Therefore, immunity against hCG does not prevent fertilization. However, it was found that anti-hCG antibodies prevent marmoset embryos from implanting in the endometrium of their mother's uterus. The main function of hCG is to sustain the ovarian corpus luteum during pregnancy past the time it would normally decay as part of the regular menstrual cycle. For the first 7\u20139 weeks in humans, the corpus luteum secretes the progesterone necessary to maintain the viability of the endometrium for pregnancy. Therefore,",
      "cell surfaces which allow for adhesion to the extracellular matrix of the uterine wall. This interaction allows for implantation and triggers further specification into the three different cell types, preparing the blastocyst for gastrulation. Level of human chorionic gonadotropin secreted by the blastocyst during implantation is the factor measured in a pregnancy test. HCG can be measured in both blood and urine to determine whether a woman is pregnant. More hCG is secreted in a multiple pregnancy. Blood tests of hCG can also be used to check for abnormal pregnancies. \"In vitro\" fertilization (IVF) is an alternative to traditional \"in",
      "This allows the corpus luteum to secrete the hormone progesterone during the first trimester. Progesterone enriches the uterus with a thick lining of blood vessels and capillaries so that it can sustain the growing fetus. Due to its highly negative charge, hCG may repel the immune cells of the mother, protecting the fetus during the first trimester. It has also been hypothesized that hCG may be a placental link for the development of local maternal immunotolerance. For example, hCG-treated endometrial cells induce an increase in T cell apoptosis (dissolution of T cells). These results suggest that hCG may be a"
    ]
  ],
  [
    "The son of a bricklayer, Walter R. Tucker Jr., went from being a teacher at a medical school to a tenured professor across the country through hard work, dedication to education, and collaboration with colleagues in the field of psychology and education.",
    [
      "that Tucker came from a family of educators. His father was principal of the local high school in Haskell, Oklahoma. His mother was a teacher there. Out of eight children, all obtained college degrees. Tucker and two of his brothers were doctors and five other siblings obtained master's degrees and became educators. Circa 1970 Tucker bought land and built a medical building with his brothers, Booker T., an oral surgeon, and S. Edward, an OBGYN. At the time, they were in a small group of African American doctors in Los Angeles who owned their own medical building. Serving the Watts",
      "Following the death of his wife, he left the University for his first teaching position in the United States at Brooklyn College working on the effects of brain damage in 1942. Five years later, he left the University to take a more prestigious position at Clark University in the Department of Psychology and Education. His research at the University, as many of his other positions, focused primarily on perception and language, and collaborated with his colleagues Seymour Wapner and Bernard Kaplan on several projects. This was the position he held for the longest, remaining at the University for 17 years.",
      "character, which was exhibited in all his writing.\" Born May 14, 1836, in Lima, New York, he became a country schoolteacher at the age of 17, leaving that position after an outbreak of typhoid fever killed his mother in 1851. He graduated from Genesee College (now part of Syracuse University) in 1858, and became a school principal in Oswego County in 1859. After being seriously injured in the American civil war, he returned to a school principalship in 1862, in Newark, New York, and in 1866 moved to another school in Elmira, New York. In 1872 he gave up teaching"
    ]
  ],
  [
    "The ratio of A:T bases in the newly synthesized complimentary RNA strand will also be 3:2.",
    [
      "a template for RNA synthesis. As transcription proceeds, RNA polymerase traverses the template strand and uses base pairing complementarity with the DNA template to create an RNA copy (which elongates during the traversal). Although RNA polymerase traverses the template strand from 3' \u2192 5', the coding (non-template) strand and newly formed RNA can also be used as reference points, so transcription can be described as occurring 5' \u2192 3'. This produces an RNA molecule from 5' \u2192 3', an exact copy of the coding strand (except that thymines are replaced with uracils, and the nucleotides are composed of a ribose",
      "product complementary to the DNA template strand, the mRNA is complementary to the DNA antisense strand. The mRNA is what is used for translation (protein synthesis).Hence, a base triplet 3'-TAC-5' in the DNA antisense strand can be used as a template which will result in an 5'-AUG-3' base triplet in mRNA (AUG is the codon for methionine, the start codon). The DNA sense strand will have the triplet ATG, which looks just like AUG but will not be used to make methionine because it will not be used to make mRNA. The DNA sense strand is called a \"sense\" strand",
      "meaning that they form matching sequences of base pairs, with A only binding to T, and C only to G. Because the formation of correctly matched base pairs is energetically favorable, nucleic acid strands are expected in most cases to bind to each other in the conformation that maximizes the number of correctly paired bases. The sequences of bases in a system of strands thus determine the pattern of binding and the overall structure in an easily controllable way. In DNA nanotechnology, the base sequences of strands are rationally designed by researchers so that the base pairing interactions cause the"
    ]
  ],
  [
    "During one complete turn of the tricarboxylic acid cycle (Krebs' cycle), 4 NADH, 1 FADH2, and 1 GTP molecule are generated.",
    [
      "the cytoplasm, the viscous fluid that fills living cells, where the glycolytic reactions take place. The citric acid cycle, also known as the Krebs cycle or the TCA (tricarboxylic acid) cycle is an 8-step process that takes the pyruvate generated by glycolysis and generates 4 NADH, FADH2, and GTP, which is further converted to ATP. It is only in step 5, where GTP is generated, by succinyl-CoA synthetase, and then converted to ATP, that ADP is used (GTP + ADP \u2192 GDP + ATP). Oxidative phosphorylation produces 26 of the 30 equivalents of ATP generated in cellular respiration by transferring",
      "and the Krebs cycle. While the ATP count is glycolysis and the Krebs cycle is two ATP molecules, the electron transport chain contributes, at most, twenty-eight ATP molecules. A contributing factor is due to the energy potentials of NADH and FADH. As they are brought from the initial process, glycolysis, to the electron transport chain, the energy stored in them are now utilized. A second contributing factor is that cristae, the inner membranes of mitochondria, increase the surface area and therefore the amount of proteins in the membrane that assist in the synthesis of ATP. Along the electron transport chain,",
      "molecules of water (HO). By accepting the electrons, oxygen allows the electron transport chain to continue functioning. The electrons from each NADH molecule can form a total of 3 ATP's from ADPs and phosphate groups through the electron transport chain, while each FADH molecule can produce a total of 2 ATPs. As a result, 10 NADH molecules (from glycolysis and the Krebs cycle), along with 2 FADH molecules, can form a total of 34 ATPs during aerobic respiration (from a single electron transport chain). This means that combined with the Krebs Cycle and glycolysis, the efficiency for the electron transport"
    ]
  ],
  [
    "Some concerning signs during an evaluation of a child's home situation include an unusual change in sleep habits, significant weight gain or loss, unexplained aches and pains, and an inability to concentrate on tasks or activities.",
    [
      "evaluation must be given by a medical or mental health professional such as a physiologist or psychiatrist. Following the bases of symptoms, signs include, but are not limited to, an unusual change in sleep habits (for example, trouble sleeping or overly indulged sleeping hours); a significant amount of weight gain/loss by lack or excessive eating; experiencing aches/pains for no apparent reason that can found; and an inability to concentrate on tasks or activities. If these symptoms are present for a period of two weeks or longer, it is safe to make the assumption that the child, or anybody else for",
      "is divided into six dimensions. The dimensions in order are: Abandonment, Fear of Being Alone, Fear of Physical Illness, Worry about Calamitous Events, Frequency of Calamitous Events, and Safety Signal Index. The first five dimensions have a total of five items while the last one contains nine items. The scale goes beyond assessing symptoms; it focuses on individual cases and treatment planning. As noted by Altman, McGoey & Sommer, it is important to observe the child, \"in multiple contexts, on numerous occasions, and in their everyday environments (home, daycare, preschool)\". It is beneficial to view parent and child interactions and",
      "a result of the child custody evaluation. Custody evaluation Custody evaluation (also known as \"parenting evaluation\") is a legal process, in which a court-appointed mental health expert or an expert chosen by the parties, evaluates a family and makes a recommendation to the court for custody matters, usually including residential custody, visitation and a parenting plan. When performing the custody evaluation, the evaluator is expected to act in the child's best interests. If the issue of child custody is not settled before trial and the parents have serious concerns about each other's ability to parent the children involved, especially for"
    ]
  ],
  [
    "Arguments favoring policies of redistribution of wealth from the rich to the poor often appeal to the concept of undeservingness by claiming that certain individuals may be undeserving of their wealth due to irresponsible conduct, promoting the idea that redistributive systems may lead to laziness and free riding rather than promoting fairness and equality.",
    [
      "of their own irresponsible conduct, and the charge is that theories favoring policies of redistribution of wealth from the rich to the poor ignore this crucial point, i.e. that people might be unequally deserving because of their actions. Sometimes the claim is that the redistributive systems often favored by egalitarian political theorists might have disastrous consequences in that they promote sloth and allow free riding on the productive by the lazy. These arguments are \"instrumental\" in their appeal to undeservingness. They refer to the allegedly bad consequences of a redistributive social system and do not necessarily involve any reference to",
      "prime motivation is to redistribute wealth, when redistribution is not their goal. The phrase is often coupled with the term \"class warfare,\" with high income earners and the wealthy portrayed as victims of unfairness and discrimination. Redistribution tax policy should not be confused with predistribution policies. \"Predistribution\" is the idea that the state should try to prevent inequalities occurring in the first place rather than through the tax and benefits system once they have occurred. For example, a government predistribution policy might require employers to pay all employees a living wage, not just a minimum wage, as a \"bottom-up\" response",
      "saying, \"People are poor because they are poor\". At each stage of the development of human society, there have always been different possibilities for a more equitable distribution of wealth. Which of those possibilities have been realised is not just a question of technique or productivity, but also of the assertion of power, ideology, and morals within the prevailing system of social relations governing legitimate cooperation and competition. The wealth of some may \"depend\" on the poverty of others. Some scarcity is truly \"physical\" scarcity; other scarcity is purely \"socially constructed\", i.e. people are excluded from wealth not by physical"
    ]
  ],
  [
    "Premature birth is the leading cause of death in preterm infants.",
    [
      "Infant mortality Infant mortality is the death of young children under the age of 1. This death toll is measured by the infant mortality rate (IMR), which is the number of deaths of children under one year of age per 1000 live births. The under-five mortality rate is also an important statistic, considering the infant mortality rate focuses only on children under one year of age. Premature birth is the biggest contributor to the IMR. Other leading causes of infant mortality are birth asphyxia, pneumonia, congenital malformations, term birth complications such as abnormal presentation of the foetus umbilical cord prolapse,",
      "2 to 12 weeks, and recovery takes 6 months to a year. Five causes make up about 80% of newborn deaths. They include prematurity and low-birth-weight, infections, lack of oxygen at birth, and trauma during birth. Preterm birth is the birth of an infant at fewer than 37 weeks gestational age. It is estimated that 1 in 10 babies are born prematurely. Premature birth is the leading cause of death in children under 5 years of age though many that survive experience disabilities including learning defects and visual and hearing problems. Causes for early birth may be unknown or may",
      "be related to certain chronic conditions such as diabetes, infections, and other known causes. The World Health Organization has developed guidelines with recommendations to improve the chances of survival and health outcomes for preterm infants. Newborns are prone to infection in the first month of life. The organism S. agalactiae (Group B Streptococcus) or (GBS) is most often the cause of these occasionally fatal infections. The baby contracts the infection from the mother during labor. In 2014 it was estimated that about one in 2000 newborn babies have GBS bacterial infections within the first week of life, usually evident as"
    ]
  ],
  [
    "84g of ethane would react with 168g of oxygen to produce 176g of carbon dioxide based on the principle of conservation of mass and stoichiometry.",
    [
      "of carbon, say 100 grams, may react with 133 grams of oxygen to produce one oxide, or with 266 grams of oxygen to produce the other. The ratio of the masses of oxygen that can react with 100 grams of carbon is 266:133 = 2:1, a ratio of small whole numbers. Dalton interpreted this result in his atomic theory by proposing (correctly in this case) that the two oxides have one and two oxygen atoms respectively for each carbon atom. In modern notation the first is CO (carbon monoxide) and the second is CO (carbon dioxide). John Dalton first expressed",
      "two oxygen molecules are converted into one molecule of carbon dioxide () and two of water (). The number of molecules as result from the reaction can be derived from the principle of conservation of mass, as initially four hydrogen atoms, 4 oxygen atoms and one carbon atom are present (as well as in the final state), then the number water molecules produced must be exactly two per molecule of carbon dioxide produced. Many engineering problems are solved by following the mass distribution in time of a given system, this practice is known as mass balance. An important idea in",
      "products by explosive chemical reactions and the energy released by those reactions. The gaseous products of complete reaction are typically carbon dioxide, steam, and nitrogen. Gaseous volumes computed by the ideal gas law tend to be too large at high pressures characteristic of explosions. Ultimate volume expansion may be estimated at three orders of magnitude, or one liter per gram of explosive. Explosives with an oxygen deficit will generate soot or gases like carbon monoxide and hydrogen, which may react with surrounding materials such as atmospheric oxygen. Attempts to obtain more precise volume estimates must consider the possibility of such"
    ]
  ],
  [
    "It is important for a person to cool down gradually and relax after sauna use to allow the body to adjust and prevent any potential health risks.",
    [
      "sleep or otherwise relax during which one may choose to wear a towel or bathrobe. Outside the actual sauna or steam room it is always acceptable to wear a bathrobe or towel and flip-flops. This ritual may be repeated several times during the day, starting with the warm showering each time, followed by the foot bath, actual sauna, cooling down and relaxing. A full sauna cycle takes at least 45\u201360 minutes. After each cycle one might also use the hot bath, salt scrub, massage facilities, swimming pool or bar and dining facilities. Before doing this, showers are also the norm.",
      "can be confused with perspiration. Cooling down is a part of the sauna cycle and is as important as the heating. Among users it is considered good practice to take a few moments after exiting a sauna before entering a cold plunge, and to enter a plunge pool by stepping into it gradually, rather than immediately immersing fully. In summer, a session is often started with a cold shower. Therapeutic sauna has been shown to aid adaptation, reduce stress hormones, lower blood pressure and improve cardiovascular conditions. Today there are a wide variety of sauna options. Heat sources include wood,",
      "unwell. The drinking of water is not done in the sauna to prevent the available blood from flowing to the abdomen rather than to the head, which increases the chance of losing ones consciousness by vasodilation. After the sauna one will take a few minutes to walk in the outdoor area or cold / snow room to cool down somewhat. Sometimes small portions of fruit will be offered. After a few minutes outside the sauna one will submerse oneself in the cold bath or take a cold shower bath. When cooled down sufficient one usual uses the relax area to"
    ]
  ],
  [
    "Sarcosine is not a naturally encoded amino acid found in the genetic code of any organism.",
    [
      "Non-proteinogenic amino acids In biochemistry, non-coded or non-proteinogenic amino acids are those not naturally encoded or found in the genetic code of any organism. Despite the use of only 22 amino acids (21 in eukaryotes) by the translational machinery to assemble proteins (the proteinogenic amino acids), over 140 amino acids are known to occur naturally in proteins and thousands more may occur in nature or be synthesized in the laboratory. Many non-proteinogenic amino acids are noteworthy because they are; Technically, any organic compound with an amine (-NH) and a carboxylic acid (-COOH) functional group is an amino acid. The proteinogenic",
      "of vitamins (cofactor auxotrophy). The osmolytes, sarcosine and glycine betaine are derived from amino acids, but have a secondary and quaternary amine respectively. Non-proteinogenic amino acids In biochemistry, non-coded or non-proteinogenic amino acids are those not naturally encoded or found in the genetic code of any organism. Despite the use of only 22 amino acids (21 in eukaryotes) by the translational machinery to assemble proteins (the proteinogenic amino acids), over 140 amino acids are known to occur naturally in proteins and thousands more may occur in nature or be synthesized in the laboratory. Many non-proteinogenic amino acids are noteworthy because",
      "cell walls, particularly in Gram-positive bacteria. Only 837 -amino acids were found in Swiss-Prot database (187 million amino acids analysed). The 20 amino acids that are encoded directly by the codons of the universal genetic code are called \"standard\" or \"canonical\" amino acids. A modified form of methionine (\"N\"-formylmethionine) is often incorporated in place of methionine as the initial amino acid of proteins in bacteria, mitochondria and chloroplasts. Other amino acids are called \"non-standard\" or \"non-canonical\". Most of the non-standard amino acids are also non-proteinogenic (i.e. they cannot be incorporated into proteins during translation), but two of them are proteinogenic,"
    ]
  ],
  [
    "The athlete would expend 540 kJ of energy during 5 minutes of exercise with an average steady-rate oxygen uptake of 3.0 L/min.",
    [
      "in kcal/day or in J/s = W (1000 kcal/d ~ 48.5 W). This input power can be determined by measuring oxygen uptake, or in the long term food consumption, assuming no change of weight. This includes the power needed just for living, called the basal metabolic rate BMR or roughly the resting metabolic rate. The required food can also be calculated by dividing the output power by the muscle efficiency. This is 18-26%. From the example above, if a 70 kg person is cycling at 15 km/h by expending 60 W and a muscular efficiency of 20% is assumed, roughly",
      "of oxygen consumed while sitting at rest, and is equal to 3.5 ml oxygen per kilogram body weight per minute. In other words, a means of expressing energy cost of physical activity as a multiple of the resting rate. For instance; walking on level ground at about 6 km/h or carrying groceries up a flight of stairs expends about 4 METs of activity. Generally, >7 METs of activity tolerance is considered excellent while <4 is considered poor for surgical candidates. Determining one's functional capacity can elucidate the degree of surgical risk one might undertake for procedures that risk blood loss,",
      "of oxygen consumption is representative of an increase in energy expenditure. VO2 is often measured in absolute terms (ex. Liters/min), but in weight bearing activities, such as running, body mass can have a profound influence on energy expenditure. As a result, it is common to express energy expenditure as the rate of oxygen consumption in relation to body mass (ex. ml/kg/min). Though some recent data may suggest otherwise, it is traditionally well accepted that a strong linear relationship exists between the rate of oxygen consumption and running speed (see figure 1), with energy expenditure increasing with increasing running speed. It"
    ]
  ],
  [
    "The National Academy of Medicine recommends a minimum intake of 130 g of carbohydrate per day (National Academy of Medicine).",
    [
      "carbohydrate diet less than 45%, and a high carbohydrate diet more than 45%. The National Academy of Medicine recommends a minimum intake of 130 g of carbohydrate per day. The FAO and WHO similarly recommend that the majority of dietary energy come from carbohydrates. A popular misconception driving adoption of the diet for weight loss, is that by reducing carbohydrate intake dieters can in some way avoid weight gain from the calories in other macronutrients. However any weight loss resulting from a low-carbohydrate diet probably comes merely from reduced overall calorie intake. A category of diets is known as low-glycemic-index",
      "into the subject. The American Academy of Family Physicians defines low-carbohydrate diets as diets that restrict carbohydrate intake to 20 to 60 grams per day, typically less than 20% of caloric intake. A 2016 review of low-carbohydrate diets classified diets with 50g of carbohydrate per day (less than 10% of total calories) as \"very low\" and diets with 40% of calories from carbohydrates as \"mild\" low-carbohydrate diets. In a 2015 review Richard D. Feinman and colleagues proposed that a very low carbohydrate diet had less that 10% caloric intake from carbohydrate, a low carbohydrate diet less than 26%, a medium",
      "g) \u2013 each day. The recommended maximum daily intake of sodium \u2013 the amount above which health problems appear \u2013 is 2,300 milligrams per day for adults, about 1 teaspoon of salt (5.9 g). The recommended adequate intake of sodium is 1,500 milligrams (3.9 g salt) per day, and people over 50 need even less.\" Reference Daily Intake The Reference Daily Intake (RDI) is the daily intake level of a nutrient that is considered to be sufficient to meet the requirements of 97\u201398% of healthy individuals in every demographic in the United States. While developed for the US population, it"
    ]
  ],
  [
    "Proteolysis is not used to modify protein structure after translation has occurred.",
    [
      "can recognize nonsense codons and causes the release of the polypeptide chain. The capacity of disabling or inhibiting translation in protein biosynthesis is used by some antibiotics such as anisomycin, cycloheximide, chloramphenicol, tetracycline, streptomycin, erythromycin, puromycin, etc. Events that occur during or following biosynthesis include proteolysis, post-translational modification and protein folding. Proteolysis may remove N-terminal, C-terminal or internal amino-acid residues or peptides from the polypeptide. The termini and side-chains of the polypeptide may be subjected to post-translational modification. These modifications may be required for correct cellular localisation or the natural function of the protein. During and after synthesis, polypeptide chains",
      "substrates and products bind during catalysis; what changes occur during the reaction; and even the role of particular amino acid residues in the mechanism. Some enzymes change shape significantly during the mechanism; in such cases, it is helpful to determine the enzyme structure with and without bound substrate analogues that do not undergo the enzymatic reaction. Not all biological catalysts are protein enzymes; RNA-based catalysts such as ribozymes and ribosomes are essential to many cellular functions, such as RNA splicing and translation. The main difference between ribozymes and enzymes is that RNA catalysts are composed of nucleotides, whereas enzymes are",
      "there in either of two ways depending on the protein: Co-translational translocation (translocation during the process of translation), and post-translational translocation (translocation after the process of translation is complete). Most proteins that are secretory, membrane-bound, or reside in the endoplasmic reticulum (ER), golgi or endosomes use the co-translational translocation pathway. This process begins with the N-terminal signal peptide of the protein being recognized by a signal recognition particle (SRP) \"while the protein is still being synthesized on the ribosome\". The synthesis pauses while the ribosome-protein complex is transferred to an SRP receptor on the ER in eukaryotes, and the plasma"
    ]
  ],
  [
    "During exercise or muscle contraction, the translocation of GLUT4 into the plasma membrane increases, allowing muscle cells to take in more glucose for energy production.",
    [
      "of either exercise or muscle contraction. During exercise, the body needs to convert glucose to ATP to be used as energy. As G-6-P concentrations decrease, hexokinase becomes less inhibited, and the glycolytic and oxidative pathways that make ATP are able to proceed. This also means that muscle cells are able to take in more glucose as its intracellular concentrations decrease. In order to increase glucose levels in the cell, GLUT4 is the primary transporter used in this facilitated diffusion. Although muscle contractions function in a similar way and also induce the translocation of GLUT4 into the plasma membrane, the two",
      "exercise, however, blood sugar levels are not necessarily high, and insulin is not necessarily activated, yet muscles are still able to bring in glucose. AMPK seems to be responsible in part for this exercise-induced glucose uptake. Goodyear et al. observed that with exercise, the concentration of GLUT-4 was increased in the plasma membrane, but decreased in the microsomal membranes, suggesting that exercise facilitates the translocation of vesicular GLUT-4 to the plasma membrane. While acute exercise increases GLUT-4 translocation, endurance training will increase the total amount of GLUT-4 protein available. It has been shown that both electrical contraction and AICAR treatment",
      "the enzyme glycogen phosphatase, which removes a phosphate group from glucose-6-P to release free glucose. In order for glucose to exit a cell membrane, the removal of this phosphate group is essential. Although gluconeogenesis is an important component of hepatic glucose output, it alone can not sustain exercise. For this reason, when glycogen stores are depleted during exercise, glucose levels fall and fatigue sets in. Glucose disposal, the other side of the equation, is controlled by uptake of glucose at the working skeletal muscles. During exercise, despite decreased insulin concentrations, muscle increases GLUT4 translocation of and glucose uptake. The mechanism"
    ]
  ],
  [
    "Ammonia produced from the deamination of branched chain amino acids is mostly converted into urea via the urea cycle in the human body (Oxidative deamination).",
    [
      "Deamination Deamination is the removal of an amino group from a molecule. Enzymes that catalyse this reaction are called deaminases. In the human body, deamination takes place primarily in the liver, however glutamate is also deaminated in the kidneys. In situations of excess protein intake, deamination is used to break down amino acids for energy. The amino group is removed from the amino acid and converted to ammonia. The rest of the amino acid is made up of mostly carbon and hydrogen, and is recycled or oxidized for energy. Ammonia is toxic to the human system, and enzymes convert it",
      "the soil by decaying matter. Others, such as nitrogen-fixing legumes, benefit from symbiotic relationships with rhizobia that create ammonia from atmospheric nitrogen. In certain organisms, ammonia is produced from atmospheric nitrogen by enzymes called nitrogenases. The overall process is called nitrogen fixation. Intense effort has been directed toward understanding the mechanism of biological nitrogen fixation; the scientific interest in this problem is motivated by the unusual structure of the active site of the enzyme, which consists of an FeMoS ensemble. Ammonia is also a metabolic product of amino acid deamination catalyzed by enzymes such as glutamate dehydrogenase 1. Ammonia excretion",
      "Oxidative deamination Oxidative deamination is a form of deamination that generates \u03b1-keto acids and other oxidized products from amine-containing compounds, and occurs largely in the liver and kidney. Oxidative deamination is an important step in the catabolism of amino acids, generating a more metabolizable form of the amino acid, and also generating ammonia as a toxic byproduct. The ammonia generated in this process can then be neutralized into urea via the urea cycle. Much of the oxidative deamination occurring in cells involves the amino acid glutamate, which can be oxidatively deaminated by the enzyme glutamate dehydrogenase (GDH), using NAD or"
    ]
  ],
  [
    "If a large dose of a substance that binds to cytochrome c oxidase A3 was administered to a human, it would disrupt the electron transport chain, preventing the cell from aerobically producing ATP for energy.",
    [
      "dissolved in aqueous sodium hydroxide solution to produce sodium cyanide. Many cyanides are highly toxic. The cyanide anion is an inhibitor of the enzyme cytochrome c oxidase (also known as aa) in the fourth complex of the electron transport chain (found in the membrane of the mitochondria of eukaryotic cells). It attaches to the iron within this protein. The binding of cyanide to this enzyme prevents transport of electrons from cytochrome c to oxygen. As a result, the electron transport chain is disrupted, meaning that the cell can no longer aerobically produce ATP for energy. Tissues that depend highly on",
      "Earth would be poisoned as it passed through the tail. Because of the extremely diffuse nature of the tail, there was no effect when the planet passed through it. Like other cyanides, cyanogen is very toxic, as it readily undergoes reduction to cyanide, which poisons the cytochrome c oxidase complex, thus interrupting the mitochondrial electron transfer chain. Cyanogen gas is an irritant to the eyes and respiratory system. Inhalation can lead to headache, dizziness, rapid pulse, nausea, vomiting, loss of consciousness, convulsions, and death, depending on exposure. Lethal dose through inhalation typically ranges from . Inhalation of 900 ppm over",
      "the Q cycle, one molecule of QH were used to directly reduce two molecules of cytochrome c, the efficiency would be halved, with only one proton transferred per cytochrome c reduced. Cytochrome c oxidase, also known as \"complex IV\", is the final protein complex in the electron transport chain. The mammalian enzyme has an extremely complicated structure and contains 13 subunits, two heme groups, as well as multiple metal ion cofactors \u2013 in all, three atoms of copper, one of magnesium and one of zinc. This enzyme mediates the final reaction in the electron transport chain and transfers electrons to"
    ]
  ],
  [
    "The experience of depersonalization is described as feeling disconnected from one's physicality and a sense of unreality in one's self.",
    [
      "the subjective experience of \"unreality in one's self\", or detachment from one's surroundings. People who are diagnosed with depersonalization also experience an urge to question and think critically about the nature of reality and existence. It results in significant distress. Individuals who experience depersonalization can feel divorced from their own personal physicality by sensing their body sensations, feelings, emotions and behaviors as not belonging to themselves. As such, a recognition of one's self breaks down. Depersonalization can result in very high anxiety levels, which can intensify these perceptions even further. Individuals with depersonalization describe feeling disconnected from their physicality; feeling",
      "often spoken about his diagnosis of depersonalization disorder. Depersonalization disorder Depersonalization disorder (DPD), also known as depersonalization/derealization disorder, is a mental disorder in which the person has persistent or recurrent feelings of depersonalization or derealization. Depersonalization is described as feeling disconnected or detached from one's self. Individuals experiencing depersonalization may report feeling as if they are an outside observer of their own thoughts or body, and often report feeling a loss of control over their thoughts or actions. In some cases, individuals may be unable to accept their reflection as their own, or they may have out-of-body experiences. Derealization is",
      "the self as an example of some defining social category\". Individuals who experience depersonalization feel divorced from their own personal self by sensing their body sensations, feelings, emotions, behaviors etc. as not belonging to the same person or identity. Often a person who has experienced depersonalization claims that things seem unreal or hazy. Also, a recognition of a self breaks down (hence the name). Depersonalization can result in very high anxiety levels, which further increase these perceptions. Depersonalization is a subjective experience of unreality in one's self, while derealization is unreality of the outside world. Although most authors currently regard"
    ]
  ],
  [
    " increases the capacity of the muscle to adapt to varying demands by improving general metabolism, muscle metabolism, haemoglobin levels, buffering capacity, venous return, stroke volume, and the ability of the blood bed to readily adapt to changing requirements.",
    [
      "with increased aerobic, or anaerobic capacity. As aerobic/anaerobic capacity increases, general metabolism rises, muscle metabolism is enhanced, haemoglobin rises, buffers in the bloodstream increase, venous return is improved, stroke volume is improved, and the blood bed becomes more able to adapt readily to varying demands. Each of these results of cardiovascular fitness/cardiorespiratory conditioning will have a direct positive effect on muscular endurance, and an indirect effect on strength and flexibility. To facilitate optimal delivery of oxygen to the working muscles, the person needs to train or participate in activities that will build up the energy stores needed for sport. This",
      "in and out. Endurance training typically results in an increase in tidal volume. Muscles involved in respiration, including the diaphragm and intercostal muscles, increase in strength and endurance. This results in an improved ability to breathe in more air, for longer amounts of time with less fatigue. Aerobic training typically improves the endurance of respiratory muscles, whereas anaerobic training tends to increase the size and strength of respiratory muscles. Exercise increases the vascularization of the lungs. This allows the more blood flow in and out of the lungs. This enhances the uptake of oxygen, since there is greater surface area",
      "Endurance training Endurance training is the act of exercising to increase endurance. The term endurance training generally refers to training the aerobic system as opposed to the anaerobic system. The need for endurance in sports is often predicated as the need of cardiovascular and simple muscular endurance, but the issue of endurance is far more complex. Endurance can be divided into two categories including: general endurance and specific endurance. It can be shown that endurance in sport is closely tied to the execution of skill and technique. A well conditioned athlete can be defined as, the athlete who executes his"
    ]
  ],
  [
    "The sensitivity of the nitrogenase enzyme to oxygen impacts microbial metabolism by requiring nitrogen fixers to protect nitrogenase from oxygen in vivo, which can affect their ability to fix nitrogen and use oxygen as a terminal electron acceptor for respiration.",
    [
      "sensitivity of the nitrogenase to oxygen. Microbial metabolism Microbial metabolism is the means by which a microbe obtains the energy and nutrients (e.g. carbon) it needs to live and reproduce. Microbes use many different types of metabolic strategies and species can often be differentiated from each other based on metabolic characteristics. The specific metabolic properties of a microbe are the major factors in determining that microbe's ecological niche, and often allow for that microbe to be useful in industrial processes or responsible for biogeochemical cycles. All microbial metabolisms can be arranged according to three principles: 1. How the organism obtains",
      "are irreversibly inhibited by dioxygen, which degradatively oxidizes the Fe-S cofactors. This requires mechanisms for nitrogen fixers to protect nitrogenase from oxygen \"in vivo\". Despite this problem, many use oxygen as a terminal electron acceptor for respiration. Although the ability of some nitrogen fixers such as Azotobacteraceae to employ an oxygen-labile nitrogenase under aerobic conditions has been attributed to a high metabolic rate, allowing oxygen reduction at the cell membrane, the effectiveness of such a mechanism has been questioned at oxygen concentrations above 70 \u00b5M (ambient concentration is 230 \u00b5M O), as well as during additional nutrient limitations. In addition",
      "nitrogenase (leading to overestimates of nitrogenase by ARA). Bottle or chamber-based assays may produce negative impacts on microbial systems as a result of containment or disruption of the microenvironment through handling, leading to underestimation of nitrogenase. Despite these weaknesses, such assays are very useful in assessing relative rates or temporal patterns in nitrogenase activity. Nitrogenase Nitrogenases are enzymes () that are produced by certain bacteria, such as cyanobacteria (blue-green algae). These enzymes are responsible for the reduction of nitrogen (N) to ammonia (NH). Nitrogenases are the only family of enzymes known to catalyze this reaction, which is a key step"
    ]
  ],
  [
    "The variables that must be known in order to determine the Doppler shift in perceived sound frequency are the speed of the source of the sound, the speed of the observer, and the frequency of the emitted sound.",
    [
      "Doppler effect The Doppler effect (or the Doppler shift) is the change in frequency or wavelength of a wave in relation to an observer who is moving relative to the wave source. It is named after the Austrian physicist Christian Doppler, who described the phenomenon in 1842. A common example of Doppler shift is the change of pitch heard when a vehicle sounding a horn approaches and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession. The reason for the",
      "only the words of the second sentence are heard. The ENV rates most important for speech are those below about 16 Hz, corresponding to fluctuations at the rate of syllables. On the other hand, the fundamental frequency (\u201cpitch\u201d) contour of speech sounds is primarily conveyed via TFS cues, although some information on the contour can be perceived via rapid envelope fluctuations corresponding to the fundamental frequency. For music, slow ENV rates convey rhythm and tempo information, whereas more rapid rates convey the onset and offset properties of sound (attack and decay, respectively) that are important for timbre perception. The ability",
      "Doppler shift occurs when there is relative motion between a sound source and a perceiver and slightly shifts the perceived frequency of the sound. When a flying bird is changing direction, the amplitude of the Doppler shift between it and an infrasonic source would change, enabling the bird to locate the source. This kind of mechanism would require the ability to detect very small changes in frequency. A pigeon typically flies at 20 km/hr, so a turn could cause up to a 12% modulation of an infrasonic stimulus. According to response measurements, pigeons are able to distinguish frequency changes of"
    ]
  ],
  [
    "The physiological characteristics of marathon runners, such as aerobic capacity, lactate threshold, glycogen stores, stroke volume, and oxygen-carrying capacity, play a key role in determining their success.",
    [
      "certain physiological characteristics of marathon runners exist. The differing efficiency of certain physiological features in marathon runners evidence the variety of finishing times among elite marathon runners that share similarities in many physiological characteristics. Aside from large aerobic capacities and other biochemical mechanisms, external factors such as the environment and proper nourishment of a marathon runner can further the insight as to why marathon performance is variable despite ideal physiological characteristics obtained by a runner. The first marathon ever run was an unintentional 25 mile trek performed by Pheidippides. Pheidippides was a Greek soldier who ran to Athens from the",
      "to how lactate threshold effects endurance performance. The contribution of one's blood lactate levels accumulating is attributed to potential skeletal muscle hypoxemia but also to the production of more glucose that can be used as energy. The inability to establish a singular set of physiological contributions to blood lactate accumulation's effect on the exercising individual creates a correlative role for lactate threshold in marathon performance as opposed to a causal role. In order to sustain high intensity running, a marathon runner must obtain sufficient glycogen stores. Glycogen can be found in the skeletal muscles or liver. With low levels of",
      "a greater stroke volume. A concomitant decrease in stroke volume occurs with the initial increase in heart rate at the onset of exercise. Despite an increase in cardiac dimensions, a marathoner's aerobic capacity is confined to this capped and ever decreasing heart rate. The amount of oxygen that blood can carry depends on blood volume, which increases during a race, and the amount of hemoglobin in blood. Other physiological factors affecting a marathon runner's aerobic capacity include pulmonary diffusion, mitochondria enzyme activity, and capillary density. A long-distance runner's running economy is their steady state requirement for oxygen at specific speeds"
    ]
  ],
  [
    "The common phases to cells undergoing meiosis and mitosis are interphase, which consists of G phase, S phase, and G phase, followed by the M phase (mitosis or meiosis).",
    [
      "organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of cell division The cell cycle consists of four distinct phases: G phase, S phase (synthesis), G phase (collectively known as interphase) and M phase (mitosis or meiosis). M phase is itself composed of two tightly",
      "meiosis to produce haploid cells or gametes. Haploid cells may divide again (by mitosis) to form more haploid cells, as in many yeasts, but the haploid phase is not the predominant life cycle phase. In most diplonts, mitosis occurs only in the diploid phase, i.e. gametes usually form quickly and fuse to produce diploid zygotes. In the whole cycle, gametes are usually the only haploid cells, and mitosis usually occurs only in the diploid phase. The diploid multicellular individual is a diplont, hence a gametic meiosis is also called a diplontic life cycle. Diplonts are: In sporic meiosis (also commonly",
      "a series of changes that takes place in a newly formed cell and its nucleus, before it becomes capable of division again. It is also called preparatory phase or intermitosis. Previously it was called resting stage because there is no apparent activity related to cell division.Typically interphase lasts for at least 91% of the total time required for the cell cycle. Interphase proceeds in three stages, G, S, and G, followed by the cycle of mitosis and cytokinesis. The cell's nuclear DNA contents are duplicated during S phase but may continue till G in case of heterochromatin. The first phase"
    ]
  ],
  [
    "The rate of oxygen consumption for male athletes during a training session is commonly expressed as the rate of oxygen consumption in relation to body mass (ex. ml/kg/min) and is closely related to running speed.",
    [
      "of oxygen consumption is representative of an increase in energy expenditure. VO2 is often measured in absolute terms (ex. Liters/min), but in weight bearing activities, such as running, body mass can have a profound influence on energy expenditure. As a result, it is common to express energy expenditure as the rate of oxygen consumption in relation to body mass (ex. ml/kg/min). Though some recent data may suggest otherwise, it is traditionally well accepted that a strong linear relationship exists between the rate of oxygen consumption and running speed (see figure 1), with energy expenditure increasing with increasing running speed. It",
      "coaches to fine tune his training program so that he could recover between swim events that were sometimes several minutes apart. Much similar to blood glucose for diabetes, lower priced lactate measurement devices are now available but in general the lactate measurement approach is still the domain of the professional coach and elite athlete. Endurance training Endurance training is the act of exercising to increase endurance. The term endurance training generally refers to training the aerobic system as opposed to the anaerobic system. The need for endurance in sports is often predicated as the need of cardiovascular and simple muscular",
      "of the body to the muscles. The body undergoes aerobic respiration in order to provide sufficient delivery of O to the exercising skeletal muscles and the main determining factors are shown in figure 1. The rate of maximum O uptake (Omax) depends on cardiac output, O extraction and hemoglobin mass. The cardiac output of an athlete is difficult to manipulate during competitions and the distribution of cardiac output is at the maximum rate (i.e. 80%) during competitions. In addition, the O extraction is approximately 90% at maximal exercise. Therefore, the only method to enhance the physical performance left is to"
    ]
  ],
  [
    "The ratio of purines to pyrimidines in a double-stranded molecule of DNA is 1:1, as per Chargaff's rules.",
    [
      "pyrimidines are thymine and cytosine; the purines are adenine and guanine. Both strands of double-stranded DNA store the same biological information. This information is replicated as and when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (informally, \"bases\"). It is the sequence of these four nucleobases along the backbone that encodes genetic information.",
      "Chargaff's rules Chargaff's rules state that DNA from any cell of any organisms should have a 1:1 ratio (base Pair Rule) of pyrimidine and purine bases and, more specifically, that the amount of guanine should be equal to cytosine and the amount of adenine should be equal to thymine. This pattern is found in both strands of the DNA. They were discovered by Austrian born chemist Erwin Chargaff, in the late 1940s. The first rule holds that a double-stranded DNA molecule \"globally\" has percentage base pair equality: %A = %T and %G = %C. The rigorous validation of the rule",
      "of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, pyrimidines and purines. In DNA, the"
    ]
  ],
  [
    "Sauna use is more prevalent in Finland compared to other countries, with one sauna for every three people, and potential health risks associated with sauna bathing in this population include dehydration, overheating, and cardiovascular issues.",
    [
      "as well as in most Scandinavian and in the German-speaking countries of Europe. This is true even when a swimsuit must be worn in the swimming pool area of the same complex. Saunas are very common in modern Finland, where there is one sauna for every three people and became very popular in the remainder of Europe in recent decades. German soldiers had got to know the Finnish saunas during their fight against the Soviet Union in the Continuation War, where Germany and Finland fought on the same side. Finnish hygiene depended so exclusively on saunas, that they had built",
      "the home sauna. There are at least 2 million saunas according to official registers. The Finnish Sauna Society believes the number can actually be as high as 3.2 million saunas (population 5.5 million).. Many Finns take at least one a week, and much more when they visit their summer cottage in the countryside. Here the pattern of life tends to revolve around the sauna, and a nearby lake used for cooling off. Sauna traditions in Estonia are almost identical to Finland as saunas have traditionally held a central role in the life of an individual. Ancient Estonians believed saunas were",
      "Finnic languages other than Finnish and Estonian, \"sauna\" and cognates do not necessarily mean a building or space built for bathing. It can also mean a small cabin or cottage, such as a cabin for a fisherman. The sauna known in the western world today originates from Northern Europe. In Finland, there are built-in saunas in almost every house. The oldest known saunas in Finland were made from pits dug in a slope in the ground and primarily used as dwellings in winter. The sauna featured a fireplace where stones were heated to a high temperature. Water was thrown on"
    ]
  ],
  [
    "Setting Karen up for failure and reinforcing her negative beliefs about her abilities would not be recommended by her therapist to raise her sense of self-efficacy.",
    [
      "is expected to boost individual self-efficacy. Research has shown that the effects of verbal persuasion may not prevail through a long history of failure. It has been shown to create an enduring sense of self-efficacy in situations where aid is given to facilitate successful action. Failures have a negative effect because it discredits the persuaders and undermines the individual's self-efficacy. Depending on the circumstances, stressful situations can lessen the feeling of personal competency. Poor performance, for example is usually associated with a state of high arousal. Fear-provoking thoughts can cause an individual to overestimate the intensity of a threatening situation.",
      "and undermine the therapy accordingly. A person setting themselves up for failure may do so because they have a fear of failure, an unrealistic assessment of their own abilities, or because they are naive and uninformed regarding the abilities necessary to succeed. In some cases, an individual has an unjustified expectation that they will fail, a self-reinforcing negative spiral, or failure neurosis \u2013 perhaps driven by a sense of guilt, or by the compulsion to repeat self-destructive behaviour. It is a tactic used in reality television, where situations are engineered to produce certain results. \"My Kitchen Rules\" contestant Emily Cheung",
      "believe that he/she is incapable of performing the suggested preventative behaviors that will lead to avoidance behaviors. Bandura's Research done by others have revealed \"a positive, linear effect of fear on overall intentions and behavior\", especially when the messages endorse people's self-efficacy. This effect is more positive when behaviors are performed on one-time basis instead of repeatedly. However, other researchers also pointed out that in the context of self-efficacy need to be considered carefully in relation to other stratefies. The use of other persuative techniques such as behavioral trainings might counteract against the efficacy of fear appeal in isolation. According"
    ]
  ],
  [
    "The regulation of PDHA1 by free fatty acids during exercise can impact cancer cell cytotoxicity by potentially enhancing the effectiveness of ROS-generating anticancer drugs through the manipulation of redox adaptation.",
    [
      "disorder results in the inactivation of the catalytic machinery. The PDH complex can be regulated in a variety of conditions. The PDHA1 subunit has been shown to be regulated by free fatty acids during bouts of exercise. The presence of free fatty acids increases the level of phosphorylation, thereby decreasing PDH activity. During exercise, however, these effects are overruled, and there is a much higher level of dephosphorylated PDHA1 in the cells. In certain muscles, such as the triceps, the metabolic enzyme profile seems to directly affect the level of PDH activity, which can result in higher levels of lactate",
      "for supporting cancer growth and toxicity from ROS-generating anticancer drugs. Combinations of ROS-generating drugs with pharmaceuticals that can break the redox adaptation could be a better strategy for enhancing cancer cell cytotoxicity. James Watson and others have proposed that lack of intracellular ROS due to a lack of physical exercise may contribute to the malignant progression of cancer, because spikes of ROS are needed to correctly fold proteins in the endoplasmatic reticulum and low ROS levels may thus aspecifically hamper the formation of tumor suppressor proteins. Since physical exercise induces temporary spikes of ROS, this may explain why physical exercise",
      "cancer. That same study discussed that a diet high in MUFAs is not the major determinant of erythrocyte membrane MUFAs, where most oleic acid in mammalian tissue is derived from the saturated stearic acid residue. Where key conversion is controlled by the Delta9-desaturase, which also regulates the transformation of the other common saturated fatty acids (SFAs) (myristic and palmitic). The study discussed that fat content of the diet has an important effect on Delta9-d activity, while high levels of SFAs increase Delta9-d activity by twofold to threefold, whereas polyunsaturated fatty acids (PUFAs) decrease. This conclusion was partially contradicted by a"
    ]
  ],
  [
    "The significantly higher Rf value of the products compared to the reactants in thin layer chromatography indicates a strong dispersion in the chromatographic response function.",
    [
      "The CRFs in thin layer chromatography characterize the \"equal-spreading\" of the spots. The ideal case, when the RF of the spots are uniformly distributed in <0,1> range (for example 0.25,0.5 and 0.75 for three solutes) should be characterized as the best situation possible. The simplest criteria are formula_1 and formula_1 product (Wang et al., 1996). They are the smallest difference between sorted RF values, or product of such differences. Another function is the multispot response function (MRF) as developed by De Spiegeleer et al. It is based also of differences product. This function always lies between 0 and 1. When",
      "a strong dispersion. Only four values are statistically close The average value of 9.6 cms is relatively close to the only direct measure. Lou also suggested other products for reaction (10): Some differences were noticed for reactions of type (6) accounting for the vibrational levels of the collision partners: (v=0) + HCl(v=1) \u2192 Xe + HCl + Cl + Cl (6a) \"with rate constant of k\" (v=0) + HCl(v=2) \u2192 Xe + HCl + Cl + Cl (6b) \"with rate constant of k\" XeCl(B,C;v\u22600) + HCl(v=0) \u2192 Other products and not XeCl (6c) \"with rate constant of k\" The values",
      "Response factor Response factor, usually in chromatography and spectroscopy, is the ratio between a signal produced by an analyte, and the quantity of analyte which produces the signal. Ideally, and for easy computation, this ratio is unity (one). In real-world scenarios, this is often not the case. The response factor formula_1 can be expressed on a molar, volume or mass basis. Where the true amount of sample and standard are equal: where A is the signal (e.g. peak area) and the subscript \"i\" indicates the sample and the subscript \"st\" indicates the standard. The response factor of the standard is"
    ]
  ],
  [
    "The synthesis of glucose from lactate, glycerol, or amino acids is called gluconeogenesis.",
    [
      "contain lactose. Galactose metabolism, which converts galactose into glucose, is carried out by the three principal enzymes in a mechanism known as the Leloir pathway. The enzymes are listed in the order of the metabolic pathway: galactokinase (GALK), galactose-1-phosphate uridyltransferase (GALT), and UDP-galactose-4\u2019-epimerase (GALE). In human lactation, glucose is changed into galactose via hexoneogenesis to enable the mammary glands to secrete lactose. However, most lactose in breast milk is synthesized from galactose taken up from the blood, and only 35\u00b16% is made from galactose from \"de novo\" synthesis. Glycerol also contributes some to the mammary galactose production. Glucose is the",
      "Thereafter the glucose that is released into the blood by the liver for general use by the body tissues, has to be synthesized from the glucogenic amino acids and a few other gluconeogenic substrates, which do not include fatty acids. Please note however that lipolysis releases glycerol which can enter the pathway of gluconeogenesis. Fatty acids are broken down to acetyl-CoA by means of beta oxidation inside the mitochondria, whereas fatty acids are synthesized from acetyl-CoA outside the mitochondria, in the cytosol. The two pathways are distinct, not only in where they occur, but also in the reactions that occur,",
      "regenerates the glucose, using a process called gluconeogenesis. This process is not quite the opposite of glycolysis, and actually requires three times the amount of energy gained from glycolysis (six molecules of ATP are used, compared to the two gained in glycolysis). Analogous to the above reactions, the glucose produced can then undergo glycolysis in tissues that need energy, be stored as glycogen (or starch in plants), or be converted to other monosaccharides or joined into di- or oligosaccharides. The combined pathways of glycolysis during exercise, lactate's crossing via the bloodstream to the liver, subsequent gluconeogenesis and release of glucose"
    ]
  ],
  [
    "VO2 max is the maximum rate of oxygen consumption measured during incremental exercise, typically assessed through a graded exercise test where exercise intensity is progressively increased until oxygen consumption reaches a steady state.",
    [
      "began measuring oxygen consumption during exercise. Notable contributions were made by Henry Taylor at the University of Minnesota, Scandinavian scientists Per-Olof \u00c5strand and Bengt Saltin in the 1950s and 60s, the Harvard Fatigue Laboratory, German universities, and the Copenhagen Muscle Research Centre among others. VO2 max VO max (also maximal oxygen consumption, maximal oxygen uptake, peak oxygen uptake or maximal aerobic capacity) is the maximum rate of oxygen consumption measured during incremental exercise; that is, exercise of increasing intensity. The name is derived from three abbreviations: \"V\" for volume, \"O\" for oxygen, and \"max\" for maximum. Maximal oxygen consumption reflects",
      "VO2 max VO max (also maximal oxygen consumption, maximal oxygen uptake, peak oxygen uptake or maximal aerobic capacity) is the maximum rate of oxygen consumption measured during incremental exercise; that is, exercise of increasing intensity. The name is derived from three abbreviations: \"V\" for volume, \"O\" for oxygen, and \"max\" for maximum. Maximal oxygen consumption reflects the cardiorespiratory fitness of an individual, which is a powerful modifiable determinant of life expectancy in United States adults. It is also an important determinant of endurance capacity during prolonged exercise. VO max is widely used as an indicator of cardiorespiratory fitness. In 2016,",
      "the aerobic energy system. In general clinical and athletic testing, this usually involves a graded exercise test (either on a treadmill or on a cycle ergometer) in which exercise intensity is progressively increased while measuring: VO max is reached when oxygen consumption remains at a steady state despite an increase in workload. VO max is properly defined by the Fick equation: The necessity for a subject to exert maximum effort in order to accurately measure VO max can be dangerous in those with compromised respiratory or cardiovascular systems; thus, sub-maximal tests for \"estimating\" VO max have been developed. An estimate"
    ]
  ],
  [
    "The solid-liquid equilibrium line in the phase diagram for water has a negative slope because ice (solid water) is less dense than liquid water due to a more extensive network of hydrogen bonding, causing the melting point to decrease with pressure (Phase diagram).",
    [
      "of the solid phase and enter the liquid phase. A similar concept applies to liquid\u2013gas phase changes. Water is an exception which has a solid-liquid boundary with negative slope so that the melting point decreases with pressure. This occurs because ice (solid water) is less dense than liquid water, as shown by the fact that ice floats on water. At a molecular level, ice is less dense because it has a more extensive network of hydrogen bonding which requires a greater separation of water molecules. Other exceptions are antimony and bismuth. The value of the slope dP/dT is given by",
      "on the basis of the shape of their phase diagram. The simplest phase diagrams are pressure\u2013temperature diagrams of a single simple substance, such as water. The axes correspond to the pressure and temperature. The phase diagram shows, in pressure\u2013temperature space, the lines of equilibrium or phase boundaries between the three phases of solid, liquid, and gas. The curves on the phase diagram show the points where the free energy (and other derived properties) becomes non-analytic: their derivatives with respect to the coordinates (temperature and pressure in this example) change discontinuously (abruptly). For example, the heat capacity of a container filled",
      "By charging the right amount of water and applying heat, the system can be brought to any point in the gas region of the phase diagram. If the piston is slowly lowered, the system will trace a curve of increasing temperature and pressure within the gas region of the phase diagram. At the point where gas begins to condense to liquid, the direction of the temperature and pressure curve will abruptly change to trace along the phase line until all of the water has condensed. Between two phases in equilibrium there is a narrow region where the properties are not"
    ]
  ],
  [
    "When 10.0 mL of 0.1 M HCl is added to a solution of Mg(OH)2 dissolved in water at 25 \u00b0C, the Mg(OH)2 will react with the HCl to form MgCl2 and water.",
    [
      "Example: Make 2 g/100mL of NaCl solution with 1 L water Water (properties). The density of resulting solution is considered to be equal to that of water, statement holding especially for dilute solutions, so the density information is not required. Solution In chemistry, a solution is a special type of homogeneous mixture composed of two or more substances. The term aqueous solution is when one of the solvents is water. In such a mixture, a solute is a substance dissolved in another substance, known as a solvent. The mixing process of a solution happens at a scale where the effects",
      "liter of solution) we get , thus such a solution has a pH of . For a commonplace example based on the facts that the masses of a mole of water, a mole of hydrogen ions, and a mole of hydroxide ions are respectively 18 g, 1 g, and 17 g, a quantity of 10 moles of pure (pH 7) water, or 180 tonnes (18\u00d710 g), contains close to 1 g of dissociated hydrogen ions (or rather 19 g of HO hydronium ions) and 17 g of hydroxide ions. Note that pH depends on temperature. For instance at 0 \u00b0C",
      "smaller the value, or the more negative the log value, the lower the solubility. Some salts are not fully dissociated in solution. Examples include MgSO, famously discovered by Manfred Eigen to be present in seawater as both an inner sphere complex and an outer sphere complex. The solubility of such salts is calculated by the method outlined in dissolution with reaction. For hydroxides solubility products are often given in a modified form, \"K\"*, using hydrogen ion concentration in place of hydroxide ion concentration. The two concentrations are related by the self-ionization constant for water, \"K\". For example, The common-ion effect"
    ]
  ],
  [
    "Mutations in the MT-TF gene are responsible for causing myoclonic epilepsy with ragged-red fibers (MERRF).",
    [
      "can result in mitochondrial deficiencies and associated disorders, including Myoclonic epilepsy with ragged-red fibers (MERRF), Mitochondrial encephalomyopathy, lactic acidosis, and stroke-like episodes (MELAS), Juvenile myopathy, encephalopathy, lactic acidosis, and stroke. Mutations in the \"MT-TF\" gene have been associated with myoclonic epilepsy with ragged-red fibers (MERRF). Myoclonic epilepsy with ragged-red fibers (MERRF) is a disorder that affects many parts of the body, particularly the muscles and nervous system. In most cases, the signs and symptoms of this disorder appear during childhood or adolescence. The features of MERRF vary widely among affected individuals, even among members of the same family. Common symptoms",
      "\"MT-TI\" can result in multiple mitochondrial deficiencies and associated disorders. Mutations in the \"MT-TI\" gene have been associated with myoclonic epilepsy with ragged-red fibers (MERRF). Myoclonic epilepsy with ragged-red fibers (MERRF) is a disorder that affects many parts of the body, particularly the muscles and nervous system. In most cases, the signs and symptoms of this disorder appear during childhood or adolescence. The features of MERRF vary widely among affected individuals, even among members of the same family. Common symptoms include, myoclonus, myopathy, spasticity, epilepsy, peripheral neuropathy, dementia, ataxia, atrophy, and more. Mutations in the \"MT-TI\" gene may also cause",
      "in \"MT-TT\" have been associated with myoclonic epilepsy with ragged-red fibers (MERRF), and cause mitochondrial energy deficiencies and reduced proliferation leading to oxidative phosphorylation. Myoclonic epilepsy with ragged-red fibers (MERRF) is a rare mitochondrial disorder that affects many parts of the body, particularly the muscles and nervous system. In most cases, the signs and symptoms of this disorder appear during childhood or adolescence. The features of MERRF vary widely among affected individuals, even among members of the same family. Common clinical manifestations include myoclonus, myopathy, spasticity,epilepsy, peripheral neuropathy, dementia, ataxia, atrophy and more. In addition, mutations have also been linked"
    ]
  ],
  [
    "Selective Androgen Receptor Modulators (SARMs) are a type of selective receptor modulator that have tissue-selective effects, behaving as antagonists in the prostate with little to no effects in anabolic tissues or the central nervous system, unlike anabolic steroids which have non-selective effects throughout the body.",
    [
      "may allow retaining the desired beneficial therapeutic effects of a drug while minimizing undesirable side effects. Drugs with this mixed agonist/antagonist profile of action are referred to as selective receptor modulators (SRMs). Examples include Selective Androgen Receptor Modulators (SARMs), Selective Estrogen Receptor Modulators (SERMs) and Selective Progesterone Receptor Modulators (SPRMs). The mechanism of action of SRMs may vary depending on the chemical structure of the ligand and the receptor involved, however it is thought that many SRMs work by promoting a conformation of the receptor that is closely balanced between agonism and antagonism. In tissues where the concentration of coactivator",
      "emerged recently, and are in pre-clinical development. Small molecule antiandrogens that are available today have undesirable side effects caused by complete, non-selective inhibition of AR action. To minimize these side effects, a new class of tissue selective androgen receptor modulators (SARMs) has been proposed as a novel approach for the treatment of prostate cancer. These ligands should behave as antagonists in the prostate with either no activity or agonist activity in other target tissues, so as to have little or no effects in the anabolic tissues or central nervous system (CNS). However discovering this new class of ligands might be",
      "Selective receptor modulator In the field of pharmacology, a selective receptor modulator or SRM is a type of drug that has different effects in different tissues. A SRM may behave as an agonist in some tissues while as an antagonist in others. Hence selective receptor modulators are sometimes referred to as tissue selective drugs or mixed agonists / antagonists. This tissue selective behavior is in contrast to many other drugs that behave either as agonists or antagonists regardless of the tissue in question. Note that selective estrogen receptor modulator (SERM) is the only class of these drugs currently on the"
    ]
  ],
  [
    "The arrival of the motor nerve action potential at the presynaptic neuron terminal leads to the release of neurotransmitters at the motor endplate through the opening of voltage-dependent calcium channels, influx of Ca ions into the presynaptic neuron's cytosol, fusion of neurotransmitter-containing vesicles with the cell membrane, and exocytosis of acetylcholine quanta.",
    [
      "arrival of the nerve impulse in the motor nerve terminals and the first response of the endplate The arrival of the motor nerve action potential at the presynaptic neuron terminal opens voltage-dependent calcium channels and Ca ions flow from the extracellular fluid into the presynaptic neuron's cytosol. This influx of Ca causes several hundred neurotransmitter-containing vesicles to fuse with the presynaptic neuron's cell membrane through SNARE proteins to release their acetylcholine quanta by exocytosis. The endplate depolarization by the released acetylcholine is called an endplate potential (EPP). The EPP is accomplished when ACh binds the nicotinic acetylcholine receptors (nAChR) at",
      "The neurotransmitter is released from the presynaptic nerve through exocytosis. The neurotransmitter chemical then diffuses across to receptors located on the membrane of the target cell. The neurotransmitter binds to these receptors and activates them. Depending on the type of receptors that are activated, the effect on the target cell can be to excite the target cell, inhibit it, or alter its metabolism in some way. This entire sequence of events often takes place in less than a thousandth of a second. Afterward, inside the presynaptic terminal, a new set of vesicles is moved into position next to the membrane,",
      "an action potential. When an action potential arrives at the synapse's presynaptic terminal button, it may stimulate the release of neurotransmitters. These neurotransmitters are released into the synaptic cleft to bind onto the receptors of the postsynaptic membrane and influence another cell, either in an inhibitory or excitatory way. The next neuron may be connected to many more neurons, and if the total of excitatory influences minus inhibitory influences is great enough, it will also \"fire\". That is to say, it will create a new action potential at its axon hillock, releasing neurotransmitters and passing on the information to yet"
    ]
  ],
  [
    "Sensory adaptation examples include the feeling of a table's surface on the skin no longer being felt after resting a hand on it for a while (Neural adaptation).",
    [
      "Neural adaptation Neural adaptation or sensory adaptation is a change over time in the responsiveness of the sensory system to a constant stimulus. It is usually experienced as a change in the stimulus. For example, if a hand is rested on a table, the table's surface is immediately felt on the skin. After a little while though this is no longer felt. The sensory neurons that initially respond are no longer stimulated to respond; this is an example of neural adaptation. All sensory and neural systems have a form of adaptation to constantly detect changes in the environment. Neural receptor",
      "early childhood brain injuries have shown that neural adaptations slowly occur after the injury. Children with early injuries to the linguistics, spatial cognition and affective development areas of the brain showed deficits in those areas as compared to those without injury. Due to neural adaptations, however, by early school-age, considerable development to those areas was observed. Neural adaptation Neural adaptation or sensory adaptation is a change over time in the responsiveness of the sensory system to a constant stimulus. It is usually experienced as a change in the stimulus. For example, if a hand is rested on a table, the",
      "the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves provide useful information about the sources of and distances to objects, with larger animals making and hearing lower-frequency sounds and smaller animals making and hearing higher-frequency sounds. Taste and smell respond to chemicals in the environment that were significant for fitness in the environment of evolutionary adaptedness. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range"
    ]
  ],
  [
    "The second shock would be characterized in the refractory period of membrane potential stimulation.",
    [
      "returning towards resting potential. The undershoot, or afterhyperpolarization, phase is the period during which the membrane potential temporarily becomes more negatively charged than when at rest (hyperpolarized). Finally, the time during which a subsequent action potential is impossible or difficult to fire is called the refractory period, which may overlap with the other phases. The course of the action potential is determined by two coupled effects. First, voltage-sensitive ion channels open and close in response to changes in the membrane voltage \"V\". This changes the membrane's permeability to those ions. Second, according to the Goldman equation, this change in permeability",
      "membrane can be stimulated depends on two variables: the strength of the stimulus, and the duration for which the stimulus is applied. These variables are inversely related: as the strength of the applied current increases, the time required to stimulate the membrane decreases (and vice versa) to maintain a constant effect. Mathematically, rheobase is equivalent to half the current that needs to be applied for the duration of chronaxie, which is a strength-duration time constant that corresponds to the duration of time that elicits a response when the nerve is stimulated at twice rheobasic strength. The strength-duration curve was first",
      "calcium\") within the axon membrane that remains attached to the membrane's inner surface. Katz and Miledi manipulated the concentration within the presynaptic membrane to determine whether or not residual remaining within the terminal after the first impulse caused an increase in neurotransmitter release following the second stimulus. During the first nerve impulse, concentration was either significantly below or nearing that of the second impulse. When concentration was approaching that of the second impulse, facilitation was increased. In this first experiment, stimuli were presented in intervals of 100 ms between the first and second stimuli. An absolute refractory period was reached"
    ]
  ],
  [
    "The complete breakdown of one molecule of glucose by glycolysis, citric acid cycle, and oxidative phosphorylation in eukaryotes can produce between 30 and 38 ATP molecules.",
    [
      "are: two GTP, six NADH, two QH, and four CO. The above reactions are balanced if P represents the HPO ion, ADP and GDP the ADP and GDP ions, respectively, and ATP and GTP the ATP and GTP ions, respectively. The total number of ATP molecules obtained after complete oxidation of one glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is estimated to be between 30 and 38. The theoretical maximum yield of ATP through oxidation of one molecule of glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is 38 (assuming 3 molar equivalents of ATP per equivalent",
      "for subsequent breakdown in later steps of glycolysis. At physiological conditions, this initial reaction is irreversible. In anaerobic respiration, one glucose molecule produces a net gain of two ATP molecules (four ATP molecules are produced during glycolysis through substrate-level phosphorylation, but two are required by enzymes used during the process). In aerobic respiration, a molecule of glucose is much more profitable in that a maximum net production of 30 or 32 ATP molecules (depending on the organism) through oxidative phosphorylation is generated. Tumor cells often grow comparatively quickly and consume an above-average amount of glucose by glycolysis, which leads to",
      "pathways in eukaryotes are (1) glycolysis, (2) the citric acid cycle/oxidative phosphorylation, and (3) beta-oxidation. The overall process of oxidizing glucose to carbon dioxide, the combination of pathways 1 and 2, is known as cellular respiration, produces about 30 equivalents of ATP from each molecule of glucose. ATP production by a non-photosynthetic aerobic eukaryote occurs mainly in the mitochondria, which comprise nearly 25% of the volume of a typical cell. In glycolysis, glucose and glycerol are metabolized to pyruvate. Glycolysis generates two equivalents of ATP through substrate phosphorylation catalyzed by two enzymes, PGK and pyruvate kinase. Two equivalents of NADH"
    ]
  ],
  [
    "Modification of the RNA strand could potentially disrupt the structure of the mRNA, causing it to take different paths such as being targeted for degradation rather than being translated into the transmembrane protein. (Epitranscriptome)",
    [
      "conditions change, the fraction of modified transcripts can change as well. As with other types of RNA, modifications impact the overall structure of the mRNA. Altering the its structure may cause the mRNA to take different paths. For example, a normal transcript might be fated to be translated; however, the introduction of a modified base can disrupt its structure and send it down a different path, and that particular transcript may now be targeted for degradation. Modifications can also happen in short non-coding RNAs, including small nuclear RNA (snRNA) and microRNA (miRNA). However, these modifications are less common than those",
      "to initiate translation. Therefore, modifications, such as N7-methylguanine during RNA processing, to the 5' cap may effect the ability of the ribosome to initiate translation. It is important to note that not all modifications happening to the mRNA are epigenetic, some, like the N7-methylguanosine cap, are RNA editing. mRNA molecules demonstrate something known as \"modification stoichiometry\". Modification stoichiometry is when only a portion of transcripts have a specific modification at a particular modification site. Typically, under normal cell conditions, the modification stoichiometry is very low, there are a very few number of transcripts that have specific modifications. However, as cell",
      "structure. Messenger RNA is the bridge between the genetic code and the resulting proteins, as it is what carries the necessary information that gets translated into proteins. Modifications to the actual, physical genetic code are likely to be deleterious; therefore, minor modifications, such as methylation, done to mRNA are preferable (nevertheless, modifications are still seen throughout the genome). The four major types of modifications done to mRNA are N7-methylguanine (at the 5\u2032 cap), N-methyladenosine, 5-methylcytosine, and 2\u2032-O-methylation. The modification seen at the 5' cap perfectly demonstrates how modifications to mRNA can impact its function, as the 5' cap is necessary"
    ]
  ],
  [
    "Glycolysis converts glucose into pyruvate, generating ATP and NADH through a series of enzyme-catalyzed reactions in the metabolic pathway.",
    [
      "from them. During glycolysis ATP, NADH (both an energy transport form used inside cells) as well as pyruvate are produced. Glycolysis is taking place in the cytosol, and the created pyruvate needs to be transported to the mitochondrion, where further energy can extracted through the conversion of pyruvate to lactate, and through the citric acid cycle (CAC) (see below, c.f. bioenergetic systems). The liver can also create glucose (gluconeogenesis, see below); during times of low carbohydrate supply from the digestive system, the liver creates glucose and supplies it to other organs. Most enzymes of glycolysis also participate in gluconeogenesis, as",
      "Glycolysis Glycolysis (from \"glycose\", an older term for glucose + \"-lysis\" degradation) is the metabolic pathway that converts glucose CHO, into pyruvate, CHCOCOO + H. The free energy released in this process is used to form the high-energy molecules ATP (adenosine triphosphate) and NADH (reduced nicotinamide adenine dinucleotide). Glycolysis is a sequence of ten enzyme-catalyzed reactions. Most monosaccharides, such as fructose and galactose, can be converted to one of these intermediates. The intermediates may also be directly useful. For example, the intermediate dihydroxyacetone phosphate (DHAP) is a source of the glycerol that combines with fatty acids to form fat. Glycolysis",
      "to break down one molecule of glucose into two molecules of pyruvate. This also produces a net two molecules of ATP, the energy currency of cells, along with two reducing equivalents of converting NAD (nicotinamide adenine dinucleotide: oxidised form) to NADH (nicotinamide adenine dinucleotide: reduced form). This does not require oxygen; if no oxygen is available (or the cell cannot use oxygen), the NAD is restored by converting the pyruvate to lactate (lactic acid) (e.g., in humans) or to ethanol plus carbon dioxide (e.g., in yeast). Other monosaccharides like galactose and fructose can be converted into intermediates of the glycolytic"
    ]
  ],
  [
    "The dependent variables in the experiment conducted by the psychologist are the changes of interest measured as a result of altering the independent variables.",
    [
      "random allocation of subjects to conditions allows researchers to make strong inferences about causal relationships. In an experiment, the researcher alters parameters of influence, called independent variables, and measures resulting changes of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment. Repeated-measures experiments are those which take place through intervention on multiple occasions. In research on the effectiveness of psychotherapy, experimenters often compare a given treatment with placebo treatments, or compare different treatments against each other. Treatment type is the independent variable. The dependent variables are outcomes, ideally assessed in several ways",
      "account when conducting an experiment. Because they are not controlled for, they can skew experiments results and provide a false or unreliable conclusion. For example, the psychologist Seymour Feshbach conducted an experiment to see how violence on television (the independent variable), affected aggression in adolescent boys (the dependent variable). He published his results in a paper called \"Television and Aggression\" in 1971. The paper showed that, in some cases, the lack of violence on television made the boys \"more\" aggressive. This was due to a confounding variable, which in this case was frustration. This means that extraneous variables are important",
      "Dependent and independent variables In mathematical modeling, statistical modeling and experimental sciences, the values of dependent variables depend on the values of independent variables. The dependent variables represent the output or outcome whose variation is being studied. The independent variables, also known in a statistical context as regressors, represent inputs or causes, that is, potential reasons for variation. In an experiment, any variable that the experimenter manipulates can be called an independent variable. Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables"
    ]
  ],
  [
    "Peripheral muscle fatigue during multiple sprint activities is thought to be implicated by mechanical failure of the exercising muscles, inadequate oxygen supply, lactic acid buildup, or total energy depletion in the exhausted muscles.",
    [
      "upon exercise fatigue was modeled in terms of it being due to a mechanical failure of the exercising muscles (\"peripheral muscle fatigue\"). This failure was caused either by an inadequate oxygen supply to the exercising muscles, lactic acid buildup, or total energy depletion in the exhausted muscles. Tim Noakes, a professor of exercise and sports science at the University of Cape Town, in 1997 has renewed Hill\u2019s argument on the basis of modern research. In his approach, the power output by muscles during exercise is continuously adjusted in regard to calculations made by the brain in regard to a safe",
      "considering the cost of running against wind resistance (formula_13), which is known to be: We combine the two equations to arrive at: Where formula_7 is the acceleration of the runner's body, formula_17 the forward acceleration, formula_18 the acceleration of gravity, formula_19 a proportionality constant and formula_20 the velocity. Fatigue is a prominent factor in sprinting, and it is already widely known that it hinders maximal power output in muscles, but it also affects the acceleration of runners in the ways listed below. A study on muscle coordination in which subjects performed repeated 6-second cycling sprints, or intermittent sprints of short",
      "fatigue is only caused by mechanical failure of the exercising muscles (\"peripheral fatigue\"). Instead, the brain models the metabolic limits of the body to ensure that whole body homeostasis is protected, in particular that the heart is guarded from hypoxia, and an emergency reserve is always maintained. The idea of the central governor has been questioned since \u2018physiological catastrophes\u2019 can and do occur suggesting athletes (such as Dorando Pietri, Jim Peters and Gabriela Andersen-Schiess) can over-ride the \u2018\u2018central governor\u2019. The exercise fatigue has also been suggested to be effected by: Prolonged exercise such as marathons can increase cardiac biomarkers such"
    ]
  ],
  [
    "Muscle fibers relax before tendon tension becomes high enough to cause damage through a protective mechanism that prevents avulsion of the tendon, as well as the need for ATP to bind to myosin heads for them to detach from actin filaments and allow for muscle relaxation.",
    [
      "at once, though this ratio can be affected by various physiological and psychological factors (including Golgi tendon organs and Renshaw cells). This 'low' level of contraction is a protective mechanism to prevent avulsion of the tendon\u2014the force generated by a 95% contraction of all fibers is sufficient to damage the body. In multiple fiber summation, if the central nervous system sends a weak signal to contract a muscle, the smaller motor units, being more excitable than the larger ones, are stimulated first. As the strength of the signal increases, more motor units are excited in addition to larger ones, with",
      "muscles are unable to relax properly due to myosin fibers not fully detaching from actin filaments. In skeletal muscle, ATP levels must be large enough to bind to the myosin heads for them to attach or detach from the actin and allow contraction or relaxation; the absence of enough levels of ATP means that the myosin heads remains attached to actin. The muscle must be allowed to recover (resynthesize ATP), before the myosin fibres can detach and allow the muscle to relax. Skeletal muscles work as antagonistic pairs. Contracting one skeletal muscle requires the relaxation of the opposing muscle in",
      "ability to operate as power attenuation. Tendons exhibit power attenuation that allows the muscle-tendon systems to absorb energy. This rate exceeds the muscle's maximum capacity for energy. In comparison, power amplification of tendons allow for greater output of power that can exceed the capacity of their respective muscle. This elastic mechanism can lead to the following reductions by lengthening muscles: peak power input, lengthening velocity, and force. Muscle damage has been correlated with these factors. However, the shuttling of energy through tendons before it is absorbed by muscles has been shown to provide a protective mechanism against that damage. However,"
    ]
  ],
  [
    "The function of the pyruvate dehydrogenase complex is to convert pyruvate into acetyl-CoA, which can then enter the citric acid cycle to produce energy, thereby linking glycolysis to the citric acid cycle.",
    [
      "Pyruvate dehydrogenase Pyruvate dehydrogenase is the first component enzyme of pyruvate dehydrogenase complex (PDC). The pyruvate dehydrogenase complex contributes to transforming pyruvate into acetyl-CoA by a process called pyruvate decarboxylation. Acetyl-CoA may then be used in the citric acid cycle to carry out cellular respiration, so pyruvate dehydrogenase contributes to linking the glycolysis metabolic pathway to the citric acid cycle and releasing energy via NADH. Pyruvate dehydrogenase (E1) performs the first two reactions within the pyruvate dehydrogenase complex (PDC): a decarboxylation of substrate 1 (pyruvate) and a reductive acetylation of substrate 2 (lipoic acid). Lipoic acid is covalently bound to",
      "Pyruvate dehydrogenase complex Pyruvate dehydrogenase complex (PDC) is a complex of three enzymes that converts pyruvate into acetyl-CoA by a process called pyruvate decarboxylation. Acetyl-CoA may then be used in the citric acid cycle to carry out cellular respiration, and this complex links the glycolysis metabolic pathway to the citric acid cycle. Pyruvate decarboxylation is also known as the \"pyruvate dehydrogenase reaction\" because it also involves the oxidation of pyruvate. This multi-enzyme complex is related structurally and functionally to the oxoglutarate dehydrogenase and branched-chain oxo-acid dehydrogenase multi-enzyme complexes. The reaction catalysed by pyruvate dehydrogenase complex is: Initially, pyruvate and thiamine",
      "interface. The pyruvate dehydrogenase (PDH) complex is located in the mitochondrial matrix and catalyzes the conversion of pyruvate to acetyl coenzyme A. The PDH complex thereby links glycolysis to the citric acid cycle. The PDH complex contains three catalytic subunits, E1, E2, and E3, two regulatory subunits, E1 kinase and E1 phosphatase, and a non-catalytic subunit, E3 binding protein (E3BP). This gene encodes the E3 binding protein subunit; also known as component X of the pyruvate dehydrogenase complex. This protein tethers E3 dimers to the E2 core of the PDH complex. Mutations in the PDHX gene have been known to"
    ]
  ],
  [
    "When ammonia is dissolved in water, hydrogen ions are formed through the self-ionization of water, where water molecules split to form H+ ions.",
    [
      "gives ammonia. When ammonia is dissolved in water, a tiny amount of it converts to ammonium ions: The degree to which ammonia forms the ammonium ion depends on the pH of the solution. If the pH is low, the equilibrium shifts to the right: more ammonia molecules are converted into ammonium ions. If the pH is high (the concentration of hydrogen ions is low), the equilibrium shifts to the left: the hydroxide ion abstracts a proton from the ammonium ion, generating ammonia. Formation of ammonium compounds can also occur in the vapor phase; for example, when ammonia vapor comes in",
      "NH-N removal is also commonly used in scientific publications as a short way to depict Ammonia in water, and not the measure of its quantity. Ammonium is an ionized form of ammonia. The chemical structure for ammonium is NH. The chemical structure for ammonia is NH. Ammonia is highly soluble in water. Ammonia reacts with water (HO) and forms the ionized form: The reaction is reversible. The hydroxide ion (OH) plus NH forms NH + HO. The percentage of ammonia increases with increasing alkalinity of dissolved ammonium in water. Ammonium ions are formed with increasing acidity of dissolved ammonia in",
      "concentration, measured as pH, is also responsible for the acidic or basic nature of a compound. Water molecules split to form H and hydroxide anions. This process is referred to as the self-ionization of water. Hydrogen ion A hydrogen ion is created when a hydrogen atom loses or gains an electron. A positively charged hydrogen ion (or proton) can readily combine with other particles and therefore is only seen isolated when it is in a gaseous state or a nearly particle-free space. Due to its extremely high charge density of approximately 2\u00d710 times that of a sodium ion, the bare"
    ]
  ],
  [
    "The genetic material in living organisms is encoded in DNA molecules, which are passed on through generations via cell division and replication processes.",
    [
      "Heritable traits are known to be passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long polymer that incorporates four types of bases, which are interchangeable. The sequence of bases along a particular DNA molecule specifies the genetic information: this is comparable to a sequence of letters spelling out a passage of text. Before a cell divides through mitosis, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. A portion of a DNA molecule that specifies a single functional unit is called a",
      "living organism, it utilizes polymerases, ribosomes, and other biomolecules to replicate its own genetic material and to produce more virus genetic material similar to the original virus. Thus, gene transfer may occur through many varying means. Thus, the study of this gene transfer throughout each ecosystem, whether it be through a bacterial ecosystem or through the ecosystem of an organism, genetic ecology is the study of this gene transfer and its causes. Genetic ecology Genetic ecology is the study of the stability and expression of varying genetic material within abiotic mediums. Typically, genetic data is not thought of outside of",
      "Genetic code The genetic code is the set of rules used by living cells to translate information encoded within genetic material (DNA or mRNA sequences) into proteins. Translation is accomplished by the ribosome, which links amino acids in an order specified by messenger RNA (mRNA), using transfer RNA (tRNA) molecules to carry amino acids and to read the mRNA three nucleotides at a time. The genetic code is highly similar among all organisms and can be expressed in a simple table with 64 entries. The code defines how sequences of nucleotide triplets, called \"codons\", specify which amino acid will be"
    ]
  ],
  [
    "Narcissistic personality disorder.",
    [
      "said to provide a rationale for private therapists to diagnose some personality disorders more broadly and provide ongoing treatment for them. Personality disorder Personality disorders (PD) are a class of mental disorders characterized by enduring maladaptive patterns of behavior, cognition, and inner experience, exhibited across many contexts and deviating from those accepted by the individual's culture. These patterns develop early, are inflexible, and are associated with significant distress or disability. The definitions may vary somewhat, according to source. Official criteria for diagnosing personality disorders are listed in the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM) and the of the",
      "Diagnosis is made by a healthcare professional interviewing the person in question. The condition needs to be differentiated from mania and substance use disorder. Treatments have not been well-studied. Therapy is often difficult as people with the disorder frequently do not consider themselves to have a problem. About one percent of people are believed to be affected at some point in their life. It appears to occur more often in males than females and affects young people more than older people. The personality was first described in 1925 by Robert Waelder, and the term NPD came into use in 1968.",
      "cooperate or the active subversion of demands made of the patient), waxy flexibility, impulsive actions. Sometimes the patient's speech is completely incoherent, but sometimes they are able answer questions, offering an opportunity for a physician to identify the nature of the patient's disorientation. Patients can be disoriented not only with respect to place and time, but also can be disoriented with respect to themselves and their own personality. Oneiroid syndrome most often occurs as the manifestation of an acute episode of schizophrenia. The duration of the oneiroid period is limited to a few weeks or days. The first signs of"
    ]
  ],
  [
    "An individual in the zone of proximal development is defined by Vygotsky as the range of tasks that the individual is in the process of learning to complete.",
    [
      "the 1930s. During this period Vygotsky was under particularly strong influence of holistic theories of German-American group of proponents of Gestalt psychology, most notably, the peripheral participants of the Gestalt movement Kurt Goldstein and Kurt Lewin. However, Vygotsky's work of this period remained largely fragmentary and unfinished and, therefore, unpublished. \"Zone of proximal development\" (ZPD) is Vygotsky's term for the range of tasks that a child is in the process of learning to complete. In the original Vygotsky's writings this phrase is used in three different meanings. Vygotsky viewed the ZPD as a better way to explain the relation between",
      "tests, and it originated in the writings of psychologist Lev Vygotsky (1896-1934) of his most mature and highly productive period of 1932-1934. The notion of the zone of proximal development that he introduced in 1933, roughly a year before his death, served as the banner for his proposal to diagnose development as the level of actual development that can be measured by the child's independent problem solving and, at the same time, the level of proximal, or potential development that is measured in the situation of moderately assisted problem solving by the child. The maximum level of complexity and difficulty",
      "such as internalization of knowledge. Vygotsky introduced the notion of zone of proximal development, a metaphor capable of describing the potential of human cognitive development. His work covered topics such as the origin and the psychology of art, development of higher mental functions, philosophy of science and the methodology of psychological research, the relation between learning and human development, concept formation, interrelation between language and thought development, play as a psychological phenomenon, learning disabilities, and abnormal human development (aka \"defectology\"). His scientific thinking underwent several major transformations throughout his career, but generally Vygotsky's legacy may be divided into two fairly"
    ]
  ],
  [
    "A young man would be fixated in the anal stage of Freud's psychosexual development theory if he is unable to keep his house clean according to his therapist's discernment. (Anal stage; Psychosexual development)",
    [
      "he referred to as psychosexual energy. To Freud, the libido was the driving force behind all of human behavior. In order to obtain a healthy personality later on in adulthood all of these stages need to be completed successfully. If issues are not resolved in a stage then fixation will occur resulting in an unhealthy personality. The anal stage, in Freudian psychology, is the period of human development occurring at about one to three years of age. Around this age, the child begins to toilet train, which brings about the child's fascination in the erogenous zone of the anus. The",
      "Psychosexual development In Freudian psychology, psychosexual development is a central element of the psychoanalytic sexual drive theory, that human beings, from birth, possess an instinctual libido (sexual energy) that develops in five stages. Each stagethe oral, the anal, the phallic, the latent, and the genitalis characterized by the erogenous zone that is the source of the libidinal drive. Sigmund Freud proposed that if the child experienced sexual frustration in relation to any psychosexual developmental stage, he or she would experience anxiety that would persist into adulthood as a neurosis, a functional mental disorder. Sigmund Freud (1856\u20131939) observed that during the",
      "of development that are earlier in time, and a \"formal\" one, in that the original and primitive methods of psychic expression are employed in manifesting those needs'. Behaviors associated with regression can vary greatly depending upon which stage the person is fixated at: An individual fixated at the oral stage might begin eating or smoking excessively, or might become very verbally aggressive. A fixation at the anal stage might result in excessive tidiness or messiness. Freud recognised that 'it is possible for several fixations to be left behind in the course of development, and each of these may allow an"
    ]
  ],
  [
    "A peptide bond is an amide type of covalent chemical bond linking two consecutive alpha-amino acids along a peptide or protein chain, while an isopeptide bond is a different type of amide bond between two amino acids.",
    [
      "Peptide Peptides (from Gr.: \u03c0\u03b5\u03c0\u03c4\u03cc\u03c2, \"pept\u00f3s\" \"digested\"; derived from \u03c0\u03ad\u03c3\u03c3\u03b5\u03b9\u03bd, \"p\u00e9ssein\" \"to digest\") are short chains of amino acid monomers linked by peptide (amide) bonds. The covalent chemical bonds are formed when the carboxyl group of one amino acid reacts with the amino group of another. The shortest peptides are dipeptides, consisting of 2 amino acids joined by a single peptide bond, followed by tripeptides, tetrapeptides, etc. A polypeptide is a long, continuous, and unbranched peptide chain. Hence, peptides fall under the broad chemical classes of biological oligomers and polymers, alongside nucleic acids, oligosaccharides and polysaccharides, etc. Peptides are distinguished",
      "Peptide bond A peptide bond is an amide type of covalent chemical bond linking two consecutive alpha-amino acids from C1 (carbon number one) of one alpha-amino acid and N2 (nitrogen number two) of another along a peptide or protein chain. It can also be called an eupeptide bond to separate it from an isopeptide bond, a different type of amide bond between two amino acids. When two amino acids form a \"dipeptide\" through a \"peptide bond\" it is type of condensation reaction. In this kind of condensation, two amino acids approach each other, with the non-side chain (C1) carboxylic acid",
      "less than similar compounds such as esters. Nevertheless, peptide bonds can undergo chemical reactions, usually through an attack of an electronegative atom on the carbonyl carbon, breaking the carbonyl double bond and forming a tetrahedral intermediate. This is the pathway followed in proteolysis and, more generally, in N-O acyl exchange reactions such as those of inteins. When the functional group attacking the peptide bond is a thiol, hydroxyl or amine, the resulting molecule may be called a cyclol or, more specifically, a thiacyclol, an oxacyclol or an azacyclol, respectively. Peptide bond A peptide bond is an amide type of covalent"
    ]
  ],
  [
    "The bystander effect influences how people react in emergency situations by causing individuals to look to others for cues on how to react, leading to a diffusion of responsibility and a decreased likelihood of taking action.",
    [
      "were: the fear of loss of important relationships in and out of the workplace, and a fear of \"bad consequences.\" There also were many reasons given by people who did act on the spot or come forward to authorities. This practitioners' study suggests that the \"bystander effect\" can be studied and analyzed in a much broader fashion. The broader view includes not just a) what bystanders do in singular emergencies, b) helping strangers in need, when c) there are (or are not) other people around. The reactions of bystanders can also be analyzed a) when the bystanders perceive any of",
      "study that in an emergency situation, people assumed the emergency and concern would show in their expression and behavior, but it didn't, which the authors believe partially explains the bystander effect: \"When confronted with a potential emergency, people typically play it cool, adopt a look of nonchalance, and monitor the reactions of others to determine if a crisis is really at hand. No one wants to overreact, after all, if it might not be a true emergency. However, because each individual holds back, looks nonchalant, and monitors the reactions of others, sometimes everyone concludes (perhaps erroneously) that the situation is",
      "an explanation, such as saying that their wallet had been stolen, the percentage of people giving assistance was higher (72%) than when the student just asked for a dime (34%). Additional research by Faul, Mark, et al., using data collected by EMS officials when responding to an emergency, indicated that the response of bystanders was correlated with the health severity of the situation. According to Latan\u00e9 and Darley, there are five characteristics of emergencies that affect bystanders: Due to these five characteristics, bystanders go through cognitive and behavioural processes: \"Notice\": To test the concept of \"noticing,\" Latane and Darley (1968)"
    ]
  ],
  [
    "Sauna use may not be safe for individuals with unstable angina pectoris, a recent heart attack, severe aortic stenosis, heart disease, seizure disorders, or those who use alcohol or cocaine, and there is a risk of heat prostration or hyperthermia.",
    [
      "pool, major sport, and resort complexes also contain a sauna. Therapeutic sauna sessions are often carried out in conjunction with physiotherapy or hydrotherapy, gentle exercises that do not exacerbate symptoms. Sauna use may temporarily relieve symptoms of the common cold. Saunas may not be safe in cases of unstable angina pectoris, a recent heart attack, and severe aortic stenosis. Additionally, there is risk of heat prostration or the even more serious hyperthermia. Children and older persons who have heart disease or seizure disorders or those who use alcohol or cocaine are especially vulnerable. Sauna use has been associated with loss",
      "allowed in the sauna itself. A saunameester might perform an \"opgieting\" on various times during the day. Visitors usually won't perform an \"opgieting\" by themselves and just sit or lay in the sauna when a saunameester is not available. A sauna session takes about 15\u201320 minutes. The door is expected to be closed as soon as possible when entering or leaving the sauna. Once an \"opgieting\" has started it is frowned upon to enter the sauna, since it will cause a reduction of heat when opening the door. Leaving the sauna during an \"opgieting\" is acceptable if to avoid becoming",
      "can be confused with perspiration. Cooling down is a part of the sauna cycle and is as important as the heating. Among users it is considered good practice to take a few moments after exiting a sauna before entering a cold plunge, and to enter a plunge pool by stepping into it gradually, rather than immediately immersing fully. In summer, a session is often started with a cold shower. Therapeutic sauna has been shown to aid adaptation, reduce stress hormones, lower blood pressure and improve cardiovascular conditions. Today there are a wide variety of sauna options. Heat sources include wood,"
    ]
  ],
  [
    "Muscle lactate production increases during training due to upregulation of MCT protein levels in skeletal muscle mitochondria, leading to more efficient clearance rates of lactate from the body during exercise.",
    [
      "to lactate . As found by Brooks, et al., while lactate is disposed of mainly through oxidation and only a minor fraction supports gluconeogenesis, lactate is the main gluconeogenic precursor during sustained exercise. Brooks demonstrated in his earlier studies that little difference in lactate production rates were seen in trained and untrained subjects at equivalent power outputs. What was seen, however, was more efficient clearance rates of lactate in the trained subjects suggesting an upregulation of MCT protein. Local lactate use depends on exercise exertion. During rest, approximately 50% of lactate disposal take place through lactate oxidation whereas in time",
      "that training increases MCT1 protein levels in skeletal muscle mitochondria, and that corresponded with an increase in the ability of muscle to clear lactate from the body during exercise. The affinity of MCT for pyruvate is greater than lactate, however two reactions will ensure that lactate will be present in concentrations that are orders of magnitude greater than pyruvate: first, the equilibrium constant of LDH(3.6 x 104) greatly favors the formation of lactate. Secondly, the immediate removal of pyruvate from the mitochondria (either via the Krebs\u2019 cycle or gluconeogenesis) ensures that pyruvate is not present in great concentrations within the",
      "lactate, has traditionally been thought to be detrimental to muscle function. However, this appears likely only when lactate levels are very high. Elevated lactate levels are only one of many changes that occur within and around muscle cells during intense exercise that can lead to fatigue. Fatigue, that is muscle failure, is a complex subject. Elevated muscle and blood lactate concentrations are a natural consequence of any physical exertion. The effectiveness of anaerobic activity can be improved through training. Anaerobic exercise Anaerobic exercise is a physical exercise intense enough to cause lactate to form. It is used by athletes in"
    ]
  ],
  [
    "The components of triglycerides are glycerol and three fatty acids.",
    [
      "Triglyceride A triglyceride (TG, triacylglycerol, TAG, or triacylglyceride) is an ester derived from glycerol and three fatty acids (from \"tri-\" and \"glyceride\"). Triglycerides are the main constituents of body fat in humans and other animals, as well as vegetable fat. They are also present in the blood to enable the bidirectional transference of adipose fat and blood glucose from the liver, and are a major component of human skin oils. There are many different types of triglyceride, with the main division between saturated and unsaturated types. Saturated fats are \"saturated\" with hydrogen \u2014 all available places where hydrogen atoms could",
      "particles in question are composed chiefly of dietary (exogenous) lipids or whether they originated in the liver (endogenous), through de novo synthesis of triacylglycerols. The hepatocytes are the main platform for the handling of triacylglycerols and cholesterol; the liver can also store certain amounts of glycogen and triacylglycerols. While adipocytes are the main storage cells for triacylglycerols, they do not produce any lipoproteins. Bile emulsifies fats contained in the chyme, then pancreatic lipase cleaves triacylglycerol molecules into two fatty acids and one 2-monoacylglycerol. Enterocytes readily absorb these small molecules from the chymus. Inside of the enterocytes, fatty acids and monoacylglycerides",
      "acids join to form esters. The glycerol molecule has three hydroxyl (HO\u2013) groups. Each fatty acid has a carboxyl group (\u2013COOH). In triglycerides, the hydroxyl groups of the glycerol join the carboxyl groups of the fatty acid to form ester bonds: The three fatty acids (RCOH, R\u2032COH, R\u2033COH in the above equation) are usually different, but many kinds of triglycerides are known. The chain lengths of the fatty acids in naturally occurring triglycerides vary, but most contain 16, 18, or 20 carbon atoms. Natural fatty acids found in plants and animals are typically composed of only even numbers of carbon"
    ]
  ],
  [
    "Increasing the substrate concentration in competitive inhibition reduces the impact of the inhibitor by allowing more substrate molecules to outcompete the inhibitor for binding to the active site of the enzyme.",
    [
      "or substrate can bind to. The active site will only allow one of the two complexes to bind to the site therefore either allowing for a reaction to occur or yielding it. In competitive inhibition the inhibitor resembles the substrate therefore taking its place and binding to the active site of an enzyme. Increasing the substrate concentration would diminish the \"competition\" for the substrate to properly bind to the active site and allow a reaction to occur. When the substrate is of higher concentration than that of the competitive inhibitor, it is more likely that the substrate will come into",
      "K (V/2 or affinity of enzyme to substrate complex). Competitive inhibition increases the apparent value of the Michaelis-Menten constant, formula_6, such that initial rate of reaction, formula_7, is given by where formula_9, formula_10 is the inhibitor's dissociation constant and formula_11 is the inhibitor concentration. formula_1 remains the same because the presence of the inhibitor can be overcome by higher substrate concentrations. formula_6, the substrate concentration that is needed to reach formula_14, increases with the presence of a competitive inhibitor. This is because the concentration of substrate needed to reach formula_1 with an inhibitor is greater than the concentration of substrate",
      "in convulsions due to lessened inhibition by the glycine. In competitive inhibition, the maximum velocity (formula_1) of the reaction is unchanged, while the apparent affinity of the substrate to the binding site is decreased (the formula_2 dissociation constant is apparently increased). The change in formula_3 (Michaelis-Menten constant) is parallel to the alteration in formula_2, as one increases the other must decrease. When a competitive inhibitor is bound to an enzyme the formula_3 increases. This means the binding affinity for the enzyme is decreased, but it can be overcome by increasing the concentration of the substrate. Any given competitive inhibitor concentration"
    ]
  ],
  [
    "Tyler can increase his likelihood of improving in subsequent exams by seeking help from a tutor, attending extra study sessions, and practicing regularly to strengthen his understanding of the material.",
    [
      "that is the relevant variable for obtaining favourable course evaluations. Another paper by Carrell and West (2010) use a data set from the U.S. Air Force Academy where students are randomly assigned to course sections (reducing selection problems). It found that calculus students got higher marks on common course examinations when they had instructors with high SET scores but did worse when they took later courses requiring calculus. The authors discuss a number of possible explanations for this finding, including that instructors with higher SET scores may have concentrated their teaching on the common examinations in the course rather than",
      "chance. For the first test, some will be lucky, and score more than their ability, and some will be unlucky and score less than their ability. Some of the lucky students on the first test will be lucky again on the second test, but more of them will have (for them) average or below average scores. Therefore, a student who was lucky on the first test is more likely to have a worse score on the second test than a better score. Similarly, students who score less than the mean on the first test will tend to see their scores",
      "solution. A candidate reaching the correct answer will receive full marks, regardless of the method used to answer the question.<br> All the questions that are attempted by a student and not crossed out will be assessed. However, only the six best answers will be used in the calculation of the final grade for the paper. Candidates who have received offers for Mathematics courses at the University of Cambridge sit STEP as a post-interview test. STEP papers are normally sat at a candidate's school or college. Alternatively, the test can be taken at one of Cambridge Assessment Admissions Testing's authorised test"
    ]
  ],
  [
    "Cellular activities related to microtubule stability, such as axonal transport and cytoplasmic functions, would likely be most affected by the mutant with reduced function across all families of MAPs leading to increased microtubule degradation.",
    [
      "have many consequences such as changing the expression level of tau isoforms or lead to MTs dysfunction. Mutations that alter function and isoform expression of tau lead to hyperphosphorylation. The process of tau aggregation in the absence of mutations is not known but might result from increased phosphorylation, protease action or exposure to polyanions, such as glycosaminoglycans.[6] Hyperphosphorylated tau disassembles microtubules and sequesters normal tau, MAPT 1(microtubule associated protein tau1), MAPT 2, and ubiquitin into tangles of PHFs. This insoluble structure damages cytoplasmic functions and interferes with axonal transport, which can lead to cell death. Vaccines have been found that",
      "(the \"X\" stands for Xenopus). XMAP215 has generally been linked to microtubule stabilization. During mitosis the dynamic instability of microtubules has been observed to rise approximately tenfold. This is partly due to phosphorylation of XMAP215, which makes catastrophes (rapid depolymerization of microtubules) more likely. In this way the phosphorylation of MAPs plays a role in mitosis. There are many other proteins which affect microtubule behavior, such as catastrophin, which destabilizes microtubules, katanin, which severs them, and a number of motor proteins that transport vesicles along them. Certain motor proteins were originally designated as MAPs before it was found that they",
      "may also be blocked and/or the cell may die. Descriptions of reduced function, characteristic of aging and associated with accumulation of DNA damage, are given later in this article. In contrast to DNA damage, a mutation is a change in the base sequence of the DNA. A mutation cannot be recognized by enzymes once the base change is present in both DNA strands, and thus a mutation cannot be repaired. At the cellular level, mutations can cause alterations in protein function and regulation. Mutations are replicated when the cell replicates. In a population of cells, mutant cells will increase or"
    ]
  ],
  [
    "The trigger to initiate the contractile process in skeletal muscle is the motor nerve emitting action potentials at a very high rate, leading to a sustained muscle contraction known as a tetanic contraction.",
    [
      "(meaning that they are initiated by the smooth or heart muscle cells themselves instead of being stimulated by an outside event such as nerve stimulation), although they can be modulated by stimuli from the autonomic nervous system. The mechanisms of contraction in these muscle tissues are similar to those in skeletal muscle tissues. Muscle contractions can be described based on two variables: force and length. Force itself can be differentiated as either tension or load. Muscle tension is the force exerted by the muscle on an object whereas a load is the force exerted by an object on the muscle.",
      "a single, short muscle contraction called a muscle twitch. If there is a problem at the neuromuscular junction, a very prolonged contraction may occur, such as the muscle contractions that result from tetanus. Also, a loss of function at the junction can produce paralysis. Skeletal muscles are organized into hundreds of motor units, each of which involves a motor neuron, attached by a series of thin finger-like structures called axon terminals. These attach to and control discrete bundles of muscle fibers. A coordinated and fine tuned response to a specific circumstance will involve controlling the precise number of motor units",
      "Tetanic contraction A tetanic contraction (also called tetanized state, tetanus, or physiologic tetanus, the latter to differentiate from the disease called tetanus) is a sustained muscle contraction evoked when the motor nerve that innervates a skeletal muscle emits action potentials at a very high rate. During this state, a motor unit has been maximally stimulated by its motor neuron and remains that way for some time. This occurs when a muscle's motor unit is stimulated by multiple impulses at a sufficiently high frequency. Each stimulus causes a twitch. If stimuli are delivered slowly enough, the tension in the muscle will"
    ]
  ],
  [
    "The sarcoplasmic reticulum in muscle cells acts as a reservoir for calcium ions by storing and releasing calcium ions as needed for muscle contraction and relaxation.",
    [
      "Sarcoplasmic reticulum The sarcoplasmic reticulum (SR) is a membrane-bound structure found within muscle cells that is similar to the endoplasmic reticulum in other cells. The main function of the SR is to store calcium ions (Ca). Calcium ion levels are kept relatively constant, with the concentration of calcium ions within a cell being 100,000 times smaller than the concentration of calcium ions outside the cell. This means that small increases in calcium ions within the cell are easily detected and can bring about important cellular changes (the calcium is said to be a second messenger; see calcium in biology for",
      "more details). Calcium is used to make calcium carbonate (found in chalk) and calcium phosphate, two compounds that the body uses to make teeth and bones. This means that too much calcium within the cells can lead to hardening (calcification) of certain intracellular structures, including the mitochondria, leading to cell death. Therefore, it is vital that calcium ion levels are controlled tightly, and can be released into the cell when necessary and then removed from the cell. The sarcoplasmic reticulum is a network of tubules that extend throughout muscle cells, wrapping around (but not in direct contact with) the myofibrils",
      "action potentials, which are waves in the electrical charges that extend along neurons. The waves travel to a group of cells in a muscle, letting calcium ions out from the cells' sarcoplasmic reticula (SR), which are storage areas for calcium. The released calcium lets myofibrils contract under the power of energy-carrying adenosine triphosphate (ATP) molecules. Meanwhile, the calcium is quickly pumped back into the SR by fast calcium pumps. Each muscle cell contracts fully; stronger contraction of the whole muscle requires more action potentials on more groups of cells in the muscle. When the action potentials stop, the calcium stops"
    ]
  ],
  [
    "Variations in synaptic serotonin, noradrenaline, and dopamine in the central nervous system significantly influence fatigue development in endurance athletes.",
    [
      "in endurance athletes. Existing experimental methods have provided enough evidence to suggest that variations in synaptic serotonin, noradrenaline, and dopamine are significant drivers of central nervous system fatigue. An increased synaptic dopamine concentration in the CNS is strongly ergogenic (promotes exercise performance). An increased synaptic serotonin or noradrenaline concentration in the CNS impairs exercise performance. Manipulation of norepinephrine suggests it may actually play a role in creating a feeling of fatigue. Reboxetine, an NRI inhibitor, decreased time to fatigue and increased subjective feelings of fatigue. This may be explained by a paradoxical decrease in adrenergic activity lead by feedback mechanisms.",
      "Central nervous system fatigue Central nervous system fatigue, or central fatigue, is a form of fatigue that is associated with changes in the synaptic concentration of neurotransmitters within the central nervous system (CNS; including the brain and spinal cord) which affects exercise performance and muscle function and cannot be explained by peripheral factors that affect muscle function. In healthy individuals, central fatigue can occur from prolonged exercise and is associated with neurochemical changes in the brain, primarily involving serotonin (5-HT), noradrenaline, and dopamine. Central fatigue plays an important role in endurance sports and also highlights the importance of proper nutrition",
      "5-HT. Controlling central nervous system fatigue can help scientists develop a deeper understanding of fatigue as a whole. Numerous approaches have been taken to manipulate neurochemical levels and behavior. In sports, nutrition plays a large role in athletic performance. In addition to fuel, many athletes consume performance-enhancing drugs including stimulants in order to boost their abilities. Amphetamine is a stimulant that has been found to improve both physical and cognitive performance. Amphetamine blocks the reuptake of dopamine and norepinephrine, which delays the onset of fatigue by increasing the amount of dopamine, despite the concurrent increase in norepinephrine, in the central"
    ]
  ],
  [
    "Despite normal blood oxygen levels in some patients with advanced cancer or neurodegenerative disease, oxygen is widely used in emergency medicine and medical applications because it is essential for normal cell metabolism and can prevent organ dysfunction and respiratory or cardiac arrest in individuals with compromised oxygen supply to tissues.",
    [
      "advanced cancer or neurodegenerative disease, despite having relatively normal blood oxygen levels. A 2010 trial of 239 subjects found no significant difference in reducing breathlessness between oxygen and air delivered in the same way. Oxygen is widely used in emergency medicine, both in hospital and by emergency medical services or those giving advanced first aid. In the pre-hospital environment, high flow oxygen is indicated for use in resuscitation, major trauma, anaphylaxis, major bleeding, shock, active convulsions, and hypothermia. It may also be indicated for any other people where their injury or illness has caused low oxygen levels, although in this",
      "Oxygen saturation (medicine) Oxygen saturation is the fraction of oxygen-saturated hemoglobin relative to total hemoglobin (unsaturated + saturated) in the blood. The human body requires and regulates a very precise and specific balance of oxygen in the blood. Normal blood oxygen levels in humans are considered 95\u2013100 percent. If the level is below 90 percent, it is considered low resulting in hypoxemia. Blood oxygen levels below 80 percent may compromise organ function, such as the brain and heart, and should be promptly addressed. Continued low oxygen levels may lead to respiratory or cardiac arrest. Oxygen therapy may be used to",
      "orbit is very high. Medical use of breathing gases other than air include oxygen therapy and anesthesia applications. Oxygen is required by people for normal cell metabolism. Air is typically 21% oxygen by volume. This is normally sufficient, but in some circumstances the oxygen supply to tissues is compromised. Oxygen therapy, also known as supplemental oxygen, is the use of oxygen as a medical treatment. This can include for low blood oxygen, carbon monoxide toxicity, cluster headaches, and to maintain enough oxygen while inhaled anesthetics are given. Long term oxygen is often useful in people with chronically low oxygen such"
    ]
  ],
  [
    "Mutations can produce discernible changes in the observable characteristics (phenotype) of an organism, affecting traits such as color, size, or behavior.",
    [
      "elements. Mutations may or may not produce discernible changes in the observable characteristics (phenotype) of an organism. Mutations play a part in both normal and abnormal biological processes including: evolution, cancer, and the development of the immune system, including junctional diversity. The genomes of RNA viruses are based on RNA rather than DNA. The RNA viral genome can be double stranded (as in DNA) or single stranded. In some of these viruses (such as the single stranded human immunodeficiency virus) replication occurs quickly and there are no mechanisms to check the genome for accuracy. This error-prone process often results in",
      "Modifications (genetics) Modifications are changes or differences between organisms in the same species that are due to differences in their environment. This is in contrast to mutations, which are changes in the genomes of organisms. Environmental differences that can affect an organism's characteristics (phenotype) include substrate, light, warmth, stress, exercise, and so on. Modifications are typically not heritable, however in some cases epigenetic modifications can be inherited. In both cases, there is no change to the primary DNA sequence (genotype), rather an influence on gene expression which is the cause of the altered phenotype. In heredity the genes of the",
      "gene. These changes in DNA sequence are called mutations. Mutations produce new alleles of genes. Sometimes these changes stop the functioning of that gene or make it serve another advantageous function, such as the melanin genes discussed above. These mutations and their effects on the traits of organisms are one of the causes of evolution. A population of organisms evolves when an inherited trait becomes more common or less common over time. For instance, all the mice living on an island would be a single population of mice: some with white fur, some gray. If over generations, white mice became"
    ]
  ],
  [
    "The enzymes of glycolysis are located in the muscles, adipose tissue, kidney, liver, and certain tissues such as the cortex, testis, milk glands, and phagocyte cells.",
    [
      "cortex, testis, milk glands, phagocyte cells, and red blood cells. It produces products that are used in other cell processes, while reducing NADP to NADPH. This pathway is regulated through changes in the activity of glucose-6-phosphate dehydrogenase. Fructose must undergo certain extra steps in order to enter the glycolysis pathway. Enzymes located in certain tissues can add a phosphate group to fructose. This phosphorylation creates fructose-6-phosphate, an intermediate in the glycolysis pathway that can be broken down directly in those tissues. This pathway occurs in the muscles, adipose tissue, and kidney. In the liver, enzymes produce fructose-1-phosphate, which enters the",
      "glycolysis pathway and is later cleaved into glyceraldehyde and dihydroxyacetone phosphate. Lactose, or milk sugar, consists of one molecule of glucose and one molecule of galactose. After separation from glucose, galactose travels to the liver for conversion to glucose. Galactokinase uses one molecule of ATP to phosphorylate galactose. The phosphorylated galactose is then converted to glucose-1-phosphate, and then eventually glucose-6-phosphate, which can be broken down in glycolysis. Many steps of carbohydrate metabolism allow the cells to access energy, and store it more transiently in ATP. The cofactors NAD and FAD are sometimes reduced during this process to form NADH and",
      "enzyme-catalyzed reactions. The pathway will begin in either the liver or kidney, in the mitochondria or cytoplasm of those cells, this being dependent on the substrate being used. Many of the reactions are the reverse of steps found in glycolysis. While most steps in gluconeogenesis are the reverse of those found in glycolysis, three regulated and strongly endergonic reactions are replaced with more kinetically favorable reactions. Hexokinase/glucokinase, phosphofructokinase, and pyruvate kinase enzymes of glycolysis are replaced with glucose-6-phosphatase, fructose-1,6-bisphosphatase, and PEP carboxykinase/pyruvate carboxylase. These enzymes are typically regulated by similar molecules, but with opposite results. For example, acetyl CoA and"
    ]
  ],
  [
    " is often contrasted either with totalitarianism or with collectivism, but in fact, there is a spectrum of behaviors at the societal level ranging from highly individualistic societies through mixed societies to collectivist (Rational choice theory).",
    [
      "Rational choice theory Rational choice theory, also known as choice theory or rational action theory, is a framework for understanding and often formally modeling social and economic behavior. The basic premise of rational choice theory is that aggregate social behavior results from the behavior of individual actors, each of whom is making their individual decisions. The theory also focuses on the determinants of the individual choices (methodological individualism). Rational choice theory then assumes that an individual has preferences among the available choice alternatives that allow them to state which option they prefer. These preferences are assumed to be complete (the",
      "equally powerful alternatives) lead to them still being widely used. Rational choice theory Rational choice theory, also known as choice theory or rational action theory, is a framework for understanding and often formally modeling social and economic behavior. The basic premise of rational choice theory is that aggregate social behavior results from the behavior of individual actors, each of whom is making their individual decisions. The theory also focuses on the determinants of the individual choices (methodological individualism). Rational choice theory then assumes that an individual has preferences among the available choice alternatives that allow them to state which option",
      "not. Individualism is often contrasted either with totalitarianism or with collectivism, but in fact, there is a spectrum of behaviors at the societal level ranging from highly individualistic societies through mixed societies to collectivist. Methodological individualism is the view that phenomena can only be understood by examining how they result from the motivations and actions of individual agents. In economics, people's behavior is explained in terms of rational choices, as constrained by prices and incomes. The economist accepts individuals' preferences as givens. Becker and Stigler provide a forceful statement of this view:On the traditional view, an explanation of economic phenomena"
    ]
  ],
  [
    "During multiple sprint sports, products such as high-glycemic-index carbohydrates and protein/amino acids increase in concentration in the blood.",
    [
      "following their exercise. Typically, high-glycemic-index carbohydrates are preferred for their ability to rapidly raise blood glucose levels.For the purpose of protein synthesis, protein or individual amino acids are ingested as well. Branched-chain amino acids are important since they are most responsible for the synthesis of protein. According to Lemon et al. (1995) female endurance runners have the hardest time getting enough protein in their diet. Endurance athletes in general need more protein in their diet than the sedentary person.Research has shown that endurance athletes are recommended to have 1.2 to 1.4 g of protein per kg of body weight in",
      "Sprint (running) Sprinting is running over a short distance in a limited period of time. It is used in many sports that incorporate running, typically as a way of quickly reaching a target or goal, or avoiding or catching an opponent. Human physiology dictates that a runner's near-top speed cannot be maintained for more than 30\u201335 seconds due to the depletion of phosphocreatine stores in muscles, and perhaps secondarily to excessive metabolic acidosis as a result of anaerobic glycolysis. In athletics and track and field, sprints (or dashes) are races over short distances. They are among the oldest running competitions.",
      "a greater stroke volume. A concomitant decrease in stroke volume occurs with the initial increase in heart rate at the onset of exercise. Despite an increase in cardiac dimensions, a marathoner's aerobic capacity is confined to this capped and ever decreasing heart rate. The amount of oxygen that blood can carry depends on blood volume, which increases during a race, and the amount of hemoglobin in blood. Other physiological factors affecting a marathon runner's aerobic capacity include pulmonary diffusion, mitochondria enzyme activity, and capillary density. A long-distance runner's running economy is their steady state requirement for oxygen at specific speeds"
    ]
  ],
  [
    "The rate-limiting enzyme of glycolysis is glucokinase, which is a diffusion-limited enzyme with moderate cooperativity with glucose.",
    [
      "it to regulate a \"supply-driven\" metabolic pathway. That is, the rate of reaction is driven by the supply of glucose, not by the demand for end products. Another distinctive property of glucokinase is its moderate cooperativity with glucose, with a Hill coefficient (\"n\") of about 1.7. Glucokinase has only a single binding site for glucose and is the only monomeric regulatory enzyme known to display substrate cooperativity. The nature of the cooperativity has been postulated to involve a \"slow transition\" between two different enzyme states with different rates of activity. If the dominant state depends upon glucose concentration, it would",
      "Diffusion limited enzyme A Diffusion limited enzyme is an enzyme which catalyses a reaction so efficiently that the rate limiting step is that of substrate diffusion into the active site, or product diffusion out. This is also known as kinetic perfection or catalytic perfection. Since the rate of catalysis of such enzymes is set by the diffusion-controlled reaction, it therefore represents an intrinsic, physical constraint on evolution (a maximum peak height in the fitness landscape). Diffusion limited perfect enzymes are very rare. Most enzymes catalyse their reactions to a rate that is 1,000-10,000 times slower than this limit. This is",
      "reaction order depends on the relative size of the two terms in the denominator. At low substrate concentration formula_12 so that formula_13 Under these conditions the reaction rate varies linearly with substrate concentration <chem>[S]</chem> (first-order kinetics). However at higher <chem>[S]</chem> with formula_14, the reaction becomes independent of <chem>[S]</chem> (zero-order kinetics) and asymptotically approaches its maximum rate formula_15, where <chem>[E]_0</chem> is the initial enzyme concentration. This rate is attained when all enzyme is bound to substrate. formula_10, the turnover number, is the maximum number of substrate molecules converted to product per enzyme molecule per second. Further addition of substrate does not"
    ]
  ],
  [
    "A fundamental cause of fatigue in high intensity exercise is central nervous system fatigue, which is associated with changes in the synaptic concentration of neurotransmitters within the central nervous system.",
    [
      "difficulty in accomplishing consistent exercise. The main cause of fatigue in chronic fatigue syndrome most likely lies in the central nervous system. A defect in one of its components could cause a greater requirement of input to result in sustained force. It has been shown that with very high motivation, subjects with chronic fatigue can exert force effectively. Further investigation into central nervous system fatigue may result in medical applications. Central nervous system fatigue Central nervous system fatigue, or central fatigue, is a form of fatigue that is associated with changes in the synaptic concentration of neurotransmitters within the central",
      "Fatigue Fatigue is a subjective feeling of tiredness that has a gradual onset. Unlike weakness, fatigue can be alleviated by periods of rest. Fatigue can have physical or mental causes. Physical fatigue is the transient inability of a muscle to maintain optimal physical performance, and is made more severe by intense physical exercise. Mental fatigue is a transient decrease in maximal cognitive performance resulting from prolonged periods of cognitive activity. It can manifest as somnolence, lethargy, or directed attention fatigue. Medically, fatigue is a non-specific symptom, which means that it has many possible causes and accompanies many different conditions. Fatigue",
      "fatigue is only caused by mechanical failure of the exercising muscles (\"peripheral fatigue\"). Instead, the brain models the metabolic limits of the body to ensure that whole body homeostasis is protected, in particular that the heart is guarded from hypoxia, and an emergency reserve is always maintained. The idea of the central governor has been questioned since \u2018physiological catastrophes\u2019 can and do occur suggesting athletes (such as Dorando Pietri, Jim Peters and Gabriela Andersen-Schiess) can over-ride the \u2018\u2018central governor\u2019. The exercise fatigue has also been suggested to be effected by: Prolonged exercise such as marathons can increase cardiac biomarkers such"
    ]
  ],
  [
    "The rate of blood lactate accumulation during metabolism and exercise is determined by the balance between lactate production and removal, which is influenced by factors such as monocarboxylate transporters, LDH concentration and isoform, and tissue oxidative capacity.",
    [
      "acid have a higher melting point. In animals, -lactate is constantly produced from pyruvate via the enzyme lactate dehydrogenase (LDH) in a process of fermentation during normal metabolism and exercise. It does not increase in concentration until the rate of lactate production exceeds the rate of lactate removal, which is governed by a number of factors, including monocarboxylate transporters, concentration and isoform of LDH, and oxidative capacity of tissues. The concentration of blood lactate is usually at rest, but can rise to over 20 mM during intense exertion and as high as 25 mM afterward. In addition to other biological",
      "blood lactate reaches a concentration of 2 mmol/litre (at rest it is around 1). The anaerobic energy system increases the ability to produce blood lactate during maximal exercise, resulting from an increased amount of glycogen stores and glycolytic enzymes. Lactate threshold Lactate inflection point (LIP), is the exercise intensity at which the blood concentration of lactate and/or lactic acid begins to exponentially increase. It is often expressed as 85% of maximum heart rate or 75% of maximum oxygen intake. When exercising at or below the LT, any lactate produced by the muscles is removed by the body without it building",
      "to lactate . As found by Brooks, et al., while lactate is disposed of mainly through oxidation and only a minor fraction supports gluconeogenesis, lactate is the main gluconeogenic precursor during sustained exercise. Brooks demonstrated in his earlier studies that little difference in lactate production rates were seen in trained and untrained subjects at equivalent power outputs. What was seen, however, was more efficient clearance rates of lactate in the trained subjects suggesting an upregulation of MCT protein. Local lactate use depends on exercise exertion. During rest, approximately 50% of lactate disposal take place through lactate oxidation whereas in time"
    ]
  ],
  [
    "Type I muscle fibers are high endurance but low force/power/speed production, while Type II fibers (Type IIa and Type IIb) are fast-twitch fibers with varying levels of endurance and force/power/speed production.",
    [
      "primary types of muscle fibers: Type I, Type IIa and Type IIb. As described above, Type I muscle fibers are known as slow twitch oxidative, Type IIa are fast twitch oxidative and Type IIb are fast twitch glycolytic. These three different types of fibers are specialized to have unique functionalities. Type I fibers are described as high endurance but low Force/Power/Speed production, Type IIb as low endurance but high Force/Power/Speed production and Type IIa fibers are characterized in between the two. Motor units are multiple muscle fibers bundled together. When a person wants to move their body to achieve a",
      "of properties. These types of properties\u2014while they are partly dependent on the properties of individual fibers\u2014tend to be relevant and measured at the level of the motor unit, rather than individual fiber. Traditionally, fibers were categorized depending on their varying color, which is a reflection of myoglobin content. Type I fibers appear red due to the high levels of myoglobin. Red muscle fibers tend to have more mitochondria and greater local capillary density. These fibers are more suited for endurance and are slow to fatigue because they use oxidative metabolism to generate ATP (adenosine triphosphate). Less oxidative type II fibers",
      "are bundles of contractile fibers that are attached to bones by tendons. These bundles have different types of fibers within them, and horses have adapted over the years to produce different amounts of these fibers. Type II-b fibers are fast twitch fibers. These fibers allow muscles to contract quickly, resulting in a great deal of power and speed. Type I fibers are slow-twitch fibers. They allow muscles to work for longer periods of time resulting in greater endurance. Type II-a fibers are intermediate, representing a balance between the fast-twitch fibers and the slow-twitch fibers. They allow the muscles to generate"
    ]
  ],
  [
    "The volume of the gas at atmospheric pressure after heat is added slowly until the gas temperature is uniformly 600 K can be calculated using the ideal gas law and the given thermal expansivity.",
    [
      "= 0.1 MPa) and a typical thermal expansivity is 10 K. This roughly translates into needing around ten thousand times atmospheric pressure to reduce the volume of a substance by one percent. (Although the pressures needed may be around a thousand times smaller for sandy soil and some clays.) A one percent expansion of volume typically requires a temperature increase on the order of thousands of degrees Celsius. In contrast, the density of gases is strongly affected by pressure. The density of an ideal gas is where is the molar mass, is the pressure, is the universal gas constant, and",
      "proportionally. Conversely, reducing the volume of the gas increases the pressure. Boyle's law is used to predict the result of introducing a change, in volume and pressure only, to the initial state of a fixed quantity of gas. The initial and final volumes and pressures of the fixed amount of gas, where the initial and final temperatures are the same (heating or cooling will be required to meet this condition), are related by the equation: Here \"P\" and \"V\" represent the original pressure and volume, respectively, and \"P\" and \"V\" represent the second pressure and volume. Boyle's law, Charles's law,",
      "as important to indicate the applicable reference conditions of temperature and pressure when stating the molar volume of a gas as it is when expressing a gas volume or volumetric flow rate. Stating the molar volume of a gas without indicating the reference conditions of temperature and pressure has very little meaning and can cause confusion. The molar volume of gases around STP and at atmospheric pressure can be calculated with an accuracy that is usually sufficient by using the ideal gas law. The molar volume of any ideal gas may be calculated at various standard reference conditions as shown"
    ]
  ],
  [
    "The blood pressure in a capillary of the neck is lower compared to a capillary with an equal cross-sectional area in the right knee due to the larger surface area and higher resistance to fluid flow in the capillary network.",
    [
      "about 30 \u00b5m. The smaller the radius of a tube, the larger the resistance to fluid flow. Immediately following the arterioles are the capillaries. Following the logic observed in the arterioles, we expect the blood pressure to be lower in the capillaries compared to the arterioles. Since pressure is a function of force per unit area, (\"P\" = \"F\"/\"A\"), the larger the surface area, the lesser the pressure when an external force acts on it. Though the radii of the capillaries are very small, the network of capillaries have the largest surface area in the vascular network. They are known",
      "confer the main blood pressure drop across major arteries to capillaries in the circulatory system. In the arterioles blood pressure is lower than in the major arteries. This is due to bifurcations, which cause a drop in pressure. The more bifurcations, the higher the total cross-sectional area, therefore the pressure across the surface drops. This is why the arterioles have the highest pressure-drop. The pressure drop of the arterioles is the product of flow rate and resistance: \u2206P=Q xresistance. The high resistance observed in the arterioles, which factor largely in the \u2206\"P\" is a result of a smaller radius of",
      "exits the blood. This all occurs in the lungs where blood is oxygenated. The blood pressure in blood vessels is traditionally expressed in millimetres of mercury (1 mmHg = 133 Pa). In the arterial system, this is usually around 120 mmHg systolic (high pressure wave due to contraction of the heart) and 80 mmHg diastolic (low pressure wave). In contrast, pressures in the venous system are constant and rarely exceed 10 mmHg. Vascular resistance occurs where the vessels away from the heart oppose the flow of blood. Resistance is an accumulation of three different factors: blood viscosity, blood vessel length,"
    ]
  ],
  [
    "improves middle distance running performance by acting as a buffering agent to help prevent the build-up of lactic acid in the muscles, delaying fatigue and improving overall performance.",
    [
      "with a cotton swab dipped in the solution. Sodium bicarbonate is used as a cattle feed supplement, in particular as a buffering agent for the rumen. Small amounts of sodium bicarbonate have been shown to be useful as a supplement for athletes in speed-based events, such as middle-distance running, lasting about 1\u20137 minutes. However, overdose is a serious risk because sodium bicarbonate is slightly toxic; and gastrointestinal irritation is of particular concern. Additionally, this practice causes a significant increase in dietary sodium. Sodium bicarbonate is used in a process for removing paint and corrosion called sodablasting; the process is particularly",
      "and carbon dioxide: Sodium bicarbonate reacts with bases such as sodium hydroxide to form carbonates: Sodium bicarbonate reacts with carboxyl groups in proteins to give a brisk effervescence from the formation of . This reaction is used to test for the presence of carboxylic groups in protein. Above , sodium bicarbonate gradually decomposes into sodium carbonate, water, and carbon dioxide. The conversion is fast at : Most bicarbonates undergo this dehydration reaction. Further heating converts the carbonate into the oxide (above ): These conversions are relevant to the use of NaHCO as a fire-suppression agent (\"BC powder\") in some dry-powder",
      "Once consumed, it causes internal organs of cockroaches to burst due to gas collection. Sodium bicarbonate can be an effective way of controlling fungal growth, and in the United States is registered by the Environmental Protection Agency as a biopesticide. Sodium bicarbonate can be administered to pools, spas, and garden ponds to raise the total alkalinity. This will also raise the pH level and make maintaining proper pH easier. In the event that the pH is high, sodium bicarbonate should not be used to adjust the pH. Sodium bicarbonate is one of the main components of the common \"black snake\""
    ]
  ],
  [
    "Transsexual man (Assigned female at birth (AFAB))",
    [
      "an intersex individual may also contradict their future gender identity. \"Sex assignment\" is the determination of an infant's sex at birth. Terms that may be related to sex assignment are: Assigned male at birth (AMAB): a person of any age and irrespective of current gender whose sex assignment at birth resulted in a declaration of \"male\". For example, when an attending midwife or physician announces, \"It's a boy!\" Synonyms: male assigned at birth (MAAB) and designated male at birth (DMAB). Assigned female at birth (AFAB): a person of any age and irrespective of current gender whose sex assignment at birth",
      "person who was assigned the female sex at birth on the basis of his genitals, but despite that assignment, identifies as a man and is transitioning or has transitioned to a male gender role; in the case of a \"transsexual\" man, he furthermore has or will have a masculine body. Transsexual people are sometimes referred to with directional terms, such as \"female-to-male\" for a transsexual man, abbreviated to \"F2M\", \"FTM\", and \"F to M\", or \"male-to-female\" for a transsexual woman, abbreviated \"M2F\", \"MTF\" and \"M to F\". Individuals who have undergone and completed sex reassignment surgery are sometimes referred to",
      "identity can be influenced by the need to fit into gender binaries or adhere to social ideals constructed by mainstream society. The assigned sex of a person at birth, otherwise known as natal sex, is not always interchangeable with the terms \"gender identity\" and \"gender role\". According to the Family Process Institute, Natal sex and gender identity are, however, different components of identity, and gender identity does not necessarily unfold in the direction of one's natal sex. Gender identity is not the same as gender role; gender identity is a core sense of self, whereas gender role involves the adaptation"
    ]
  ],
  [
    "Phosphate can act as an intracellular buffer to limit pH changes when the rate of glycolysis is high.",
    [
      "pH. Extracellular buffers include bicarbonate and ammonia, while proteins and phosphate act as intracellular buffers. The bicarbonate buffering system is especially key, as carbon dioxide (CO) can be shifted through carbonic acid (HCO) to hydrogen ions and bicarbonate (HCO) as shown below. Acid\u2013base imbalances that overcome the buffer system can be compensated in the short term by changing the rate of ventilation. This alters the concentration of carbon dioxide in the blood, shifting the above reaction according to Le Chatelier's principle, which in turn alters the pH. For instance, if the blood pH drops too low (\"acidemia\"), the body will",
      "the extracellular fluid, including the blood plasma, is normally tightly regulated between 7.32 and 7.42, by the chemical buffers, the respiratory system, and the renal system. Aqueous buffer solutions will react with strong acids or strong bases by absorbing excess hydrogen ions, or hydroxide ions, replacing the strong acids and bases with weak acids and weak bases. This has the effect of damping the effect of pH changes, or reducing the pH change that would otherwise have occurred. But buffers cannot correct abnormal pH levels in a solution, be that solution in a test tube or in the extracellular fluid.",
      "pathway is adjusted in response to conditions both inside and outside the cell. The internal factors that regulate glycolysis do so primarily to provide ATP in adequate quantities for the cell\u2019s needs. The external factors act primarily on the liver, fat tissue, and muscles, which can remove large quantities of glucose from the blood after meals (thus preventing hyperglycemia by storing the excess glucose as fat or glycogen, depending on the tissue type). The liver is also capable of releasing glucose into the blood between meals, during fasting, and exercise thus preventing hypoglycemia by means of glycogenolysis and gluconeogenesis. These"
    ]
  ],
  [
    "In terms of Signal Detection Theory, when a pilot cannot see the tower light at 1 mile away, it would be referred to as a lack of prevailing visibility, impacting aircraft operations.",
    [
      "this was unlikely, but added the following comment: \"Attention is being turned to the still difficult, but less unpromising, problem of radio detection and numerical considerations on the method of detection by reflected radio waves will be submitted when required\". Over the following several weeks, Wilkins considered the radio detection problem. He outlined an approach and backed it with detailed calculations of necessary transmitter power, reflection characteristics of an aircraft, and needed receiver sensitivity. He proposed using a directional receiver based on Watt's lightning detection concept, listening for powerful signals from a separate transmitter. Timing, and thus distance measurements, would",
      "4 miles,or goes from less than 4 miles to four miles or greater, tower personnel are required to take a prevailing visibility reading. If this reading differs from the ground reading (i.e., different readings for different heights), the lower value of the two is used for aircraft operations. Also visibility is defined as greatest distance at which an object of specified characteristics can be seen and identified by an observer with normal flight under normal condition in daylight. Prevailing visibility Prevailing visibility in aviation is a measurement of the greatest distance visible throughout at least half of the horizon, not",
      "runway visual range). The runway lighting is controlled by the air traffic control tower. At non-towered airports, pilot-controlled lighting may be installed that can be switched on by the pilot via radio. In both cases, the brightness of the lights can be adjusted for day and night operations. Depth perception is inoperative at the distances usually involved in flying aircraft, and so the position and distance of a runway with respect to an aircraft must be judged by a pilot using only two-dimensional cues such as perspective, as well as angular size and movement within the visual field. Approach lighting"
    ]
  ],
  [
    "in New York City, such as the one located in Manhattan, has experienced ethnic succession as different immigrant groups have replaced the original Italian population, reflecting the ongoing evolution of urban neighborhoods.",
    [
      "city) over time. Ethnic succession has taken place in most major United States cities, but is most well known in New York City, where this process has been observed since the 19th century. Because of the United States' continued attraction for immigrants, its cities have been sources of study for scholars of urban development and ethnic succession. Ethnic groups often settle together in urban neighborhoods as part of a \"chain of immigration\" to a new country, or migration to a new region, to keep personal networks, languages, foods, religions and cultures alive. They may be viewed by the dominant racial",
      "predominated among immigrants entering inner-city neighborhoods of New York City, succeeding whites of European ancestries. Puerto Ricans and Dominicans tended to settle in their own ethnic neighborhoods, perhaps because of a concentration of numbers. Immigrants from a variety of South American nations have integrated more in multi-ethnic neighborhoods. In the 21st century, numerous national groups of South American Hispanic ethnicity have begun to succeed the Puerto Ricans and Dominicans in some areas, creating the first Hispanic succession in the city. Ethnic succession theory Ethnic succession theory is a theory in sociology stating that ethnic and racial groups entering a new",
      "in certain areas. As cities modernized and grew, these areas became known for their ethnic associations, and towns like \"Little Italy\" blossomed, becoming the icons they are today. Some Italian neighborhoods may have other names, but are colloquially referred to as \"Little Italy,\" including: Little Italy Little Italy is a general name for an ethnic enclave populated primarily by Italians or people of Italian ancestry, usually in an urban neighborhood. The concept of \"Little Italy\" holds many different aspects of the Italian culture. There are shops selling Italian goods as well as Italian restaurants lining the streets. A \"Little Italy\""
    ]
  ],
  [
    "Coenzymes are organic molecules needed for enzyme activity, while cofactors can be either organic or inorganic substances required for enzyme function, and prosthetic groups are tightly bound coenzymes or cofactors that remain bound to the enzyme.",
    [
      "slightly different definitions of coenzymes, cofactors, and prosthetic groups. Some consider tightly bound organic molecules as prosthetic groups and not as coenzymes, while others define all non-protein organic molecules needed for enzyme activity as coenzymes, and classify those that are tightly bound as coenzyme prosthetic groups. These terms are often used loosely. A 1980 letter in \"Trends in Biochemistry Sciences\" noted the confusion in the literature and the essentially arbitrary distinction made between prosthetic groups and coenzymes group and proposed the following scheme. Here, cofactors were defined as an additional substance apart from protein and substrate that is required for",
      "enzyme can \"grasp\" the coenzyme to switch it between different catalytic centers. Cofactors can be divided into two major groups: organic Cofactors, such as flavin or heme, and inorganic cofactors, such as the metal ions Mg, Cu, Mn, or iron-sulfur clusters. Organic cofactors are sometimes further divided into \"coenzymes\" and \"prosthetic groups\". The term coenzyme refers specifically to enzymes and, as such, to the functional properties of a protein. On the other hand, \"prosthetic group\" emphasizes the nature of the binding of a cofactor to a protein (tight or covalent) and, thus, refers to a structural property. Different sources give",
      "Cofactor (biochemistry) A cofactor is a non-protein chemical compound or metallic ion that is required for an enzyme's activity. Cofactors can be considered \"helper molecules\" that assist in biochemical transformations. The rates at which these happen are characterized by enzyme kinetics. Cofactors can be subclassified as either inorganic ions or complex organic molecules called coenzymes, the latter of which is mostly derived from vitamins and other organic essential nutrients in small amounts. A coenzyme that is tightly or even covalently bound is termed a prosthetic group. Cosubstrates are transiently bound to the protein and will be released at some point,"
    ]
  ],
  [
    "Codons are composed of three-nucleotide sequences called \"words\" in the genetic \"language\", which correspond to specific amino acids during protein translation.",
    [
      "a \"start codon\", and three \"stop codons\" indicate the beginning and end of the protein coding region. There are 64 possible codons (four possible nucleotides at each of three positions, hence 4 possible codons) and only 20 standard amino acids; hence the code is redundant and multiple codons can specify the same amino acid. The correspondence between codons and amino acids is nearly universal among all known living organisms. Transcription produces a single-stranded RNA molecule known as messenger RNA, whose nucleotide sequence is complementary to the DNA from which it was transcribed. The mRNA acts as an intermediate between the",
      "RNA also contains the base uracil in place of thymine. RNA molecules are less stable than DNA and are typically single-stranded. Genes that encode proteins are composed of a series of three-nucleotide sequences called codons, which serve as the \"words\" in the genetic \"language\". The genetic code specifies the correspondence during protein translation between codons and amino acids. The genetic code is nearly the same for all known organisms. The total complement of genes in an organism or cell is known as its genome, which may be stored on one or more chromosomes. A chromosome consists of a single, very",
      "be true or false, or whether or not sentences can express propositions about things that do not exist, rather than the way sentences are used. It has long been known that there are different parts of speech. One part of the common sentence is the lexical word, which is composed of nouns, verbs, and adjectives. A major question in the field \u2013 perhaps the single most important question for formalist and structuralist thinkers \u2013 is, \"How does the meaning of a sentence emerge out of its parts?\" Many aspects of the problem of the composition of sentences are addressed in"
    ]
  ],
  [
    "The exercise intensity at which blood lactate concentration begins to exponentially increase is known as the Lactate Threshold or Lactate Inflection Point (LIP).",
    [
      "Lactate threshold Lactate inflection point (LIP), is the exercise intensity at which the blood concentration of lactate and/or lactic acid begins to exponentially increase. It is often expressed as 85% of maximum heart rate or 75% of maximum oxygen intake. When exercising at or below the LT, any lactate produced by the muscles is removed by the body without it building up. The onset of blood lactate accumulation (OBLA) is often confused with the lactate threshold. With a higher exercise intensity the lactate production exceeds at a rate which it cannot be broken down, the blood lactate concentration will show",
      "blood lactate reaches a concentration of 2 mmol/litre (at rest it is around 1). The anaerobic energy system increases the ability to produce blood lactate during maximal exercise, resulting from an increased amount of glycogen stores and glycolytic enzymes. Lactate threshold Lactate inflection point (LIP), is the exercise intensity at which the blood concentration of lactate and/or lactic acid begins to exponentially increase. It is often expressed as 85% of maximum heart rate or 75% of maximum oxygen intake. When exercising at or below the LT, any lactate produced by the muscles is removed by the body without it building",
      "or AerT) is sometimes defined as the exercise intensity at which blood lactate concentrations rise above resting levels. Anaerobic threshold (AnT) is sometimes defined equivalently to the lactate threshold(LT); as the exercise intensity beyond which blood lactate concentration is no longer linearly related to exercise intensity, but increases with both exercise intensity \"and\" duration. The blood lactate concentration at the anaerobic threshold is called the \"maximum steady-state lactate concentration\" (MLSS). AeT is the exercise intensity at which aerobic energy pathways start to operate, considered to be around 65-85% of an individual's maximum heart rate. Some have suggested this is where"
    ]
  ],
  [
    "The key to neural function, according to research by Rodolfo Llinas and colleagues in the 80's, is the intrinsic electrophysiological properties of mammalian neurons, which provide insights into central nervous system function. (Llinas, 1988)",
    [
      "a consequence of an article written by Rodolfo Llinas himself in 1988 and published in Science with the title \"The Intrinsic Electrophysiological Properties of Mammalian Neurons: Insights into Central Nervous System Function\", which is considered a watershed due to its more than 2000 citations in the scientific literature, marking a major shift in viewpoint in neuroscience around the functional aspect. Until then, the prevailing belief in neuroscience was that just the connections and neurotransmitters released by neurons was enough to determine their function. Research by Llin\u00e1s and colleagues during the 80's with vertebrates revealed this previously held dogma was wrong.",
      "So, scientists have been at work to understand the human operation system and try to simulate its functionalities. In order to accomplish this, one needs to model its components and functions and validate its performance with real life. Computational models of a well simulated nervous system enable learning the nervous system and apply it to real life problem solutions. What is brain and what is neural network? Section 2.1 addresses the former question from an evolutionary perspective. The answer to the second question is based on the neural doctrine proposed by Ramon y Cajal (1894). He hypothesized that the elementary",
      "theoretical thesis on the visual system under the tuition of neurosurgeon and neurophysiologist Fernando Rosas and the mathematician Carlo Federici at the National University of Colombia. Llin\u00e1s is an atheist and describes himself as a logical positivist. He has studied the electrophysiology of single neurons in the cerebellum, the thalamus, the cerebral cortex, the entorhinal cortex, the hippocampus, the vestibular system, the inferior olive and the spinal cord. He has studied synaptic transmitter release in the squid giant synapse. He has studied human brain function using magnetoencephalography (MEG) on the basis of which he introduced the concept of Thalamocortical dysrhythmia."
    ]
  ],
  [
    "The redundancy theory of truth posits that asserting a statement is true is equivalent to asserting the statement itself.",
    [
      "Redundancy theory of truth According to the redundancy theory of truth (or the disquotational theory of truth), asserting that a statement is true is completely equivalent to asserting the statement itself. For example, asserting the sentence \" 'Snow is white' is true\" is equivalent to asserting the sentence \"Snow is white\". Redundancy theorists infer from this premise that truth is a redundant concept, in other words, that \"truth\" is a mere word that is conventional to use in certain contexts of discourse but not a word that points to anything in reality. The theory is commonly attributed to Frank P.",
      "true that...' is to agree with, accept, or endorse the statement that 'it's raining.'\" According to the redundancy theory of truth, asserting that a statement is true is completely equivalent to asserting the statement itself. For example, making the assertion that \" 'Snow is white' is true\" is equivalent to asserting \"Snow is white\". Redundancy theorists infer from this premise that truth is a redundant concept; that is, it is merely a word that is traditionally used in conversation or writing, generally for emphasis, but not a word that actually equates to anything in reality. This theory is commonly attributed",
      "is white\" and the person Snow White, both of which can be true in a sense. To say \"Snow is white\" is true is to say \"Snow is white\", but to say Snow White is true is not to say Snow White. Redundancy theory of truth According to the redundancy theory of truth (or the disquotational theory of truth), asserting that a statement is true is completely equivalent to asserting the statement itself. For example, asserting the sentence \" 'Snow is white' is true\" is equivalent to asserting the sentence \"Snow is white\". Redundancy theorists infer from this premise that"
    ]
  ],
  [
    "Based on the information provided, the statement \"Logical truths are contingent on historical facts\" is logically false.",
    [
      "false. One statement logically implies another when it is logically incompatible with the negation of the other. A statement is logically true if, and only if its opposite is logically false. The opposite statements must contradict one another. In this way all logical connectives can be expressed in terms of preserving logical truth. The logical form of a sentence is determined by its semantic or syntactic structure and by the placement of logical constants. Logical constants determine whether a statement is a logical truth when they are combined with a language that limits its meaning. Therefore, until it is determined",
      "and outputs either one of two Boolean values: true or false. For example, consider the sentences \"Barack Obama is the 44th president\" and \"If it rains today, I will bring an umbrella\". The first is a statement with an associated truth value. The second is a conditional statement relying on the value of some other statement. Either of these sentences can be broken down into predicates which can be compared and form the knowledge base of a deductive language. Moreover, variables such as 'Barack Obama' or 'president' can be quantified over. For example, take 'Barack Obama' as variable 'x'. In",
      "some interpretation or truth within some logical system. A logical truth (also called an analytic truth or a necessary truth) is a statement which is true in all possible worlds or under all possible interpretations, as contrasted to a \"fact\" (also called a \"synthetic claim\" or a \"contingency\") which is only true in this world as it has historically unfolded. A proposition such as \"If p and q, then p\" is considered to be a logical truth because of the meaning of the symbols and words in it and not because of any fact of any particular world. They are"
    ]
  ],
  [
    "The most rapid method to resynthesize ATP during exercise is through anaerobic glycolysis, which can provide energy for a period ranging from 10 seconds to 2 minutes and produces ATP at a speed about 100 times faster than oxidative processes.",
    [
      "Anaerobic glycolysis Anaerobic glycolysis is the transformation of glucose to lactate when limited amounts of oxygen (O) are available. Anaerobic glycolysis is only an effective means of energy production during short, intense exercise, providing energy for a period ranging from 10 seconds to 2 minutes. The anaerobic glycolysis (lactic acid) system is dominant from about 10\u201330 seconds during a maximal effort. It replenishes very quickly over this period and produces 2 ATP molecules per glucose molecule or about 5% of glucose's energy potential (38 ATP molecules). The speed at which ATP is produced is about 100 times that of oxidative",
      "and intensity and by the individual's cardiorespiratory fitness level. Three exercise energy systems can be selectively recruited, depending on the amount of oxygen available, as part of the cellular respiration process to generate the ATP for the muscles. They are ATP, the anaerobic system and the aerobic system. ATP is the usable form of chemical energy for muscular activity. It is stored in most cells, particularly in muscle cells. Other forms of chemical energy, such as those available from food, must be transformed into ATP before they can be utilized by the muscle cells. Since energy is released when ATP",
      "system that relates to its anaerobic quality is that only a few moles of ATP can be resynthesized from the breakdown of sugar as compared to the yield possible when oxygen is present. This system cannot be relied on for extended periods of time. The lactic acid system, like the ATP-CP system, is important primarily because it provides a rapid supply of ATP energy. For example, exercises that are performed at maximum rates for between 1 and 3 minutes depend heavily upon the lactic acid system for ATP energy. In activities such as running 1500 meters or a mile, the"
    ]
  ],
  [
    "H+ returns to the inside of the mitochondria in the process of oxidative phosphorylation through the passage of electrons from NADH and FADH through the electron transport chain, which pumps protons out of the mitochondrial matrix and into the intermembrane space.",
    [
      "pass electrons to a more electronegative acceptor, which in turn donates these electrons to another acceptor, a process that continues down the series until electrons are passed to oxygen, the most electronegative and terminal electron acceptor in the chain. Passage of electrons between donor and acceptor releases energy, which is used to generate a proton gradient across the mitochondrial membrane by actively \"pumping\" protons into the intermembrane space, producing a thermodynamic state that has the potential to do work. This entire process is called oxidative phosphorylation, since ADP is phosphorylated to ATP using the energy of hydrogen oxidation in many",
      "inner mitochondrial membrane is impermeable to NADH and NAD. Instead of transferring the generated NADH, a malate dehydrogenase enzyme converts oxaloacetate to malate, which is translocated to the mitochondrial matrix. Another malate dehydrogenase-catalyzed reaction occurs in the opposite direction, producing oxaloacetate and NADH from the newly transported malate and the mitochondrion's interior store of NAD. A transaminase converts the oxaloacetate to aspartate for transport back across the membrane and into the intermembrane space. In oxidative phosphorylation, the passage of electrons from NADH and FADH through the electron transport chain pumps protons out of the mitochondrial matrix and into the intermembrane",
      "phosphorylation. This second step requires oxygen. The net result is ATP, the energy carrier used by the cell for metabolic activities and to perform work, such as muscle contraction. When the energy in ATP is used during cell work via ATP hydrolysis, hydrogen ions, (positively charged protons) are released. The mitochondria normally incorporate these free hydrogen nuclei back into ATP, thus preventing buildup of unbound hydrogen cations, and maintaining neutral pH. If oxygen supply is inadequate (hypoxia), the mitochondria are unable to continue creating ATP at a rate sufficient to meet the cell's energy needs. In this situation, glycolysis is"
    ]
  ],
  [
    "The molecule that will stop being produced first when oxygen is no longer supplied to the cell is ATP. (Basal metabolic rate)",
    [
      "occur. Oxygen cells behave in a similar way to electrical batteries in that they have a finite lifespan which is dependent upon use. The chemical reaction described above causes the cell to create an electrical output that has a predicted voltage which is dependent on the materials used. In theory they should give that voltage from the day they are made until they are exhausted, except that one component of the planned chemical reaction has been left out of the assembly: oxygen. Oxygen is one of the fuels of the cell so the more oxygen there is at the reaction",
      "in water\u2014that is, 2 to 1\u2014all of the oxygen consumed by the cells is used to oxidize the carbon in the carbohydrate molecule to form carbon dioxide. Consequently, during the complete oxidation of a glucose molecule, six molecules of carbon dioxide and six molecules of water are produced and six molecules of oxygen are consumed. The overall equation for this reaction is: Because the gas exchange in this reaction is equal, the respiratory quotient (R.Q.) for carbohydrate is unity or 1.0: The chemical composition for fats differs from that of carbohydrates in that fats contain considerably fewer oxygen atoms in",
      "for subsequent breakdown in later steps of glycolysis. At physiological conditions, this initial reaction is irreversible. In anaerobic respiration, one glucose molecule produces a net gain of two ATP molecules (four ATP molecules are produced during glycolysis through substrate-level phosphorylation, but two are required by enzymes used during the process). In aerobic respiration, a molecule of glucose is much more profitable in that a maximum net production of 30 or 32 ATP molecules (depending on the organism) through oxidative phosphorylation is generated. Tumor cells often grow comparatively quickly and consume an above-average amount of glucose by glycolysis, which leads to"
    ]
  ],
  [
    "Substance abuse during adolescence can impact a young adult's executive functioning and impulse control by disproportionately strengthening the incentive-rewards systems in the brain, leading to increased likelihood of acting on impulses and engaging in risky behavior before considering consequences.",
    [
      "who engage in drug use favorably, the chances of them developing an addiction increases. Family conflict and home management is also a cause for one to become engaged in drug or alcohol use. Adolescence represents a period of unique vulnerability for developing an addiction. In adolescence, the incentive-rewards systems in the brain mature well before the cognitive control center. This consequentially grants the incentive-rewards systems a disproportionate amount of power in the behavioral decision-making process. Therefore, adolescents are increasingly likely to act on their impulses and engage in risky, potentially addicting behavior before considering the consequences. Not only are adolescents",
      "of time-based prospective memory from childhood into young adulthood; however, a decrease begins in later adulthood. Substance abuse refers to the harmful use of substances, including alcohol and illicit drugs. Multiple studies have shown that abuse of substances can damage our memory system. Current research has looked at the effect of substances on prospective memory; ability to remember to do something in the future. In particular it has been found that cannabis, ecstasy, methamphetamine and alcohol are substances that directly affect time-based prospective memory. Marijuana is the world's most commonly abused illicit drug. The effects of cannabis are associated with",
      "the child's cognitive functioning or ability to cope with negative or disruptive emotions may be impaired. Over time, the child may adopt substance use as a coping mechanism, particularly during adolescence. A study of 900 court cases involving children who experienced abuse found that a vast amount of them went on to suffer from some form of addiction in their adolescence or adult life. This pathway towards addiction that is opened through stressful experiences during childhood can be avoided by a change in environmental factors throughout an individual's life and opportunities of professional help. If one has friends or peers"
    ]
  ],
  [
    "The initial result of glycogen breakdown in muscle during exercise is the production of glucose-6-phosphate, which serves as a rapidly accessible energy source for movement. (muscle during exercise)",
    [
      "muscle during exercise. In the muscles, glycogen ensures a rapidly accessible energy source for movement. Glycogenesis refers to the process of synthesizing glycogen. In humans, excess glucose is converted to glycogen via this process. Glycogen is a highly branched structure, consisting of glucose, in the form of glucose-6-phosphate, linked together. The branching of glycogen increases its solubility, and allows for a higher number of glucose molecules to be accessible for breakdown. Glycogenesis occurs primarily in the liver, skeletal muscles, and kidney. The pentose phosphate pathway is an alternative method of oxidizing glucose. It occurs in the liver, adipose tissue, adrenal",
      "is referred to as \"hitting the wall\". Glycogen depletion can be forestalled in three possible ways. First, during exercise, carbohydrates with the highest possible rate of conversion to blood glucose (high glycemic index) are ingested continuously. The best possible outcome of this strategy replaces about 35% of glucose consumed at heart rates above about 80% of maximum. Second, through endurance training adaptations and specialized regimens (e.g. fasting low-intensity endurance training), the body can condition type I muscle fibers to improve both fuel use efficiency and workload capacity to increase the percentage of fatty acids used as fuel, sparing carbohydrate use",
      "the process of glycolysis breaks down the sugars from carbohydrates for energy without the use of oxygen. This type of exercise occurs in physical activity such as power sprints, strength resistances and quick explosive movement where the muscles are being used for power and speed, with short-time energy use. After this type of exercise, there is a need to refill glycogen storage sites in the body (the long simple sugar chains in the body that store energy), although they are not likely fully depleted. To compensate for this glycogen reduction, athletes will often take in large amounts of carbohydrates, immediately"
    ]
  ],
  [
    "Severely reducing intake of fluids while maintaining rigorous daily workouts is NOT a good method for a wrestler to restrict his caloric intake while trying to lose weight for a match.",
    [
      "competition begins. If done properly, a wrestler who does cut weight can gain a very significant strength and weight advantage over opponents who do not. Athletes can cut weight in an unhealthy way, with negative short and long-term effects. Dehydration can result when a wrestler severely reduces intake of fluids while maintaining rigorous daily workouts. This may result in cramps or, in extreme cases, heatstroke and swelling of the brain which causes seizures and hypovolemic shock. Malnutrition can also result if cutting weight over long periods of time. Long term weight cutting can mean that a wrestler does not intake",
      "class and wrestles at two weight classes above it, he forfeits his previous lowest weight class for the one weight class below where he wrestled. If a contestant wishes to weigh-in and wrestle at only one weight class above his certified weight class and later return to his lowest certified weight class, he may do so. However, the wrestler may only return to that certified weight class according to the weight-loss plan of the National Wrestling Coaches Association. This weight loss plan takes into account potential dehydration during the wrestling season and minimum amounts of body fat. All of this",
      "Catchweight A catchweight is a term used in combat sports, such as boxing or mixed martial arts, to describe a weight limit that does not adhere to the traditional limits for weight classes. In boxing, a catchweight is negotiated prior to weigh-ins, which are conducted one day before the fight. The term 'catchweight' is also less regularly used in professional wrestling, but used to describe catch wrestling; which can be used to mean no weight limit. An example of a catchweight division in professional wrestling is Pro Wrestling Pride's Catch Division Championship. Strictly speaking, a catchweight in boxing is used"
    ]
  ],
  [
    "Proteins are treated with a detergent before being run through an electrophoresis gel in order to denature the proteins, coat them with a negative charge, and allow for separation based on size.",
    [
      "of a charged ion in an electric field. In practice, the proteins are denatured in a solution containing a detergent (SDS). In these conditions, the proteins are unfolded and coated with negatively charged detergent molecules. The proteins in SDS-PAGE are separated on the sole basis of their size. In analytical methods, the protein migrate as bands based on size. Each band can be detected using stains such as Coomassie blue dye or silver stain. Preparative methods to purify large amounts of protein, require the extraction of the protein from the electrophoretic gel. This extraction may involve excision of the gel",
      "to positive EMF on the sample. Proteins therefore, are usually denatured in the presence of a detergent such as sodium dodecyl sulfate (SDS) that coats the proteins with a negative charge. Generally, the amount of SDS bound is relative to the size of the protein (usually 1.4g SDS per gram of protein), so that the resulting denatured proteins have an overall negative charge, and all the proteins have a similar charge-to-mass ratio. Since denatured proteins act like long rods instead of having a complex tertiary shape, the rate at which the resulting SDS coated proteins migrate in the gel is",
      "substrate. The electrophoresis takes place in non-reducing conditions, and the enzymes are protected from denaturation using leupeptin. After protein concentration is determined, equal amounts of tissue protein are loaded into a gel. The protein is then allowed to migrate through the gel. After electrophoresis, the gel is put into a renaturing buffer in order to return the cathepsins to their native conformation. The gel is then put into an activation buffer of a specific pH and left to incubate overnight at 37 \u00b0C. This activation step allows the cathepsins to degrade the gelatin substrate. When the gel is stained using"
    ]
  ],
  [
    "The initial energy source for very high force contractions lasting 1-2 seconds is the anaerobic muscle fibers, which use their fuel faster than aerobic fibers at maximum load, shutting out the aerobic fibers and relying solely on anaerobic processes for energy.",
    [
      "the extremes, a muscle will fire fibres of both the aerobic or anaerobic types on any given exercise, in varying ratio depending on the load on the intensity of the contraction. This is known as the energy system continuum. At higher loads, the muscle will recruit all muscle fibres possible, both anaerobic (\"fast-twitch\") and aerobic (\"slow-twitch\"), in order to generate the most force. However, at maximum load, the anaerobic processes contract so forcefully that the aerobic fibers are completely shut out, and all work is done by the anaerobic processes. Because the anaerobic muscle fibre uses its fuel faster than",
      "the stronger the eccentric contraction will be, which in turn produces even greater tension. This tension, which is potential force, is then given back in the return movement when the muscular contractions switch to the concentric or shortening regime. However, for maximum return of energy, minimum time must elapse from when the force is received to when they are returned. The greater the time between receiving the forces and giving them back, the less is the return and the less the height that can be achieved in the jump. Most of the lengthening and shortening occurs in the respective muscle",
      "Pulsed power Pulsed power is the science and technology of accumulating energy over a relatively long period of time and releasing it very quickly, thus increasing the instantaneous power. Energy is typically stored within electrostatic fields (capacitors), magnetic fields (inductor), as mechanical energy (using large flywheels connected to special purpose high current alternators), or as chemical energy (high-current lead-acid batteries, or explosives). By releasing the stored energy over a very short interval (a process that is called energy compression), a huge amount of peak power can be delivered to a load. For example, if one joule of energy is stored"
    ]
  ],
  [
    "Negative gender-based characteristics, such as misogyny and the societal belief that aggression and toughness are essential for success in sports, play a significant role in determining success, as they often lead to the shaming, objectifying, and degrading of women athletes.",
    [
      "be men but also not to be women (California Institute of Integral Studies). In other words, he meant that coaches ensure that young males gain enough strength to face their opponents and discourage these players to fear, which is a character trait associated with femininity. In sporting activities, to be masculine means to possess skills, energy, and toughness in order to become the victor in many tournaments. Misogyny in sports is a significant factor that contributes to shaming, objectifying, and degrading women, regardless of their performance on the playing field. Misogyny refers to hatred or prejudice towards women. It is",
      "3 headed \u2018What Sports Illustrate,\u2019 she argues that the pre-determined societal beliefs surrounding gender have turned competitive sports into a way for men to legitimize aggression and create their masculine identity. Conversely, Lorber believes that the female sex in competitive sports is made a mockery of and continually takes a \u201csecondary status\u201d to competitive male sports. Lorber uses the example of basketball to confirm her belief. She correlates the female secondary status to assumptions surrounding women\u2019s physiology. As well Lorber believes this assumption of physiology influences rules in women's sports, rules in sporting competitions, and how women are treated in",
      "\u2013 and what is not. A boy's rank in the hierarchy is chiefly determined by his athletic ability. One site where gender is performed and socialized is in sport. Violent sports such as football are fundamental in naturalizing the equation of maleness with violence. Displays of strength and violence, through sports like football, help to naturalize elements of competition and hierarchy as inherently male behaviour. There is considerable evidence that males are hormonally predisposed to higher levels of aggression on average that females, due to the effects of testosterone. However, the violent and competitive nature of sports like football can"
    ]
  ],
  [
    "The lining of the digestive tract and respiratory tract develop from the endoderm germ layer.",
    [
      "gut. The blood vessels supplying these structures remain constant throughout development. The gastrointestinal tract has a form of general histology with some differences that reflect the specialization in functional anatomy. The GI tract can be divided into four concentric layers in the following order: The mucosa is the innermost layer of the gastrointestinal tract. The mucosa surrounds the lumen, or open space within the tube. This layer comes in direct contact with digested food (chyme). The mucosa is made up of: The mucosae are highly specialized in each organ of the gastrointestinal tract to deal with the different conditions. The",
      "the plant becomes older, more endoderm will lignify. The following chart shows the tissues produced by the endoderm. The embryonic endoderm develops into the interior linings of two tubes in the body, the digestive and respiratory tube. Liver and pancreas cells are believed to derive from a common precursor. In humans, the endoderm can differentiate into distinguishable organs after 5 weeks of embryonic development. Endoderm Endoderm is one of the three primary germ layers in the very early embryo. The other two layers are the ectoderm (outside layer) and mesoderm (middle layer), with the endoderm being the innermost layer. Cells",
      "however, derived from the ectoderm. Germ layer A germ layer is a primary layer of cells that form during embryogenesis. The three germ layers in vertebrates are particularly pronounced; however, all eumetazoans (animals more complex than the sponge) produce two or three primary germ layers. Some animals, like cnidarians, produce two germ layers (the ectoderm and endoderm) making them diploblastic. Other animals such as chordates produce a third layer (the mesoderm), between these two layers. making them triploblastic. Germ layers eventually give rise to all of an animal\u2019s tissues and organs through the process of organogenesis. Caspar Friedrich Wolff observed"
    ]
  ],
  [
    "Phenylalanine is most likely to be inside the transmembrane domain of the protein C15orf52.",
    [
      "proteins in humans. Phenylalanine, Tyrosine, and Asparagine were all found in lower frequencies than other proteins in humans. Glycine and Arginine were found at higher frequencies than other proteins in humans. The isoelectric point of the protein is 9.457, indicating a basic protein at a normal physiological pH of 7.4. C15orf52 has a coiled coil domain spanning amino acids 60-97 containing alpha helices. The tertiary structure of this protein is still unknown to the scientific community and is often up for debate. There are no transmembrane sequences detected in the C15orf52 protein. C15orf52 is also predicted to be a non-cytoplasmic",
      "to cluster in. Before post-translational modifications, C3orf62 is an acidic protein. No charge clusters are present in C3orf62, and no specific spacing of cysteine is found. The isoelectric point of C3orf62 is 5.211000. There are no known transmembrane domains for C3orf62. C3orf62 has a KKXX-like motif in the C-terminus meaning C3orf62 may be responsible for retrieval of endoplasmic reticulum (ER) membrane proteins from the Golgi apparatus. Roughly 7 alpha helices are predicted for C3orf62 through Pele Protein Structure Protein Prediction and strengthened through orthologous secondary structure predictions by Ali2D. C3orf62 is predicted to be localized in the nucleus. The k-nearest",
      "kiloDaltons. Glycine and tyrosine residues are relatively less prevalent compared to other proteins in the human proteome, while methionine is more prevalent. The protein product is predicted to have multiple \u03b1-helices, coiled coil, and one \u03b2-sheet. It is suggested that the protein does not contain transmembrane regions or helices, meaning that the protein is not anchored to the cell membrane nor an intracellular membrane like the Golgi apparatus. In the predicted protein product, C12orf60 contains a conserved protein domain of 225 amino acids. This domain (DUF4533) is within in the pfam15047 family of proteins. Only one other gene is listed"
    ]
  ],
  [
    "Pseudouridine is a modified base found in RNA that is not part of the standard nucleotide bases.",
    [
      "defined by containing ribose nucleic acid. In some occasions, DNA and RNA may contain some minor bases. Methylated forms of the major bases are most common in DNA. In viral DNA, some bases may be hydroxymethylated or glucosylated. In RNA, minor or modified bases occur more frequently. Some examples include hypoxanthine, dihydrouracil, methylated forms of uracil, cytosine, and guanine, as well as modified nucleoside pseudouridine. Nucleotides with phosphate groups in positions other than on the 5' carbon have also been observed. Examples include ribonucleoside 2',3'-cyclic monophosphates which are isolatable intermediates, and ribonucleoside 3'-monophosphates which are end products of the hydrolysis",
      "present: CpG islands in DNA (are often methylated), all eukaryotic mRNA (capped with a methyl-7-guanosine), and several bases of rRNAs (are methylated). Often, tRNAs are heavily modified postranscriptionally in order to improve their conformation or base pairing, in particular in/near the anticodon: inosine can base pair with C, U, and even with A, whereas thiouridine (with A) is more specific than uracil (with a purine). Other common tRNA base modifications are pseudouridine (which gives its name to the T\u03a8C loop), dihydrouridine (which does not stack as it is not aromatic), queuosine, wyosine, and so forth. Nevertheless, these are all modifications",
      "cytosine, adenine, and uridine. Another commonly modified base in tRNA is the position adjacent to the anticodon. Position 37 is often hypermodified with bulky chemical modifications. These modifications prevent frameshifting and increase anticodon-codon binding stability through stacking interactions. Ribosomal RNA modifications are made throughout the ribosome synthesis. Modifications primarily play a role in the structure of the rRNA in order to protect translational efficiency. There are over 160 RNA modifications identified. Chemical modifications can range from simple methylations (m6A) to hypermodifications (i6A) that require several steps for synthesis. Hypermodified bases are primarily seen at position 34 and 37 of the"
    ]
  ],
  [
    "Fast-twitch fibers have a low myoglobin content and rely on an anaerobic, short term, glycolytic system for energy transfer.",
    [
      "The fast twitch fibers rely on a well-developed, anaerobic, short term, glycolytic system for energy transfer and can contract and develop tension at 2\u20133 times the rate of slow twitch fibers. Fast twitch muscles are much better at generating short bursts of strength or speed than slow muscles, and so fatigue more quickly. The slow twitch fibers generate energy for ATP re-synthesis by means of a long term system of aerobic energy transfer. These mainly include the ATPase type I and MHC type I fibers. They tend to have a low activity level of ATPase, a slower speed of contraction",
      "are white due to relatively low myoglobin and a reliance on glycolytic enzymes. Fibers can also be classified on their twitch capabilities, into fast and slow twitch. These traits largely, but not completely, overlap the classifications based on color, ATPase, or MHC. Some authors define a fast twitch fiber as one in which the myosin can split ATP very quickly. These mainly include the ATPase type II and MHC type II fibers. However, fast twitch fibers also demonstrate a higher capability for electrochemical transmission of action potentials and a rapid level of calcium release and uptake by the sarcoplasmic reticulum.",
      "with a less well developed glycolytic capacity. They contain high mitochondrial volumes, and the high levels of myoglobin that give them a red pigmentation. They have been demonstrated to have high concentrations of mitochondrial enzymes, thus they are fatigue resistant. Slow twitch muscles fire more slowly than fast twitch fibers, but are able to contract for a longer time before fatiguing. Individual muscles tend to be a mixture of various fiber types, but their proportions vary depending on the actions of that muscle and the species. For instance, in humans, the quadriceps muscles contain ~52% type I fibers, while the"
    ]
  ],
  [
    "The net products formed when one molecule of glucose is converted to two molecules of pyruvate are two molecules of pyruvate.",
    [
      "glucose degrading into two molecules of pyruvate, through various steps, with the help of different enzymes. It occurs in ten steps and proves that phosphorylation is a much required and necessary step to attain the end products. Phosphorylation initiates the reaction in step 1 of the preparatory step (first half of glycolysis), and initiates step 6 of payoff phase (second phase of glycolysis). Glucose, by nature, is a small molecule with the ability to diffuse in and out of the cell. By phosphorylating glucose (adding a negatively charged phosphate group), glucose is converted to glucose-6-phosphate and trapped within the cell",
      "it into the energy rich molecule pyruvate. Inside the overall reaction there lie many steps that need to be followed in order for the original glucose molecule to be transformed into pyruvate. The glucose first gathers a phosphate group from an ATP molecule in order to become glucose-6-phosphate. It is then changed into fructose 6-phosphate, with the assistance of phosphoglucose isomerase, which is then changed into fructose 1,6-biphosphate when the fructose molecule receives a phosphate group from another ATP. The next step in the chain is crucial for cells in order to make more energy than they expend through the",
      "potassium hydrogen sulfate, by the oxidation of propylene glycol by a strong oxidizer (e.g., potassium permanganate or bleach), or by the hydrolysis of acetyl cyanide, formed by reaction of acetyl chloride with potassium cyanide: Pyruvate is an important chemical compound in biochemistry. It is the output of the metabolism of glucose known as glycolysis. One molecule of glucose breaks down into two molecules of pyruvate, which are then used to provide further energy, in one of two ways. Pyruvate is converted into acetyl-coenzyme A, which is the main input for a series of reactions known as the Krebs cycle (also"
    ]
  ],
  [
    "The subtypes of Acute Myeloid Leukemia based on the type of cell from which the leukemia developed and its degree of maturity include M0 through M7, with biphenotypic acute leukemia also being a subtype.",
    [
      "biphenotypic acute leukemia) occur when the leukemic cells can not be classified as either myeloid or lymphoid cells, or where both types of cells are present. The French-American-British (FAB) classification system divides AML into eight subtypes, M0 through to M7, based on the type of cell from which the leukemia developed and its degree of maturity. This is done by examining the appearance of the malignant cells with light microscopy and/or by using cytogenetics to characterize any underlying chromosomal abnormalities. The subtypes have varying prognoses and responses to therapy. Although the WHO classification (see above) may be more useful, the",
      "Acute myeloblastic leukemia with maturation M2 is a subtype of AML (Acute Myeloid Leukemia). It is also known as \"Acute Myeloblastic Leukemia with Maturation\". Acute myeloid leukemia (AML) is a type of cancer affecting blood cells that eventually develop into non-lymphocyte white blood cells. The disease originates from the bone marrow, the soft inner portion of select bones where blood stem cells develop into either lymphocyte or in this particular condition, myeloid cells. This acute disease prevents bone marrow cells from properly maturing, thus causing an accumulation of immature myeloblast cells in the bone marrow. Acute myeloid leukemia is more",
      "structure and function of the cell\u2019s chromosomes. The criteria for an acute myeloid leukemia case to fall under the M2 subtype is the following: 20%+ nonerythroid cells in peripheral blood or bone marrow are myeloblasts; monocytic precursors are < 20% in bone marrow and granulocytes are 10%+ of cells (Mihova, 2013). This subtype is characterized by a translocation of a part of chromosome 8 to chromosome 21, written as t(8;21). On both sides of the splice the DNA coded for different proteins, RUNX1 and ETO, These two sequences are then transcribed and translated into a single large protein, \"M2 AML\""
    ]
  ],
  [
    "The second phase of glycolysis promotes ATP generation by producing pyruvate, which can then be converted into lactate or enter the mitochondria for further ATP production through oxidative phosphorylation, providing more energy to contracting muscles under heavy workloads. (Carbohydrate catabolism)",
    [
      "second phase of glycolysis to promote ATP generation. This, in effect, provides more energy to contracting muscles under heavy workloads. The production and removal of lactate from the cell also ejects a proton consumed in the LDH reaction- the removal of excess protons produced in the wake of this fermentation reaction serves to act as a buffer system for muscle acidosis. Once proton accumulation exceeds the rate of uptake in lactate production and removal through the LDH symport, muscular acidosis occurs. LDH is a protein that normally appears throughout the body in small amounts. Many cancers can raise LDH levels,",
      "phosphorylation. This second step requires oxygen. The net result is ATP, the energy carrier used by the cell for metabolic activities and to perform work, such as muscle contraction. When the energy in ATP is used during cell work via ATP hydrolysis, hydrogen ions, (positively charged protons) are released. The mitochondria normally incorporate these free hydrogen nuclei back into ATP, thus preventing buildup of unbound hydrogen cations, and maintaining neutral pH. If oxygen supply is inadequate (hypoxia), the mitochondria are unable to continue creating ATP at a rate sufficient to meet the cell's energy needs. In this situation, glycolysis is",
      "ATP. For the glucose molecule to oxidize into pyruvate, an input of ATP molecules is required. This is known as the investment phase, in which a total of two ATP molecules are consumed. At the end of glycolysis, the total yield of ATP is four molecules, but the net gain is two ATP molecules. Even though ATP is synthesized, the two ATP molecules produced are few compared to the second and third pathways, Krebs cycle and oxidative phosphorylation. Even if there is no oxygen present, glycolysis can continue to generate ATP. However, for glycolysis to continue to produce ATP, there"
    ]
  ],
  [
    "The main occurrence in prophase of cell division is the condensation of chromatin and the disappearance of the nucleolus.",
    [
      "Prophase Prophase (from the Greek \u03c0\u03c1\u03cc, \"before\" and \u03c6\u03ac\u03c3\u03b9\u03c2, \"stage\") is the first stage of cell division in both mitosis and meiosis. Beginning after interphase, DNA has already been replicated when the cell enters prophase. The main occurrences in prophase are the condensation of the chromatin and the disappearance of the nucleolus. Microscopy can be used to visualize condensed chromosomes as they move through meiosis and mitosis. Various DNA stains are used to treat cells such that condensing chromosomes can be visualized as the move through prophase. The giemsa G-banding technique is commonly used to identify mammalian chromosomes, utilizing the",
      "the meiotic recombination checkpoint) that prevent the cell from entering metaphase I with errors due to recombination. Prophase Prophase (from the Greek \u03c0\u03c1\u03cc, \"before\" and \u03c6\u03ac\u03c3\u03b9\u03c2, \"stage\") is the first stage of cell division in both mitosis and meiosis. Beginning after interphase, DNA has already been replicated when the cell enters prophase. The main occurrences in prophase are the condensation of the chromatin and the disappearance of the nucleolus. Microscopy can be used to visualize condensed chromosomes as they move through meiosis and mitosis. Various DNA stains are used to treat cells such that condensing chromosomes can be visualized as",
      "the first time, by Kurt Michelwith, using a phase-contrast microscope. Cell division Cell division is the process by which a parent cell divides into two or more daughter cells. Cell division usually occurs as part of a larger cell cycle. In eukaryotes, there are two distinct types of cell division: a vegetative division, whereby each daughter cell is genetically identical to the parent cell (mitosis), and a reproductive cell division, whereby the number of chromosomes in the daughter cells is reduced by half to produce haploid gametes (meiosis). Meiosis results in four haploid daughter cells by undergoing one round of"
    ]
  ],
  [
    "The correlation between social networks and alumni networks can impact admission rates to exceptionally selective colleges by providing first-generation college students with academic and emotional support, access to information about college, and connections to college graduates, ultimately increasing their chances of attending and completing college.",
    [
      "difficulty creates difficulty in most other subjects. In this way they fall further and further behind in school, dropping out at a much higher rate than their peers. In the words of Stanovich: In network science, the Matthew effect is used to describe the preferential attachment of earlier nodes in a network, which explains that these nodes tend to attract more links early on. \"Because of preferential attachment, a node that acquires more connections than another one will increase its connectivity at a higher rate, and thus an initial difference in the connectivity between two nodes will increase further as",
      "around each other despite social class differences. Studies show that the strongest predictors for college attendance and completion are academic preparation, social support, access to information, parental involvement and knowledge about college, and financial aid. Another form of social support is the prevalence of strong social networks that support a student's academic and emotional development. Federally funded programs such as Upward Bound, Talent Search, Gear Up, and non-profit organizations such as AVID have been implemented at the secondary level to prepare first-generation students for college through academic counseling, college field trips, study skill development, and support from college graduates. In",
      "students, then the effect of the college attendance can be attributed to the college choice. Many statistical methods have been developed for causal inference, such as propensity score matching and nearest-neighbor matching (which often uses the Mahalanobis metric, also called Mahalanobis matching). These methods attempt to correct for the assignment mechanism by finding control units similar to treatment units. In the example, matching finds graduates of a public college most similar to graduates of a private college, so that like is compared only with like. Causal inference methods make few assumptions other than that one unit's outcomes are unaffected by"
    ]
  ],
  [
    "Passive immunity is demonstrated when a patient is given a serum containing rabies antibodies grown inside a horse.",
    [
      "blood which contains antibodies (known as the serum), and injecting this serum into the person for whom immunity is desired. This is known as passive immunity, and the serum that is isolated from one subject and injected into another is sometimes called antiserum. Antiserum from other mammals, notably horses, has been used in humans with generally good and often life-saving results, but there is some risk of anaphylactic shock and even death from this procedure because the human body sometimes recognizes antibodies from other animals as foreign proteins. Passive immunity is temporary, because the antibodies which are transferred have a",
      "species in mind (polyvalent). In the United States for example, all species of venomous snakes are pit vipers, with the exception of the coral snake. To produce antivenom, a mixture of the venoms of the different species of rattlesnakes, copperheads, and cottonmouths is injected into the body of a horse in ever-increasing dosages until the horse is immunized. Blood is then extracted from the immunized horse. The serum is separated and further purified and freeze-dried. It is reconstituted with sterile water and becomes antivenom. For this reason, people who are allergic to horses are more likely to suffer an allergic",
      "to be more than 1,000.00 USD per dose. It is made from the blood plasma of people or horses who have high levels of the antibody in their blood. The horse version is less expensive but has a higher rate of side effects. Rabies immunoglobulin Rabies immunoglobulin (RIG) is a medication made up of antibodies against the rabies virus. It is used to prevent rabies following exposure. It is given after the wound is cleaned with soap and water or providone-iodine and is followed by a course of rabies vaccine. It is given by injection into the site of the"
    ]
  ],
  [
    "The semi-periphery countries in the world-system theory are the industrializing, mostly capitalist countries positioned between the periphery and core countries.",
    [
      "Kawana, Brewer (2000). And this is the semi-periphery listing according to Babones (2005), who notes that this list is composed of countries that \"have been consistently classified into a single one of the three zones [core, semi-periphery or periphery] of the world economy over the entire 28-year study period\". Semi-periphery countries In world-systems theory, the semi-periphery countries (sometimes referred to as just the semi-periphery) are the industrializing, mostly capitalist countries which are positioned between the periphery and core countries. Semi-periphery countries have organizational characteristics of both core countries and periphery countries and are often geographically located between core and peripheral",
      "Semi-periphery countries In world-systems theory, the semi-periphery countries (sometimes referred to as just the semi-periphery) are the industrializing, mostly capitalist countries which are positioned between the periphery and core countries. Semi-periphery countries have organizational characteristics of both core countries and periphery countries and are often geographically located between core and peripheral regions as well as between two or more competing core regions. Semi-periphery regions play a major role in mediating economic, political, and social activities that link core and peripheral areas. These regions allow for the possibility of innovative technology, reforms in social and organizational structure, and dominance over peripheral",
      "in the world-system\" that appeared in the American Sociological Review (Dunn, Kawana, Brewer (2000)). And this is the periphery listing according to Babones (2005), who notes that this list is composed of countries that \"have been consistently classified into a single one of the three zones [core, semi-periphery or periphery] of the world economy over the entire 28-year study period\". Periphery countries In world systems theory, the periphery countries (sometimes referred to as just the periphery) are those that are less developed than the semi-periphery and core countries. These countries usually receive a disproportionately small share of global wealth. They"
    ]
  ],
  [
    "The role of a dephospho-(reductase kinase) kinase in enzymology is to catalyze the transfer of phosphate groups from ATP to dephospho-{[hydroxymethylglutaryl-CoA reductase (NADPH)] kinase}, leading to phosphorylation of the substrate.",
    [
      "Dephospho-(reductase kinase) kinase In enzymology, a dephospho-[reductase kinase] kinase () is an enzyme that catalyzes the chemical reaction Thus, the two substrates of this enzyme are ATP and dephospho-{[hydroxymethylglutaryl-CoA reductase (NADPH)] kinase}, whereas its two products are ADP and {[hydroxymethylglutaryl-CoA reductase (NADPH)] kinase}. This enzyme belongs to the family of transferases, specifically those transferring a phosphate group to the sidechain oxygen atom of serine or threonine residues in proteins (protein-serine/threonine kinases). The systematic name of this enzyme class is ATP:dephospho-{[hydroxymethylglutaryl-CoA reductase (NADPH)] kinase} phosphotransferase. Other names in common use include AMP-activated kinase, AMP-activated protein kinase kinase, hydroxymethylglutaryl coenzyme A reductase",
      "kinase kinase, hydroxymethylglutaryl coenzyme A reductase kinase kinase, (phosphorylating), reductase kinase, reductase kinase kinase, and STK30. Dephospho-(reductase kinase) kinase In enzymology, a dephospho-[reductase kinase] kinase () is an enzyme that catalyzes the chemical reaction Thus, the two substrates of this enzyme are ATP and dephospho-{[hydroxymethylglutaryl-CoA reductase (NADPH)] kinase}, whereas its two products are ADP and {[hydroxymethylglutaryl-CoA reductase (NADPH)] kinase}. This enzyme belongs to the family of transferases, specifically those transferring a phosphate group to the sidechain oxygen atom of serine or threonine residues in proteins (protein-serine/threonine kinases). The systematic name of this enzyme class is ATP:dephospho-{[hydroxymethylglutaryl-CoA reductase (NADPH)] kinase} phosphotransferase.",
      "Kinase In biochemistry, a kinase is an enzyme that catalyzes the transfer of phosphate groups from high-energy, phosphate-donating molecules to specific substrates. This process is known as phosphorylation, where the substrate gains a phosphate group and the high-energy ATP molecule donates a phosphate group. This transesterification produces a phosphorylated substrate and ADP. Conversely, it is referred to as dephosphorylation when the phosphorylated substrate donates a phosphate group and ADP gains a phosphate group (producing a dephosphorylated substrate and the high energy molecule of ATP). These two processes, phosphorylation and dephosphorylation, occur four times during glycolysis. Kinases are part of the"
    ]
  ],
  [
    "The main fate of lactate that leaves muscle and enters the circulation is to be transported to the liver, where it can be converted back into pyruvate and then into glucose through the Cori cycle. (Cori cycle)",
    [
      "intracellular lactate is released into the blood, maintenance of electroneutrality of the blood requires that a cation be released into the blood, as well. This can reduce blood pH. Glycolysis coupled with lactate production is neutral in the sense that it does not produce excess hydrogen cations; however, pyruvate production does produce them. Lactate production is buffered intracellularly, e.g. the lactate-producing enzyme, lactate dehydrogenase, binds one hydrogen cation per pyruvate molecule converted. When such buffer systems become saturated, cells will transport lactate into the bloodstream. Hypoxia certainly causes both a buildup of lactate and acidification, and lactate is therefore a",
      "the cell via diffusion into the blood stream, where it is transported in three ways: HO also diffuses out of the cell into the blood stream, from where it is excreted in the form of perspiration, water vapor in breath, or urine from the kidneys. Water, along with some dissolved solutes, are removed from blood circulation in the nephrons of the kidney and eventually excreted as urine. The products of fermentation can be processed different ways, depending on the cellular conditions. Lactic acid tends to accumulate in the muscles, which causes pain of the muscle and joint as well as",
      "pyruvate and ultimately to glucose, which can travel back to the peripheral tissues, completing the Cori cycle. Thus, lactate has traditionally considered a toxic metabolic byproduct that could give rise to fatigue and muscle pain during anaerobic respiration. Lactate can be thought of essentially as payment for \"oxygen debt\", defined by Hill and Lupton as the \"total amount of oxygen used, after cessation of exercise in recovery there from\". Highly malignant tumors rely heavily on aerobic glycolysis (metabolism of glucose to lactic acid even under presence of oxygen; Warburg effect) and thus need to efflux lactic acid via MCTs to"
    ]
  ],
  [
    "The three exercise energy systems work together to supply energy for muscle activity based on the individual's cardiorespiratory fitness level by selectively recruiting ATP, anaerobic, and aerobic systems depending on the amount of oxygen available during cellular respiration to generate ATP for the muscles.",
    [
      "and intensity and by the individual's cardiorespiratory fitness level. Three exercise energy systems can be selectively recruited, depending on the amount of oxygen available, as part of the cellular respiration process to generate the ATP for the muscles. They are ATP, the anaerobic system and the aerobic system. ATP is the usable form of chemical energy for muscular activity. It is stored in most cells, particularly in muscle cells. Other forms of chemical energy, such as those available from food, must be transformed into ATP before they can be utilized by the muscle cells. Since energy is released when ATP",
      "to supply its self with enough energy to support all the corresponding changes in the body at work. The 3 energy systems involved in exercise are the Phosphogenic, Anaerobic and Aerobic energy pathways. The simultaneous action of these three energy pathways prioritizes one specific pathway over the others depending on the type of exercise an individual is partaking in. This differential prioritization is based on the duration and intensity of the particular exercise being performed. The variable usage of these energy pathways is central to the mechanisms that allow for long, sustained exercise, such as a marathon running, to be",
      "in such a way that the energy released by the one is always used by the other. Three methods can synthesize ATP: Aerobic and anaerobic systems usually work concurrently. When describing activity, it is not a question of which energy system is working, but which predominates. The term metabolism refers to the various series of chemical reactions that take place within the body. Aerobic refers to the presence of oxygen, whereas anaerobic means with series of chemical reactions that does not require the presence of oxygen. The ATP-CP series and the lactic acid series are anaerobic, whereas the oxygen series"
    ]
  ],
  [
    "During exercise, the breakdown of glycogen in muscle cells is initiated and regulated by hormonal responses, such as the fight-or-flight response, and by the need for immediate energy for muscle contraction.",
    [
      "the fight-or-flight response and the regulation of glucose levels in the blood. In myocytes (muscle cells), glycogen degradation serves to provide an immediate source of glucose-6-phosphate for glycolysis, to provide energy for muscle contraction. In hepatocytes (liver cells), the main purpose of the breakdown of glycogen is for the release of glucose into the bloodstream for uptake by other cells. The phosphate group of glucose-6-phosphate is removed by the enzyme glucose-6-phosphatase, which is not present in myocytes, and the free glucose exits the cell via GLUT2 facilitated diffusion channels in the hepatocyte cell membrane. Glycogenolysis is regulated hormonally in response",
      "the other hand, in the skeletal muscle, glycogen is used as an energy source to perform muscular contraction during exercise. The different functions of glycogen in muscles or liver make the regulation mechanisms of its metabolism differ in each tissue. These mechanisms are based mainly in the differences on structure and on the regulations of the enzymes that catalyze the way for its synthesis, glycogen synthase (GS), and for its degradation, glycogen phosphorylase (GF). Glycogenin is the initiator of the glycogen biosynthesis. This protein is a glycosyl transferase that have the ability of autoglycosilation using UDP-glucose, which helps in the",
      "the process of glycolysis breaks down the sugars from carbohydrates for energy without the use of oxygen. This type of exercise occurs in physical activity such as power sprints, strength resistances and quick explosive movement where the muscles are being used for power and speed, with short-time energy use. After this type of exercise, there is a need to refill glycogen storage sites in the body (the long simple sugar chains in the body that store energy), although they are not likely fully depleted. To compensate for this glycogen reduction, athletes will often take in large amounts of carbohydrates, immediately"
    ]
  ],
  [
    "The child who is provided with protection and a sense of security by the male psychologist is most likely to develop a secure attachment to him.",
    [
      "met psychological, emotional, physical and attachment problems can arise. This was shown in the Spitz orphan study. (Shepard, 2013) \"Attachment theory states that a child's first relationship is a love relationship that will have profound, long-lasting effects on an individual's subsequent development.\" (Colin, 1991) The closeness of the child to the person providing protection and a sense of security will becomes this figure to the child. This figure lays the foundation for the child to be able to form other secure relationships in the future. There are four main types of attachment, including: secure, anxious/avoidant, anxious/ambivalent and disorganized/disoriented. In securely",
      "same questionnaire used in other countries, the authoritative style continues to be the best one for children. Attachment theory was created by John Bowlby and Mary Ainsworth. This theory focuses on the attachment of parents and children (specifically through infancy), and the aspect of children staying in close distance with their caregiver who will protect them from the outside world. This theory includes to Possible Types of Attachment: Although research for Attachment Theory is primarily focused on infancy and early childhood, research shows that there are affects on adolescent and parent relationships based on whether they have a secure or",
      "factors (i.e., why development happens), which are the focus of mainstream evolutionary psychology. Attachment theory, originally developed by John Bowlby, focuses on the importance of open, intimate, emotionally meaningful relationships. Attachment is described as a biological system or powerful survival impulse that evolved to ensure the survival of the infant. A child who is threatened or stressed will move toward caregivers who create a sense of physical, emotional and psychological safety for the individual. Attachment feeds on body contact and familiarity. Later Mary Ainsworth developed the Strange Situation protocol and the concept of the secure base. Theorists have proposed four"
    ]
  ],
  [
    "Creatine is synthesized from cyanamide and sarcosine in an industrial setting by stabilizing cyanamide with a buffer, such as monosodium phosphate, to avoid problematic impurities like chloroacetic acid, iminodiacetic acid, and dihydrotriazine.",
    [
      "of cyanamide with sarcosine In the industrial synthesis of creatine: This synthesis route mostly avoids problematic impurities like chloroacetic acid, iminodiacetic acid or dihydrotriazine that occur in other routes. The physiological precursor guanidinoacetic is obtained analogously by reacting cyanamide with glycine. Since the mid-1960s there are methods to stabilize cyanamide in order to make it available on an industrial scale. Due to the strong affinity towards self-condensation in alkaline media (see above) solutions of cyanamide are stabilized by the addition of 0.5 wt% of monosodium phosphate as buffer. Solid cyanamide is produced by careful evaporation of the solvent and subsequent",
      "also be converted to glycocyamine with \"S\"-methylisothiourea or with \"O\"-alkylisoureas as a guanylation agent. The recent patent literature describes the synthesis of glycocyamine by catalytic oxidation of ethanolamine to glycine and subsequent reaction with cyanamide in aqueous solution in high yield, analogous to the synthesis of creatine starting from 2-methylaminoethanol via sarcosine. This synthetic route suppresses the formation of toxic dihydrotriazine and other undesired by-products (such as iminodiacetic acid). Industrially produced guanidinoacetic acid is sold as a white (to yellowish) fine powder, which is granulated for improve handling, metering and uptake with starch into aggregates with a mean diameter of",
      "Creatinine Creatinine ( or ; from ) is a breakdown product of creatine phosphate in muscle, and is usually produced at a fairly constant rate by the body (depending on muscle mass). Serum creatinine (a blood measurement) is an important indicator of renal health because it is an easily measured byproduct of muscle metabolism that is excreted unchanged by the kidneys. Creatinine itself is produced via a biological system involving creatine, phosphocreatine (also known as creatine phosphate), and adenosine triphosphate (ATP, the body's immediate energy supply). Creatine is synthesized primarily in the liver from the methylation of glycocyamine (guanidino acetate,"
    ]
  ],
  [
    "response helps to calm the body down and return it to a state of balance and relaxation.",
    [
      "Reaction - The body executes the \u201cFight-or-flight\u201d reaction to get the body out of danger quickly. When the timing between the \"threat\" and the resolution of the \"threat\" are close, the \u201cfight-or-flight\" reaction is executed, the \"threat\" is handled, and the body returns to its previous state (taking care of the business of life - digestion, relaxation, tissue repair etc.). The body has evolved to stay in this mode for only a short time. Chronic Stress State - When the timing between the \"threat\" and the resolution of the \"threat\" are more distant (the \"threat\" or the perception of \"threat\"",
      "in the body's stress-related mechanisms. Whether one should interpret these mechanisms as the body's response to a stressor or embody the act of stress itself is part of the ambiguity in defining what exactly stress is. The central nervous system works closely with the body's endocrine system to regulate these mechanisms. The sympathetic nervous system becomes primarily active during a stress response, regulating many of the body's physiological functions in ways that ought to make an organism more adaptive to its environment. Below there follows a brief biological background of neuroanatomy and neurochemistry and how they relate to stress. Stress,",
      "Stress (biology) Physiological or biological stress is an organism's response to a stressor such as an environmental condition. Stress is the body's method of reacting to a condition such as a threat, challenge or physical and psychological barrier. Stimuli that alter an organism's environment are responded to by multiple systems in the body. The autonomic nervous system and hypothalamic-pituitary-adrenal (HPA) axis are two major systems that respond to stress. The sympathoadrenal medullary (SAM) axis may activate the fight-or-flight response through the sympathetic nervous system, which dedicates energy to more relevant bodily systems to acute adaptation to stress, while the parasympathetic"
    ]
  ],
  [
    "The Task Force on Hate Crimes recommended developing regulations to implement the \"Hate Crimes Reporting Act\", coordinating training efforts, increasing submission of hate crime data, and working with community organizations and victims' groups to reduce prejudice and hate crimes.",
    [
      "unit of the State Police must also collect, summarize and report hate crime data to the state Attorney General and to several legislative committees. The reports are available on public record. In 1991, the Governor created the Task Force on Hate Crimes. The task force's principal tasks are (1) developing regulations to implement the \"Hate Crimes Reporting Act\", (2) coordinating training efforts, (3) increasing submission of hate crime data, and (4) working with community organizations and victims' groups. Initiatives for 2000 include pilot programs in high schools, youth diversion programs, a new correctional diversity awareness program, outreach coordination, a victimization",
      "allowed the government to count the incidence of hate crimes based on religion, race, national origin, and sexual orientation. However, a sentence was added onto the end of bill stating that federal funds should not be used to \"promote or encourage homosexuality\". According to FBI statistics, of the over 113,000 hate crimes since 1991, 55% were motivated by racial bias, 17% by religious bias, 14% sexual orientation bias, 14% ethnicity bias, and 1% disability bias. Although not necessarily on the same scale as Matthew Shepard's murder, violent incidences against gays and lesbians occur frequently. Gay and lesbian people are often",
      "term \"hate crime\" is somewhat misleading since it assumes there is a hateful motivation which is not present in many occasions; in her view, laws to punish people who commit hate crimes may not be the best remedy for preventing them because the threat of future punishment does not usually deter such criminal acts. Some on the political left have been critical of hate crime laws for expanding the criminal justice system and dealing with violence against minority groups through punitive measures. Hate crime A hate crime (also known as a bias-motivated crime or bias crime) is a prejudice-motivated crime"
    ]
  ],
  [
    "The compounds that provide the energy for muscle contraction include adenosine triphosphate (ATP), glycogen, and creatine phosphate.",
    [
      "They include molecules such as adenosine triphosphate (ATP), glycogen and creatine phosphate. ATP binds to the myosin head and causes the \u2018ratchetting\u2019 that results in contraction according to the sliding filament model. Creatine phosphate stores energy so ATP can be rapidly regenerated within the muscle cells from adenosine diphosphate (ADP) and inorganic phosphate ions, allowing for sustained powerful contractions that last between 5\u20137 seconds. Glycogen is the intramuscular storage form of glucose, used to generate energy quickly once intramuscular creatine stores are exhausted, producing lactic acid as a metabolic byproduct. Substrate shortage is one of the causes of metabolic fatigue.",
      "can require a breakdown, and also to build in the rest period, which occurs during the strengthening phase associated with muscular contraction. ATP is composed of adenine, a nitrogen containing base, ribose, a five carbon sugar (collectively called adenosine), and three phosphate groups. ATP is a high energy molecule because it stores large amounts of energy in the chemical bonds of the two terminal phosphate groups. The breaking of these chemical bonds in the Krebs Cycle provides the energy needed for muscular contraction. Because the ratio of hydrogen to oxygen atoms in all carbohydrates is always the same as that",
      "within the muscle cells themselves. New research from scientists at Columbia University suggests that muscle fatigue is caused by calcium leaking out of the muscle cell. This makes less calcium available for the muscle cell. In addition, the Columbia researchers propose that an enzyme activated by this released calcium eats away at muscle fibers. Substrates within the muscle generally serve to power muscular contractions. They include molecules such as adenosine triphosphate (ATP), glycogen and creatine phosphate. ATP binds to the myosin head and causes the \u2018ratchetting\u2019 that results in contraction according to the sliding filament model. Creatine phosphate stores energy"
    ]
  ],
  [
    "are produced from genes through transcription and translation, resulting in the incorporation of different protein domains that contribute to protein stability and complex function.",
    [
      "proteins are transcribed and produced from blueprints in the cell, called genes. Mosaic proteins can be made when two adjacent genes are transcribed together and are therefore made into the same protein. This can benefit the protein because it makes the protein more stable and gives the protein a more complex function. For example, if the protein is an enzyme, it will be able to act more efficiently with its substrates. Additionally, these proteins are most utilized outside of the cell membrane or on the outer side of membrane proteins. This suggests that these multifunctional proteins played a part in",
      "This ensures that the body will build an immunity, and that next time the germ is introduced, it will be more equipped to fight it off. Mosaic proteins of the germs can be designed in order to maximize antibody production and quality. Mosaic protein Proteins are made up of amino acids which connect by peptide bonds and form into polypeptide chains. There are four levels of protein organization: primary structure- the amino acid sequences; secondary structure- different types of folded shapes (ex: alpha helices and beta sheets); tertiary structure- includes all the different folding that happens on one polypeptide chain;",
      "mosaic protein is a protein that is made up of different protein domains, giving the protein multiple functions. These proteins have quaternary structures, as they are made up of multiple tertiary structured protein domains. Protein domains can combine to form different types of proteins, creating a diversity of proteins. These domains are spread throughout the genome because they are mobile, which is why some domains can be found in a variety of proteins, even though they are seemingly unrelated. This also allows the domains to fold independently, and so they don't become deformed and unfolded in a new environment. All"
    ]
  ],
  [
    "Phosphocreatine plays a role in the muscle cell by serving as a rapidly mobilizable reserve of high-energy phosphates to recycle adenosine triphosphate, the energy currency of the cell.",
    [
      "Phosphocreatine Phosphocreatine, also known as creatine phosphate (CP) or PCr (Pcr), is a phosphorylated creatine molecule that serves as a rapidly mobilizable reserve of high-energy phosphates in skeletal muscle and the brain to recycle adenosine triphosphate, the energy currency of the cell. In the kidneys, the enzyme catalyzes the conversion of two amino acids \u2014 arginine and glycine \u2014 into guanidinoacetate (also called glycocyamine or GAA), which is then transported in the blood to the liver. A methyl group is added to GAA from the amino acid methionine by the enzyme GAMT, forming non-phosphorylated creatine. This is then released into",
      "use of phosphocreatine (PCr) through a reversible reaction with the enzyme creatine kinase (CK). In skeletal muscle, PCr concentrations may reach 20\u201335 mM or more. Additionally, in most muscles, the ATP regeneration capacity of CK is very high and is therefore not a limiting factor. Although the cellular concentrations of ATP are small, changes are difficult to detect because ATP is continuously and efficiently replenished from the large pools of PCr and CK. Creatine has the ability to increase muscle stores of PCr, potentially increasing the muscle\u2019s ability to resynthesize ATP from ADP to meet increased energy demands. Genetic deficiencies",
      "ATP (by cytoplasmic creatine kinase) to be used as energy for muscle contraction. In some vertebrates, arginine phosphate plays a similar role. Creatine phosphate shuttle The creatine phosphate shuttle is an intracellular energy shuttle which facilitates transport of high energy phosphate from muscle cell mitochondria to myofibrils. This is part of phosphocreatine metabolism. In mitochondria, ATP levels are very high as a result of glycolysis, TCA cycle, oxidative phosphorylation processes, whereas creatine phosphate levels are low. This makes conversion of creatine to phosphocreatine a highly favored reaction. Phosphocreatine is a very-high-energy compound. It then diffuses from mitochondria to myofibrils. In"
    ]
  ],
  [
    "Steroid hormone.",
    [
      "Adrenocortical hormones are considered steroid hormones because of the shared characteristic of a cholesterol backbone. The structures of different steroids differ by the types and locations of additional atoms on a cholesterol backbone. The cholesterol backbone consists of four hydrocarbon rings, three cyclohexane rings and one cyclopentane, that contribute to its insolubility in aqueous environments. However, the hydrophobic nature allows them to readily diffuse through the plasma membrane of cells. This is important to the function of steroid hormones as they rely on cellular response pathways to restore the homeostatic imbalance that initiated the hormone release. The synthesis of adrenocortical",
      "Hormone A hormone (from the Greek participle \u201c\u201d, \"to set in motion, urge on\") is any member of a class of signaling molecules produced by glands in multicellular organisms that are transported by the circulatory system to target distant organs to regulate physiology and behavior. Hormones have diverse chemical structures, mainly of 3 classes: eicosanoids, steroids, and amino acid/protein derivatives (amines, peptides, and proteins). The glands that secrete hormones comprise the endocrine signaling system. The term hormone is sometimes extended to include chemicals produced by cells that affect the same cell (autocrine or intracrine signalling) or nearby cells (paracrine signalling).",
      "unbound hormones when these are eliminated). The discovery of hormones and endocrine signaling occurred during studies of how the digestive system regulates its activities, as explained at \"Secretin \u00a7 Discovery\". Hormone A hormone (from the Greek participle \u201c\u201d, \"to set in motion, urge on\") is any member of a class of signaling molecules produced by glands in multicellular organisms that are transported by the circulatory system to target distant organs to regulate physiology and behavior. Hormones have diverse chemical structures, mainly of 3 classes: eicosanoids, steroids, and amino acid/protein derivatives (amines, peptides, and proteins). The glands that secrete hormones comprise"
    ]
  ],
  [
    "The term for the coding sequences of genes that specify the protein sequence is \"structural gene.\" (Structural gene A structural gene is a gene that codes for any RNA or protein product other than a regulatory factor)",
    [
      "the process of gene duplication and divergence. A gene is a sequence of DNA that contains genetic information and can influence the phenotype of an organism. Within a gene, the sequence of bases along a DNA strand defines a messenger RNA sequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and the amino-acid sequences of proteins is determined by the rules of translation, known collectively as the genetic code. The genetic code consists of three-letter 'words' called \"codons\" formed from a sequence of three nucleotides (e.g. ACT, CAG, TTT). In transcription, the",
      "added next during protein synthesis. With some exceptions, a three-nucleotide codon in a nucleic acid sequence specifies a single amino acid. The vast majority of genes are encoded with a single scheme (see the RNA codon table). That scheme is often referred to as the canonical or standard genetic code, or simply \"the\" genetic code, though variant codes (such as in human mitochondria) exist. While the \"genetic code\" determines a protein's amino acid sequence, other genomic regions determine when and where these proteins are produced according to various \"gene regulatory codes\". Efforts to understand how proteins are encoded began after",
      "Structural gene A structural gene is a gene that codes for any RNA or protein product other than a regulatory factor (i.e. regulatory protein). A term derived from the \"lac\" operon, structural genes are typically viewed as those containing sequences of DNA corresponding to the amino acids of a protein that will be produced, as long as said protein does not function to regulate gene expression. Structural gene products include enzymes and structural proteins. Also encoded by structural genes are non-coding RNAs, such as rRNAs and tRNAs (but excluding any regulatory miRNAs and siRNAs). In prokaryotes, structural genes of related"
    ]
  ],
  [
    "The transmission of knowledge through observation and imitation is NOT an example of the transmission of knowledge through symbolic culture.",
    [
      "the way \"knowledge\" is used in natural languages. He saw knowledge as a case of a family resemblance. Following this idea, \"knowledge\" has been reconstructed as a cluster concept that points out relevant features but that is not adequately captured by any definition. Symbolic representations can be used to indicate meaning and can be thought of as a dynamic process. Hence the transfer of the symbolic representation can be viewed as one ascription process whereby knowledge can be transferred. Other forms of communication include observation and imitation, verbal exchange, and audio and video recordings. Philosophers of language and semioticians construct",
      "networks adopt new ideas, practices and products. In anthropology, the concept of diffusion also explores the spread of ideas among cultures. Knowledge transfer is often used as a synonym for training. Furthermore, information should not be confused with knowledge, nor is it, strictly speaking, possible to \"transfer\" experiential knowledge to other people. Information might be thought of as facts or understood data; however, knowledge has to do with flexible and adaptable skills\u2014a person's unique ability to wield and apply information. This fluency of application is in part what differentiates information from knowledge. Knowledge tends to be both tacit and personal;",
      "is used by the World Intellectual Property Organization to refer to \"any form of artistic and literary expression in which traditional culture and knowledge are embodied. They are transmitted from one generation to the next, and include handmade textiles, paintings, stories, legends, ceremonies, music, songs, rhythms and dance.\" Tradition A tradition is a belief or behavior passed down within a group or society with symbolic meaning or special significance with origins in the past. Common examples include holidays or impractical but socially meaningful clothes (like lawyers' wigs or military officers' spurs), but the idea has also been applied to social"
    ]
  ],
  [
    "DNA methylation and histone modification can lead to a decrease in gene expression by altering the structure of chromatin, making it more difficult for transcription factors and RNA polymerase to access the DNA and transcribe the gene. (Histone methylation; Chromatin remodeling)",
    [
      "Histone methylation Histone methylation is a process by which methyl groups are transferred to amino acids of histone proteins that make up nucleosomes, which the DNA double helix wraps around to form chromosomes. Methylation of histones can either increase or decrease transcription of genes, depending on which amino acids in the histones are methylated, and how many methyl groups are attached. Methylation events that weaken chemical attractions between histone tails and DNA increase transcription, because they enable the DNA to uncoil from nucleosomes so that transcription factor proteins and RNA polymerase can access the DNA. This process is critical for",
      "the transcriptional potential of a cell that are not necessarily heritable. Examples of mechanisms that produce such changes are DNA methylation and histone modification, each of which alters how genes are expressed without altering the underlying DNA sequence. Gene expression can be controlled through the action of repressor proteins that attach to silencer regions of the DNA. DNA methylation turns a gene \"off\" \u2013 it results in the inability of genetic information to be read from DNA; removing the methyl tag can turn the gene back \"on\". Epigenetics has a strong influence on the development of an organism and can",
      "to epigenetic effects. There are several layers of regulation of gene expression. One way that genes are regulated is through the remodeling of chromatin. Chromatin is the complex of DNA and the histone proteins with which it associates. If the way that DNA is wrapped around the histones changes, gene expression can change as well. Chromatin remodeling is accomplished through two main mechanisms: Mechanisms of heritability of histone state are not well understood; however, much is known about the mechanism of heritability of DNA methylation state during cell division and differentiation. Heritability of methylation state depends on certain enzymes (such"
    ]
  ],
  [
    "ATP is generated through anaerobic metabolism by processes such as the ATP-CP series and the lactic acid series, and the energy released is used in the body for muscular activity through the hydrolysis of ATP into ADP and inorganic phosphate, releasing energy for cellular processes.",
    [
      "in such a way that the energy released by the one is always used by the other. Three methods can synthesize ATP: Aerobic and anaerobic systems usually work concurrently. When describing activity, it is not a question of which energy system is working, but which predominates. The term metabolism refers to the various series of chemical reactions that take place within the body. Aerobic refers to the presence of oxygen, whereas anaerobic means with series of chemical reactions that does not require the presence of oxygen. The ATP-CP series and the lactic acid series are anaerobic, whereas the oxygen series",
      "and intensity and by the individual's cardiorespiratory fitness level. Three exercise energy systems can be selectively recruited, depending on the amount of oxygen available, as part of the cellular respiration process to generate the ATP for the muscles. They are ATP, the anaerobic system and the aerobic system. ATP is the usable form of chemical energy for muscular activity. It is stored in most cells, particularly in muscle cells. Other forms of chemical energy, such as those available from food, must be transformed into ATP before they can be utilized by the muscle cells. Since energy is released when ATP",
      "bonds\". The hydrolysis of ATP into ADP and inorganic phosphate releases 30.5 kJ/mol of enthalpy, with a change in free energy of 3.4 kJ/mol. The energy released by cleaving either a phosphate (P) or pyrophosphate (PP) unit from ATP at standard state of 1 M are: These abbreviated equations can be written more explicitly (R = adenosyl): With a typical intracellular concentration of 1\u201310 mM, ATP is abundant. The dephosphorylation of ATP and rephosphorylation of ADP and AMP occur repeatedly in the course of aerobic metabolism. ATP can be produced by a number of distinct cellular processes; the three main"
    ]
  ],
  [
    "RNA (except some cells such as mature red blood cells) does not form part of DNA.",
    [
      "molecules are probably the largest individual molecules known. Well-studied biological nucleic acid molecules range in size from 21 nucleotides (small interfering RNA) to large chromosomes (human chromosome 1 is a single molecule that contains 247 million base pairs). In most cases, naturally occurring DNA molecules are double-stranded and RNA molecules are single-stranded. There are numerous exceptions, however\u2014some viruses have genomes made of double-stranded RNA and other viruses have single-stranded DNA genomes, and, in some circumstances, nucleic acid structures with three or four strands can form. Nucleic acids are linear polymers (chains) of nucleotides. Each nucleotide consists of three components: a",
      "RNA (except some cells such as mature red blood cells), while viruses contain either DNA or RNA, but usually not both. The basic component of biological nucleic acids is the nucleotide, each of which contains a pentose sugar (ribose or deoxyribose), a phosphate group, and a nucleobase. Nucleic acids are also generated within the laboratory, through the use of enzymes (DNA and RNA polymerases) and by solid-phase chemical synthesis. The chemical methods also enable the generation of altered nucleic acids that are not found in nature, for example peptide nucleic acids. Nucleic acids are generally very large molecules. Indeed, DNA",
      "Nucleic acid molecules are usually unbranched, and may occur as linear and circular molecules. For example, bacterial chromosomes, plasmids, mitochondrial DNA, and chloroplast DNA are usually circular double-stranded DNA molecules, while chromosomes of the eukaryotic nucleus are usually linear double-stranded DNA molecules. Most RNA molecules are linear, single-stranded molecules, but both circular and branched molecules can result from RNA splicing reactions. The total amount of pyrimidine is equal to the total amount of purines. The diameter of the helix is about 20A. One DNA or RNA molecule differs from another primarily in the sequence of nucleotides. Nucleotide sequences are of"
    ]
  ],
  [
    "Gold (Au) possesses 10 electrons with a quantum number l = 2.",
    [
      "numbers often describe specifically the energy levels of electrons in atoms, but other possibilities include angular momentum, spin, etc. An important family is flavour quantum numbers \u2013 internal quantum numbers which determine the type of a particle and its interactions with other particles through the forces. Any quantum system can have one or more quantum numbers; it is thus difficult to list all possible quantum numbers. The question of \"how many quantum numbers are needed to describe any given system\" has no universal answer. Hence for each system one must find the answer for a full analysis of the system.",
      "only by particle accelerators. Although gold is the most noble of the noble metals, it still forms many diverse compounds. The oxidation state of gold in its compounds ranges from \u22121 to +5, but Au(I) and Au(III) dominate its chemistry. Au(I), referred to as the aurous ion, is the most common oxidation state with soft ligands such as thioethers, thiolates, and tertiary phosphines. Au(I) compounds are typically linear. A good example is Au(CN), which is the soluble form of gold encountered in mining. The binary gold halides, such as AuCl, form zigzag polymeric chains, again featuring linear coordination at Au.",
      "sets of quantum numbers may be used for the description of the same system. Four quantum numbers can describe an electron in an atom completely. As per the following model, these nearly-compatible quantum numbers are: The spin-orbital interaction, however, relates these numbers. Thus, a complete description of the system can be given with fewer quantum numbers, if orthogonal choices are made for these basis vectors. Many different models have been proposed throughout the history of quantum mechanics, but the most prominent system of nomenclature spawned from the Hund-Mulliken molecular orbital theory of Friedrich Hund, Robert S. Mulliken, and contributions from"
    ]
  ],
  [
    "The article does not mention weight loss as a benefit of sauna use.",
    [
      "least 48 scientific or medical articles on \"Infrared Sauna\", 327 on \"Far Infrared Therapy\" and over twenty one thousand on \"Infrared Therapy\" some of which investigate health benefits, such as prophylactic physiotherapeutic treatment schemes, appetite loss and depression, cardio vascular system improvements, and Photothermal Therapy. A study of patients with rheumatoid arthritis and ankylosing spondylitis showed a reduction in pain, stiffness and fatigue during infrared therapy, but since results from the study did not reach statistical significance there is no clear or certain relationship between reducing symptoms and using infrared therapy. \"Toxins\" and \"toxicity\" are general terms used in alternative",
      "Infrared sauna \"Infrared therapy\" uses infrared heaters to emit infrared light experienced as radiant heat which is absorbed by the surface of the skin. Saunas heat the body primarily by conduction and convection from the heated air and by radiation of the heated surfaces in the sauna room. Because of this, infrared therapy is not considered a sauna by Finnish sauna societies. Nevertheless, \"Infrared Sauna\" and \"Infrared Therapy\" are respectively a methodology and a terminology in the application of the infrared electromagnetic field spectrum to the human body. There are claims of detoxification which lack scientific evidence. There are at",
      "bathing is more effective than exercise, relaxation therapy, or mudpacks. Most of the studies on balneotherapy have methodological flaws and are not reliable. A 2009 review of all published clinical evidence concluded that existing research is not sufficiently strong to draw firm conclusions about the efficacy of balneotherapy. Balneotherapy Balneotherapy ( \"bath\") is the presumed benefit from disease by bathing, a traditional medicine technique usually practiced at spas. While it is considered distinct from hydrotherapy, there are some overlaps in practice and in underlying principles. Balneotherapy may involve hot or cold water, massage through moving water, relaxation, or stimulation. Many"
    ]
  ],
  [
    "As the number of sprints increases, fatigue sets in and hinders maximal power output in muscles, affecting acceleration and potentially leading to changes in running technique to maintain high velocities.",
    [
      "Biomechanics of sprint running Sprinting involves a quick acceleration phase followed by a velocity maintenance phase. During the initial stage of sprinting, the runners have their upper body tilted forward in order to direct ground reaction forces more horizontally. As they reach their maximum velocity, the torso straightens out into an upright position. The goal of sprinting is to reach and maintain high top speeds to cover a set distance in the shortest possible time. A lot of research has been invested in quantifying the biological factors and mathematics that govern sprinting. In order to achieve these high velocities, it",
      "technology companies like I Measure U are creating solutions using biomechanics data to analyse the gait of a runner in real time and provide feedback on how to change the running technique to reduce injury risk. Biomechanics of sprint running Sprinting involves a quick acceleration phase followed by a velocity maintenance phase. During the initial stage of sprinting, the runners have their upper body tilted forward in order to direct ground reaction forces more horizontally. As they reach their maximum velocity, the torso straightens out into an upright position. The goal of sprinting is to reach and maintain high top",
      "considering the cost of running against wind resistance (formula_13), which is known to be: We combine the two equations to arrive at: Where formula_7 is the acceleration of the runner's body, formula_17 the forward acceleration, formula_18 the acceleration of gravity, formula_19 a proportionality constant and formula_20 the velocity. Fatigue is a prominent factor in sprinting, and it is already widely known that it hinders maximal power output in muscles, but it also affects the acceleration of runners in the ways listed below. A study on muscle coordination in which subjects performed repeated 6-second cycling sprints, or intermittent sprints of short"
    ]
  ],
  [
    "The final velocity of the water as it exits the fire hose with a nozzle that decreases the area of the water stream depends on the flow rate and pressure of the water being discharged.",
    [
      "to apply water to a blaze, before leaving it in place to attend to other tasks. A deluge gun can discharge 2,000 US gallons (7,600 liters) per minute, or more. A master stream is a fire service term for a water stream of 350 US gallons (1,300 liters) per minute or greater. It is delivered by a master stream device, such as a deck gun, deluge gun, or fire monitor. Master streams are often found at the end of aerial ladders, tele-squirt nozzles, or monitor nozzles. The high pressure that they require renders them unsuitable for handline use. A master",
      "500 L/min for each fire hose. The aim is to absorb as much heat as possible at the beginning to stop the expansion of the fire and to reduce the smoke. If the flow is too low, the cooling is insufficient, and the steam that is produced can burn firefighters (the drop of pressure is too small and the vapor is pushed back in their direction). Although it may seem paradoxical, the use of a strong flow with an efficient fire hose and an efficient strategy (diffuse spray, small droplets) requires a smaller amount of water. This is because once",
      "of fire apparatus. Fire hose A fire hose (or firehose) is a high-pressure hose that carries water or other fire retardant (such as foam) to a fire to extinguish it. Outdoors, it attaches either to a fire engine or a fire hydrant. Indoors, it can permanently attach to a building's standpipe or plumbing system. The usual working pressure of a firehose can vary between while per the NFPA 1961 Fire Hose Standard, its bursting pressure is in excess of 110 bar, (11,000kPa; 1600psi) After use, a fire hose is usually hung to dry, because standing water that remains in a"
    ]
  ],
  [
    "NADH and FADH2 contribute to the production of ATP through oxidative phosphorylation by delivering their electrons to the inner membranes of the mitochondrion, where they power the synthesis of ATP.",
    [
      "of electrons, is used to reduce NAD+ and FAD to NADH and FADH, respectively. NADH and FADH contain the stored energy harnessed from the initial glucose molecule and is used in the electron transport chain where the bulk of the ATP is produced. The last process in aerobic respiration is oxidative phosphorylation, also known as the electron transport chain. Here NADH and FADH, which contain the energy in the form of electrons, deliver their electrons to the inner membranes of the mitochondrion to power the production of ATP. Oxidative phosphorylation contributes the majority of the ATP produced, compared to glycolysis",
      "total NADH+ molecules allow the rejuvenation of 30 ATP, and 2 FADH+ molecules allow for 4 ATP molecules to be rejuvenated (the total being 34 from oxidative phosphorylation, plus the 4 from the previous 2 stages meaning a total of 38 ATP being produced during the aerobic system). The NADH+ and FADH+ get oxidized to allow the NAD and FAD to return to be used in the aerobic system again, and electrons and hydrogen ions are accepted by oxygen to produce water, a harmless byproduct. Bioenergetic systems Bioenergetic systems are metabolic processes that relate to the flow of energy in",
      "by oxidative phosphorylation. The oxidation of NADH results in the synthesis of 2\u20133 equivalents of ATP, and the oxidation of one FADH yields between 1\u20132 equivalents of ATP. The majority of cellular ATP is generated by this process. Although the citric acid cycle itself does not involve molecular oxygen, it is an obligately aerobic process because O is used to recycle the NADH and FADH. In the absence of oxygen, the citric acid cycle ceases. The generation of ATP by the mitochondrion from cytosolic NADH relies on the malate-aspartate shuttle (and to a lesser extent, the glycerol-phosphate shuttle) because the"
    ]
  ],
  [
    "The energy released from the breakdown of high-energy phosphates can sustain maximal exertion exercise for approximately the first 30 seconds. (Phosphogenic (ATP-PC) anaerobic energy pathway)",
    [
      "High-energy phosphate High-energy phosphate can mean one of two things: High-energy phosphate bonds are pyrophosphate bonds, acid anhydride linkages formed by taking phosphoric acid derivatives and dehydrating them. As a consequence, the hydrolysis of these bonds is exergonic under physiological conditions, releasing energy. Except for PP \u2192 2 P, these reactions are, in general, not allowed to go uncontrolled in the human cell but are instead coupled to other processes needing energy to drive them to completion. Thus, high-energy phosphate reactions can: The one exception is of value because it allows a single hydrolysis, ATP + HO \u2192 AMP +",
      "system. High energy phosphates are stored in limited quantities within muscle cells. Anaerobic glycolysis exclusively uses glucose (and glycogen) as a fuel in the absence of oxygen, or more specifically when ATP is needed at rates that exceed those provided by aerobic metabolism. The consequence of such rapid glucose breakdown is the formation of lactic acid (or more appropriately, its conjugate base lactate at biological pH levels). Physical activities that last up to about thirty seconds rely primarily on the former, . Beyond this time both aerobic and anaerobic glycolysis-based metabolic systems begin to predominate. The by-product of anaerobic glycolysis,",
      "maintained. The Phosphogenic (ATP-PC) anaerobic energy pathway restores ATP after its breakdown via Creatine Phosphate stored in skeletal muscle. This pathway is anaerobic because it does not require oxygen to synthesize or utilize ATP. ATP restoration only lasts for approximately the first 30 seconds of exercise. This rapid rate of ATP production is essential at the onset of exercise. The amount of creatine phosphate and ATP stored in the muscle is small, readily available, and is utilized quickly due these two factors. An example of exercise that would primarily use this energy pathway could be weight lifting or performing sprints."
    ]
  ],
  [
    "In genetic sex-determination systems, an organism's sex is determined by the genome it inherits, typically through the presence or absence of sex chromosomes.",
    [
      "intersex individuals are unusual cases and are not typically fertile in both male and female aspects. In genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring. Humans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible",
      "Sex-determination system A sex-determination system is a biological system that determines the development of sexual characteristics in an organism. Most organisms that create their offspring using sexual reproduction have two sexes. Occasionally, there are hermaphrodites in place of one or both sexes. There are also some species that are only one sex due to parthenogenesis, the act of a female reproducing without fertilization. In many species, sex determination is genetic: males and females have different alleles or even different genes that specify their sexual morphology. In animals this is often accompanied by chromosomal differences, generally through combinations of XY, ZW,",
      "as female, as humans have an XY sex-determination system. X0 sex-determination system The X0 sex-determination system is a system that determines the sex of offspring among: In this system, there is only one sex chromosome, referred to as X. Males only have one X chromosome (X0), while females have two (XX). The zero (sometimes, the letter O) signifies the lack of a second X. Maternal gametes always contain an X chromosome, so the sex of the animals\u2019 offspring depends on whether a sex chromosome is present in the male gamete. Its sperm normally contain either one X chromosome or no"
    ]
  ],
  [
    "Exercise can help control blood glucose levels in individuals with diabetes mellitus by increasing glucose disposal, reducing total plasma glucose concentrations, and increasing sensitivity to insulin for approximately 12-24 hours post-exercise, independent of insulin.",
    [
      "Exercise for diabetes: Exercise is a particularly potent tool for glucose control in those who have diabetes mellitus. In a situation of elevated blood glucose (hyperglycemia), moderate exercise can induce greater glucose disposal than appearance, thereby decreasing total plasma glucose concentrations. As stated above, the mechanism for this glucose disposal is independent of insulin, which makes it particularly well-suited for people with diabetes. In addition, there appears to be an increase in sensitivity to insulin for approximately 12\u201324 hours post-exercise. This is particularly useful for those who have type II diabetes and are producing sufficient insulin but demonstrate peripheral resistance",
      "normal glucose control. Although nobody is technically cured of diabetes, individuals can live normal lives without the fear of diabetic complications; however, regain of weight would assuredly result in diabetes signs and symptoms. Vigorous physical activity (such as exercise or hard labor) increases the body's demand for oxygen. The first-line physiologic response to this demand is an increase in heart rate, breathing rate, and depth of breathing. Oxygen consumption (VO) during exercise is best described by the Fick Equation: VO=Q x (a-vOdiff), which states that the amount of oxygen consumed is equal to cardiac output (Q) multiplied by the difference",
      "is accompanied by exercise. G.I. Diet: lowering the glycemic index of one's diet can improve the control of diabetes. This includes avoidance of such foods as potatoes cooked in certain ways and white bread. It instead favors multi-grain and sourdough breads, legumes and whole grains that are converted more slowly to glucose in the bloodstream. Low Carb Diet: It has been suggested that the removal of carbohydrates from the diet and replacement with fatty foods such as nuts, seeds, meats, fish, oils, eggs, avocados, olives, and vegetables may help reverse diabetes. Fats would become the primary calorie source for the"
    ]
  ],
  [
    "Base excision repair (BER) is responsible for removing incorrect bases shortly after the RNA primer is removed during DNA replication.",
    [
      "be processed by either short-patch (where a single nucleotide is replaced) or long-patch BER (where 2\u201310 new nucleotides are synthesized). Single bases in DNA can be chemically damaged by a variety of mechanisms, the most common ones being deamination, oxidation, and alkylation. These modifications can affect the ability of the base to hydrogen-bond, resulting in incorrect base-pairing, and, as a consequence, mutations in the DNA. For example, incorporation of adenine across from 8-oxoguanine (right) during DNA replication causes a G:C base pair to be mutated to T:A. Other examples of base lesions repaired by BER include: In addition to base",
      "Base excision repair In biochemistry and genetics, base excision repair (BER) is a cellular mechanism that repairs damaged DNA throughout the cell cycle. It is responsible primarily for removing small, non-helix-distorting base lesions from the genome. The related nucleotide excision repair pathway repairs bulky helix-distorting lesions. BER is important for removing damaged bases that could otherwise cause mutations by mispairing or lead to breaks in DNA during replication. BER is initiated by DNA glycosylases, which recognize and remove specific damaged or inappropriate bases, forming AP sites. These are then cleaved by an AP endonuclease. The resulting single-strand break can then",
      "Thus, this reaffirmed that Pol I was more likely to be involved in repairing DNA damage rather than DNA replication. In the replication process, RNase H removes the RNA primer (created by primase) from the lagging strand and then polymerase I fills in the necessary nucleotides between the Okazaki fragments (see \"DNA replication\") in a 5'\u21923' direction, proofreading for mistakes as it goes. It is a template-dependent enzyme\u2014it only adds nucleotides that correctly base pair with an existing DNA strand acting as a template. It is crucial that these nucleotides are in the proper orientation and geometry to base pair"
    ]
  ],
  [
    "The statement \"A logically implies the negation of B\" is logically false.",
    [
      "false. One statement logically implies another when it is logically incompatible with the negation of the other. A statement is logically true if, and only if its opposite is logically false. The opposite statements must contradict one another. In this way all logical connectives can be expressed in terms of preserving logical truth. The logical form of a sentence is determined by its semantic or syntactic structure and by the placement of logical constants. Logical constants determine whether a statement is a logical truth when they are combined with a language that limits its meaning. Therefore, until it is determined",
      "not be untrue and no situation could arise which would cause us to reject a logical truth. It must be true in every sense of intuition, practices, and bodies of beliefs. However, it is not universally agreed that there are any statements which are \"necessarily\" true. A logical truth is considered by some philosophers to be a statement which is true in all possible worlds. This is contrasted with facts (which may also be referred to as \"contingent claims\" or \"synthetic claims\") which are true in \"this\" world, as it has historically unfolded, but which is not true in at",
      "A logically implies a sentence B then this can be discovered (for example, by searching for a proof until one is found, using some effective, sound, complete proof system). However, if A does not logically imply B, this does not mean that A logically implies the negation of B. There is no effective procedure that, given formulas A and B, always correctly decides whether A logically implies B. A rule of inference states that, given a particular formula (or set of formulas) with a certain property as a hypothesis, another specific formula (or set of formulas) can be derived as"
    ]
  ],
  [
    "The McDonaldization of society impacts human experiences and relationships by imposing a rationalized structure on interactions and thinking, leading to an increasingly rationalized world.",
    [
      "only represent the process of rationalization, the structure they impose on human interaction and thinking furthers the process, leading to an increasingly rationalized world. The process affects all aspects of everyday life. The McDonaldization of Society The McDonaldization of Society is a 1993 book by sociologist George Ritzer. Ritzer suggests that in the later part of the 20th century the socially-structured form of the fast-food restaurant has become the organizational force representing and extending the process of rationalization into the realm of everyday interaction and individual identity. McDonald's of the 1990s serves as the case model. The book introduced the",
      "The McDonaldization of Society The McDonaldization of Society is a 1993 book by sociologist George Ritzer. Ritzer suggests that in the later part of the 20th century the socially-structured form of the fast-food restaurant has become the organizational force representing and extending the process of rationalization into the realm of everyday interaction and individual identity. McDonald's of the 1990s serves as the case model. The book introduced the term McDonaldization to learned discourse as a way to describe a social process which produces \"mind-numbing sameness\", according to a 2002 review of a related academic text. In \"McDonaldization\" Ritzer expands and",
      "While Wal-Marts have attracted considerable criticism for effectively displacing more traditional stores, these subjective social-value concerns have held minimal effectiveness in limiting expansion of the enterprise, particularly in more rationalized nations, due to the preferences of the public for lower prices over the advantages sociologists claim for more traditional stores. The sociologist George Ritzer has used the term McDonaldization to refer, not just to the actions of the fast food restaurant, but to the general process of rationalization. Ritzer distinguishes four primary components of McDonaldization: As capitalism itself is a rationalized economic policy, so is the process of commercialization it"
    ]
  ],
  [
    "The descending loop of Henle of the nephron of the kidney is permeable to water and noticeably less permeable to salt.",
    [
      "Descending limb of loop of Henle Within the nephron of the kidney, the descending limb of loop of Henle is the portion of the renal tubule constituting the first part of the loop of Henle. The permeability is as follows: Also, the medullary interstitium is highly concentrated (because of the activity of the ascending limb), leading to a strong osmotic gradient from the descending limb to the medulla. Because of these factors, the concentration of the urine increases dramatically in the descending limb. Osmolality can reach up to 1400 mOsmol/kg by the end of the descending limb. The epithelium of",
      "into the medulla as the descending limb, and then returns to the cortex as the ascending limb to empty into the distal convoluted tubule. The primary role of the loop of Henle is to concentrate the salt in the interstitium, the tissue surrounding the loop. Considerable differences aid in distinguishing the descending and ascending limbs of the loop of Henle. The descending limb is permeable to water and noticeably less permeable to salt, and thus only indirectly contributes to the concentration of the interstitium. As the filtrate descends deeper into the hypertonic interstitium of the renal medulla, water flows freely",
      "duct in the collecting duct system. Water present in the filtrate in the papillary duct flows through aquaporin channels out of the duct, moving passively down its concentration gradient. This process reabsorbs water and creates a concentrated urine for excretion. The loop of Henle can be divided into four parts: The tissue type of the loop is simple squamous epithelium. The \"thick\" and \"thin\" terminology does not refer to the size of the lumen, but to the size of the epithelial cells. The loop is also sometimes called the Nephron loop. The loop of Henle is supplied by blood in"
    ]
  ],
  [
    "The rate expression for a zero-order chemical reaction A \u2192 B that is second-order in A and zero-order in CO is \"r\" = \"k\"[A]^2[].",
    [
      "is second-order in and zero-order in CO, with rate equation \"r\" = \"k\"[]. This suggests that the rate is determined by a step in which two molecules react, with the CO molecule entering at another, faster, step. A possible mechanism in two elementary steps that explains the rate equation is: In this mechanism the reactive intermediate species is formed in the first step with rate \"r\" and reacts with CO in the second step with rate \"r\". However can also react with NO if the first step occurs in the \"reverse direction\" (NO + \u2192 2 ) with rate \"r\",",
      "where Ar indicates an aryl group. A reaction is said to be second order when the overall order is two. The rate of a second-order reaction may be proportional to one concentration squared formula_21, or (more commonly) to the product of two concentrations formula_22. As an example of the first type, the reaction NO + CO \u2192 NO + CO is second-order in the reactant NO and zero order in the reactant CO. The observed rate is given by formula_23, and is independent of the concentration of CO. For the rate proportional to a single concentration squared, the time dependence",
      "such an expression can look like For the reaction the observed rate equation (or rate expression) is: formula_7 As for many reactions, the experimental rate equation does not simply reflect the stoichiometric coefficients in the overall reaction: It is third order overall: first order in H and second order in NO, even though the stoichiometric coefficients of both reactants are equal to 2. In chemical kinetics, the overall reaction rate is often explained using a mechanism consisting of a number of elementary steps. Not all of these steps affect the rate of reaction; normally the slowest elementary step controls the"
    ]
  ],
  [
    " is useful for determining the structure of organic molecules by measuring changes in vibration of molecules, including stretching, bending or twisting motions, to identify the kinds of bonds or functional groups present.",
    [
      "Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule. The study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for",
      "Geology applications of Fourier transform infrared spectroscopy Fourier transform infrared spectroscopy (FTIR) is a spectroscopic technique that has been used for analyzing the fundamental molecular structure of geological samples in recent decades. As in other infrared spectroscopy, the molecules in the sample are excited to a higher energy state due to the absorption of infrared (IR) radiation emitted from the IR source in the instrument, which results in vibrations of molecular bonds. The intrinsic physicochemical property of each particular molecule determines its corresponding IR absorbance peak, and therefore can provide characteristic fingerprints of functional groups (e.g. C-H, O-H, C=O, etc.).",
      "correlates with the frequency of its vibrational mode. That is, increase in bond strength leads to corresponding frequency increase and vice versa. Infrared spectroscopy is a simple and reliable technique widely used in both organic and inorganic chemistry, in research and industry. It is used in quality control, dynamic measurement, and monitoring applications such as the long-term unattended measurement of CO concentrations in greenhouses and growth chambers by infrared gas analyzers. It is also used in forensic analysis in both criminal and civil cases, for example in identifying polymer degradation. It can be used in determining the blood alcohol content"
    ]
  ],
  [
    "The triple point temperature and pressure for a single-component system such as carbon monoxide is 5.2 bar and 217 K.",
    [
      "and vapour, can exist together in equilibrium (). If there is only one component, there are no degrees of freedom () when there are three phases. Therefore, in a single-component system, this three-phase mixture can only exist at a single temperature and pressure, which is known as a triple point. Here there are two equations , which are sufficient to determine the two variables T and p. In the diagram for CO the triple point is the point at which the solid, liquid and gas phases come together, at 5.2 bar and 217 K. It is also possible for other",
      "Triple point In thermodynamics, the triple point of a substance is the temperature and pressure at which the three phases (gas, liquid, and solid) of that substance coexist in thermodynamic equilibrium. For example, the triple point of mercury occurs at a temperature of \u221238.83440 \u00b0C and a pressure of 0.2 mPa. In addition to the triple point for solid, liquid, and gas phases, a triple point may involve more than one solid phase, for substances with multiple polymorphs. Helium-4 is a special case that presents a triple point involving two different fluid phases (lambda point). The triple point of water",
      "point cells of hydrogen, neon, oxygen, argon, mercury, and water for delineating six of its defined temperature points. This table lists the gas\u2013liquid\u2013solid triple points of several substances. Unless otherwise noted, the data come from the U.S. National Bureau of Standards (now NIST, National Institute of Standards and Technology). Triple point In thermodynamics, the triple point of a substance is the temperature and pressure at which the three phases (gas, liquid, and solid) of that substance coexist in thermodynamic equilibrium. For example, the triple point of mercury occurs at a temperature of \u221238.83440 \u00b0C and a pressure of 0.2 mPa."
    ]
  ],
  [
    "2MnO\u2084\u207b + 16I\u207b + 24H\u207a \u2192 2Mn\u00b2\u207a + 8IO\u2083\u207b + 12H\u2082O",
    [
      "the tetravalent state. Mn has been oxidised to 4+, and MnO(OH) appears as a brown precipitate. There is some uncertainty about whether the oxidised manganese is tetravalent or trivalent. Some sources claim that Mn(OH) is the brown precipitate, but hydrated MnO may also give the brown colour. The second part of the Winkler test reduces (acidifies) the solution. The precipitate will dissolve back into solution as the H reacts with the O and OH to form water. The acid facilitates the conversion by the brown, Manganese-containing precipitate of the Iodide ion into elemental Iodine. The Mn(SO) formed by the acid",
      "the balanced equation: Cu and Ag are in a 1:2 ratio. Now that the moles of Ag produced is known to be 0.5036 mol, we convert this amount to grams of Ag produced to come to the final answer: This set of calculations can be further condensed into a single step: For propane (CH) reacting with oxygen gas (O), the balanced chemical equation is: The mass of water formed if 120 g of propane (CH) is burned in excess oxygen is then Stoichiometry is also used to find the right amount of one reactant to \"completely\" react with the other",
      "one (g/g = 1), with the resulting amount in moles (the unit that was needed), as shown in the following equation, Stoichiometry is often used to balance chemical equations (reaction stoichiometry). For example, the two diatomic gases, hydrogen and oxygen, can combine to form a liquid, water, in an exothermic reaction, as described by the following equation: Reaction stoichiometry describes the 2:1:2 ratio of hydrogen, oxygen, and water molecules in the above equation. The molar ratio allows for conversion between moles of one substance and moles of another. For example, in the reaction the amount of water that will be"
    ]
  ],
  [
    "Knowing the power application P(t) can help obtain kinetic information in a chemical reaction involving the production of compound C by yielding the rate of production d[C] /dt, which opens the way for obtaining kinetic information.",
    [
      "only by integrating P(t) that Q can be obtained. By definition of \u0394H, one has therefore a direct link between P(t) and d[C]/dt, the rate of production of the compound C: where V is the volume of the cell where the reaction takes place. Therefore, knowing P(t) yields d[C] /dt, which opens the way for obtaining kinetic information. The kinetics of the bimolecular reaction A + B \u2194 C is governed by the two parameters k and k which leads to: This equation may be complemented with two conservation equations: [A] +[C] = [A] and [B] +[C] = [B] ,",
      "be obtained: \u0394G\u00b0 = \u0394H\u00b0 - T\u0394S\u00b0 = RT lnK (2) and thus also \u0394S\u00b0 = (\u0394H\u00b0 - \u0394G\u00b0 )/T. As important the four quantities \u0394H\u00b0, \u0394G\u00b0, \u0394S\u00b0 and K may be, particularly for drug design, it is important to emphasize that they are typical thermodynamic quantities that do not say anything about the kinetics of the reaction. An important feature of ITC is that the instrument does not measure directly the total heat Q evolved during the reaction, but its rate of production (or absorption) during the reaction. Therefore, the primary signal is the heat power: and this is",
      "has to be taken into account for obtaining K(T) from the Van 't Hoff equation. If, for example, a chemical process cannot be represented by equation (1), but needs to be represented by: A + B \u2194 C \u2194 C (6) all usual experimental methods used alone yield information on the global process A + B \u2194 C, which ignores the intermediate step. On the contrary, as shown in, \"kinITC\" used alone is able to dissect such a composite process both thermodynamically and kinetically. This means that \"kinITC\" allows dissecting the overall enthalpic term \u0394H (obtained by a classical use"
    ]
  ],
  [
    "The EPR spectrum of a solution of a rigid nitronyl nitroxide diradical with J << a will consist of one line.",
    [
      "isotropic hyperfine splitting pattern for a radical freely tumbling in a solution (isotropic system) can be predicted. While it is easy to predict the number of lines, the reverse problem, unraveling a complex multi-line EPR spectrum and assigning the various spacings to specific nuclei, is more difficult. In the often encountered case of \"I\" = 1/2 nuclei (e.g., H, F, P), the line intensities produced by a population of radicals, each possessing \"M\" equivalent nuclei, will follow Pascal's triangle. For example, the spectrum at the right shows that the three H nuclei of the CH radical give rise to 2\"MI\"",
      "+ 1 = 2(3)(1/2) + 1 = 4 lines with a 1:3:3:1 ratio. The line spacing gives a hyperfine coupling constant of \"a\" = 23 \"G\" for each of the three H nuclei. Note again that the lines in this spectrum are \"first derivatives\" of absorptions. As a second example, the methoxymethyl radical, HCOCH the OC\"H\" center will give an overall 1:2:1 EPR pattern, each component of which is further split by the three methoxy hydrogens into a 1:3:3:1 pattern to give a total of 3\u00d74 = 12 lines, a triplet of quartets. A simulation of the observed EPR spectrum",
      "peak is positive in first-derivative spectra, the high-frequency peak is negative, and the central peak is bipolar. Such situations are commonly observed in powders, and the spectra are therefore called \"powder-pattern spectra\". In crystals, the number of EPR lines is determined by the number of crystallographically equivalent orientations of the EPR spin (called \"EPR center\"). Since the source of an EPR spectrum is a change in an electron's spin state, the EPR spectrum for a radical (S = 1/2 system) would consist of one line. Greater complexity arises because the spin couples with nearby nuclear spins. The magnitude of the"
    ]
  ],
  [
    "Cobalt-60 is used in radiation therapy as a high activity gamma ray emitter, and it is produced by neutron irradiation of ordinary cobalt metal in a reactor.",
    [
      "radiation therapy. Cobalt-57 (Co-57 or Co) is a cobalt radioisotope most often used in medical tests, as a radiolabel for vitamin B uptake, and for the Schilling test. Cobalt-57 is used as a source in M\u00f6ssbauer spectroscopy and is one of several possible sources in X-ray fluorescence devices. Nuclear weapon designs could intentionally incorporate Co, some of which would be activated in a nuclear explosion to produce Co. The Co, dispersed as nuclear fallout, is sometimes called a cobalt bomb. Cobalt is essential to the metabolism of all animals. It is a key constituent of cobalamin, also known as vitamin",
      "Project during the World War II made possible the creation of artificial radioisotopes for radiotherapy. Cobalt-60, produced by neutron irradiation of ordinary cobalt metal in a reactor, is a high activity gamma ray emitter, emitting 1.17 and 1.33 MeV gamma rays with an activity of 44 TBq/g (about 1100 Ci/g). The main reason for its wide use in radiotherapy is that it has a longer half-life, 5.27 years, than many other gamma emitters. However this half life still requires cobalt sources to be replaced about every 5 years. In 1949, Dr. Harold E. Johns of the University of Saskatchewan sent",
      "where linacs are common. Cobalt-57 (Co-57 or Co) is a radioactive metal that is used in medical tests; it is used as a radiolabel for vitamin B uptake. It is useful for the Schilling test. Cobalt-60 (Co-60 or Co) is useful as a gamma ray source because it can be produced in predictable quantities, and for its high radioactive activity simply by exposing natural cobalt to neutrons in a reactor for a given time. The uses for industrial cobalt include: Cobalt-57 is used as a source in M\u00f6ssbauer spectroscopy of iron-containing samples. The electron capture decay of the Co forms"
    ]
  ],
  [
    "Both paramagnetism and ferromagnetism require exchange interactions between neighboring dipoles to spontaneously align or anti-align and form magnetic domains.",
    [
      "exchange between neighbouring dipoles, they will interact, and may spontaneously align or anti-align and form magnetic domains, resulting in ferromagnetism (permanent magnets) or antiferromagnetism, respectively. Paramagnetic behavior can also be observed in ferromagnetic materials that are above their Curie temperature, and in antiferromagnets above their N\u00e9el temperature. At these temperatures, the available thermal energy simply overcomes the interaction energy between the spins. In general, paramagnetic effects are quite small: the magnetic susceptibility is of the order of 10 to 10 for most paramagnets, but may be as high as 10 for synthetic paramagnets such as ferrofluids. In conductive materials, the",
      "unpaired. The Bohr\u2013van Leeuwen theorem proves that there cannot be any diamagnetism or paramagnetism in a purely classical system. The paramagnetic response has then two possible quantum origins, either coming from permanents magnetic moments of the ions or from the spatial motion of the conduction electrons inside the material. Both description are given below. For low levels of magnetization, the magnetization of paramagnets follows what is known as Curie's law, at least approximately. This law indicates that the susceptibility, formula_1, of paramagnetic materials is inversely proportional to their temperature, i.e. that materials become more magnetic at lower temperatures. The mathematical",
      "properly measured. Even for iron it is not uncommon to say that \"iron becomes a paramagnet\" above its relatively high Curie-point. In that case the Curie-point is seen as a phase transition between a ferromagnet and a 'paramagnet'. The word paramagnet now merely refers to the linear response of the system to an applied field, the temperature dependence of which requires an amended version of Curie's law, known as the Curie\u2013Weiss law: This amended law includes a term \u03b8 that describes the exchange interaction that is present albeit overcome by thermal motion. The sign of \u03b8 depends on whether ferro-"
    ]
  ],
  [
    "The ratio of concentrations of dimethylsulphoxide (DMSO) to acetonitrile (AN) in the mixture can be calculated using mass attenuation coefficients and the Beer-Lambert law applied to measurements at multiple wavelengths.",
    [
      "solution, the concentrations of each can be calculated using a light absorption analysis. First, the mass attenuation coefficients of each individual solute or solvent, ideally across a broad spectrum of wavelengths, must be measured or looked up. Second, the attenuation coefficient of the actual solution must be measured. Finally, using the formula the spectrum can be fitted using \"\u03c1\", \"\u03c1\", \u2026 as adjustable parameters, since \"\u03bc\" and each are functions of wavelength. If there are \"N\" solutes or solvents, this procedure requires \"at least\" \"N\" measured wavelengths to create a solvable system of simultaneous equations, although using more wavelengths gives",
      "alkylammonium and alkylamide ions in ammines mixtures, etc. Mixing ratio In chemistry and physics, the dimensionless mixing ratio is the abundance of one component of a mixture relative to that of all other components. The term can refer either to mole ratio or mass ratio. In atmospheric chemistry, mixing ratio usually refers to the mole ratio \"r\", which is defined as the amount of a constituent \"n\" divided by the total amount of all \"other\" constituents in a mixture: The mole ratio is also called amount ratio. If \"n\" is much smaller than \"n\" (which is the case for atmospheric",
      "is better to use linear least squares to determine the two amount concentrations from measurements made at more than two wavelengths. Mixtures containing more than two components can be analyzed in the same way, using a minimum of \"N\" wavelengths for a mixture containing \"N\" components. The law is used widely in infra-red spectroscopy and near-infrared spectroscopy for analysis of polymer degradation and oxidation (also in biological tissue) as well as to measure the concentration of various compounds in different food samples. The carbonyl group attenuation at about 6 micrometres can be detected quite easily, and degree of oxidation of"
    ]
  ],
  [
    "The T1 of a nucleus is sometimes longer than its T2 due to the difference in the actual relaxation mechanisms involved, such as intermolecular versus intramolecular magnetic dipole-dipole interactions, leading to slower spin-lattice relaxation in T1.",
    [
      "of the spins. After the nuclear spin population has relaxed, it can be probed again, since it is in the initial, equilibrium (mixed) state. The precessing nuclei can also fall out of alignment with each other and gradually stop producing a signal. This is called \"T\" or \"transverse relaxation\". Because of the difference in the actual relaxation mechanisms involved (for example, intermolecular versus intramolecular magnetic dipole-dipole interactions ), \"T\" is usually (except in rare cases) longer than \"T\" (that is, slower spin-lattice relaxation, for example because of smaller dipole-dipole interaction effects). In practice, the value of \"T\"* which is the",
      "to nuclei appearing in the time interval will be: where formula_11 is the second of the two parameters in this simple model, it is the, assumed constant, growth velocity of a crystal.The integration of this equation between formula_12 and formula_13 will yield the total extended volume that appears in the time interval Only a fraction of this extended volume is real; some portion of it lies on previously transformed material and is virtual. Since nucleation occurs randomly, the fraction of the extended volume that forms during each time increment that is real will be proportional to the volume fraction of",
      "37% of \"V\". The larger a time constant is, the slower the rise or fall of the potential of a neuron. A long time constant can result in temporal summation, or the algebraic summation of repeated potentials. A short time constant rather produces a coincidence detector through spatial summation. In exponential decay, such as of a radioactive isotope, the time constant can be interpreted as the mean lifetime. The half-life \"T\" is related to the exponential time constant formula_11 by The reciprocal of the time constant is called the decay constant, and is denoted formula_53 A time constant is the"
    ]
  ],
  [
    "The chemical shift of trifluoroacetic acid is 10.816 ppm on a 400 MHz spectrometer.",
    [
      "shift is usually expressed in parts per million (ppm) by frequency, because it is calculated from: where is the absolute resonance frequency of the sample and is the absolute resonance frequency of a standard reference compound, measured in the same applied magnetic field . Since the numerator is usually expressed in hertz, and the denominator in megahertz, is expressed in ppm. The detected frequencies (in Hz) for H, C, and Si nuclei are usually referenced against TMS (tetramethylsilane), TSP (Trimethylsilylpropanoic acid), or DSS, which by the definition above have a chemical shift of zero if chosen as the reference. Other",
      "we are dividing Hz by MHz, the resulting number would be too small, and thus it is multiplied by a million. This operation therefore gives a locator number called the \"chemical shift\" with units of parts per million. In general, chemical shifts for protons are highly predictable since the shifts are primarily determined by simpler shielding effects (electron density), but the chemical shifts for many heavier nuclei are more strongly influenced by other factors including excited states (\"paramagnetic\" contribution to shielding tensor). The chemical shift provides information about the structure of the molecule. The conversion of the raw data to",
      "is the case for NMR the chemical shift reflects the electron density at the atomic nucleus. Chemical shift In nuclear magnetic resonance (NMR) spectroscopy, the chemical shift is the resonant frequency of a nucleus relative to a standard in a magnetic field. Often the position and number of chemical shifts are diagnostic of the structure of a molecule. Chemical shifts are also used to describe signals in other forms of spectroscopy such as photoemission spectroscopy. Some atomic nuclei possess a magnetic moment (nuclear spin), which gives rise to different energy levels and resonance frequencies in a magnetic field. The total"
    ]
  ],
  [
    "It takes approximately 15 seconds for the 13C nuclei in a molecule in a 600 MHz spectrometer to reach a polarization value equal to twice the thermal equilibrium polarization at 298 K, given a T1 of 5.0 s.",
    [
      "Rotational correlation time Rotational correlation time (formula_1) is the average time it takes for a molecule to rotate one radian. In solution, rotational correlation times are in the order of picoseconds. For example, the formula_2 1.7 ps for water, and 100 ps for a pyrroline nitroxyl radical in a DMSO-water mixture. Rotational correlation times are employed in the measurement of microviscosity (viscosity at the molecular level) and in protein characterization. Rotational correlation times may be measured by rotational (microwave), dielectric, and nuclear magnetic resonance (NMR) spectroscopy. Rotational correlation times of probe molecules in media have been measured by fluorescence lifetime",
      "or for radicals, from the linewidths of electron spin resonances. Rotational correlation time Rotational correlation time (formula_1) is the average time it takes for a molecule to rotate one radian. In solution, rotational correlation times are in the order of picoseconds. For example, the formula_2 1.7 ps for water, and 100 ps for a pyrroline nitroxyl radical in a DMSO-water mixture. Rotational correlation times are employed in the measurement of microviscosity (viscosity at the molecular level) and in protein characterization. Rotational correlation times may be measured by rotational (microwave), dielectric, and nuclear magnetic resonance (NMR) spectroscopy. Rotational correlation times of",
      "B. Dynamic nuclear polarisation Dynamic nuclear polarization (DNP) results from transferring spin polarization from electrons to nuclei, thereby aligning the nuclear spins to the extent that electron spins are aligned. Note that the alignment of electron spins at a given magnetic field and temperature is described by the Boltzmann distribution under the thermal equilibrium. It is also possible that those electrons are aligned to a higher degree of order by other preparations of electron spin order such as: chemical reactions (leading to Chemical-Induced DNP, CIDNP), optical pumping and spin injection. DNP is considered one of several techniques for hyperpolarization. DNP"
    ]
  ],
  [
    "The estimated \u03b2-hyperfine value for the cyclobutyl radical (C4H7\u2022) assuming the radical is flat and the HCH angle is 115\u00b0 is approximately 32.5 kcal/mole.",
    [
      "() has six pi electrons and is more stable than either the cycloheptatrienyl radical () or its anion. The cyclopropenyl cation () and the triboracyclopropenyl dianion () are considered examples of a 2\u03c0-electron system Planar ring molecules with 4\"n\" pi electrons do not obey H\u00fcckel's rule, and theory predicts that they are less stable and have triplet ground states with two unpaired electrons. In practice such molecules distort from planar regular polygons. Cyclobutadiene (CH) with four pi electrons is stable only at temperatures below 35 K and is rectangular rather than square. Cyclooctatetraene (CH) with eight pi electrons has a",
      "paid to tri-carbon species, including CH. Later reports of astronomical detections of the CH radical are given in chronological order below. In 1987, Yamamoto et al. report measurements of the rotational spectra of the cyclic CH radical (c-CH) in the laboratory and in interstellar space towards TMC-1. This publication marks the first terrestrial measurement of CH. Yamamoto et al. precisely determine molecular constants and identify 49 lines in the c-CH rotational spectrum. Both fine and hyperfine components are detected toward TMC-1, and the column density for the line of sight toward TMC-1 is estimated to be 6x10cm, which is comparable",
      "the energy of a doubly-occupied \u03c0 orbital (2\u03b1 + 2\u03b2) with the energy of electrons in two isolated p orbitals (2\u03b1), a value of |\u03b2| = 32.5 kcal/mole can be inferred. On the other hand, using the resonance energy of benzene (36 kcal/mole, derived from heats of hydrogenation) and comparing benzene (6\u03b1 + 8\u03b2) with a hypothetical \"non-aromatic 1,3,5-cyclohexatriene\" (6\u03b1 + 6\u03b2), a much smaller value of |\u03b2| = 18 kcal/mole emerges. These differences are not surprising, given the substantially shorter bond length of ethylene (1.33 \u00c5) compared to benzene (1.40 \u00c5). The shorter distance between the interacting p orbitals"
    ]
  ],
  [
    "The ratio of line intensities in the EPR spectrum of the methoxymethyl radical, HCOCH, with isotropic hyperfine splitting patterns is 1:2:1.",
    [
      "isotropic hyperfine splitting pattern for a radical freely tumbling in a solution (isotropic system) can be predicted. While it is easy to predict the number of lines, the reverse problem, unraveling a complex multi-line EPR spectrum and assigning the various spacings to specific nuclei, is more difficult. In the often encountered case of \"I\" = 1/2 nuclei (e.g., H, F, P), the line intensities produced by a population of radicals, each possessing \"M\" equivalent nuclei, will follow Pascal's triangle. For example, the spectrum at the right shows that the three H nuclei of the CH radical give rise to 2\"MI\"",
      "+ 1 = 2(3)(1/2) + 1 = 4 lines with a 1:3:3:1 ratio. The line spacing gives a hyperfine coupling constant of \"a\" = 23 \"G\" for each of the three H nuclei. Note again that the lines in this spectrum are \"first derivatives\" of absorptions. As a second example, the methoxymethyl radical, HCOCH the OC\"H\" center will give an overall 1:2:1 EPR pattern, each component of which is further split by the three methoxy hydrogens into a 1:3:3:1 pattern to give a total of 3\u00d74 = 12 lines, a triplet of quartets. A simulation of the observed EPR spectrum",
      "peak is positive in first-derivative spectra, the high-frequency peak is negative, and the central peak is bipolar. Such situations are commonly observed in powders, and the spectra are therefore called \"powder-pattern spectra\". In crystals, the number of EPR lines is determined by the number of crystallographically equivalent orientations of the EPR spin (called \"EPR center\"). Since the source of an EPR spectrum is a change in an electron's spin state, the EPR spectrum for a radical (S = 1/2 system) would consist of one line. Greater complexity arises because the spin couples with nearby nuclear spins. The magnitude of the"
    ]
  ],
  [
    "Spin trapping is often used to detect free radical intermediates because it forms stable adducts with short-lived radicals that can be detected using electron paramagnetic resonance (EPR) spectroscopy (Spin trapping).",
    [
      "Spin trapping Spin trapping is an analytical technique employed in chemistry and biology for the detection and identification of short-lived free radicals through the use of electron paramagnetic resonance (EPR) spectroscopy. EPR spectroscopy detects paramagnetism species such as the unpaired electrons of free radicals. However, when the half-life of radicals is too short to detect with EPR, compounds known as Spin Traps are used to react covalently with the radical products and form more stable adduct that will also have paramagnetic resonance spectra detectable by EPR spectroscopy. The use of radical-addition reactions to detect short-lived radicals was developed by several",
      "introduced or altered. It is worth noting that the radical adduct (or products such as the hydroxylamine) can often be stable enough to allow non-EPR detection techniques. The groups of London, and Berliner & Khrahmtsov have used NMR to study such adducts and Timmins and co-workers used charge changes upon DBNBS trapping to isolate protein adducts for study. A major advance has been the development of anti-DMPO antibodies by Mason's group, allowing study of spin trapping reactions by a simple immuno-based techniques. Spin trapping Spin trapping is an analytical technique employed in chemistry and biology for the detection and identification",
      "spin trap resulting in the formation of a spin adduct, a nitroxide-based persistent radical, that can be detected using EPR. The spin adduct usually yields a distinctive EPR spectrum characteristic of a particular free radical that is trapped. The identity of the radical can be inferred based on the EPR spectral profile of their respective spin adducts such as the \"g\" value, but most importantly, the hyperfine-coupling constants of relevant nuclei. Unambiguous assignments of the identity of the trapped radical can often be made by using stable isotope substitution of the radicals parent compound, so that further hyperfine couplings are"
    ]
  ],
  [
    "The magnetic moment of a proton is approximately 2.79284734462 \u00d7 10^-26 J/T.",
    [
      "requires a relativistic treatment. A calculation of nucleon magnetic moments from first principles is not yet available. Proton magnetic moment The proton magnetic moment is the magnetic dipole moment of the proton, symbol \"\u03bc\". Protons and neutrons, both nucleons, comprise the nucleus of an atom, and both nucleons act as small magnets whose strength is measured by their magnetic moments. The magnitude of the proton's magnetic moment indicates that the proton is not an elementary particle. CODATA's recommended value for the magnetic moment of the proton is . A more precise measurement has since been claimed, with a result of",
      "Proton magnetic moment The proton magnetic moment is the magnetic dipole moment of the proton, symbol \"\u03bc\". Protons and neutrons, both nucleons, comprise the nucleus of an atom, and both nucleons act as small magnets whose strength is measured by their magnetic moments. The magnitude of the proton's magnetic moment indicates that the proton is not an elementary particle. CODATA's recommended value for the magnetic moment of the proton is . A more precise measurement has since been claimed, with a result of , for an 11-fold improvement in precision. In these values, \"\u03bc\" is the nuclear magneton, a physical",
      "elementary charge and \"\u0127\" is the reduced Planck constant. The magnetic moment of this particle is parallel to its spin. Since the proton has charge +1 \"e\", it should have magnetic moment equal to 1 \"\u03bc\" by this expression. The larger magnetic moment of the proton indicates that it is not an elementary particle. The sign of the proton's magnetic moment is that of a positively charged particle. Similarly, the fact that the magnetic moment of the neutron, , is finite and negative indicates that it too is not an elementary particle. Protons and neutrons are composed of quarks, and"
    ]
  ],
  [
    "The asymmetric stretch (v3) and the bending vibrations in the plane of the molecule (v2) of a carbon dioxide molecule are infrared-active.",
    [
      "at a given temperature can be calculated using the Boltzmann distribution. Performing such a calculation shows that, for relatively low temperatures (such as those used for most routine spectroscopy), most of the molecules occupy the ground vibrational state. Such a molecule can be excited to a higher vibrational mode through the direct absorption of a photon of the appropriate energy. This is the mechanism by which IR spectroscopy operates: infrared radiation is passed through the sample, and the intensity of the transmitted light is compared with that of the incident light. A reduction in intensity at a given wavelength of",
      "identified as either IR or Raman active. A vibration will be active in the IR if there is a change in the dipole moment of the molecule and if it has the same symmetry as one of the x, y, z coordinates. To determine which modes are IR active, the irreducible representation corresponding to x, y, and z are checked with the reducible representation of \u0393. IR mode is active if the same irreducible representation is present in both. Furthermore, a vibration will be active in the Raman if there is a change in the polarizability of the molecule and",
      "= B = \u03a3 \"v\" = B = \u03a0 \"v\" = B = \u03a0 8. Label whether the modes are either IR active or Raman active: \"v\" = Raman active \"v\" = IR active \"v\" = IR active \"v\" = IR active Vibrational spectroscopy of linear molecules To determine the vibrational spectroscopy of linear molecules, the rotation and vibration of linear molecules are taken into account to predict which vibrational (normal) modes are active in the infrared spectrum and the Raman spectrum. The location of a molecule in a 3-dimensional space can be described by the total number of coordinates."
    ]
  ],
  [
    "A total of 30 isomers are possible for an octahedral complex with six different ligands.",
    [
      "of the octahedron surrounding the metal atom, so that any two of these three ligands are mutually cis, and a meridional isomer (\"mer\") in which each set of three identical ligands occupies a plane passing through the metal atom. More complicated complexes, with several different kinds of ligands or with bidentate ligands can also be chiral, with pairs of isomers which are non-superimposable mirror images or enantiomers of each other. For MLLL, a total of six isomers are possible. The number of possible isomers can reach 30 for an octahedral complex with six different ligands (in contrast, only two stereoisomers",
      "coordinated to an octahedral metal centre (M), the complex can exist as isomers. The naming system for these isomers depends upon the number and arrangement of different ligands. For MLL, two isomers exist. These isomers of MLL are \"cis\", if the L ligands are mutually adjacent, and \"trans\", if the L groups are situated 180\u00b0 to each other. It was the analysis of such complexes that led Alfred Werner to the 1913 Nobel Prize\u2013winning postulation of octahedral complexes. For MLL, two isomers are possible - a facial isomer (\"fac\") in which each set of three identical ligands occupies one face",
      "ligand, Werner succeeded in explaining the number of isomers observed. For example, he explained the existence of two tetrammine isomers, \"Co(NH)Cl\", one green and one purple. Werner proposed that these are two geometric isomers of formula [Co(NH)Cl]Cl, with one Cl ion dissociated as confirmed by conductivity measurements. The Co atom is surrounded by four NH and two Cl ligands at the vertices of an octahedron. The green isomer is \"trans\" with the two Cl ligands at opposite vertices, and the purple is \"cis\" with the two Cl at adjacent vertices. Werner also prepared complexes with optical isomers, and in 1914"
    ]
  ],
  [
    "Theories on the nature of logical truth and its relationship to philosophical logic include correspondence theories, deflationary theories, and epistemic theories.",
    [
      "Logical truth Logical truth is one of the most fundamental concepts in logic, and there are different theories on its nature. A logical truth is a statement which is true, and remains true under all reinterpretations of its components other than its logical constants. It is a type of analytic statement. All of philosophical logic can be thought of as providing accounts of the nature of logical truth, as well as logical consequence. Logical truths (including tautologies) are truths which are considered to be necessarily true. This is to say that they are considered to be such that they could",
      "was carried out in November 2009 (taken by 3226 respondents, including 1803 philosophy faculty members and/or PhDs and 829 philosophy graduate students) 45% of respondents accept or lean towards correspondence theories, 21% accept or lean towards deflationary theories and 14% epistemic theories. Logic is concerned with the patterns in reason that can help tell us if a proposition is true or not. However, logic does not deal with truth in the absolute sense, as for instance a metaphysician does. Logicians use formal languages to express the truths which they are concerned with, and as such there is only truth under",
      "History of Western Philosophy\". Logic Logic (from the ), originally meaning \"the word\" or \"what is spoken\", but coming to mean \"thought\" or \"reason\", is a subject concerned with the most general laws of truth, and is now generally held to consist of the systematic study of the form of valid inference. A valid inference is one where there is a specific relation of logical support between the assumptions of the inference and its conclusion. (In ordinary discourse, inferences may be signified by words such as \"therefore\", \"hence\", \"ergo\", and so on.) There is no universal agreement as to the"
    ]
  ],
  [
    "The Larmor frequency for a proton in a magnetic field of 1 T is 42.58 MHz/T.",
    [
      "MHz/T. The quantity \u03b3/2\u03c0 (\"gamma bar\") is therefore convenient, which has the value MHz\u00b7T. When a proton is put into a magnetic field produced by an external source, it is subject to a torque tending to orient its magnetic moment parallel to the field (hence its spin also parallel to the field). Like any magnet, the amount of this torque is proportional both to the magnetic moment and the external magnetic field. Since the proton has spin angular momentum, this torque will cause the proton to precess with a well-defined frequency, called the Larmor frequency. It is this phenomenon that",
      "frequency or reference sample (see also chemical shift referencing), usually a molecule with a barely distorted electron distribution. The operating (or Larmor) frequency of a magnet is calculated from the Larmor equation where is the actual strength of the magnet in units like Teslas or Gauss, and is the gyromagnetic ratio of the nucleus being tested which is in turn calculated from its magnetic moment and spin number with the nuclear magneton and the Planck constant : Thus for example, the proton operating frequency for a 1 T magnet is calculated as: MRI scanners are often referred to by their",
      "in Larmor precession frequency formula_1, is of great relevance to nuclear magnetic resonance analysis. Some isotopes in the human body have unpaired protons or neutrons (or both, as the magnetic moments of a proton and neutron do not cancel perfectly) Note that in the table below, the measured magnetic dipole moments, expressed in a ratio to the nuclear magneton, may be divided by the half-integral nuclear spin to calculate dimensionless g-factors. These g-factors may be multiplied by MHz/T, which is the nuclear magneton divided by Planck's constant, to yield Larmor frequencies in MHz/T. If divided instead by the reduced Planck"
    ]
  ],
  [
    "n-type extrinsic semiconductor.",
    [
      "Extrinsic semiconductor An extrinsic semiconductor is one that has been \"doped\"; during manufacture of the semiconductor crystal a trace element or chemical called a doping agent has been incorporated chemically into the crystal, for the purpose of giving it different electrical properties than the pure semiconductor crystal, which is called an \"intrinsic semiconductor\". In an extrinsic semiconductor it is these foreign dopant atoms in the crystal lattice that mainly provide the charge carriers which carry electric current through the crystal. The doping agents used are of two types, resulting in two types of extrinsic semiconductor. An \"electron donor\" dopant is",
      "doping is the process that changes an intrinsic semiconductor to an extrinsic semiconductor. During doping, impurity atoms are introduced to an intrinsic semiconductor. Impurity atoms are atoms of a different element than the atoms of the intrinsic semiconductor. Impurity atoms act as either donors or acceptors to the intrinsic semiconductor, changing the electron and hole concentrations of the semiconductor. Impurity atoms are classified as either donor or acceptor atoms based on the effect they have on the intrinsic semiconductor. Donor impurity atoms have more valence electrons than the atoms they replace in the intrinsic semiconductor lattice. Donor impurities \"donate\" their",
      "Donor (semiconductors) In semiconductor physics, a donor is a dopant atom that, when added to a semiconductor, can form a n-type region. For example, when silicon (Si), having four valence electrons, needs to be doped as a n-type semiconductor, elements from group V like phosphorus (P) or arsenic (As) can be used because they have five valence electrons. A dopant with five valence electrons is also called a pentavalent impurity. Other pentavalent dopants are antimony (Sb) and bismuth (Bi). When substituting a Si atom in the crystal lattice, four of the valence electrons of phosphorus form covalent bonds with the"
    ]
  ],
  [
    "The 1H spectrum of 12CHCl3 is a singlet because all the protons in the molecule are equivalent and coupled to the same carbon atom, resulting in only one spectral line due to the paired electrons in the system. (Singlet state)",
    [
      "is sufficient to prevent the emergence of causality paradoxes. Singlet state In quantum mechanics, a singlet state usually refers to a system in which all electrons are paired. The term 'singlet' originally meant a linked set of particles whose net angular momentum is zero, that is, whose overall spin quantum number formula_1. As a result, there is only one spectral line of a singlet state. In contrast, a doublet state contains one unpaired electron and shows splitting of spectral lines into a doublet; and a triplet state has two unpaired electrons and shows threefold splitting of spectral lines. Singlets and",
      "frequently in atomic physics and nuclear physics, where one often needs to determine the total spin of a collection of particles. Since the only observed fundamental particle with zero spin is the extremely inaccessible Higgs boson, singlets in everyday physics are necessarily composed of sets of particles whose individual spins are non-zero, e.g. 1/2 or 1. The origin of the term \"singlet\" is that bound quantum systems with zero net angular momentum emit photons within a single spectral line, as opposed to double lines (doublet state) or triple lines (triplet state). The number of spectral lines formula_2 in this singlet-style",
      "Singlet state In quantum mechanics, a singlet state usually refers to a system in which all electrons are paired. The term 'singlet' originally meant a linked set of particles whose net angular momentum is zero, that is, whose overall spin quantum number formula_1. As a result, there is only one spectral line of a singlet state. In contrast, a doublet state contains one unpaired electron and shows splitting of spectral lines into a doublet; and a triplet state has two unpaired electrons and shows threefold splitting of spectral lines. Singlets and the related spin concepts of doublets and triplets occur"
    ]
  ],
  [
    "The solubility product, K_sp, for lead iodide at 25\u00b0C is 1.4 x 10^-8 (Lead iodide).",
    [
      "700 \u00b0C. A thin film of can also be prepared by depositing a film of lead sulfide and exposing it to iodine vapor, by the reaction The sulfur is then washed with dimethyl sulfoxide. Lead iodide prepared from cold solutions of and salts usually consists of many small hexagonal platelets, giving the yellow precipitate a silky appearance. Larger crystals can be obtained by exploiting the fact that solubility of lead iodide in water (like those of lead chloride and lead bromide) increases dramatically with temperature. The compound is colorless when dissolved in hot water, but crystallizes on cooling as thin",
      "Lead(II) iodide Lead(II) iodide or lead iodide is a salt with the formula . At room temperature, it is a bright yellow odorless crystalline solid, that becomes orange and red when heated. It was formerly called plumbous iodide. The compound currently has a few specialized applications, such as the manufacture of solar cells and X-ray and gamma-ray detectors. Its preparation is a popular demonstration in basic chemistry education, to teach topics such as double displacement reactions and stoichiometry. It is decomposed by light at moderately high temperatures and this effect has been used in a patented photographic process. Lead iodide",
      "weigh out the potassium iodide so it can be used in an emergency situation. KI crystals are simply added to water until no more KI will dissolve and instead sits at the bottom of the container. With pure water, the concentration of KI in the solution depends only on the temperature. Potassium iodide is highly soluble in water so SSKI is a concentrated source of KI. At 20 degrees Celsius the solubility of KI is 140-148 grams per 100 grams of water. Because the volumes of KI and water are approximately additive, the resulting SSKI solution will contain about 1.00"
    ]
  ],
  [
    "The isomer of C6H14 that has lines with five distinct chemical shifts in its 13C spectrum is 6-methylene-6,13-dihydropentacene.",
    [
      "much research. A tautomeric chemical equilibrium exists between \"6-methylene-6,13-dihydropentacene\" and 6-methylpentacene. This equilibrium is entirely in favor of the methylene compound. Only by heating a solution of the compound to 200 \u00b0C does a small amount of the pentacene develop, as evidenced by the emergence of a red-violet color. According to one study the reaction mechanism for this equilibrium is not based on an intramolecular 1,5-hydride shift, but on a bimolecular free radical hydrogen migration. In contrast, isotoluenes with the same central chemical motif easily aromatize. Pentacene reacts with elemental sulfur in 1,2,4-trichlorobenzene to the compound hexathiapentacene. X-ray crystallography shows",
      "lab suggested between three and four. Most of them are at the 2(3) positions, as in the formula and sometimes a cartoon representation uses the methylene bridge criss crossing across the bond between these two positions to indicate that it could bind either of these two positions. A large number of isomers, differing in the positions of the cationic groups, are possible. Alcian blue 7GX carries fewer isothiouronium groups than 8GX. Similarly 5GX and 2GX may have even fewer side groups but it was not rigorously proven. The phthalocyanine aromatic nucleus has a large conjugated system with a CBN (Conjugated",
      "carbon isotopomers of a compound can be studied by carbon-13 NMR to learn about the different carbon atoms in the structure. Each individual structure that contains a single C isotope provides data about the structure in its immediate vicinity. A large sample of a chemical contains a mixture of all such isotopomers, so a single spectrum of the sample contains data about all carbons in it. Nearly all of the carbon in normal samples of carbon-based chemicals is C, with only about 1% abundance of C, so there is only about a 1% abundance of the total of the singly-substituted"
    ]
  ],
  [
    "dioxide (silica) or silicates being the most common naturally-occurring form in which silicon is found.",
    [
      "its discovery: silicon was first prepared and characterized in pure form in 1824, and given the name silicium (from , flints), with an -ium word-ending to suggest a metal. However, its final name, suggested in 1831, reflects the more physically similar elements carbon and boron. Silicon is the eighth most common element in the universe by mass, but very rarely occurs as the pure free element in nature. It is most widely distributed in dusts, sands, planetoids, and planets as various forms of silicon dioxide (silica) or silicates. Over 90% of the Earth's crust is composed of silicate minerals, making",
      "pure monomolecular silicon \"{not to be confused with Silicone}\". Silicium, with the abbreviation SI14 in the periods system, is a natural product and is, after H2O, the most frequently occurring element on earth. Silicium is found in sand and glass as well as in the human body. Even in the medical field, silicium in used in the research and development of new medicines. Permanon products with silicium do not undergo any chemical reaction with the material to be coated. Through a complex procedure, it was developed to create a static attractive force on all surfaces, without damaging the surface materials.",
      "\u00b0C respectively are the second-highest among all the metalloids and nonmetals, being only surpassed by boron. Silicon is the eighth most common element in the universe by mass, but very rarely occurs as the pure element in the Earth's crust. It is most widely distributed in dusts, sands, planetoids, and planets as various forms of silicon dioxide (silica) or silicates. More than 90% of the Earth's crust is composed of silicate minerals, making silicon the second most abundant element in the Earth's crust (about 28% by mass) after oxygen. Most silicon is used commercially without being separated, and often with"
    ]
  ],
  [
    "The nuclide with an NMR frequency of 115.5 MHz in a 20.0 T magnetic field is carbon-13 (C-13).",
    [
      "placed in a magnetic field, NMR active nuclei (such as H or C) absorb electromagnetic radiation at a frequency characteristic of the isotope. The resonant frequency, energy of the radiation absorbed, and the intensity of the signal are proportional to the strength of the magnetic field. For example, in a 21 Tesla magnetic field, hydrogen atoms (commonly referred to as protons) resonate at 900 MHz. It is common to refer to a 21 T magnet as a 900 MHz magnet since hydrogen is the most common nucleus detected, however different nuclei will resonate at different frequencies at this field strength",
      "T is therefore about 42 ppm. However a frequency scale is commonly used to designate the NMR signals, even though the spectrometer may operate by sweeping the magnetic field, and thus the 42 ppm is 4200 Hz for a 100 MHz reference frequency (rf). However, given that the location of different NMR signals is dependent on the external magnetic field strength and the reference frequency, the signals are usually reported relative to a reference signal, usually that of TMS (tetramethylsilane). Additionally, since the distribution of NMR signals is field dependent, these frequencies are divided by the spectrometer frequency. However, since",
      "shift is usually expressed in parts per million (ppm) by frequency, because it is calculated from: where is the absolute resonance frequency of the sample and is the absolute resonance frequency of a standard reference compound, measured in the same applied magnetic field . Since the numerator is usually expressed in hertz, and the denominator in megahertz, is expressed in ppm. The detected frequencies (in Hz) for H, C, and Si nuclei are usually referenced against TMS (tetramethylsilane), TSP (Trimethylsilylpropanoic acid), or DSS, which by the definition above have a chemical shift of zero if chosen as the reference. Other"
    ]
  ],
  [
    "predicts that the vapor pressures of ideal mixtures will vary linearly with composition ratio, assuming that the constituents stick to each other to the same degree as they do to themselves.",
    [
      "when a mixture deviates from Raoult's law, the equality of compositions in liquid phase and vapor phases, in vapour-liquid equilibrium and Dalton's law the equality of pressures for total pressure being equal to the sum of the partial pressures in real mixtures. In other words: Raoult's law predicts the vapor pressures of ideal mixtures as a function of composition ratio. More simply: per Raoult's law molecules of the constituents stick to each other to the same degree as they do to themselves. For example, if the constituents are X and Y, then X sticks to Y with roughly equal energy",
      "its mole fraction \"x\", and Henry's law can be written as This can be compared with Raoult's law: where \"p\"* is the vapor pressure of the pure component. At first sight, Raoult's law appears to be a special case of Henry's law, where \"K\" = \"p\"*. This is true for pairs of closely related substances, such as benzene and toluene, which obey Raoult's law over the entire composition range: such mixtures are called \"ideal mixtures\". The general case is that both laws are limit laws, and they apply at opposite ends of the composition range. The vapor pressure of the",
      "two liquids, A and B, Raoult's law predicts that if no other gases are present, then the total vapor pressure formula_11 above the solution is equal to the weighted sum of the \"pure\" vapor pressures of the two components, formula_12 and formula_13. Thus the total pressure above the solution of A and B would be Since the sum of the mole fractions is equal to one, This is a linear function of the mole fraction formula_16, as shown in the graph. Raoult's law was originally discovered as an idealised experimental law. Using Raoult's law as the definition of an ideal"
    ]
  ],
  [
    "The chemical shift of phosphorus when the pH of the solution equals the pKa of H2PO4\u203e would be around 2.15.",
    [
      "Phosphate A phosphate is chemical derivative of phosphoric acid. The phosphate ion, (), is an inorganic chemical, the conjugate base that can form many different salts. In organic chemistry, a phosphate, or organophosphate, is an ester of phosphoric acid. Of the various phosphoric acids and phosphates, organic phosphates are important in biochemistry and biogeochemistry (and, consequently, in ecology), and inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry. At elevated temperatures in the solid state, phosphates can condense to form pyrophosphates. In biology, adding phosphates to\u2014and removing them from\u2014proteins in cells are both pivotal in the",
      "pH values. For a neutral pH, as in the cytosol, pH = 7.0 so that only and ions are present in significant amounts (62% , 38% . Note that in the extracellular fluid (pH = 7.4), this proportion is inverted (61% , 39% ). Phosphate can form many polymeric ions such as pyrophosphate), , and triphosphate, . The various metaphosphate ions (which are usually long linear polymers) have an empirical formula of and are found in many compounds. In biological systems, phosphorus is found as a free phosphate ion in solution and is called inorganic phosphate, to distinguish it from",
      "be lost from the molecule as H ions (alternatively referred to as protons). When all three H ions are lost from orthophosphoric acid, an orthophosphate ion (PO) is formed. Orthophosphate is the simplest in a series of phosphates, and is usually just called phosphate by both non-technical people and many chemists alike; see a separate article on phosphate for details. Because orthophosphoric acid can undergo as many as three dissociations or ionizations (losses of H ions), it has three acid dissociation constants called K, K, and K. Another way to provide acid dissociation constant data is to list pK, pK,"
    ]
  ],
  [
    "It is important to buffer the analyte solution and the titrant solution at the same pH for EDTA titrations to maintain a constant pH throughout the reaction process and ensure accurate results. (source)",
    [
      "systems, which include a buffer solution. In a buffer, a weak acid and its conjugate base (in the form of a salt), or a weak base and its conjugate acid, are used in order to limit the pH change during a titration process. Buffers have both organic and non-organic chemical applications. For example, besides buffers being used in lab processes, our blood acts as a buffer to maintain pH. The most important buffer in our bloodstream is the carbonic acid-bicarbonate buffer, which prevents drastic pH changes when is introduced. This functions as such: Furthermore, here is a table of common",
      "used for special purposes (as in petrochemistry). Concentrated analytes are often diluted to improve accuracy. Many non-acid\u2013base titrations require a constant pH throughout the reaction. Therefore, a buffer solution may be added to the titration chamber to maintain the pH. In instances where two reactants in a sample may react with the titrant and only one is the desired analyte, a separate masking solution may be added to the reaction chamber which masks the unwanted ion. Some redox reactions may require heating the sample solution and titrating while the solution is still hot to increase the reaction rate. For instance,",
      "measured directly, but may be calculated using theoretical methods. Buffer solutions are used extensively to provide solutions at or near the physiological pH for the study of biochemical reactions; the design of these solutions depends on a knowledge of the p\"K\" values of their components. Important buffer solutions include MOPS, which provides a solution with pH 7.2, and tricine, which is used in gel electrophoresis. Buffering is an essential part of acid base physiology including acid\u2013base homeostasis, and is key to understanding disorders such as acid\u2013base imbalance. The isoelectric point of a given molecule is a function of its p\"K\""
    ]
  ],
  [
    "According to Henry's law, the concentration of dissolved CO2 in water increases with increasing pressure.",
    [
      "is increased due to pressure increase by \u0394p = 2\u03b3/r; see Young\u2013Laplace equation). Henry's law is valid for gases that do not undergo speciation on dissolution. Sieverts' law shows a case when this assumption does not hold. The carbon dioxide solubility in seawater is also affected by temperature and by the carbonate buffer. The decrease of solubility of carbon dioxide in seawater when temperature increases is also an important retroaction factor (positive feedback) exacerbating past and future climate changes as observed in ice cores from the Vostok site in Antarctica. At the geological time scale, because of the Milankovich cycles,",
      "and vice versa. At equilibrium the rate of transfer of CO from the gas to the liquid phase is equal to the rate from liquid to gas. In this case, the equilibrium concentration of CO in the liquid is given by Henry's law, which states that the solubility of a gas in a liquid is directly proportional to the partial pressure of that gas above the liquid. This relationship is written as where \"k\" is a temperature-dependent constant, \"p\" is the partial pressure and \"c\" is the concentration of the dissolved gas in the liquid Thus the partial pressure of",
      "Henry's law In chemistry, Henry's law is a gas law that states that the amount of dissolved gas is proportional to its partial pressure in the gas phase. The proportionality factor is called the Henry's law constant. It was formulated by the English chemist William Henry, who studied the topic in the early 19th century. In his publication about the quantity of gases absorbed by water, he described the results of his experiments: An example where Henry's law is at play is in the depth-dependent dissolution of oxygen and nitrogen in the blood of underwater divers that changes during decompression,"
    ]
  ],
  [
    "The concentration of a sample solution can be determined using titration methods and a series of standard solutions by measuring the volume of titrant used and applying the formula C1V1 = C2V2, where C1 is the concentration of the analyte, V1 is the volume of the analyte used, C2 is the concentration of the titrant, and V2 is the volume of the titrant used.",
    [
      "The general procedure is that the concentration in question is measured for a series of solutions with known analytical concentrations of the reactants. Typically, a titration is performed with one or more reactants in the titration vessel and one or more reactants in the burette. Knowing the analytical concentrations of reactants initially in the reaction vessel and in the burette, all analytical concentrations can be derived as a function of the volume (or mass) of titrant added. The equilibrium constants may be derived by best-fitting of the experimental data with a chemical model of the equilibrium system. There are four",
      "and used to calculate the concentration of analyte by where C is the concentration of the analyte, typically in molarity; C is the concentration of the titrant, typically in molarity; V is the volume of the titrant used, typically in liters; M is the mole ratio of the analyte and reactant from the balanced chemical equation; and V is the volume of the analyte used, typically in liters. Typical titrations require titrant and analyte to be in a liquid (solution) form. Though solids are usually dissolved into an aqueous solution, other solvents such as glacial acetic acid or ethanol are",
      "is possible to quantify the amount or concentration of a substance within a sample by determining the absorbance on the Spec 20 and finding the corresponding concentration on the calibration curve. Alternatively, the logarithm of percent transmittance can be plotted versus concentration to create a standard curve using the same procedure. The absorbance measured by the Spectronic 20 is the sum of the absorbance of each of the constituents of the solution. Therefore, the Spectronic 20 can be used to analyze more complex solutions. For example, if a sample solution has two light-absorbing compounds in it, then the user performs"
    ]
  ],
  [
    "Hyperfine interactions can be measured in atomic and molecular spectra by observing small shifts and splittings in the energy levels of atoms, molecules, and ions due to the interaction between the state of the nucleus and the state of the electron clouds.",
    [
      "higher-J transitions, there are small but significant changes in the relative intensities and positions of each individual hyperfine component. Hyperfine interactions can be measured, among other ways, in atomic and molecular spectra and in electron paramagnetic resonance spectra of free radicals and transition-metal ions. As the hyperfine splitting is very small, the transition frequencies are usually not located in the optical, but are in the range of radio- or microwave (also called sub-millimeter) frequencies. Hyperfine structure gives the 21 cm line observed in H I regions in interstellar medium. Carl Sagan and Frank Drake considered the hyperfine transition of hydrogen",
      "with the interaction between the magnetic moments associated with different magnetic nuclei in a molecule, as well as between the nuclear magnetic moments and the magnetic field generated by the rotation of the molecule. Hyperfine structure contrasts with \"fine structure\", which results from the interaction between the magnetic moments associated with electron spin and the electrons' orbital angular momentum. Hyperfine structure, with energy shifts typically orders of magnitudes smaller than those of a fine-structure shift, results from the interactions of the nucleus (or nuclei, in molecules) with internally generated electric and magnetic fields. The optical hyperfine structure was observed in",
      "Hyperfine structure In atomic physics, hyperfine structure refers to small shifts and splittings in the energy levels of atoms, molecules, and ions, due to interaction between the state of the nucleus and the state of the electron clouds. In atoms, hyperfine structure arises from the energy of the nuclear magnetic dipole moment interacting with the magnetic field generated by the electrons and the energy of the nuclear electric quadrupole moment in the electric field gradient due to the distribution of charge within the atom. Molecular hyperfine structure is generally dominated by these two effects, but also includes the energy associated"
    ]
  ],
  [
    "The chemical species with the greatest affinity for electrons based on its reduction potential is the one with the most positive potential.",
    [
      "its rate is so slow. Under those circumstances, kinetic evaluations are necessary. However, the equilibrium conditions can be used to evaluate the direction of spontaneous changes and the magnitude of the driving force behind them. Reduction potential Reduction potential (also known as redox potential, oxidation / reduction potential, ORP, pE, \u03b5, or formula_1) is a measure of the tendency of a chemical species to acquire electrons and thereby be reduced. Reduction potential is measured in volts (V), or millivolts (mV). Each species has its own intrinsic reduction potential; the more positive the potential, the greater the species' affinity for electrons",
      "Reduction potential Reduction potential (also known as redox potential, oxidation / reduction potential, ORP, pE, \u03b5, or formula_1) is a measure of the tendency of a chemical species to acquire electrons and thereby be reduced. Reduction potential is measured in volts (V), or millivolts (mV). Each species has its own intrinsic reduction potential; the more positive the potential, the greater the species' affinity for electrons and tendency to be reduced. ORP is a common measurement for water quality. In aqueous solutions, reduction potential is a measure of the tendency of the solution to either gain or lose electrons when it",
      "Electron affinity (data page) This page deals with the electron affinity as a property of isolated atoms or molecules (i.e. in the gas phase). Solid state electron affinities are not listed here. Electron affinity can be defined in two equivalent ways. First, as the energy that is released by adding an electron to an isolated gaseous atom. The second (reverse) definition is that electron affinity is the energy required to remove an electron from a singly charged gaseous negative ion. Either convention can be used. Whereas ionization energies are always concerned with the formation of positive ions, electron affinities are"
    ]
  ],
  [
    "Tetramethylsilane is used as a 1H chemical shift reference in NMR spectroscopy because all twelve hydrogen atoms in a tetramethylsilane molecule are equivalent, resulting in a single peak in the H NMR spectrum.",
    [
      "chemical vapor deposition, TMS is the precursor to silicon dioxide or silicon carbide, depending on the deposition conditions. Tetramethylsilane is the accepted internal standard for calibrating chemical shift for H, C and Si NMR spectroscopy in organic solvents (where TMS is soluble). In water, where it is not soluble, sodium salts of DSS, 2,2-dimethyl-2-silapentane-5-sulfonate, are used instead. Because of its high volatility, TMS can easily be evaporated, which is convenient for recovery of samples analyzed by NMR spectroscopy. Because all twelve hydrogen atoms in a tetramethylsilane molecule are equivalent, its H NMR spectrum consists of a singlet. The chemical shift",
      "Protein chemical shift re-referencing Protein chemical shift re-referencing is a post-assignment process of adjusting the assigned NMR chemical shifts to match IUPAC and BMRB recommended standards in protein chemical shift referencing. In NMR chemical shifts are normally referenced to an internal standard that is dissolved in the NMR sample. These internal standards include tetramethylsilane (TMS), 4,4-dimethyl-4-silapentane-1-sulfonic acid (DSS) and trimethylsilyl propionate (TSP). For protein NMR spectroscopy the recommended standard is DSS, which is insensitive to pH variations (unlike TSP). Furthermore, the DSS 1H signal may be used to indirectly reference 13C and 15N shifts using a simple ratio calculation [1].",
      "be a significant, and as yet unresolved problem for the biomolecular NMR community. Protein chemical shift re-referencing Protein chemical shift re-referencing is a post-assignment process of adjusting the assigned NMR chemical shifts to match IUPAC and BMRB recommended standards in protein chemical shift referencing. In NMR chemical shifts are normally referenced to an internal standard that is dissolved in the NMR sample. These internal standards include tetramethylsilane (TMS), 4,4-dimethyl-4-silapentane-1-sulfonic acid (DSS) and trimethylsilyl propionate (TSP). For protein NMR spectroscopy the recommended standard is DSS, which is insensitive to pH variations (unlike TSP). Furthermore, the DSS 1H signal may be used"
    ]
  ],
  [
    "The g value of the atom based on the X-band EPR spectrum can be determined by measuring the field and frequency at which resonance occurs using the equation \"gB\" + \"gB\" + \"gB\".",
    [
      "formula_52 resonance condition (above) is rewritten as follows: The quantity formula_54 is denoted formula_55 and called simply the \"g\"-factor, so that the final resonance equation becomes This last equation is used to determine formula_55 in an EPR experiment by measuring the field and the frequency at which resonance occurs. If formula_55 does not equal formula_8, the implication is that the ratio of the unpaired electron's spin magnetic moment to its angular momentum differs from the free-electron value. Since an electron's spin magnetic moment is constant (approximately the Bohr magneton), then the electron must have gained or lost angular momentum through",
      "between the two energy levels by either absorbing or emitting a photon of energy formula_14 such that the resonance condition, formula_15, is obeyed. This leads to the fundamental equation of EPR spectroscopy: formula_16. Experimentally, this equation permits a large combination of frequency and magnetic field values, but the great majority of EPR measurements are made with microwaves in the 9000\u201310000 MHz (9\u201310 GHz) region, with fields corresponding to about 3500 G (0.35 T). Furthermore, EPR spectra can be generated by either varying the photon frequency incident on a sample while holding the magnetic field constant or doing the reverse. In",
      "number of its components from 9 to 3: \"g\", \"g\" and \"g\". For a single spin experiencing only Zeeman interaction with an external magnetic field, the position of the EPR resonance is given by the expression \"gB\" + \"gB\" + \"gB\". Here \"B\", \"B\" and \"B\" are the components of the magnetic field vector in the coordinate system (\"x\",\"y\",\"z\"); their magnitudes change as the field is rotated, so does the frequency of the resonance. For a large ensemble of randomly oriented spins, the EPR spectrum consists of three peaks of characteristic shape at frequencies \"gB\", \"gB\" and \"gB\": the low-frequency"
    ]
  ],
  [
    "The total angular momentum quantum number of 43Ca is 7/2 ((the \"total angular momentum quantum number\")).",
    [
      "vector with the appropriate length and \"z\"-component is drawn, forming a cone. The expected value of the angular momentum for a given ensemble of systems in the quantum state characterized by formula_45 and formula_46 could be somewhere on this cone while it cannot be defined for a single system (since the components of formula_47 do not commute with each other). The quantization rules are technically true even for macroscopic systems, like the angular momentum L of a spinning tire. However they have no observable effect. For example, if formula_48 is roughly 100000000, it makes essentially no difference whether the precise",
      "value is an integer like 100000000 or 100000001, or a non-integer like 100000000.2\u2014the discrete steps are too small to notice. The most general and fundamental definition of angular momentum is as the \"generator\" of rotations. More specifically, let formula_49 be a rotation operator, which rotates any quantum state about axis formula_50 by angle formula_51. As formula_52, the operator formula_49 approaches the identity operator, because a rotation of 0\u00b0 maps all states to themselves. Then the angular momentum operator formula_54 about axis formula_50 is defined as: where 1 is the identity operator. Also notice that \"R\" is an additive morphism :",
      "(the \"principal quantum number\"), formula_32 (the \"total angular momentum quantum number\"), \"formula_33\" (the \"orbital angular momentum quantum number\"), \"formula_34\" (the \"spin quantum number\"), and formula_35 (the \"\"z\" component of total angular momentum\"). To evaluate the energies, we note that for hydrogenic wavefunctions (here formula_37 is the Bohr radius divided by the nuclear charge \"Z\"); and We can now say that where For the exact relativistic result, see the solutions to the Dirac equation for a hydrogen-like atom. A crystalline solid (semiconductor, metal etc.) is characterized by its band structure. While on the overall scale (including the core levels) the spin\u2013orbit"
    ]
  ],
  [
    "The relationship between electric polarization and magnetic field in the context of magnetoelectric susceptibility is described by the formula_42. formula, which takes into account the frequency dependence of the susceptibility and imposes Kramers-Kronig constraints due to causality.",
    [
      "formula_38. It is more convenient in a linear system to take the Fourier transform and write this relationship as a function of frequency. Due to the convolution theorem, the integral becomes a product, This frequency dependence of the susceptibility leads to frequency dependence of the permittivity. The shape of the susceptibility with respect to frequency characterizes the dispersion properties of the material. Moreover, the fact that the polarization can only depend on the electric field at previous times (i.e. formula_36 for formula_37), a consequence of causality, imposes Kramers\u2013Kronig constraints on the susceptibility formula_42. Electric susceptibility In electricity (electromagnetism), the electric",
      "Electric susceptibility In electricity (electromagnetism), the electric susceptibility (formula_1; Latin: \"susceptibilis\" \"receptive\") is a dimensionless proportionality constant that indicates the degree of polarization of a dielectric material in response to an applied electric field. The greater the electric susceptibility, the greater the ability of a material to polarize in response to the field, and thereby reduce the total electric field inside the material (and store energy). It is in this way that the electric susceptibility influences the electric permittivity of the material and thus influences many other phenomena in that medium, from the capacitance of capacitors to the speed of",
      "can also be expressed generally (in SI) as formula_9. The polarizability of individual particles is related to the average electric susceptibility of the medium by the Clausius-Mossotti relation. Polarizability for anisotropic or non-spherical media cannot in general be represented as a scalar quantity. Defining formula_1 as a scalar implies both that applied electric fields can only induce polarization components parallel to the field and that the formula_11 and formula_12 directions respond in the same way to the applied electric field. For example, an electric field in the formula_13-direction can only produce an formula_13 component in formula_2 and if that same"
    ]
  ],
  [
    "The linewidth of an NMR line with a T2 of 15 ms can be calculated using the equation: Linewidth = 1/(\u03c0*T2), yielding a value of approximately 21.2 Hz.",
    [
      "mineralogy, NMR-logging measurements respond to the presence of hydrogen. Because hydrogen atoms primarily occur in pore fluids, NMR effectively responds to the volume, composition, viscosity, and distribution of these fluids, for example oil, gas or water. NMR logs provide information about the quantities of fluids present, the properties of these fluids, and the sizes of the pores containing these fluids. From this information, it is possible to infer or estimate: The basic core and log measurement is the \"T\" decay, presented as a distribution of \"T\" amplitudes versus time at each sample depth, typically from 0.3 ms to 3 s.",
      "or a mixture of statistical and physics principles PSVS. In addition to structures, nuclear magnetic resonance can yield information on the dynamics of various parts of the protein. This usually involves measuring relaxation times such as T and T to determine order parameters, correlation times, and chemical exchange rates. NMR relaxation is a consequence of local fluctuating magnetic fields within a molecule. Local fluctuating magnetic fields are generated by molecular motions. In this way, measurements of relaxation times can provide information of motions within a molecule on the atomic level. In NMR studies of protein dynamics the nitrogen-15 isotope is",
      "that at this extreme case, T equals T. As follows from the BPP theory, measuring the T times leads to internuclear distances r. One of the examples is accurate determinations of the metal \u2013 hydride (M-H) bond lengths in solutions by measurements of H selective and non-selective T times in variable-temperature relaxation experiments via the equation: Relaxation (NMR) In nuclear magnetic resonance (NMR) spectroscopy and magnetic resonance imaging (MRI) the term relaxation describes how signals change with time. In general signals deteriorate with time, becoming weaker and broader. The deterioration reflects the fact that the NMR signal, which results from"
    ]
  ],
  [
    "The possible outcomes for the z-component of the spin angular momentum of 43Ca can be all positive and negative half-integer values.",
    [
      "components are the possible outcomes (in units of formula_39) of a measurement of the spin projected onto one of the basis directions. Rotations (of ordinary space) about an axis formula_32 through angle \"\u03b8\" about the unit vector formula_41 in space acting on a multicomponent wave function (spinor) at a point in space is represented by: ) = \\exp\\left( - \\frac{i}{\\hbar}\\theta \\hat{\\mathbf{a}} \\cdot \\widehat{\\mathbf{S}}\\right) </math> However, unlike orbital angular momentum in which the \"z\"-projection quantum number \"\" can only take positive or negative integer values (including zero), the \"z\"-projection spin quantum number \"s\" can take all positive and negative half-integer values.",
      "vector with the appropriate length and \"z\"-component is drawn, forming a cone. The expected value of the angular momentum for a given ensemble of systems in the quantum state characterized by formula_45 and formula_46 could be somewhere on this cone while it cannot be defined for a single system (since the components of formula_47 do not commute with each other). The quantization rules are technically true even for macroscopic systems, like the angular momentum L of a spinning tire. However they have no observable effect. For example, if formula_48 is roughly 100000000, it makes essentially no difference whether the precise",
      "component of the angular momentum thus has two eigenspinors. By convention, the z direction is chosen as having the formula_6 and formula_7 states as its eigenspinors. The eigenspinors for the other two orthogonal directions follow from this convention: formula_3: formula_1: formula_2: All of these results are but special cases of the eigenspinors for the direction specified by \"\u03b8\" and \"\u03c6\" in spherical coordinates - those eigenspinors are: Suppose there is a spin 1/2 particle in a state formula_19. To determine the probability of finding the particle in a spin up state, we simply multiply the state of the particle by"
    ]
  ],
  [
    "Lanthanum has the second-lowest melting point after cerium among all the lanthanides, at 920 \u00b0C.",
    [
      "(with the exceptions of the last two, ytterbium and lutetium, where the 4f shell is completely full). Furthermore, since the melting points of the trivalent lanthanides are related to the extent of hybridisation of the 6s, 5d, and 4f electrons, lanthanum has the second-lowest (after cerium) melting point among all the lanthanides: 920 \u00b0C. The lanthanides become harder as the series is traversed: as expected, lanthanum is a soft metal. Lanthanum has a relatively high resistivity of 615 n\u03a9m at room temperature; in comparison, the value for the good conductor aluminium is only 26.50 n\u03a9m. Lanthanum is the least volatile",
      "series, (lanthanum (920 \u00b0C) \u2013 lutetium (1622 \u00b0C)) to the extent of hybridization of the 6s, 5d, and 4f orbitals. The hybridization is believed to be at its greatest for cerium, which has the lowest melting point of all, 795 \u00b0C. The lanthanide metals are soft; their hardness increases across the series. Europium stands out, as it has the lowest density in the series at 5.24 g/cm and the largest metallic radius in the series at 208.4 pm. It can be compared to barium, which has a metallic radius of 222 pm. It is believed that the metal contains the",
      "one of the least abundant elements in the universe; only about % of all matter in the universe is europium. Europium is a ductile metal with a hardness similar to that of lead. It crystallizes in a body-centered cubic lattice. Some properties of europium are strongly influenced by its half-filled electron shell. Europium has the second lowest melting point and the lowest density of all lanthanides. Europium becomes a superconductor when it is cooled below 1.8 K and compressed to above 80 GPa. This is because europium is divalent in the metallic state, and is converted into the trivalent state"
    ]
  ],
  [
    "The protons and neutrons collectively make up the nucleons in nuclear binding energy calculations.",
    [
      "of the theory delivers an equation which attempts to predict the binding energy of a nucleus in terms of the numbers of protons and neutrons it contains. This equation has five terms on its right hand side. These correspond to the cohesive binding of all the nucleons by the nuclear force, a surface energy term, the electrostatic (Coulomb) mutual repulsion of the protons, an asymmetry term (derivable from the protons and neutrons occupying independent quantum momentum states) and a pairing term (partly derivable from the protons and neutrons occupying independent quantum spin states). If we consider the sum of the",
      "Nucleon In chemistry and physics, a nucleon is either a proton or a neutron, considered in its role as a component of an atomic nucleus. The number of nucleons in a nucleus defines an isotope's mass number (nucleon number). Until the 1960s, nucleons were thought to be elementary particles, not made up of smaller parts. Now they are known to be composite particles, made of three quarks bound together by the so-called strong interaction. The interaction between two or more nucleons is called internucleon interaction or nuclear force, which is also ultimately caused by the strong interaction. (Before the discovery",
      "is the total number of nucleons (neutrons and protons collectively). Chemistry concerns itself with how electron sharing binds atoms into structures such as crystals and molecules. Nuclear physics deals with how protons and neutrons arrange themselves in nuclei. The study of subatomic particles, atoms and molecules, and their structure and interactions, requires quantum mechanics. Analyzing processes that change the numbers and types of particles requires quantum field theory. The study of subatomic particles \"per se\" is called particle physics. The term \"high-energy physics\" is nearly synonymous to \"particle physics\" since creation of particles requires high energies: it occurs only as"
    ]
  ],
  [
    "The hyperfine value for the EPR spectrum of fully deuteriated benzene radical anion C6D6\u2022- is isotropic.",
    [
      "from the detailed analysis of experimental emission spectra of 1,3,5- trifluoro- and hexafluoro (and chloro) benzene radical cations. For the parent benzene cation one has to rely on photoelectron spectra with comparatively lower resolution because this species does not fluoresce (see also Section on ). Rather detailed ab initio calculations have been carried out which document the JT stabilization energies for the various (four) JT active modes and also quantify the moderate barriers for the JT pseudorotation. Finally, a somewhat special role is played by systems with a fivefold symmetry axis like the cyclopentadienyl radical. Careful laser spectroscopic investigations have",
      "isotropic hyperfine splitting pattern for a radical freely tumbling in a solution (isotropic system) can be predicted. While it is easy to predict the number of lines, the reverse problem, unraveling a complex multi-line EPR spectrum and assigning the various spacings to specific nuclei, is more difficult. In the often encountered case of \"I\" = 1/2 nuclei (e.g., H, F, P), the line intensities produced by a population of radicals, each possessing \"M\" equivalent nuclei, will follow Pascal's triangle. For example, the spectrum at the right shows that the three H nuclei of the CH radical give rise to 2\"MI\"",
      "of an ion or molecule are often not conveniently measured. Electron spin resonance is instead the definitive and most widely used technique for characterizing radicals. The nature of the atom bearing the unpaired electron and its neighboring atoms can often be deduced by the EPR spectrum. The presence of radicals can also be detected or inferred by chemical reagents that trap (i.e. combine with) radicals. Often these traps are themselves radicals, such as TEMPO. Radical (chemistry) In chemistry, a radical is an atom, molecule, or ion that has an unpaired valence electron. With some exceptions, these unpaired electrons make radicals"
    ]
  ],
  [
    "NH3 (g) (ammonia) is the strongest base in liquid ammonia.",
    [
      "this ability as well. This occurs typically in compounds such as butyl lithium, alkoxides, and metal amides such as sodium amide. Bases of carbon, nitrogen and oxygen without resonance stabilization are usually very strong, or superbases, which cannot exist in a water solution due to the acidity of water. Resonance stabilization, however, enables weaker bases such as carboxylates; for example, sodium acetate is a weak base. A strong base is a basic chemical compound that can remove a proton (H) from (or \"deprotonate\") a molecule of even a very weak acid (such as water) in an acid-base reaction. Common examples",
      "liquid ammonia are similar to the reactions in water: Nitric acid can be a base in liquid sulfuric acid: The unique strength of this definition shows in describing the reactions in aprotic solvents; for example, in liquid : Because the solvent system definition depends on the solute as well as on the solvent itself, a particular solute can be either an acid or a base depending on the choice of the solvent: is a strong acid in water, a weak acid in acetic acid, and a weak base in fluorosulfonic acid; this characteristic of the theory has been seen as",
      "more completely and are thus stronger bases. As stated above, pH of the solution depends on the H concentration, which is related to the OH concentration by the self-ionization constant (K = 1.0x10). A strong base has a lower H concentration because they are fully protonated and less hydrogen ions remain in the solution. A lower H concentration also means a higher OH concentration and therefore, a larger K. NaOH (s) (sodium hydroxide) is a stronger base than (CHCH)NH (l) (diethylamine) which is a stronger base than NH (g) (ammonia). As the bases get weaker, the smaller the K values"
    ]
  ],
  [
    "The populations of the 1H energy levels of a molecule after a 5.0 \u03bcs pulse when B1 = 4.697 mT can be calculated using the formula for the energy level spacing in a hydrogen-like ion.",
    [
      "state. Note: The \"energy\" units in the above table are actually the reciprocal of the wavelength of a photon emitted in a transition to the lowest energy state. The actual energy can be found by multiplying the given statistic by the product of \"c\" (the speed of light) and \"h\" (Planck's constant), i.e., about 1.99 \u00d7 10 Joule metres, and then multiplying by a further factor of 100 to convert from cm to m. The aforementioned fluorescence occurs in distinct regions of the electromagnetic spectrum, called \"emission bands\": each band corresponds to a particular transition from a higher electronic state",
      "and formula_10 is the wavelength (in vacuum) of the emitted or absorbed light. A refinement of the Bohr model takes into account the fact that the mass of the atomic nucleus is not actually infinite compared to the mass of the electron. Then the formula is: where formula_12 and \"M\" is the total mass of the nucleus. This formula comes from substituting the reduced mass for the mass of the electron. A generalization of the Bohr model describes a hydrogen-like ion; that is, an atom with atomic number \"Z\" that has only one electron, such as C. In this case,",
      "only multiples of 1/\"T\". This is the classical radiation law: the frequencies emitted are integer multiples of 1/\"T\". In quantum mechanics, this emission must be in quanta of light, of frequencies consisting of integer multiples of 1/\"T\", so that classical mechanics is an approximate description at large quantum numbers. This means that the energy level corresponding to a classical orbit of period 1/\"T\" must have nearby energy levels which differ in energy by \"h/T\", and they should be equally spaced near that level, Bohr worried whether the energy spacing 1/\"T\" should be best calculated with the period of the energy"
    ]
  ],
  [
    "The Q-factor of a resonator is inversely related to the bandwidth of the cavity resonance, with a higher Q-factor resulting in a narrower bandwidth around the resonant frequency.",
    [
      "of the resonant frequency to the bandwidth of the cavity resonance. The average lifetime of a resonant photon in the cavity is proportional to the cavity's \"Q\". If the \"Q\" factor of a laser's cavity is abruptly changed from a low value to a high one, the laser will emit a pulse of light that is much more intense than the laser's normal continuous output. This technique is known as \"Q\"-switching. Q factor In physics and engineering the quality factor or \"Q\" factor is a dimensionless parameter that describes how underdamped an oscillator or resonator is, and characterizes a resonator's",
      "a parameter that describes the resonance behavior of an underdamped harmonic oscillator (resonator). Sinusoidally driven resonators having higher \"Q\" factors resonate with greater amplitudes (at the resonant frequency) but have a smaller range of frequencies around that frequency for which they resonate; the range of frequencies for which the oscillator resonates is called the bandwidth. Thus, a high-\"Q\" tuned circuit in a radio receiver would be more difficult to tune, but would have more selectivity; it would do a better job of filtering out signals from other stations that lie nearby on the spectrum. High-\"Q\" oscillators oscillate with a smaller",
      "\"Q\" factor or \"quality factor\" is a dimensionless parameter that describes how under-damped an oscillator or resonator is, or equivalently, characterizes a resonator's bandwidth relative to its center frequency. Higher \"Q\" indicates a lower rate of energy loss relative to the stored energy of the oscillator, i.e., the oscillations die out more slowly. A pendulum suspended from a high-quality bearing, oscillating in air, has a high \"Q\", while a pendulum immersed in oil has a low \"Q\". To sustain a system in resonance in constant amplitude by providing power externally, the energy provided in each cycle must be less than"
    ]
  ],
  [
    "The solvent has the largest effect on the chemical shift in fluorine NMR spectroscopy, with a potential variation of \u00b12 ppm or more.",
    [
      "chemical shifts in the literature vary strongly, commonly by over 1 ppm, even within the same solvent. Although the reference compound for F NMR spectroscopy, neat CFCl (0 ppm), has been used since the 1950s, clear instructions on how to measure and deploy it in routine measurements were not present until recently. An investigation of the factors influencing the chemical shift in fluorine NMR spectroscopy revealed the solvent to have the largest effect (\u0394\u03b4 = \u00b12 ppm or more). A solvent-specific reference table with 5 internal reference compounds has been prepared (CFCl, CHF, PhCF, CF and CFCOH) to allow reproducible",
      "information. F NMR chemical shifts are more difficult to predict, reflecting contributions from excited states. In contrast, 1H NMR shifts are dominated by the diamagnetic term.H NMR. Data presented in this section are referenced to CFCl as the standard (i.e. \u03b4 = 0). For vinylic fluorine substituents, the following formula allows for estimation of F chemical shfits:formula_1where Z is the statistical substituent chemical shift (SSCS) for the substituent in the listed position, and S is the interaction factor. Some representative values for use in this equation are provided in the table below: When determining the F chemical shifts of aromatic",
      "and one for the asymmetric. The carbon\u2013fluorine bands are so strong that they may obscure any carbon\u2013hydrogen bands that might be present. Organofluorine compounds can also be characterized using NMR spectroscopy, using carbon-13, fluorine-19 (the only natural fluorine isotope), or hydrogen-1 (if present). The chemical shifts in F NMR appear over a very wide range, depending on the degree of substitution and functional group. The table below shows the ranges for some of the major classes. Carbon\u2013fluorine bond The carbon\u2013fluorine bond is a polar covalent bond between carbon and fluorine that is a component of all organofluorine compounds. It is"
    ]
  ],
  [
    "The solution with a strong electrolyte such as HCl will have the highest ionic strength based on complete dissociation of ions. (Dissociation (chemistry); Ionic strength)",
    [
      "solution completely or nearly completely as ions. Again, the strength of an electrolyte is defined as the percentage of solute that is ions, rather than molecules. The higher the percentage, the stronger the electrolyte. Thus, even if a substance is not very soluble, but does dissociate completely into ions, the substance is defined as a strong electrolyte. Similar logic applies to a weak electrolyte. Strong acids and bases are good examples such as HCl, and HSO. These will all exist as ions in an aqueous medium. The degree of dissociation in gases is denoted by the symbol \u03b1 where \u03b1",
      "Ionic strength The concept of ionic strength was first introduced by Lewis and Randall in 1921 while describing the activity coefficients of strong electrolytes. The ionic strength of a solution is a measure of the concentration of ions in that solution. Ionic compounds, when dissolved in water, dissociate into ions. The total electrolyte concentration in solution will affect important properties such as the dissociation constant or the solubility of different salts. One of the main characteristics of a solution with dissolved ions is the ionic strength. Ionic strength can be molar (mol/L) or molal (mol/kg water) and to avoid confusion",
      "the units should be stated explicitly. The molar ionic strength, \"I\", of a solution is a function of the concentration of \"all\" ions present in that solution. where one half is because we are including both cations and anions, \"c\" is the molar concentration of ion i (M, mol/L), \"z\" is the charge number of that ion, and the sum is taken over all ions in the solution. For a 1:1 electrolyte such as sodium chloride, where each ion is singly-charged, the ionic strength is equal to the concentration. For the electrolyte MgSO, however, each ion is doubly-charged, leading to"
    ]
  ],
  [
    "The optimal pulse width for a 90\u00b0 pulse in chemistry experiments is typically determined based on the specific experimental setup and desired outcomes.",
    [
      "biased to early arriving photons, the photon count rate is kept low (usually less than 1% of excitation rate). This electrical pulse comes after the second laser pulse excites the molecule to a higher energy state, and a photon is eventually emitted from a single molecule upon returning to its original state. Thus, the longer a molecule takes to emit a photon, the higher the voltage of the resulting pulse. The central concept of this technique is that only a single photon is needed to discharge the capacitor. Thus, this experiment must be repeated many times to gather the full",
      "that cold-junction compensation may be applied to correct the apparent measured EMF to the standard cold-junction temperature of 0 degrees C. To make a potentiometric determination of an analyte in a solution, the potential of the cell is measured. This measurement must be corrected for the reference and junction potentials. It can also used in standardisation methods. The concentration of the analyte can then be calculated from the Nernst Equation. Many varieties of this basic principle exist for quantitative measurements. A metre bridge is a simple type of potentiometer which may be used in school science laboratories to demonstrate the",
      "some of the details that a pulse response could show. It is easy to numerically integrate an experimental pulse response to obtain a very high-quality estimate of the step response, but the reverse is not the case because any noise in the concentration measurement will be amplified by numeric differentiation. In chemical reactors, the goal is to make components react with a high yield. In a homogeneous, first-order reaction, the probability that an atom or molecule will react depends only on its residence time: for a rate constant formula_51. Given a RTD, the average probability is equal to the ratio"
    ]
  ],
  [
    "Planck's quantum theory explained the photoelectric effect, the nuclear model of the atom, and the structure of atomic orbits, among other experimental observations.",
    [
      "which they would occur, would need to be derived from an actual theory of quantum gravity. The reciprocal of the Planck time, which is Planck frequency, can be interpreted as an upper bound on the frequency of a wave. This follows from the interpretation of the Planck length as a minimal length, and hence a lower bound on the wavelength. All scientific experiments and human experiences occur over time scales that are dozens of orders of magnitude longer than the Planck time, making any events happening at the Planck scale undetectable with current scientific knowledge. , the smallest time interval",
      "him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld. This phase is known as old quantum theory. According to Planck, each energy element (\"E\")\" \"is proportional to its frequency (\"\u03bd\"): where \"h\" is Planck's constant. Planck",
      "Michelson\u2013Morley experiment on Earth's drift through a luminiferous ether. Conversely, Einstein was awarded the Nobel Prize for explaining the photoelectric effect, previously an experimental result lacking a theoretical formulation. A physical theory is a model of physical events. It is judged by the extent to which its predictions agree with empirical observations. The quality of a physical theory is also judged on its ability to make new predictions which can be verified by new observations. A physical theory differs from a mathematical theorem in that while both are based on some form of axioms, judgment of mathematical applicability is not"
    ]
  ],
  [
    "Six hours (meaning 94% of it decays to Tc in 24 hours).",
    [
      "6.0058 hours (meaning 93.7% of it decays to Tc in 24 hours). The relatively \"short\" physical half-life of the isotope and its biological half-life of 1 day (in terms of human activity and metabolism) allows for scanning procedures which collect data rapidly but keep total patient radiation exposure low. The same characteristics make the isotope suitable only for diagnostic but never therapeutic use. Technetium-99m was discovered as a product of cyclotron bombardment of molybdenum. This procedure produced molybdenum-99, a radionuclide with a longer half-life (2.75 days), which decays to Tc-99m. At present, molybdenum-99 (Mo-99) is used commercially as the easily",
      "six hours (meaning 94% of it decays to Tc in 24 hours). The \"short\" physical half-life of the isotope and its biological half-life of 1 day (in terms of human activity and metabolism) allows for scanning procedures which collect data rapidly, but keep total patient radiation exposure low. Diagnostic treatment involving technetium-99m will result in radiation exposure to technicians, patients, and passers-by. Typical quantities of technetium administered for immunoscintigraphy tests, such as SPECT tests, range from (millicurie or mCi; and Mega-Becquerel or MBq) for adults. These doses result in radiation exposures to the patient around 10 mSv (1000 mrem), the",
      "is the time it takes for a substance (drug, radioactive nuclide, or other) to lose one-half of its pharmacologic, physiologic, or radiological activity. In a medical context, the half-life may also describe the time that it takes for the concentration of a substance in blood plasma to reach one-half of its steady-state value (the \"plasma half-life\"). The relationship between the biological and plasma half-lives of a substance can be complex, due to factors including accumulation in tissues, active metabolites, and receptor interactions. While a radioactive isotope decays almost perfectly according to so-called \"first order kinetics\" where the rate constant is"
    ]
  ],
  [
    "The orbital angular momentum quantum number, l, of the electron that is most easily removed when ground-state aluminum is ionized is 1.",
    [
      "have half-lives well under an hour. Three metastable states are known, all with half-lives under a minute. An aluminium atom has 13 electrons, arranged in an electron configuration of [Ne]3s3p, with three electrons beyond a stable noble gas configuration. Accordingly, the combined first three ionization energies of aluminium are far lower than the fourth ionization energy alone. Aluminium can relatively easily surrender its three outermost electrons in many chemical reactions (see below). The electronegativity of aluminium is 1.61 (Pauling scale). A free aluminium atom has a radius of 143 pm. With the three outermost electrons removed, the radius shrinks to",
      "of the two electrons will leave \"d\"(1,2) invariant. In such a case neither l nor l is a constant of motion in general, but the total orbital angular momentum L = l + l is. Given the eigenstates of l and l, the construction of eigenstates of L (which still is conserved) is the \"coupling of the angular momenta of electrons 1 and 2\". The total orbital angular momentum quantum number L is restricted to integer values and must satisfy the triangular condition that formula_1, such that the three nonnegative integer values could correspond to the three sides of a",
      "is, approximately, the mass of the electron (more accurately, it is the reduced mass of the system consisting of the electron and the nucleus), and formula_8 is the reduced Planck constant. Different values of \"l\" give solutions with different angular momentum, where \"l\" (a non-negative integer) is the quantum number of the orbital angular momentum. The magnetic quantum number \"m\" (satisfying formula_9) is the (quantized) projection of the orbital angular momentum on the \"z\"-axis. See here for the steps leading to the solution of this equation. In addition to \"l\" and \"m\", a third integer \"n\" > 0, emerges from"
    ]
  ],
  [
    "At a pH below 2, the majority of the species present in solutions of hypochlorous acid are HOCl and Cl2.",
    [
      "is more stable in dilute solutions that contain solvated and ions. The density of the solution is 1.093 g/mL at 5% concentration, and 1.21 g/mL at 14%, 20 deg C. Stoichometric solutions are fairly alkaline, with pH 11 or more since hypochlorous acid is a weak acid: The following species and equilibria are present in solutions of : The second equilibrium equation above will be shifted to the right if the chlorine is allowed to escape as gas. The ratios of , HOCl, and in solution are also pH dependent. At pH below 2, the majority of the chlorine in",
      "aqueous solutions. Their primary applications are as bleaching, disinfection, and water treatment agents but they are also used in chemistry for chlorination and oxidation reactions. Acidification of hypochlorites generates hypochlorous acid. This exists in an equilibrium with chlorine gas, which can bubble out of solution. The equilibrium is subject to Le Chatelier's principle; thus a high pH drives the reaction to the left by consuming ions, promoting the disproportionation of chlorine into chloride and hypochlorite, whereas a low pH drives the reaction to the right, promoting the release of chlorine gas. Hypochlorous acid also exists in equilibrium with its anhydride;",
      "the solution is in the form of dissolved elemental . At pH greater than 7.4, the majority is in the form of hypochlorous acid . The equilibrium can be shifted by adding acids (such as hydrochloric acid) or bases (such as sodium hydroxide) to the solution: At a pH of about 4, such as obtained by the addition of strong acids like hydrochloric acid, the amount of undissociated (nonionized) HOCl is highest. The reaction can be written as: Sodium hypochlorite solutions combined with acid evolve chlorine gas, particularly strongly at pH < 2, by the reactions: At pH > 8,"
    ]
  ],
  [
    "The relationship between the EPR splitting pattern of a silyl radical bearing an Si-H\u00b7 fragment and the C=C resonance in divalent species is that electron density is shifted out of the C=C bond into the ring and empty Si 3p orbital, weakening the C=C bond and causing the observed energy drop and shape change in the C=C resonance.",
    [
      "isotropic hyperfine splitting pattern for a radical freely tumbling in a solution (isotropic system) can be predicted. While it is easy to predict the number of lines, the reverse problem, unraveling a complex multi-line EPR spectrum and assigning the various spacings to specific nuclei, is more difficult. In the often encountered case of \"I\" = 1/2 nuclei (e.g., H, F, P), the line intensities produced by a population of radicals, each possessing \"M\" equivalent nuclei, will follow Pascal's triangle. For example, the spectrum at the right shows that the three H nuclei of the CH radical give rise to 2\"MI\"",
      "sprio-[tBuN-CH=CH-tBuN]Si. The four-coordinate silanes exhibit C=C stretches at 1620 cm. Conversely [tBuN-CH=CH-tBuN]Si:'s C=C resonance is found bifurcated at 1566 cm and 1573 cm at a much greater intensity. In the divalent species, electron density is shifted out of the C=C bond into the ring and empty Si 3p orbital, weakening the C=C bond and causing the observed energy drop and shape change in the C=C resonance.Several computational studies suggest aromatic delocalization over the unsaturated NHSi rings. DFT calculations of NHSi-phosphonyl radical adducts show far greater radical electron delocalization over the rings of unsaturated NHSis (14% Si, 65% ring) than the",
      "configuration interaction (CI) and coupled cluster (CC). Initially the question was whether disilyne, SiH had the same structure as ethyne (acetylene), CH. In early studies, by Binkley and Lischka and Kohler, it became clear that linear SiH was a transition structure between two equivalent trans-bent structures and that the ground state was predicted to be a four-membered ring bent in a 'butterfly' structure with hydrogen atoms bridged between the two silicon atoms. Interest then moved to look at whether structures equivalent to vinylidene (Si=SiH) existed. This structure is predicted to be a local minimum, i. e. an isomer of SiH,"
    ]
  ],
  [
    "Elements with partially filled 4f or 5f orbitals such as the lanthanides and actinides are not included in the complexity of the superactinides due to the extreme energy level similarities of their orbitals.",
    [
      "is larger due to the fact that 32 electrons are filled in the deeply buried 5g and 6f shells, instead of just 14 electrons being filled in the 4f and 5f shells in the lanthanides and actinides respectively. Pekka Pyykk\u00f6 divides these superactinides into three series: a 5g series (elements 121 to 138), an 8p series (elements 139 to 140), and a 6f series (elements 141 to 155), also noting that there would be a great deal of overlapping between energy levels and that the 6f, 7d, or 8p orbitals could well also be occupied in the early superactinide atoms",
      "be so large that occupation of the 6f, 7d, and 8p subshells cannot be avoided in any superactinide. Because of the lack of radial nodes in the 5g orbitals, analogous to the 4f but not the 5f orbitals, the position of unbiunium in the periodic table is expected to be more akin to that of lanthanum than that of actinium among its congeners, and some have proposed to rename the superactinides as \"superlanthanides\" for that reason. The lack of radial nodes in the 4f orbitals contribute to their core-like behavior in the lanthanide series, unlike the more valence-like 5f orbitals",
      "5f configuration of the Np ion. Similar behavior is observed in the chemical inactivity of the 4f electrons in lanthanides; this is a consequence of the 5g orbitals being small and deeply buried in the electron cloud. The presence of electrons in g-orbitals, which do not exist in the ground state electron configuration of any currently known element, should allow presently unknown hybrid orbitals to form and influence the chemistry of the superactinides in new ways, although the absence of \"g\" electrons in known elements makes predicting superactinide chemistry more difficult. In the later superactinides, the oxidation states should become"
    ]
  ],
  [
    "The spin density on the central carbon atom of malonic acid radical can be determined by analyzing the hyperfine value for the \u03b1-hydrogen atom, which is 21.9 G.",
    [
      "atom caused by the interaction between the magnetic moment of the nucleus and the combined spin and orbital magnetic moment of the electron. The hyperfine splitting in hydrogen, measured using Ramsey's hydrogen maser, is known with great precision. Unfortunately, the influence of the proton's internal structure limits how precisely the splitting can be predicted theoretically. This leads to the extracted value of \u03b1 being dominated by theoretical uncertainty: The hyperfine splitting in muonium, an \"atom\" consisting of an electron and an antimuon, provides a more precise measurement of \u03b1 because the muon has no internal structure: The Lamb shift is",
      "hydrocarbons. His theoretical and experimental studies of nuclear hyperfine interactions in such compounds showed conclusively that this interaction gave a measure of the unpaired electron spin densities on the carbon atoms (see McConnell equation for details). His theoretical and experimental investigations of the anisotropic nuclear hyperfine interactions laid a firm foundation for the analysis of the paramagnetic resonance spectra of organic free radicals in. molecular crystals. His work also provided the first experimental demonstration of a negative spin density at a proton. He also realized that certain nitric oxide free radicals had the potential of providing labels for studying molecular",
      "one electron in each of the p orbitals and the high energy form is the singlet state with an electron pair filling one p orbital and the other one vacant. As with carbenes, a strong correlation exists between the spin density on the nitrogen atom which can be calculated in silico and the zero-field splitting parameter \"D\" which can be derived experimentally from electron spin resonance. Small nitrenes such as NH or CFN have D values around 1.8 cm with spin densities close to a maximum value of 2. At the lower end of the scale are molecules with low"
    ]
  ],
  [
    "The quantum yield in fluorescence spectroscopy is defined as the ratio of the number of photons emitted to the number of photons absorbed.",
    [
      "the system initially prepared in some other quantum state. For example, a singlet to triplet transition quantum yield is the fraction of molecules that, after being photoexcited into a singlet state, cross over to the triplet state. The fluorescence quantum yield is defined as the ratio of the number of photons emitted to the number of photons absorbed. Experimentally, relative fluorescence quantum yields can be determined by measuring fluorescence of a fluorophore of known quantum yield with the same experimental parameters (excitation wavelength, slit widths, photomultiplier voltage etc.) as the substance in question. The quantum yield is then calculated by:",
      "Quantum yield The quantum yield (\u03a6) of a radiation-induced process is the number of times a specific event occurs per photon absorbed by the system. The \"event\" is typically a kind of chemical reaction. The quantum yield for the decomposition of a reactant molecule in a decomposition reaction is defined as: Quantum yield can also be defined for other events, such as fluorescence: Here, quantum yield is the emission efficiency of a given fluorophore. Quantum yield is used in modeling photosynthesis: In a chemical photodegradation process, when a molecule dissociates after absorbing a light quantum, the quantum yield is the",
      "of photons emitted to the number of photons absorbed. The maximum possible fluorescence quantum yield is 1.0 (100%); each photon absorbed results in a photon emitted. Compounds with quantum yields of 0.10 are still considered quite fluorescent. Another way to define the quantum yield of fluorescence is by the rate of excited state decay: where formula_6 is the rate constant of spontaneous emission of radiation and is the sum of all rates of excited state decay. Other rates of excited state decay are caused by mechanisms other than photon emission and are, therefore, often called \"non-radiative rates\", which can include:"
    ]
  ],
  [
    "Yes, logical truth is considered to be necessarily true and remain true under all reinterpretations of its components other than its logical constants.",
    [
      "Logical truth Logical truth is one of the most fundamental concepts in logic, and there are different theories on its nature. A logical truth is a statement which is true, and remains true under all reinterpretations of its components other than its logical constants. It is a type of analytic statement. All of philosophical logic can be thought of as providing accounts of the nature of logical truth, as well as logical consequence. Logical truths (including tautologies) are truths which are considered to be necessarily true. This is to say that they are considered to be such that they could",
      "some interpretation or truth within some logical system. A logical truth (also called an analytic truth or a necessary truth) is a statement which is true in all possible worlds or under all possible interpretations, as contrasted to a \"fact\" (also called a \"synthetic claim\" or a \"contingency\") which is only true in this world as it has historically unfolded. A proposition such as \"If p and q, then p\" is considered to be a logical truth because of the meaning of the symbols and words in it and not because of any fact of any particular world. They are",
      "same interpretation, but it is possible that the truth value of the same sentence can be different under different interpretations. A sentence is \"consistent\" if it is true under at least one interpretation; otherwise it is \"inconsistent\". A sentence \u03c6 is said to be \"logically valid\" if it is satisfied by every interpretation (if \u03c6 is satisfied by every interpretation that satisfies \u03c8 then \u03c6 is said to be a \"logical consequence\" of \u03c8). Some of the logical symbols of a language (other than quantifiers) are truth-functional connectives that represent truth functions \u2014 functions that take truth values as arguments"
    ]
  ],
  [
    "The magnetic moment of a system with a total angular momentum quantum number J is given by the total magnetic moment formula.",
    [
      "quantum number, \"m\", and can take values from +\"S\" to \u2212\"S\", in integer steps. Hence for any given nucleus, there are a total of angular momentum states. The \"z\"-component of the angular momentum vector (formula_2) is therefore , where \"\u0127\" is the reduced Planck constant. The \"z\"-component of the magnetic moment is simply: Consider nuclei with a spin of one-half, like , or . Each nucleus has two linearly independent spin states, with \"m\" = or \"m\" = \u2212 (also referred to as spin-up and spin-down, or sometimes \u03b1 and \u03b2 spin states, respectively) for the z-component of spin. In",
      "is, approximately, the mass of the electron (more accurately, it is the reduced mass of the system consisting of the electron and the nucleus), and formula_8 is the reduced Planck constant. Different values of \"l\" give solutions with different angular momentum, where \"l\" (a non-negative integer) is the quantum number of the orbital angular momentum. The magnetic quantum number \"m\" (satisfying formula_9) is the (quantized) projection of the orbital angular momentum on the \"z\"-axis. See here for the steps leading to the solution of this equation. In addition to \"l\" and \"m\", a third integer \"n\" > 0, emerges from",
      "used; in Gaussian units, the prefactor would be 1/2\"c\" instead, where \"c\" is the speed of light.) In a quantum-mechanical context, this can also be written as where \u2212\"e\" and \"m\" are the charge and mass of the electron, \u03a8 is the ground-state wave function, and L is the angular momentum operator. The total magnetic moment is where the spin contribution is intrinsically quantum-mechanical and is given by where \"g\" is the electron spin g-factor, \"\u03bc\" is the Bohr magneton, \"\u0127\" is the reduced Planck constant, and S is the electron spin operator. The orbital magnetization M is defined as"
    ]
  ],
  [
    "The protons in a molecule with a rotational correlation time of 1 ns would have the fastest spin-lattice relaxation at a magnetic field strength where the tumbling frequency equals the Larmor frequency.",
    [
      "the two relaxation time constants for hydrogen nuclear spins in nonpathological human tissues. Following is a table of the approximate values of the two relaxation time constants for chemicals that commonly show up in human brain magnetic resonance spectroscopy (MRS) studies, physiologically or pathologically. The discussion above describes relaxation of nuclear magnetization in the presence of a constant magnetic field B. This is called relaxation in the laboratory frame. Another technique, called relaxation in the rotating frame, is the relaxation of nuclear magnetization in the presence of the field B together with a time-dependent magnetic field B. The field B",
      "on pure substances, but not for complicated environments such as the human body. This theory makes the assumption that the autocorrelation function of the microscopic fluctuations causing the relaxation is proportional to formula_18, where formula_19 is called the correlation time. From this theory, one can get T > T for magnetic dipolar relaxation: where formula_22 is the Larmor frequency in correspondence with the strength of the main magnetic field formula_23. formula_24 is the correlation time of the molecular tumbling motion. formula_25 is defined for spin-1/2 nuclei and is a constant with formula_26 being the magnetic permeability of free space of",
      "to the average rotation rate formula_3, spin-spin relaxation is not heavily dependent on magnetic field strength. This directly contrasts with spin-lattice relaxation, which occurs at tumbling frequencies equal to the Larmor frequency formula_4. Some frequency shifts, such as the NMR chemical shift, occur at frequencies proportional to the Larmor frequency, and the related but distinct parameter T can be heavily dependent on field strength due to the difficulty of correcting for inhomogeneity in stronger magnet bores. Assuming isothermal conditions, spins tumbling faster through space will generally have a longer T. Since slower tumbling displaces the spectral energy at high tumbling"
    ]
  ],
  [
    "The ionic substance with the greatest lattice enthalpy is chloride.",
    [
      "chloride, metals such as iron, or covalently linked materials such as diamond is considerably greater in magnitude than for solids such as sugar or iodine, whose neutral molecules interact only by weaker dipole-dipole or van der Waals forces. The relationship between the molar lattice energy and the molar lattice enthalpy is given by the following equation: where formula_2 is the molar lattice energy, formula_3 the molar lattice enthalpy and formula_4 the change of the volume per mole. Therefore, the lattice enthalpy further takes into account that work has to be performed against an outer pressure formula_5. The lattice energy of",
      "Lattice energy The lattice energy of a crystalline solid is a measure of the energy released when ions are combined to make a compound. It is a measure of the cohesive forces that bind ions. Lattice energy is relevant to many practical properties including solubility, hardness, and volatility. The lattice energy is usually deduced from the Born\u2013Haber cycle. The lattice energy is exothermic, i.e., the value of \"\u0394H\" is negative because it corresponds to the coalescing of infinitely separated gaseous ions in vacuum to form the ionic lattice. The lattice \"enthalpy\" is reported as a positive value. The concept of",
      "enthalpy change involved in the formation of an ionic compound from gaseous ions (an exothermic process), or sometimes defined as the energy to break the ionic compound into gaseous ions (an endothermic process). A Born\u2013Haber cycle applies Hess's law to calculate the lattice enthalpy by comparing the standard enthalpy change of formation of the ionic compound (from the elements) to the enthalpy required to make gaseous ions from the elements. This latter calculation is complex. To make gaseous ions from elements it is necessary to atomise the elements (turn each into gaseous atoms) and then to ionise the atoms. If"
    ]
  ],
  [
    "The proton resonating at 1.55 kHz away from TMS on a spectrometer with a 12.0 T magnet is the proton that is 3 bonds away from another proton with a signal at 1 ppm, with a coupling constant of 7 Hz.",
    [
      "of a proton which has a signal at 1 ppm. This proton is in a hypothetical molecule where three bonds away exists another proton (in a CH-CH group for instance), the neighbouring group (a magnetic field) causes the signal at 1 ppm to split into two, with one peak being a few hertz higher than 1 ppm and the other peak being the same number of hertz lower than 1 ppm. These peaks each have half the area of the former singlet peak. The magnitude of this splitting (difference in frequency between peaks) is known as the coupling constant. A",
      "typical coupling constant value would be 7 Hz. The coupling constant is independent of magnetic field strength because it is caused by the magnetic field of another nucleus, not the spectrometer magnet. Therefore, it is quoted in hertz (frequency) and not ppm (chemical shift). In another molecule a proton resonates at 2.5 ppm and that proton would also be split into two by the proton at 1 ppm. Because the magnitude of interaction is the same the splitting would have the same coupling constant 7 Hz apart. The spectrum would have two signals, each being a doublet. Each doublet will",
      "gyromagnetic ratio of that isotope. The signal strength is proportional both to the stimulating magnetic field and the number of nuclei of that isotope in the sample. Thus in the 21 tesla magnetic field that may be found in high resolution laboratory NMR spectrometers, protons resonate at 900 MHz. However, in the Earth's magnetic field the same nuclei resonate at audio frequencies of around 2 kHz and generate very weak signals. The location of a nucleus within a complex molecule affects the 'chemical environment' (i.e. the rotating magnetic fields generated by the other nuclei) experienced by the nucleus. Thus different"
    ]
  ],
  [
    "The equilibrium polarization of 13C nuclei in a 20.0 T magnetic field at 300 K is determined by dynamic nuclear polarization, where spin polarization is transferred from electrons to nuclei.",
    [
      "in a number of ways. Another way of circumventing dipolar truncation in case of rare nuclei like 13C is to study the systems at their natural isotopic abundance utilising DNP assisted solid-state NMR under magic-angle spinning, where the probability of finding a third spin is almost 100 times lower. The chemical shielding is a local property of each nucleus, and depends on the external magnetic field. Specifically, the external magnetic field induces currents of the electrons in molecular orbitals. These induced currents create local magnetic fields that often vary across the entire molecular framework such that nuclei in distinct molecular",
      "expression is: where: Curie's law is valid under the commonly encountered conditions of low magnetization (\"\u03bc\"\"H\" \u2272 \"k\"\"T\"), but does not apply in the high-field/low-temperature regime where saturation of magnetization occurs (\"\u03bc\"\"H\" \u2273 \"k\"\"T\") and magnetic dipoles are all aligned with the applied field. When the dipoles are aligned, increasing the external field will not increase the total magnetization since there can be no further alignment. For a paramagnetic ion with noninteracting magnetic moments with angular momentum \"J\", the Curie constant is related the individual ions' magnetic moments, where \"n\" is the number of atoms per unit volume. The parameter",
      "B. Dynamic nuclear polarisation Dynamic nuclear polarization (DNP) results from transferring spin polarization from electrons to nuclei, thereby aligning the nuclear spins to the extent that electron spins are aligned. Note that the alignment of electron spins at a given magnetic field and temperature is described by the Boltzmann distribution under the thermal equilibrium. It is also possible that those electrons are aligned to a higher degree of order by other preparations of electron spin order such as: chemical reactions (leading to Chemical-Induced DNP, CIDNP), optical pumping and spin injection. DNP is considered one of several techniques for hyperpolarization. DNP"
    ]
  ],
  [
    "The unknown nucleotide with a nuclear spin of 1 and a magnetic moment of 2.884 x 10^-27 J T-1 is a proton.",
    [
      "constant and standard unit for the magnetic moments of nuclear components. In SI units, the CODATA value of (). A magnetic moment is a vector quantity, and the direction of the proton's magnetic moment is defined by its spin. The torque on the proton resulting from an external magnetic field is towards aligning the proton's spin vector in the same direction as the magnetic field vector. The nuclear magneton is the spin magnetic moment of a Dirac particle, a charged, spin 1/2 elementary particle, with a proton's mass \"m\". In SI units, the nuclear magneton is where \"e\" is the",
      "requires a relativistic treatment. A calculation of nucleon magnetic moments from first principles is not yet available. Proton magnetic moment The proton magnetic moment is the magnetic dipole moment of the proton, symbol \"\u03bc\". Protons and neutrons, both nucleons, comprise the nucleus of an atom, and both nucleons act as small magnets whose strength is measured by their magnetic moments. The magnitude of the proton's magnetic moment indicates that the proton is not an elementary particle. CODATA's recommended value for the magnetic moment of the proton is . A more precise measurement has since been claimed, with a result of",
      "which means they are fermions and, like electrons (and unlike bosons), are subject to the Pauli exclusion principle, a very important phenomenon in nuclear physics: protons and neutrons in an atomic nucleus cannot all be in the same quantum state; instead they spread out into nuclear shells analogous to electron shells in chemistry. Also important, this spin (of proton and neutron) is the source of nuclear spin in larger nuclei. Nuclear spin is best known for its crucial role in the NMR/MRI technique for chemical and biochemical analyses. The magnetic moment of a proton, denoted \u03bc, is , while the"
    ]
  ],
  [
    "Mass spectrometry is characterized by measuring the mass-to-charge ratio of charged particles using operating pressures in the range of 310,000 to 775,000 torr (6000 to 15000 psi).",
    [
      "operating pressures in the range of 310.000 to 775.000 torr (6000 to 15000 psi). Mass spectrometry (MS) is an analytical technique that measures the mass-to-charge ratio (\"m/z)\" of charged particles (ions). Although there are many different kinds of mass spectrometers, all of them make use of electric or magnetic fields to manipulate the motion of ions produced from an analyte of interest and determine their \"m/z.\" The basic components of a mass spectrometer are the ion source, the mass analyzer, the detector, and the data and vacuum systems.The ion source is where the components of a sample introduced in a",
      "instrument are available for detection. Mass spectrum A mass spectrum is an intensity vs. \"m/z\" (mass-to-charge ratio) plot representing a chemical analysis. Hence, the mass spectrum of a sample is a pattern representing the distribution of ions by mass (more correctly: mass-to-charge ratio) in a sample. It is a histogram usually acquired using an instrument called a mass spectrometer. Not all mass spectra of a given substance are the same. For example, some mass spectrometers break the analyte molecules into fragments; others observe the intact molecular masses with little fragmentation. A mass spectrum can represent many different types of information",
      "Mass spectrum A mass spectrum is an intensity vs. \"m/z\" (mass-to-charge ratio) plot representing a chemical analysis. Hence, the mass spectrum of a sample is a pattern representing the distribution of ions by mass (more correctly: mass-to-charge ratio) in a sample. It is a histogram usually acquired using an instrument called a mass spectrometer. Not all mass spectra of a given substance are the same. For example, some mass spectrometers break the analyte molecules into fragments; others observe the intact molecular masses with little fragmentation. A mass spectrum can represent many different types of information based on the type of"
    ]
  ],
  [
    "Light-scattering techniques are a type of spectroscopy known as Fourier-transform spectroscopy.",
    [
      "any absorption spectroscopy (FTIR, ultraviolet-visible (\"UV-Vis\") spectroscopy, etc.) is to measure how well a sample absorbs light at each wavelength. The most straightforward way to do this, the \"dispersive spectroscopy\" technique, is to shine a monochromatic light beam at a sample, measure how much of the light is absorbed, and repeat for each different wavelength. (This is how some UV\u2013vis spectrometers work, for example.) Fourier-transform spectroscopy is a less intuitive way to obtain the same information. Rather than shining a monochromatic beam of light (a beam composed of only a single wavelength) at the sample, this technique shines a beam",
      "more sensitive and has a much shorter sampling time than conventional spectroscopic techniques, but is only applicable in a laboratory environment). The term \"Fourier-transform spectroscopy\" reflects the fact that in all these techniques, a Fourier transform is required to turn the raw data into the actual spectrum, and in many of the cases in optics involving interferometers, is based on the Wiener\u2013Khinchin theorem. One of the most basic tasks in spectroscopy is to characterize the spectrum of a light source: how much light is emitted at each different wavelength. The most straightforward way to measure a spectrum is to pass",
      "cases spectral lines are well separated and distinguishable, but spectral lines can also overlap and appear to be a single transition if the density of energy states is high enough. Named series of lines include the principal, sharp, diffuse and fundamental series. Spectroscopy is a sufficiently broad field that many sub-disciplines exist, each with numerous implementations of specific spectroscopic techniques. The various implementations and techniques can be classified in several ways. The types of spectroscopy are distinguished by the type of radiative energy involved in the interaction. In many applications, the spectrum is determined by measuring changes in the intensity"
    ]
  ],
  [
    "Violating the uncertainty principle implies the violation of the second law of thermodynamics, as the irreversibility of thermodynamics must be statistical in nature and not impossible. (Uncertainty principle; irreversible process)",
    [
      "free will. One critique, however, is that apart from the basic role of quantum mechanics as a foundation for chemistry, nontrivial biological mechanisms requiring quantum mechanics are unlikely, due to the rapid decoherence time of quantum systems at room temperature. The standard view, however, is that this decoherence is overcome by both screening and decoherence-free subspaces found in biological cells. There is reason to believe that violating the uncertainty principle also strongly implies the violation of the second law of thermodynamics. Uncertainty principle In quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle) is any of a variety",
      "uncertainty principle). The irreversibility of thermodynamics must be statistical in nature; that is, that it must be merely highly unlikely, but not impossible, that a system will lower in entropy. The German physicist Rudolf Clausius, in the 1850s, was the first to mathematically quantify the discovery of irreversibility in nature through his introduction of the concept of entropy. In his 1854 memoir \"On a Modified Form of the Second Fundamental Theorem in the Mechanical Theory of Heat,\" Clausius states: Simply, Clausius states that it is impossible for a system to transfer heat from a cooler body to a hotter body.",
      "uncertainty principle is a mathematical relation asserting an upper limit to the accuracy of the simultaneous measurement of any pair of conjugate variables, e.g. position and momentum. In the formalism of operator notation, this limit is the evaluation of the commutator of the variables' corresponding operators. The uncertainty principle arose as an answer to the question: How does one measure the location of an electron around a nucleus if an electron is a wave? When quantum mechanics was developed, it was seen to be a relation between the classical and quantum descriptions of a system using wave mechanics. In March"
    ]
  ],
  [
    "Bond-dissociation energy is the energy required to break a single chemical bond in a molecule, and it is typically measured by determining the enthalpy change involved in breaking up one molecule of a specific compound into its constituent atoms.",
    [
      "Bond energy In chemistry, bond energy (\"E\") or bond enthalpy (\"H\") is the measure of bond strength in a chemical bond. IUPAC defines bond energy as the average value of the gas-phase bond dissociation energies (usually at a temperature of 298 K) for all bonds of the same type within the same chemical species. For example, the carbon\u2013hydrogen bond energy in methane \"H\"(C\u2013H) is the enthalpy change involved with breaking up one molecule of methane into a carbon atom and four hydrogen radicals, divided by 4. Tabulated bond energies are generally values of bond energies averaged over a number of",
      "selected typical chemical species containing that type of bond. Bond energy (\"E\") or bond enthalpy (\"H\") should not be confused with bond-dissociation energy. Bond energy is the average of all the bond-dissociation energies in a molecule, and will show a different value for a given bond than the bond-dissociation energy would. This is because the energy required to break a single bond in a specific molecule differs for each bond in that molecule. For example, methane has four C\u2013H bonds and the bond-dissociation energies are 435 kJ/mol for \"D\"(CH\u2013H), 444 kJ/mol for \"D\"(CH\u2013H), 444 kJ/mol for \"D\"(CH\u2013H) and 339 kJ/mol",
      "since the free energy changes for the aforementioned thermochemical steps can be determined from parameters like acid dissociation constants (p\"K\") and standard redox potentials (\u2107\u00b0) that are measured in solution. Except for diatomic molecules, the bond-dissociation energy differs from the \"bond energy\". While the bond-dissociation energy is the energy of a single chemical bond, the bond energy is the average of all the bond-dissociation energies of the bonds of the same type for a given molecule. For a homoleptic compound EX\"\", the E\u2013X bond energy is (1/\"n\") multiplied by the enthalpy change of the reaction EX\"\" \u2192 E + \"n\"X."
    ]
  ],
  [
    "The magnetogyric ratio of 23Na is 11.26 rad\u00b7s\u00b7T.",
    [
      ". The gyromagnetic ratio, symbol \u03b3, of a particle or system is the ratio of its magnetic moment to its spin angular momentum, or For nucleons, the ratio is conventionally written in terms of the proton mass and charge, by the formula The proton's gyromagnetic ratio, symbol \u03b3, is rad\u00b7s\u00b7T. The gyromagnetic ratio is also the ratio between the observed angular frequency of Larmor precession (in rad s) and the strength of the magnetic field in proton NMR applications, such as in MRI imaging or proton magnetometers. For this reason, the value of \"\u03b3\" is often given in units of",
      "Gyromagnetic ratio In physics, the gyromagnetic ratio (also sometimes known as the magnetogyric ratio in other disciplines) of a particle or system is the ratio of its magnetic moment to its angular momentum, and it is often denoted by the symbol \u03b3, gamma. Its SI unit is the radian per second per tesla (rad\u22c5s\u22c5T) or, equivalently, the coulomb per kilogram (C\u22c5kg). The term \"gyromagnetic ratio\" is often used as a synonym for a \"different\" but closely related quantity, the g-factor. The g-factor, unlike the gyromagnetic ratio, is dimensionless. For more on the g-factor, see below, or see the article g-factor.",
      "nuclear magneton. For the neutron, I is 1/2 \"\u0127\", so the neutron's \"g\"-factor, symbol \"g\", is . The gyromagnetic ratio, symbol \u03b3, of a particle or system is the ratio of its magnetic moment to its spin angular momentum, or For nucleons, the ratio is conventionally written in terms of the proton mass and charge, by the formula The neutron's gyromagnetic ratio, symbol \u03b3, is . The gyromagnetic ratio is also the ratio between the observed angular frequency of Larmor precession (in rad s) and the strength of the magnetic field in nuclear magnetic resonance applications, such as in MRI"
    ]
  ],
  [
    "Systems involving three or more particles, such as all other atomic or molecular systems, cannot have exact solutions of the Schr\u00f6dinger equation in quantum mechanics and quantum chemistry.",
    [
      "the wave function is the most complete description that can be given of a physical system. Solutions to Schr\u00f6dinger's equation describe not only molecular, atomic, and subatomic systems, but also macroscopic systems, possibly even the whole universe. Schr\u00f6dinger's equation is central to all applications of quantum mechanics including quantum field theory which combines special relativity with quantum mechanics. Theories of quantum gravity, such as string theory, also do not modify Schr\u00f6dinger's equation. The Schr\u00f6dinger equation is not the only way to study quantum mechanical systems and make predictions, as there are other quantum mechanical formulations such as matrix mechanics, introduced",
      "Schr\u00f6dinger equation can only be obtained for the hydrogen atom (though exact solutions for the bound state energies of the hydrogen molecular ion have been identified in terms of the generalized Lambert W function). Since all other atomic, or molecular systems, involve the motions of three or more \"particles\", their Schr\u00f6dinger equations cannot be solved exactly and so approximate solutions must be sought. The foundation of quantum mechanics and quantum chemistry is the wave model, in which the atom is a small, dense, positively charged nucleus surrounded by electrons. The wave model is derived from the wave function, a set",
      "the potential grows to infinity, the motion is classically confined to a finite region. Viewed far enough away, every solution is reduced to an exponential; the condition that the exponential is decreasing restricts the energy levels to a discrete set, called the allowed energies. The Schr\u00f6dinger equation for this situation is It is a notable quantum system to solve for; since the solutions are exact (but complicated \u2013 in terms of Hermite polynomials), and it can describe or at least approximate a wide variety of other systems, including vibrating atoms, molecules, and atoms or ions in lattices, and approximating other"
    ]
  ],
  [
    "The discrepancy between the predicted molar heat capacity of a diatomic gas based on the equipartition theorem and the experimental values is that the theoretical prediction is around 7 cal/(mol\u00b7K) while the experimental values are typically about 5 cal/(mol\u00b7K) and fall to 3 cal/(mol\u00b7K) at very low temperatures.",
    [
      "raised concerns about the validity of the equipartition theorem. The theorem predicts that the molar heat capacity of simple monatomic gases should be roughly 3 cal/(mol\u00b7K), whereas that of diatomic gases should be roughly 7 cal/(mol\u00b7K). Experiments confirmed the former prediction, but found that molar heat capacities of diatomic gases were typically about 5 cal/(mol\u00b7K), and fell to about 3 cal/(mol\u00b7K) at very low temperatures. Maxwell noted in 1875 that the disagreement between experiment and the equipartition theorem was much worse than even these numbers suggest; since atoms have internal parts, heat energy should go into the motion of these",
      "the heat capacity of a mole of diatomic molecules with no other degrees of freedom should be (7/2)\"N\"\"k\" = (7/2)\"R\" and, thus, the predicted molar heat capacity should be roughly 7 cal/(mol\u00b7K). However, the experimental values for molar heat capacities of diatomic gases are typically about 5 cal/(mol\u00b7K) and fall to 3 cal/(mol\u00b7K) at very low temperatures. This disagreement between the equipartition prediction and the experimental value of the molar heat capacity cannot be explained by using a more complex model of the molecule, since adding more degrees of freedom can only \"increase\" the predicted specific heat, not decrease it.",
      "the gas pressure at each instant will have practically the same value (\"p\" = 1 atm) throughout. For a thermally perfect diatomic gas, the molar specific heat capacity at constant pressure (\"c\") is / or 29.1006 J mol deg. The molar heat capacity at constant volume (\"c\") is / or 20.7862 J mol deg. The ratio formula_15 of the two heat capacities is 1.4. The heat \"Q\" required to bring the gas from 300 to 600 K is The increase in internal energy is Therefore, formula_18 Also formula_19, which of course is identical to the difference between \u0394\"H\" and \u0394\"U\"."
    ]
  ],
  [
    "Nihonium exhibits a more stable +1 oxidation state compared to the +3 oxidation state. (Inert pair effect; less stable, as the energy released in forming two additional bonds and attaining the +3 state is not always enough to outweigh the energy needed to involve the s-electrons. Hence, for aluminium and gallium +3 is the most stable state, but +1 gains importance for indium and by thallium it becomes more stable than the +3 state. Nihonium is expected to continue this trend and have +1 as its most stable oxidation state.)",
    [
      "of the group readily portray their group oxidation state of +7 and the state becomes more stable as the group is descended. Thus bohrium is expected to form a stable +7 state. Technetium also shows a stable +4 state whilst rhenium exhibits stable +4 and +3 states. Bohrium may therefore show these lower states as well. The higher +7 oxidation state is more likely to exist in oxyanions, such as perbohrate, , analogous to the lighter permanganate, pertechnetate, and perrhenate. Nevertheless, bohrium(VII) is likely to be unstable in aqueous solution, and would probably be easily reduced to the more stable",
      "it was proposed, there were no known compounds of Group 13 elements with the intermediate, +2, oxidation state. This is no longer true since the discovery of certain complexes of Ga(II) and In(II), such as halides of the form [MX]. These complex ions are stabilized by the formation of a covalent M\u2013M bond. It follows that the instability of simple complexes of ions such as Ga is due to kinetic factors, namely that the Ga, having an unpaired electron, behaves as a free radical and is rapidly destroyed by reaction with another free radical. If Drago's explanation as described above",
      "less stable, as the energy released in forming two additional bonds and attaining the +3 state is not always enough to outweigh the energy needed to involve the s-electrons. Hence, for aluminium and gallium +3 is the most stable state, but +1 gains importance for indium and by thallium it becomes more stable than the +3 state. Nihonium is expected to continue this trend and have +1 as its most stable oxidation state. The simplest possible nihonium compound is the monohydride, NhH. The bonding is provided by the 7p electron of nihonium and the 1s electron of hydrogen. The SO"
    ]
  ],
  [
    "The structure of deuterium chloride (DCl) differs from hydrogen chloride (HCl) in that DCl forms zigzag chains in the solid state, while HCl forms face-centered arrays, leading to different infrared absorption frequencies.",
    [
      "the chlorine atoms are in a face-centered array. However, the hydrogen atoms could not be located. Analysis of spectroscopic and dielectric data, and determination of the structure of DCl (deuterium chloride) indicates that HCl forms zigzag chains in the solid, as does HF (see figure on right). The infrared spectrum of gaseous hydrogen chloride, shown on the left, consists of a number of sharp absorption lines grouped around 2886 cm (wavelength ~3.47 \u00b5m). At room temperature, almost all molecules are in the ground vibrational state \"v\" = 0. Including anharmonicity the vibrational energy can be written as. To promote an",
      "that \u0394\"J\" is only able to take values of \u00b11. The value of the rotational constant \"B\" is much smaller than the vibrational one \"\u03bd\", such that a much smaller amount of energy is required to rotate the molecule; for a typical molecule, this lies within the microwave region. However, the vibrational energy of HCl molecule places its absorptions within the infrared region, allowing a spectrum showing the rovibrational transitions of this molecule to be easily collected using an infrared spectrometer with a gas cell. The latter can even be made of quartz as the HCl absorption lies in a",
      "of the molecule can be conformational rather than structural. That is, for instance, a protein molecule with a helical secondary structure can have a CD that changes with changes in the conformation. By definition, where \u0394A (Delta Absorbance) is the difference between absorbance of left circularly polarized (LCP) and right circularly polarized (RCP) light (this is what is usually measured). \u0394A is a function of wavelength, so for a measurement to be meaningful the wavelength at which it was performed must be known. It can also be expressed, by applying Beer's law, as: where Then is the molar circular dichroism."
    ]
  ],
  [
    "s for radiation are 50 mSv in a single year with a maximum of 100 mSv in a consecutive five-year period for occupational exposure, and an average of 1 mSv per year for public exposure.",
    [
      "limits are \"situational\", for planned, emergency and existing situations. Within these situations, limits are given for the following groups; For occupational exposure, the limit is 50 mSv in a single year with a maximum of 100 mSv in a consecutive five-year period, and for the public to an average of 1 mSv (0.001 Sv) of effective dose per year, not including medical and occupational exposures. For comparison, natural radiation levels inside the US capitol building are such that a human body would receive an additional dose rate of 0.85 mSv/a, close to the regulatory limit, because of the uranium content",
      "welding and brazing operations, for example. Excessive exposure to natural or artificial UV-radiation means immediate (acute) and long-term (chronic) damage to the eye and skin. Occupational exposure limits may be one of two types: rate limited or dose limited. Rate limits characterize the exposure based on effective energy (radiance or irradiance, depending on the type of radiation and the health effect of concern) per area per time, and dose limits characterize the exposure as a total acceptable dose. The latter is applied when the intensity of the radiation is great enough to produce a thermal injury. The European Union (EU)",
      "Occupational exposure limit An occupational exposure limit is an upper limit on the acceptable concentration of a hazardous substance in workplace air for a particular material or class of materials. It is typically set by competent national authorities and enforced by legislation to protect occupational safety and health. It is an important tool in risk assessment and in the management of activities involving handling of dangerous substances. There are many dangerous substances for which there are no formal occupational exposure limits. In these cases, hazard banding or control banding strategies can be used to ensure safe handling. Occupational Exposure Limits"
    ]
  ],
  [
    "Non-toxic pollutants in water include salts, metals, organic chemicals, pesticides, herbicides, and radioactive contaminants.",
    [
      "is ingested into the body. Nonpoint source pollution can often have toxic contaminants and chemicals present in the water. Additionally, there can be both bacteria and viruses (aka pathogens) found in the water. Effluent from nonpoint sources may include toxic contaminates and chemical compounds including heavy metals like lead, mercury, zinc, and cadmium, organics like polychlorinated biphenyls (PCBs) and polycyclic aromatic hydrocarbons (PAHs), and other substances resistant to breakdown . There are many health effects associated with many of these toxic substances. Pathogens are bacteria and viruses that can be found in water and cause diseases in humans. Pathogens found",
      "tends to be focused on water that is treated for human consumption, industrial use, or in the environment. Contaminants that may be in untreated water include microorganisms such as viruses, protozoa and bacteria; inorganic contaminants such as salts and metals; organic chemical contaminants from industrial processes and petroleum use; pesticides and herbicides; and radioactive contaminants. Water quality depends on the local geology and ecosystem, as well as human uses such as sewage dispersion, industrial pollution, use of water bodies as a heat sink, and overuse (which may lower the level of the water). The United States Environmental Protection Agency (EPA)",
      "are toxic to viruses, fungi, bacteria and cancerous cells are considered a part of the immune system. In nuclear physics, a poison is a substance that obstructs or inhibits a nuclear reaction. For an example, see nuclear poison. Environmentally hazardous substances are not necessarily poisons, and vice versa. For example, food-industry wastewater\u2014which may contain potato juice or milk\u2014can be hazardous to the ecosystems of streams and rivers by consuming oxygen and causing eutrophication, but is nonhazardous to humans and not classified as a poison. Biologically speaking, any substance, if given in large enough amounts, is poisonous and can cause death."
    ]
  ],
  [
    "Aromatic amino acids absorb ultraviolet radiation above 250 nm in the UV region of the spectrum. (Gregorio Weber)",
    [
      "do not absorb light in the visible region of the electromagnetic spectrum, however, all absorb light from the infrared region due to their conjugated double bonds and the aromatic amino acids also absorb ultraviolet radiation. The aromatic amino acids phenylalanine, tyrosine, and tryptophan have the ability to significantly absorb ultraviolet radiation above 250 nm.This characteristic of aromatic amino acids is used to quantify the concentration of proteins in an unknown sample. These amino acids are able to absorb light which excites its electron to the excited state. When the electron falls back to its ground state, it will either emit",
      "can be estimated by measuring the OD at 280 nm due to the presence of tryptophan, tyrosine and phenylalanine. This method is not very accurate since the composition of proteins varies greatly and proteins with none of these amino acids do not have maximum absorption at 280 nm. Nucleic acid contamination can also interfere. This method requires a spectrophotometer capable of measuring in the UV region with quartz cuvettes. Ultraviolet-visible (UV-vis) spectroscopy involves energy levels that excite electronic transitions. Absorption of UV-vis light excites molecules that are in ground-states to their excited-states Visible region 400\u2013700 nm spectrophotometry is used extensively",
      "the region 3000 - 4000 \u00c5. At about the same time Weber and Teale were carrying out their studies, Shore and Pardee adapted a Beckman DU spectrophotometer to view the ultraviolet fluorescence of tyrosine, tryptophan and a number of proteins through a filter that passed wavelengths greater than about 300 nm. Shore and Pardee could not record emission spectra with their apparatus, however, and the excitation spectra obtained were very approximate. In 1957, Weber and Teale published the first emission spectra of the aromatic amino acids, and the first accurate excitation spectra (figure 7 from this paper has been reproduced"
    ]
  ],
  [
    "The estimated rotational correlation time for a small protein with a relative molecular mass of 8400 in water at 298 K is approximately 4-8 picoseconds.",
    [
      "Rotational correlation time Rotational correlation time (formula_1) is the average time it takes for a molecule to rotate one radian. In solution, rotational correlation times are in the order of picoseconds. For example, the formula_2 1.7 ps for water, and 100 ps for a pyrroline nitroxyl radical in a DMSO-water mixture. Rotational correlation times are employed in the measurement of microviscosity (viscosity at the molecular level) and in protein characterization. Rotational correlation times may be measured by rotational (microwave), dielectric, and nuclear magnetic resonance (NMR) spectroscopy. Rotational correlation times of probe molecules in media have been measured by fluorescence lifetime",
      "or for radicals, from the linewidths of electron spin resonances. Rotational correlation time Rotational correlation time (formula_1) is the average time it takes for a molecule to rotate one radian. In solution, rotational correlation times are in the order of picoseconds. For example, the formula_2 1.7 ps for water, and 100 ps for a pyrroline nitroxyl radical in a DMSO-water mixture. Rotational correlation times are employed in the measurement of microviscosity (viscosity at the molecular level) and in protein characterization. Rotational correlation times may be measured by rotational (microwave), dielectric, and nuclear magnetic resonance (NMR) spectroscopy. Rotational correlation times of",
      "factor of two to four; thus, at 25 \u00b0C the reorientational correlation time of water increases from 2 to 4-8 picoseconds. Generally, this leads to significant losses in translational and rotational entropy of water molecules and makes the process unfavorable in terms of the free energy in the system. By aggregating together, nonpolar molecules reduce the surface area exposed to water and minimize their disruptive effect. The hydrophobic effect can be quantified by measuring the partition coefficients of non-polar molecules between water and non-polar solvents. The partition coefficients can be transformed to free energy of transfer which includes enthalpic and"
    ]
  ],
  [
    "Gas chromatography with a Flame Ionization Detector (FID) would be best suited to measure trace amounts of CFCs in an air sample due to its high sensitivity and ability to separate and identify different compounds based on retention times.",
    [
      "procedure is recommended: Inject a sample containing known amounts of CH, CO and CO to check conversion efficiency and peak shape. The retention times of these compounds should be known. If not, and light hydrocarbons are present in the sample, there might be some confusion in identification. The user should be aware, also, that the FID does respond slightly to O so at high sensitivities an air peak might also be evident. As a very rough indication, 1% O gives a signal similar to that of 1 ppm CO or CO. If there is any doubt about retention times, the",
      "air-conditioning, refrigeration, blowing agents in foams, insulations and packing materials, propellants in aerosol cans, and as solvents. The entry of CFCs into the ocean makes them extremely useful as transient tracers to estimate rates and pathways of ocean circulation and mixing processes. However, due to production restrictions of CFCs in the 1980s, atmospheric concentrations of CFC-11 and CFC-12 has stopped increasing, and the CFC-11 to CFC-12 ratio in the atmosphere have been steadily decreasing, making water dating of water masses more problematic. Incidentally, production and release of sulfur hexafluoride (SF) have rapidly increased in the atmosphere since the 1970s. Similar",
      "refrigeration applications, and are becoming available in new split system air conditioners. Various other solvents and methods have replaced the use of CFCs in laboratory analytics. In Metered-dose inhalers (MDI), a non-ozone effecting substitute was developed as a propellant, known as \"hydrofluoroalkane.\" As previously discussed, CFCs were phased out via the Montreal Protocol due to their part in ozone depletion. However, the atmospheric impacts of CFCs are not limited to its role as an active ozone reducer. This anthropogenic compound is also a greenhouse gas, with a much higher potential to enhance the greenhouse effect than CO. Infrared absorption bands"
    ]
  ],
  [
    "The structural units that make up the solid-state structures of the principal allotropes of elemental boron are icosahedral B clusters, along with two- and three-centre covalent bonds between icosahedra.",
    [
      "size and high ionization energies, the basic structural unit of boron (and nearly all of its allotropes) is the icosahedral B cluster. Of the 36 electrons associated with 12 boron atoms, 26 reside in 13 delocalized molecular orbitals; the other 10 electrons are used to form two- and three-centre covalent bonds between icosahedra. The same motif can be seen, as are deltahedral variants or fragments, in metal borides and hydride derivatives, and in some halides. The bonding in boron has been described as being characteristic of behaviour intermediate between metals and nonmetallic covalent network solids (such as diamond). The energy",
      "solids. The crystal structures of many boron-rich borides can be attributed to certain types including MgAlB, YB, REBSi, BC and other, more complex types such as REBCSi. Some of these formulas, for example BC, YB and MgAlB, historically reflect the idealistic structures, whereas the experimentally determined composition is nonstoichiometric and corresponds to fractional indexes. Boron-rich borides are usually characterized by large and complex unit cells, which can contain more than 1500 atomic sites and feature extended structures shaped as \"tubes\" and large modular polyhedra (\"superpolyhedra\"). Many of those sites have partial occupancy, meaning that the probability to find them occupied",
      "structure with 64 atoms per unit cell. The unit cell is orthorhombic and its most salient feature is four boron-containing icosahedra. Each icosahedron contains 12 boron atoms. Eight more boron atoms connect the icosahedra to the other elements in the unit cell. The occupancy of metal sites in the lattice is lower than one, and thus, while the material is usually identified with the formula AlMgB, its chemical composition is closer to AlMgB. Such non-stoichiometry is common for borides (see crystal structure of boron-rich metal borides and boron carbide). The unit cell parameters of BAM are \"a\" = 1.0313 nm,"
    ]
  ],
  [
    "Doubling the volume of a buffer solution made from equal concentrations of a weak acid and its conjugate base will result in an increase in pH.",
    [
      "reduced, making the soap less effective. A buffer solution contains an acid and its conjugate base or a base and its conjugate acid. Addition of the conjugate ion will result in a change of pH of the buffer solution. For example, if both sodium acetate and acetic acid are dissolved in the same solution they both dissociate and ionize to produce acetate ions. Sodium acetate is a strong electrolyte, so it dissociates completely in solution. Acetic acid is a weak acid, so it only ionizes slightly. According to Le Chatelier's principle, the addition of acetate ions from sodium acetate will",
      "Buffer solution A buffer solution (more precisely, pH buffer or hydrogen ion buffer) is an aqueous solution consisting of a mixture of a weak acid and its conjugate base, or vice versa. Its pH changes very little when a small amount of strong acid or base is added to it. Buffer solutions are used as a means of keeping pH at a nearly constant value in a wide variety of chemical applications. In nature, there are many systems that use buffering for pH regulation. For example, the bicarbonate buffering system is used to regulate the pH of blood. Buffer solutions",
      "largely in the undissociated form, releasing far fewer H ions into the solution than the original strong acid would have done. The pH of a buffer solution depends solely on the \"ratio\" of the molar concentrations of the weak acid to the weak base. The higher the concentration of the weak acid in the solution (compared to the weak base) the lower the resulting pH of the solution. Similarly, if the weak base predominates the higher the resulting pH. This principle is exploited to \"regulate\" the pH of the extracellular fluids (rather than just \"buffering\" the pH). For the carbonic"
    ]
  ],
  [
    "In a conjugate acid-base pair, the acid donates a proton to form its conjugate base, and the base accepts a proton to form its conjugate acid.",
    [
      "Acid-base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA H + A. In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant \"K\" is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that [HO] means \"the concentration of HO\". The acid dissociation",
      "is proton acceptor, i.e. a substance which tends to combine with a proton. When an acid HB dissociates it yields a proton together with the conjugate base B of the acid: Alternatively, the base B will combine with a proton to yield the conjugate acid HB of the base B, for every base has its conjugate acid and, every acid has its conjugate base. It follows from these definitions that an acid may be either: A base may be either: Substances which are potentially acidic can function as acids only in the presence of a base to which they can",
      "is possible, and thus the acid/base and conjugated base/acid are always in equilibrium. The equilibrium is determined by the acid and base dissociation constants (\"K\" and \"K\") of the involved substances. A special case of the acid-base reaction is the neutralization where an acid and a base, taken at exactly same amounts, form a neutral salt. Acid-base reactions can have different definitions depending on the acid-base concept employed. Some of the most common are: Precipitation is the formation of a solid in a solution or inside another solid during a chemical reaction. It usually takes place when the concentration of"
    ]
  ],
  [
    "Materials that contain d- or f-elements, such as salts of these elements, often exhibit paramagnetic behavior due to unquenched spins.",
    [
      "use of quantum statistics. Materials that are called \"paramagnets\" are most often those that exhibit, at least over an appreciable temperature range, magnetic susceptibilities that adhere to the Curie or Curie\u2013Weiss laws. In principle any system that contains atoms, ions, or molecules with unpaired spins can be called a paramagnet, but the interactions between them need to be carefully considered. The narrowest definition would be: a system with unpaired spins that \"do not interact\" with each other. In this narrowest sense, the only pure paramagnet is a dilute gas of monatomic hydrogen atoms. Each atom has one non-interacting unpaired electron.",
      "only possible if the interactions of the spins that lead either to quenching or to ordering are kept at bay by structural isolation of the magnetic centers. There are two classes of materials for which this holds: As stated above, many materials that contain d- or f-elements do retain unquenched spins. Salts of such elements often show paramagnetic behavior but at low enough temperatures the magnetic moments may order. It is not uncommon to call such materials 'paramagnets', when referring to their paramagnetic behavior above their Curie or N\u00e9el-points, particularly if such temperatures are very low or have never been",
      "field is linear in the field strength and rather weak. It typically requires a sensitive analytical balance to detect the effect and modern measurements on paramagnetic materials are often conducted with a SQUID magnetometer. Paramagnetism is due to the presence of unpaired electrons in the material, so all atoms with incompletely filled atomic orbitals are paramagnetic. Due to their spin, unpaired electrons have a magnetic dipole moment and act like tiny magnets. An external magnetic field causes the electrons' spins to align parallel to the field, causing a net attraction. Paramagnetic materials include aluminium, oxygen, titanium, and iron oxide (FeO)."
    ]
  ],
  [
    "The ratio of the equilibrium polarizations |pe / pH| for an electron and a proton placed in a 1.0 T magnetic field at 300 K can be determined using the magnetoelectric susceptibility formula, with the tensor being the same for both equations.",
    [
      "equation: Where formula_2 is the elementary magnetic moment and \"formula_3\" is the volume element; in other words, the M-field is the distribution of magnetic moments in the region or manifold concerned. This is better illustrated through the following relation: where m is an ordinary magnetic moment and the triple integral denotes integration over a volume. This makes the M-field completely analogous to the electric polarisation field, or P-field, used to determine the electric dipole moment p generated by a similar region or manifold with such a polarization: Where formula_6 is the elementary electric dipole moment. Those definitions of P and",
      "The polarizability formula_1 in isotropic media is defined as the ratio of the induced dipole moment formula_2 of an atom to the electric field formula_3 that produces this dipole moment. formula_4 Polarizability has the SI units of C\u00b7m\u00b7V = A\u00b7s\u00b7kg while its cgs unit is cm. Usually it is expressed in cgs units as a so-called polarizability volume, sometimes expressed in \u00c5 = 10 cm. One can convert from SI units to cgs units as follows: formula_5 \u2243 8.988\u00d710 \u00d7 formula_6 where formula_7, the vacuum permittivity, is ~8.854 \u00d7 10 (F/m). If the polarizability volume is denoted formula_8 the relation",
      "resp. a magnetic field, there is also the possibility of a magnetoelectric susceptibility formula_3 which describes a linear response of the electric polarization to a magnetic field, and vice versa: The tensor formula_6 must be the same in both equations. Here, P is the electric polarization, M the magnetization, E and H the electric and magnetic fields. The SI Unit of \u03b1 is [s/m] which can be converted to the practical unit [V/(cm Oe)] by [s/m]=1.1 x10 \u03b5 [V/(cm Oe)]. For the CGS unit, [unitless] = 3 x 10 [s/m]/(4 x \u03c0) The first material where an intrinsic linear magnetoelectric"
    ]
  ],
  [
    "The observed NMR signal decay time is inversely related to the static magnetic field inhomogeneity.",
    [
      "actually observed decay time of the observed NMR signal, or free induction decay (to of the initial amplitude immediately after the resonant RF pulse), also depends on the static magnetic field inhomogeneity, which is quite significant. (There is also a smaller but significant contribution to the observed FID shortening from the RF inhomogeneity of the resonant pulse). In the corresponding FT-NMR spectrum\u2014meaning the Fourier transform of the free induction decay\u2014the \"T\"* time is inversely related to the width of the NMR signal in frequency units. Thus, a nucleus with a long \"T\" relaxation time gives rise to a very sharp",
      "an NMR signal is analyzed in terms of two separate processes, each with their own time constants. One process, associated with \"T\", is responsible for the loss of signal intensity. The other process, associated with \"T\", is responsible for the broadening of the signal. Stated more formally, \"T\" is the time constant for the physical processes responsible for the relaxation of the components of the nuclear spin magnetization vector M parallel to the external magnetic field, B (which is conventionally oriented along the \"z\" axis). \"T\" relaxation affects the components of M perpendicular to B. In conventional NMR spectroscopy \"T\"",
      "lead to a distribution of resonance frequencies around the ideal. Over time, this distribution can lead to a dispersion of the tight distribution of magnetic spin vectors, and loss of signal (Free Induction Decay). In fact, for most magnetic resonance experiments, this \"relaxation\" dominates. This results in dephasing. However, decoherence because of magnetic field inhomogeneity is not a true \"relaxation\" process; it is not random, but dependent on the location of the molecule in the magnet. For molecules that aren't moving, the deviation from ideal relaxation is consistent over time, and the signal can be recovered by performing a spin"
    ]
  ],
  [
    "The molecular geometry of thionyl chloride, SOCl2, is trigonal pyramidal with C molecular symmetry.",
    [
      "of chloride ions. The major industrial synthesis involves the reaction of sulfur trioxide and sulfur dichloride: Other methods include syntheses from phosphorus pentachloride, chlorine and sulfur dichloride, or phosgene: The first of the above four reactions also affords phosphorus oxychloride (phosphoryl chloride), which resembles thionyl chloride in many of its reactions. SOCl adopts a trigonal pyramidal molecular geometry with C molecular symmetry. This geometry is attributed to the effects of the lone pair on the central sulfur(IV) center. In the solid state SOCl forms monoclinic crystals with the space group P2/c. Thionyl chloride has a long shelf life, however \"aged\"",
      "Sulfuryl chloride Sulfuryl chloride is an inorganic compound with the formula SOCl. At room temperature, it is a colorless liquid with a pungent odor. Sulfuryl chloride is not found in nature, as can be inferred from its rapid hydrolysis. Sulfuryl chloride is commonly confused with thionyl chloride, SOCl. The properties of these two sulfur oxychlorides are quite different: sulfuryl chloride is a source of chlorine whereas thionyl chloride is a source of chloride ions. An alternative IUPAC name is sulfuroyl dichloride. Sulfur is tetrahedral in SOCl and the oxidation state of the sulfur atom is +6, as in sulfuric acid.",
      "on four corners of the cube and the bridging Cl atoms sit on the other four corners. The bridging Se-Cl distances are longer than the terminal Se-Cl distances, but all Cl-Se-Cl angles are approximately 90\u00b0. SeCl has often been used as an example for teaching VSEPR rules of hypervalent molecules. As such, one would predict four bonds but five electron groups giving rise to a seesaw geometry. This clearly is not the case in the crystal structure. Others have suggested that the crystal structure can be represented as SeCl and Cl. This formulation would predict a pyramidal geometry for the"
    ]
  ],
  [
    ".\n\nRandom errors in measured results in chemistry can be minimized by using data reconciliation techniques to correct measurement errors due to measurement noise and ensuring that the formulation of survey questions and scales are exact to reduce unintended mistakes and systematic errors.",
    [
      "is a technique that targets at correcting measurement errors that are due to measurement noise, i.e. random errors. From a statistical point of view the main assumption is that no systematic errors exist in the set of measurements, since they may bias the reconciliation results and reduce the robustness of the reconciliation. Given formula_11 measurements formula_12, data reconciliation can mathematically be expressed as an optimization problem of the following form: formula_13 where formula_14 is the reconciled value of the formula_15-th measurement (formula_16), formula_17 is the measured value of the formula_15-th measurement (formula_16), formula_20 is the formula_21-th unmeasured variable (formula_22), and",
      "that there may contain quite large measurement errors (). These errors can be random or systematic. Random errors are caused by unintended mistakes by respondents, interviewers and/or coders. Systematic error can occur if there is a systematic reaction of the respondents to the scale used to formulate the survey question. Thus, the exact formulation of a survey question and its scale are crucial, since they affect the level of measurement error (). Different tools are available for the researchers to help them decide about this exact formulation of their questions, for instance estimating the quality of a question using MTMM",
      "conditions and with highly trained experts, it is possible to achieve small (a few tenths to a few percent) errors in measurements. However, in many practical situations, there is the likelihood of errors on the order of 10 percent Several types of error are at play when taking physical measurements. The three basic types of error noted as the limiting factors of accuracy of measurement are random, systematic, and periodic errors \"Random errors\" are variations about that mean. In the case of spectroradiometric measurements, this could be thought of as noise from the detector, internal electronics, or the light source"
    ]
  ],
  [
    "The predicted resonance magnetic field position for a microwave frequency of 9388.2 MHz in an X-band EPR spectrum is approximately 0.3350 teslas (3350 gausses).",
    [
      "a spectrometer's microwave source (see Table). EPR experiments often are conducted at X and, less commonly, Q bands, mainly due to the ready availability of the necessary microwave components (which originally were developed for radar applications). A second reason for widespread X and Q band measurements is that electromagnets can reliably generate fields up to about 1 tesla. However, the low spectral resolution over \"g\"-factor at these wavebands limits the study of paramagnetic centers with comparatively low anisotropic magnetic parameters. Measurements at formula_27 > 40 GHz, in the millimeter wavelength region, offer the following advantages: This was demonstrated experimentally in",
      "it is this absorption that is monitored and converted into a spectrum. The upper spectrum below is the simulated absorption for a system of free electrons in a varying magnetic field. The lower spectrum is the first derivative of the absorption spectrum. The latter is the most common way to record and publish continuous wave EPR spectra. For the microwave frequency of 9388.2 MHz, the predicted resonance occurs at a magnetic field of about formula_19 = 0.3350 teslas = 3350 gausses. Because of electron-nuclear mass differences, the magnetic moment of an electron is substantially larger than the corresponding quantity for",
      "between the two energy levels by either absorbing or emitting a photon of energy formula_14 such that the resonance condition, formula_15, is obeyed. This leads to the fundamental equation of EPR spectroscopy: formula_16. Experimentally, this equation permits a large combination of frequency and magnetic field values, but the great majority of EPR measurements are made with microwaves in the 9000\u201310000 MHz (9\u201310 GHz) region, with fields corresponding to about 3500 G (0.35 T). Furthermore, EPR spectra can be generated by either varying the photon frequency incident on a sample while holding the magnetic field constant or doing the reverse. In"
    ]
  ],
  [
    "The contamination of the ground state by excited states affects the relative occupancies of the \u03b1 and \u03b2 spin energy levels for a radical species with g = 2.05 at L- and W-band frequencies by leading to a higher occupancy of the \u03b2 spin energy level due to the contamination by excited states, resulting in a deviation from the expected occupancies.",
    [
      "the \u03b1 spin electrons and a set of molecular orbitals and orbital energies for the \u03b2 electrons. This method has one drawback. A single Slater determinant of different orbitals for different spins is not a satisfactory eigenfunction of the total spin operator - formula_16. The ground state is contaminated by excited states. If there is one more electron of \u03b1 spin than \u03b2 spin, the ground state is a doublet. The average value of formula_16, written formula_18, should be formula_19 but will actually be rather more than this value as the doublet state is contaminated by a quadruplet state. A",
      "states. As shown in (3) of the figure 1, in order for there to be an appearance of B term, there must be another state K, which is closely related in energy to either the ground state or the excited states. Also, the energy between the ground state and the excited state should be high enough so that the excited state is not highly populated. Usually, B term would have the absorption band shape. Otherwise, it will lead to the C term (4) of figure 1, which requires the degeneracy of the ground state. This happens due to a change",
      "sites. The proposition that the ground state is an eigenstate of the Hamiltonian is confirmed. One might guess that the first excited state of the Hamiltonian has one randomly selected spin at position rotated so that but in fact this arrangement of spins is not an eigenstate. The reason is that such a state is transformed by the spin raising and lowering operators. The operator formula_12 will increase the -projection of the spin at position back to its low-energy orientation, but the operator formula_13 will lower the -projection of the spin at position . The combined effect of the two"
    ]
  ],
  [
    "A primary standard for use in standardizing bases in chemistry is a reagent which can be weighed easily and is so pure that its weight represents the number of moles of substance contained.",
    [
      "Primary standard A primary standard in metrology is a standard that is sufficiently accurate such that it is not calibrated by or subordinate to other standards. Primary standards are defined via other quantities like length, mass and time. Primary standards are used to calibrate other standards referred to as working standards. See Hierarchy of Standards. Standards are used in analytical chemistry. Here, a primary standard is typically a reagent which can be weighed easily, and which is so pure that its weight is truly representative of the number of moles of substance contained. Features of a primary standard include: Some",
      "examples of primary standards according to the European Pharmacopoeia 5, ch. 4.2: Such standards are often used to make standard solutions. Primary standards are used in titration and are essential for determining unknown concentrations or preparing working standards. Primary standard A primary standard in metrology is a standard that is sufficiently accurate such that it is not calibrated by or subordinate to other standards. Primary standards are defined via other quantities like length, mass and time. Primary standards are used to calibrate other standards referred to as working standards. See Hierarchy of Standards. Standards are used in analytical chemistry. Here,",
      "Standard state In chemistry, the standard state of a material (pure substance, mixture or solution) is a reference point used to calculate its properties under different conditions. In principle, the choice of standard state is arbitrary, although the International Union of Pure and Applied Chemistry (IUPAC) recommends a conventional set of standard states for general use. IUPAC recommends using a standard pressure \"p\" = 10 Pa. Strictly speaking, temperature is not part of the definition of a standard state. For example, as discussed below, the standard state of a gas is conventionally chosen to be unit pressure (usually in bar)"
    ]
  ],
  [
    "Argon has stretching frequencies at 670, 710, 760, 820, 860, and 920 nm, while neon has stretching frequencies at 650, 700, 850, and 880 nm.",
    [
      "are emitted. Each of these frequencies are related to energy by the formula: where formula_2 is the energy of the photon, formula_3 is its frequency, and formula_4 is Planck's constant. This concludes that only photons with specific energies are emitted by the atom. The principle of the atomic emission spectrum explains the varied colors in neon signs, as well as chemical flame test results (described below). The frequencies of light that an atom can emit are dependent on states the electrons can be in. When excited, an electron moves to a higher energy level or orbital. When the electron falls",
      "so, when pumping Nd:YAG with xenon, the continuum radiation must be used. All gases produce spectral lines which are specific to the gas, superimposed on a background of continuum radiation. With all gases, low current-densities produce mostly spectral lines, with the highest output being concentrated in the near-IR between 650 and 1000 nm. Krypton's strongest peaks are around 760 and 810 nm. Argon has many strong peaks at 670, 710, 760, 820, 860, and 920 nm. Neon has peaks around 650, 700, 850, and 880 nm. As current densities become higher, the output of continuum radiation will increase more than",
      "drops, allowing electrons to recombine with atoms and light emission to cease due to this lack of free electrons. This makes for a 160-picosecond light pulse for argon (even a small drop in temperature causes a large drop in ionization, due to the large ionization energy relative to photon energy). This description is simplified from the literature above, which details various steps of differing duration from 15 microseconds (expansion) to 100 picoseconds (emission). Computations based on the theory presented in the review produce radiation parameters (intensity and duration time versus wavelength) that match experimental results with errors no larger than"
    ]
  ],
  [
    "The chemical shift of [Ru(NH3)6]2+ relative to [Ru(CN)6]2- at 0 ppm is 200 ppm.",
    [
      "380.5 ppm upfield from CHNO (\u03b4 = \u03b4 + 380.5 ppm). Chemical shifts for N are somewhat erratic but typically they span a range of -400 ppm to 1100 ppm with respect to CHNO. Below is a summary of N chemical shifts for common organic groups referenced with respect to NH, whose chemical shift is assigned 0 ppm. Unlike most nuclei, the gyromagnetic ratio for N is negative. With the spin precession phenomenon, the sign of \u03b3 determines the sense (clockwise vs counterclockwise) of precession. Most common nuclei have positive gyromagnetic ratios such as H and C. N NMR is",
      "(about 200 ppm, relative to nitric acid on the sigma scale, on which increased shielding corresponds to increased chemical shift). NMR shows that the chemical shift of N1-H drops slightly, whereas the chemical shift of N3-H drops considerably (about 190 vs. 145 ppm). This indicates that the N1-H tautomer is preferred, it is presumed due to hydrogen bonding to the neighboring ammonium. The shielding at N3 is substantially reduced due to the second-order paramagnetic effect, which involves a symmetry-allowed interaction between the nitrogen lone pair and the excited \u03c0* states of the aromatic ring. As the pH rises above 9,",
      "(\u00b5in/in)/\u00b0C; the numeric value representing a relative proportion does not change with the adoption of a different unit of measure. Similarly, a metering pump that injects a trace chemical into the main process line at the proportional flow rate \"Q\" = 125 ppm, is doing so at a rate that may be expressed in a variety of volumetric units, including 125 \u00b5L/L, 125 \u00b5gal/gal, 125 cm/m, etc. In nuclear magnetic resonance spectroscopy (NMR), chemical shift is usually expressed in ppm. It represents the difference of a measured frequency in parts per million from the reference frequency. The reference frequency depends"
    ]
  ],
  [
    "As the frequency increases in a chemistry system with an inverted voltage and a 180\u00b0 phase shift at resonance, the phase delay also increases until it levels out at 360\u00b0.",
    [
      "is such that the secondary winding produces an inverted voltage to the primary. That is, at resonance the phase shift is now 180\u00b0. As the frequency continues to increase, the phase delay also continues to increase and the input and output start to come back into phase as a whole cycle delay is approached. At high frequencies L and L' approach open-circuit and C approaches short-circuit and the phase delay tends to level out at 360\u00b0. The relationship between phase shift (\u03c6) and time delay (T) with angular frequency (\u03c9) is given by the simple relation, It is required that",
      "same as a shift in time, such as a time delay. If formula_3 is delayed (time-shifted) by formula_4 of its cycle, it becomes: whose \"phase\" is now formula_6. It has been shifted by formula_7 radians (the variable formula_8 here just represents the amplitude of the wave). Phase difference is the difference, expressed in degrees or radians, between two waves having the same frequency and referenced to the same point in time. Two oscillators that have the same frequency and no phase difference are said to be in phase. Two oscillators that have the same frequency and different phases have a",
      "with one signal time-delayed by a small and gradually changing amount, usually smaller than 20 milliseconds. This produces a swept comb filter effect: peaks and notches are produced in the resultant frequency spectrum, related to each other in a linear harmonic series. Varying the time delay causes these to sweep up and down the frequency spectrum. Part of the output signal is usually fed back to the input (a re-circulating delay line), producing a resonance effect which further enhances the intensity of the peaks and troughs. The phase of the fed-back signal is sometimes inverted, producing another variation on the"
    ]
  ],
  [
    "A necessary, but not sufficient, condition for a process to actually occur is that it is interdependent with other phenomena including other processes.",
    [
      "that in the absence of 'Q', 'P' does not occur, meaning that 'Q' is the necessary condition for 'P'. The rule of inference for necessary condition is \"modus tollens\": Premise (1): If P, then Q Premise (2): not Q Conclusion: Therefore, not P An example traditionally used by logicians contrasting sufficient and necessary conditions is the statement \"If there is fire, then oxygen is present\". An oxygenated environment is necessary for fire or combustion, but simply because there is an oxygenated environment does not necessarily mean that fire or combustion is occurring. While one can infer that fire stipulates the",
      "\"Q\"\". This can also be expressed as \"\"P\" only if \"Q\"\", \"\"P\" implies \"Q\"\" or several other variants. It may be the case that several sufficient conditions, when taken together, constitute a single necessary condition, as illustrated in example 5. A condition can be either necessary or sufficient without being the other. For instance, \"being a mammal\" (\"N\") is necessary but not sufficient to \"being human\" (\"S\"), and that a number formula_11 \"is rational\" (\"S\") is sufficient but not necessary to formula_11 \"being a real number\" (\"N\") (since there are real numbers that are not rational). A condition can be",
      "factor which cannot be avoided: as a matter of fact, a process is always interdependent with other phenomena including other processes. Process (engineering) In engineering, a process is a series of interrelated tasks that, together, transform inputs into outputs. These tasks may be carried out by people, nature or machines using various resources; an engineering process must be considered in the context of the agents carrying out the tasks and the resource attributes involved. Systems engineering normative documents and those related to Maturity Models are typically based on processes, for example, systems engineering processes of the EIA-632 and processes involved"
    ]
  ],
  [
    "The EPR spectrum of a solution of a rigid nitronyl nitroxide diradical with J >> a will consist of a single line.",
    [
      "isotropic hyperfine splitting pattern for a radical freely tumbling in a solution (isotropic system) can be predicted. While it is easy to predict the number of lines, the reverse problem, unraveling a complex multi-line EPR spectrum and assigning the various spacings to specific nuclei, is more difficult. In the often encountered case of \"I\" = 1/2 nuclei (e.g., H, F, P), the line intensities produced by a population of radicals, each possessing \"M\" equivalent nuclei, will follow Pascal's triangle. For example, the spectrum at the right shows that the three H nuclei of the CH radical give rise to 2\"MI\"",
      "+ 1 = 2(3)(1/2) + 1 = 4 lines with a 1:3:3:1 ratio. The line spacing gives a hyperfine coupling constant of \"a\" = 23 \"G\" for each of the three H nuclei. Note again that the lines in this spectrum are \"first derivatives\" of absorptions. As a second example, the methoxymethyl radical, HCOCH the OC\"H\" center will give an overall 1:2:1 EPR pattern, each component of which is further split by the three methoxy hydrogens into a 1:3:3:1 pattern to give a total of 3\u00d74 = 12 lines, a triplet of quartets. A simulation of the observed EPR spectrum",
      "peak is positive in first-derivative spectra, the high-frequency peak is negative, and the central peak is bipolar. Such situations are commonly observed in powders, and the spectra are therefore called \"powder-pattern spectra\". In crystals, the number of EPR lines is determined by the number of crystallographically equivalent orientations of the EPR spin (called \"EPR center\"). Since the source of an EPR spectrum is a change in an electron's spin state, the EPR spectrum for a radical (S = 1/2 system) would consist of one line. Greater complexity arises because the spin couples with nearby nuclear spins. The magnitude of the"
    ]
  ],
  [
    "Based on Higuchi and coworkers' research on X-ray crystallography and spectroscopic data of \"Desulfovibrio vulgaris\" Miyazaki F, the metal ions that could be found at the active site of redox enzymes are nickel (Ni) and iron (Fe).",
    [
      "proposed by Higuchi and coworkers based on X-ray crystallography and spectroscopic data of \"Desulfovibrio vulgaris\" Miyazaki F. During the catalytic process, the Fe metal ion in the active site does not change its oxidation state while the Ni metal ion participates in redox chemistry. There are two main groups of redox states that [NiFe] hydrogenases pass through during catalysis 1) Inactive redox states and 2) Active redox states (Figure 5). Ni-A (the \u201cunready\u201d state) and Ni-B (the \u201cready\u201d state) are the most oxidized forms of the [NiFe] metal center and are activated via one-electron reduction with proton transfer. The rate",
      "which the generic ligand X is either an oxide, sulfur, hydroperoxide, or a hydroxide found in an oxidized state only (Figure 3). While the nickel atom participates in redox reactions, the iron atom is consistently in a Fe(II) coordination state. The exact geometry of the three non-protein ligands (denoted as L) coordinating to the Fe metal ion is not known; however, they were identified as one carbon monoxide (C\u2261O) molecule and two cyanide (C\u2261N) molecules. Almost all hydrogenases contain at least one iron-sulfur cluster (Fe-S cluster). As previously mentioned, these Fe-S clusters connect the nickel active site of the enzyme",
      "that the metal ion is bound to the protein with one labile coordination site. As with all enzymes, the shape of the active site is crucial. The metal ion is usually located in a pocket whose shape fits the substrate. The metal ion catalyzes reactions that are difficult to achieve in organic chemistry. In aqueous solution, carbon dioxide forms carbonic acid This reaction is very slow in the absence of a catalyst, but quite fast in the presence of the hydroxide ion A reaction similar to this is almost instantaneous with carbonic anhydrase. The structure of the active site in"
    ]
  ],
  [
    "The appearance of the R-branch and P-branch in the vibrational-rotational spectrum of a diatomic molecule is determined by the symmetry of the molecular rotors, with the R-branch appearing similar to the pure rotation spectrum and the P-branch appearing as a nearly mirror image of the R-branch.",
    [
      "Q-branch the energy of rotational transitions is added to the energy of the vibrational transition. This is known as the R-branch of the spectrum for \u0394J = +1. The P-branch for \u0394J = \u22121 lies on the low wavenumber side of the Q branch. The appearance of the R-branch is very similar to the appearance of the pure rotation spectrum, and the P-branch appears as a nearly mirror image of the R-branch. The appearance of rotational fine structure is determined by the symmetry of the molecular rotors which are classified, in the same way as for pure rotational spectroscopy, into",
      "cases the P- and R- branch wavenumbers follow the same trend as in diatomic molecules. The two classes differ in the selection rules that apply to ro-vibrational transitions. For parallel transitions the selection rule is the same as for diatomic molecules, namely, the transition corresponding to the Q-branch is forbidden. An example is the C-H stretching mode of hydrogen cyanide. For a perpendicular vibration the transition \u0394\"J\"=0 is allowed. This means that the transition is allowed for the molecule with the same rotational quantum number in the ground and excited vibrational state, for all the populated rotational states. This makes",
      "Numerical analysis of ro-vibrational spectral data would appear to be complicated by the fact that the wavenumber for each transition depends on two rotational constants, formula_5 and formula_6. However combinations which depend on only one rotational constant are found by subtracting wavenumbers of pairs of lines (one in the P-branch and one in the R-branch) which have either the same lower level or the same upper level. For example, in a diatomic molecule the line denoted \"P\"(\"J\" + 1) is due to the transition (\"v\" = 0, \"J\" + 1) \u2192 (\"v\" = 1, \"J\"), and the line \"R\"(\"J\" \u2212"
    ]
  ],
  [
    "Chlorine has the highest electron affinity.",
    [
      "magnitude but have the opposite sign. This is because those atoms with a high electron affinity are less inclined to give up an electron, and so take more energy to remove the electron from the atom. In this case, the atom with the more positive energy value has the higher electron affinity. As one progresses from left to right across a period, the electron affinity will increase. Although it may seem that Fluorine should have the greatest electron affinity, the small size of fluorine generates enough repulsion that Chlorine has the greatest electron affinity. Electronegativity is a measure of the",
      "the electrons within that shell. The exceptions are the elements in the boron and oxygen family, which require slightly less energy than the general trend. The electron affinity of an atom can be described either as the energy released by an atom when an electron is added to it, conversely as the energy required to detach an electron from a singly charged anion. The sign of the electron affinity can be quite confusing, as atoms that become more stable with the addition of an electron (and so are considered to have a higher electron affinity) show a decrease in potential",
      "compared to thallium and bismuth, appears to be an artifact of data selection and data availability. Methods of calculation other than the Pauling method show the normal periodic trends for these elements. The electron affinity of an atom is the amount of energy released when an electron is added to a neutral atom to form a negative ion. Although electron affinity varies greatly, some patterns emerge. Generally, nonmetals have more positive electron affinity values than metals. Chlorine most strongly attracts an extra electron. The electron affinities of the noble gases have not been measured conclusively, so they may or may"
    ]
  ],
  [
    "The cation with the smallest radius among cations and anions is the hydrogen cation.",
    [
      "molecule. Cations and anions are measured by their ionic radius and they differ in relative size: \"Cations are small, most of them less than 10 m (10 cm) in radius. But most anions are large, as is the most common Earth anion, oxygen. From this fact it is apparent that most of the space of a crystal is occupied by the anion and that the cations fit into the spaces between them.\" A cation has radius less than 0.8 \u00d7 10 m (0.8 \u00c5) while an anion has radius greater than 1.3 \u00d7 10 m (1.3 \u00c5). Ions are ubiquitous",
      "size is determined by its electron cloud. Cations are smaller than the corresponding parent atom or molecule due to the smaller size of the electron cloud. One particular cation (that of hydrogen) contains no electrons, and thus consists of a single proton - \"very much smaller\" than the parent hydrogen atom. Since the electric charge on a proton is equal in magnitude to the charge on an electron, the net electric charge on an ion is equal to the number of protons in the ion minus the number of electrons. An (\u2212) (), from the Greek word \u1f04\u03bd\u03c9 (\"\u00e1n\u014d\"), meaning",
      "transferred to the electron-deficient nonmetal atoms. This reaction produces metal cations and nonmetal anions, which are attracted to each other to form a \"salt\". Ion An ion () is an atom or molecule that has a non-zero net electrical charge. Since the charge of the electron (considered \"negative\" by convention) is equal and opposite to that of the proton (considered \"positive\" by convention), the net charge of an ion is non-zero due to its total number of electrons being unequal to its total number of protons. A cation is a positively charged ion, with fewer electrons than protons, while an"
    ]
  ],
  [
    "The NMR frequency of 31P in a 20.0 T magnetic field would be approximately 243.2 MHz.",
    [
      "placed in a magnetic field, NMR active nuclei (such as H or C) absorb electromagnetic radiation at a frequency characteristic of the isotope. The resonant frequency, energy of the radiation absorbed, and the intensity of the signal are proportional to the strength of the magnetic field. For example, in a 21 Tesla magnetic field, hydrogen atoms (commonly referred to as protons) resonate at 900 MHz. It is common to refer to a 21 T magnet as a 900 MHz magnet since hydrogen is the most common nucleus detected, however different nuclei will resonate at different frequencies at this field strength",
      "T is therefore about 42 ppm. However a frequency scale is commonly used to designate the NMR signals, even though the spectrometer may operate by sweeping the magnetic field, and thus the 42 ppm is 4200 Hz for a 100 MHz reference frequency (rf). However, given that the location of different NMR signals is dependent on the external magnetic field strength and the reference frequency, the signals are usually reported relative to a reference signal, usually that of TMS (tetramethylsilane). Additionally, since the distribution of NMR signals is field dependent, these frequencies are divided by the spectrometer frequency. However, since",
      "displayed directly as field strength or output as digital data. The relationship between the frequency of the induced current and the strength of the magnetic field is called the \"proton gyromagnetic ratio\", and is equal to 0.042576 Hz nT. Because the precession frequency depends only on atomic constants and the strength of the ambient magnetic field, the accuracy of this type of magnetometer can reach 1 ppm. The frequency of Earth's field NMR for protons varies between approximately 900 Hz near the equator to 4.2 kHz near the geomagnetic poles. These magnetometers can be moderately sensitive if several tens of"
    ]
  ],
  [
    "The number of lines in the EPR spectrum of a solution of dimethylnitroxide (CH3)2NO\u2022 is influenced by the number of crystallographically equivalent orientations of the EPR spin, as well as spin coupling with nearby nuclear spins.",
    [
      "peak is positive in first-derivative spectra, the high-frequency peak is negative, and the central peak is bipolar. Such situations are commonly observed in powders, and the spectra are therefore called \"powder-pattern spectra\". In crystals, the number of EPR lines is determined by the number of crystallographically equivalent orientations of the EPR spin (called \"EPR center\"). Since the source of an EPR spectrum is a change in an electron's spin state, the EPR spectrum for a radical (S = 1/2 system) would consist of one line. Greater complexity arises because the spin couples with nearby nuclear spins. The magnitude of the",
      "linewidth is used. For symmetric lines, halfwidth formula_63, and full inclination width formula_64. EPR/ESR spectroscopy is used in various branches of science, such as biology, chemistry and physics, for the detection and identification of free radicals and paramagnetic centers such as F-centers. EPR is a sensitive, specific method for studying both radicals formed in chemical reactions and the reactions themselves. For example, when ice (solid HO) is decomposed by exposure to high-energy radiation, radicals such as H, OH, and HO are produced. Such radicals can be identified and studied by EPR. Organic and inorganic radicals can be detected in electrochemical",
      "+ 1 = 2(3)(1/2) + 1 = 4 lines with a 1:3:3:1 ratio. The line spacing gives a hyperfine coupling constant of \"a\" = 23 \"G\" for each of the three H nuclei. Note again that the lines in this spectrum are \"first derivatives\" of absorptions. As a second example, the methoxymethyl radical, HCOCH the OC\"H\" center will give an overall 1:2:1 EPR pattern, each component of which is further split by the three methoxy hydrogens into a 1:3:3:1 pattern to give a total of 3\u00d74 = 12 lines, a triplet of quartets. A simulation of the observed EPR spectrum"
    ]
  ],
  [
    "A compound of group 16 elements with a formula DX, such as an ether or thioether, is LEAST likely to behave as a Lewis acid.",
    [
      "compounds of group 13 elements with a formula AX can behave as Lewis acids. Similarly, compounds of group 15 elements with a formula DY, such as amines, NR, and phosphines, PR, can behave as Lewis bases. Adducts between them have the formula XA\u2190DY with a dative covalent bond, shown symbolically as \u2190, between the atoms A (acceptor) and D (donor). Compounds of group 16 with a formula DX may also act as Lewis bases; in this way, a compound like an ether, RO, or a thioether, RS, can act as a Lewis base. The Lewis definition is not limited to",
      "show that there is no single order of Lewis base strengths or Lewis acid strengths. The ECW model accommodates the failure of single parameter descriptions of acid-base interactions. A related method adopting the E and C formalism of Drago and co-workers quantitatively predicts the formation constants for complexes of many metal ions plus the proton with a wide range of unidentate Lewis acids in aqueous solution, and also offered insights into factors governing HSAB behavior in solution. Another quantitative system has been proposed, in which Lewis acid strength toward Lewis base fluoride is based on gas-phase affinity for fluoride. Additional",
      "widest variety of and most stable organometallic compounds, which are bonded covalently. Organolithium compounds are electrically non-conducting volatile solids or liquids that melt at low temperatures, and tend to form oligomers with the structure (RLi) where R is the organic group. As the electropositive nature of lithium puts most of the charge density of the bond on the carbon atom, effectively creating a carbanion, organolithium compounds are extremely powerful bases and nucleophiles. For use as bases, butyllithiums are often used and are commercially available. An example of an organolithium compound is methyllithium ((CHLi)), which exists in tetrameric (\"x\" = 4,"
    ]
  ],
  [
    "A 95% confidence interval is a range of values within which the true population parameter is expected to lie in 95% of all possible cases, based on repeated sampling and analysis under the same conditions.",
    [
      "in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does \"not\" imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not",
      "to a single point, they are narrower (at this point) than a confidence band which is supposed to hold simultaneously at many points.\" Suppose our aim is to estimate a function \"f\"(\"x\"). For example, \"f\"(\"x\") might be the proportion of people of a particular age \"x\" who support a given candidate in an election. If \"x\" is measured at the precision of a single year, we can construct a separate 95% confidence interval for each age. Each of these confidence intervals covers the corresponding true value \"f\"(\"x\") with probability 0.95. Taken together, these confidence intervals constitute a \"95% pointwise confidence",
      "5% of the cases it will not be. The actual confidence interval is calculated by entering the measured masses in the formula. Our 0.95 confidence interval becomes: In other words, the 95% confidence interval is between the lower endpoint 249.22 g and the upper endpoint 251.18 g. As the desired value 250 of \"\u03bc\" is within the resulted confidence interval, there is no reason to believe the machine is wrongly calibrated. The calculated interval has fixed endpoints, where \u03bc might be in between (or not). Thus this event has probability either 0 or 1. One cannot say: \"with probability (1"
    ]
  ],
  [
    "Some alternative unit root tests to the Dickey-Fuller test include the Phillips\u2013Perron test (PP) and the ADF-GLS test procedure (ERS) developed by Elliott, Rothenberg, and Stock (1996).",
    [
      "the unit root that can occur with other testing strategies, and discusses how to use prior knowledge about the existence or not of long-run growth (or shrinkage) in \"y\". Hacker and Hatemi-J (2010) provide simulation results on these matters, including simulations covering the Enders (2004) and Elder and Kennedy (2001) unit-root testing strategies. Simulation results are presented in Hacker (2010) which indicate that using an information criterion such as the Schwarz information criterion may be useful in determining unit root and trend status within a Dickey\u2013Fuller framework. Dickey\u2013Fuller test In statistics, the Dickey\u2013Fuller test tests the null hypothesis that a",
      "In this case the formula_8 and null hypothesis is not rejected. A model that includes a constant and a time trend is estimated using sample of 50 observations and yields the formula_16 statistic of \u22124.57. This is more negative than the tabulated critical value of \u22123.50, so at the 95 per cent level the null hypothesis of a unit root will be rejected. There are alternative unit root tests such as the Phillips\u2013Perron test (PP) or the ADF-GLS test procedure (ERS) developed by Elliott, Rothenberg and Stock (1996). Augmented Dickey\u2013Fuller test In statistics and econometrics, an augmented Dickey\u2013Fuller test (ADF)",
      "Dickey\u2013Fuller test In statistics, the Dickey\u2013Fuller test tests the null hypothesis that a unit root is present in an autoregressive model. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is named after the statisticians David Dickey and Wayne Fuller, who developed the test in 1979. A simple AR(1) model is where formula_2 is the variable of interest, formula_3 is the time index, formula_4 is a coefficient, and formula_5 is the error term. A unit root is present if formula_6. The model would be non-stationary in this case."
    ]
  ],
  [
    "Partial leverage measures the contribution of individual independent variables to the leverage of each observation in a regression model, and data points with high partial leverage can exert undue influence on the model.",
    [
      "Partial leverage In regression analysis, partial leverage is a measure of the contribution of the individual independent variables to the leverage of each observation. That is, if \"h\" is the \"i\" element of the diagonal of the hat matrix, the partial leverage is a measure of how \"h\" changes as a variable is added to the regression model. The partial leverage is computed as: where Note that the partial leverage is the leverage of the \"i\" point in the partial regression plot for the \"j\" variable. Data points with large partial leverage for an independent variable can exert undue influence",
      "is not necessarily an influential point. The leverage is typically defined as the diagonal of the hat matrix, which is Partial leverage In regression analysis, partial leverage is a measure of the contribution of the individual independent variables to the leverage of each observation. That is, if \"h\" is the \"i\" element of the diagonal of the hat matrix, the partial leverage is a measure of how \"h\" changes as a variable is added to the regression model. The partial leverage is computed as: where Note that the partial leverage is the leverage of the \"i\" point in the partial",
      "Leverage (statistics) In statistics and in particular in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations. High-leverage points are those observations, if any, made at extreme or outlying values of the independent variables such that the lack of neighboring observations means that the fitted regression model will pass close to that particular observation. Modern computer packages for statistical analysis include, as part of their facilities for regression analysis, various quantitative measures for identifying influential observations: among these measures is partial leverage, a measure of"
    ]
  ],
  [
    "The logit model is more commonly used in econometric analysis compared to the linear probability model.",
    [
      "logit model or the probit model are more commonly used. Linear probability model In statistics, a linear probability model is a special case of a binomial regression model. Here the dependent variable for each observation takes values which are either 0 or 1. The probability of observing a 0 or 1 in any one case is treated as depending on one or more explanatory variables. For the \"linear probability model\", this relationship is a particularly simple one, and allows the model to be fitted by simple linear regression. The model assumes that, for a binary outcome (Bernoulli trial), formula_1, and",
      "of the odds, the logit (\"L\") is expressed as This relationship shows that \"L\" is linear in relation to \"X\", but the probabilities are not linear in terms of \"X\". Another model that was developed to offset the disadvantages of the LPM is the probit model. The probit model uses the same approach to non-linearity as does the logit model; however, it uses the normal CDF instead of the logistic CDF. Dummy variable (statistics) In statistics and econometrics, particularly in regression analysis, a dummy variable (also known as an indicator variable, design variable, one-hot encoding, Boolean indicator, binary variable, or",
      "the logit model. In the logit model, the cumulative distribution of the error term in the regression equation is logistic. The regression is more realistic in that it is non-linear. The logit model is estimated using the maximum likelihood approach. In this model, formula_21, which is the probability of the dependent variable taking the value of 1 given the independent variable is: where formula_23. The model is then expressed in the form of the odds ratio: what is modeled in the logistic regression is the natural logarithm of the odds, the odds being defined as formula_24. Taking the natural log"
    ]
  ],
  [
    "is significant in the design of experiments because it helps researchers account for the tendency of extreme values to move closer to the mean upon repeated measurements, ensuring more accurate and reliable results.",
    [
      "the mean\". Regression toward the mean In statistics, regression toward (or to) the mean is the phenomenon that if a variable is extreme on its first measurement, it will tend to be closer to the mean or average on its second measurement, and if it is extreme on its second measurement, it will tend to have been closer to the average on its first. To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. Historically, what is now called regression toward the mean has also been called reversion to the mean",
      "Regression toward the mean In statistics, regression toward (or to) the mean is the phenomenon that if a variable is extreme on its first measurement, it will tend to be closer to the mean or average on its second measurement, and if it is extreme on its second measurement, it will tend to have been closer to the average on its first. To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. Historically, what is now called regression toward the mean has also been called reversion to the mean and reversion",
      "the average than their parents. This population genetic phenomenon of regression to the mean is best thought of as a combination of a binomially distributed process of inheritance plus normally distributed environmental influences. In contrast, the term \"regression to the mean\" is now often used to describe the phenomenon by which an initial sampling bias may disappear as new, repeated, or larger samples display sample means that are closer to the true underlying population mean. Regression toward the mean is a significant consideration in the design of experiments. Take a hypothetical example of 1,000 individuals of a similar age who"
    ]
  ],
  [
    "For testing autocorrelation in the residuals of a model, the augmented Dickey\u2013Fuller test is shown to be better for all sample sizes, including small ones.",
    [
      "valid in large samples is the augmented Dickey\u2013Fuller test. The optimal finite sample tests for a unit root in autoregressive models were developed by Denis Sargan and Alok Bhargava by extending the work by John von Neumann, and James Durbin and Geoffrey Watson. In the observed time series cases, for example, Sargan-Bhargava statistics test the unit root null hypothesis in first order autoregressive models against one-sided alternatives, i.e., if the process is stationary or explosive under the alternative hypothesis. Other popular tests include: Unit root tests are closely linked to serial correlation tests. However, while all processes with a unit",
      "for comparing means under relaxed conditions when less is assumed. Welch's t test assumes the least and is therefore the most commonly used test in a two-sample hypothesis test where the mean of a metric is to be optimized. While the mean of the variable to be optimized is the most common choice of estimator, others are regularly used. For a comparison of two binomial distributions such as a click-through rate one would use Fisher's exact test. Like most fields, setting a date for the advent of a new method is difficult because of the continuous evolution of a topic.",
      "is subject to asymptotic properties, i.e. large samples. If the sample size is too small then the results will not be reliable and one should use Auto Regressive Distributed Lags (ARDL). Peter C. B. Phillips and Sam Ouliaris (1990) show that residual-based unit root tests applied to the estimated cointegrating residuals do not have the usual Dickey\u2013Fuller distributions under the null hypothesis of no-cointegration. Because of the spurious regression phenomenon under the null hypothesis, the distribution of these tests have asymptotic distributions that depend on (1) the number of deterministic trend terms and (2) the number of variables with which"
    ]
  ],
  [
    "The disadvantages of parsimony in the context of volatility and correlation modelling include the potential for overfitting with an excessive number of parameters and the risk of model misspecification leading to a failure to accurately represent the future distribution.",
    [
      "of a 'volatility surface' is one such adaptation. But they find it preferable to fudge one parameter, namely volatility, and make it a function of time to expiry and strike price, rather than have to precisely estimate another.\" However, Cherubini and Della Lunga describe the disadavantages of parsimony in the context of volatility and correlation modelling. Using an excessive number of parameters may induce overfitting while choosing a severely specified model may easily induce model misspecification and a systematic failure to represent the future distribution. Fender and Kiff (2004) note that holding complex financial instruments, such as CDOs, \"translates into",
      "there is no way to tell if the result is going to be biased, or the degree to which it will be biased, based on the estimate itself. With parsimony too, there is no way to tell that the data are positively misleading, without comparison to other evidence. Parsimony is often characterized as implicitly adopting the position that evolutionary change is rare, or that homoplasy (convergence and reversal) is minimal in evolution. This is not entirely true: parsimony minimizes the number of convergences and reversals that are assumed by the preferred tree, but this may result in a relatively large",
      "parsimony is in most cases not compromised by this. Parsimony analysis uses the number of character changes on trees to choose the best tree, but it does not require that exactly that many changes, and no more, produced the tree. As long as the changes that have not been accounted for are randomly distributed over the tree (a reasonable null expectation), the result should not be biased. In practice, the technique is robust: maximum parsimony exhibits minimal bias as a result of choosing the tree with the fewest changes. An analogy can be drawn with choosing among contractors based on"
    ]
  ],
  [
    "The formula for calculating the continuously compounded rate of return or instantaneous rate of return is RC = ln(P/P) = ln(1 + RS).",
    [
      "of return formula_8 is The continuously compounded real rate of return is just the continuously compounded nominal rate of return minus the continuously compounded inflation rate. Continuously compounded nominal and real returns Let \"P\" be the price of a security at time \"t\", including any cash dividends or interest, and let \"P\" be its price at \"t\" \u2212 1. Let \"RS\" be the simple rate of return on the security from \"t\" \u2212 1 to \"t\". Then The continuously compounded rate of return or instantaneous rate of return \"RC\" obtained during that period is If this instantaneous return is received",
      "within each sub-period, instead of the ending value, resulting in a different formula: where and The term \"time-weighted\" is best illustrated with \"continuous (logarithmic) rates of return\". The overall rate of return is the time-weighted average of the continuous rate of return in each sub-period. In the absence of flows, where formula_5 is the \"continuous rate of return\" and formula_22 is the length of time. Over a period of a decade, a portfolio grows by a continuous rate of return of 5% p.a. over three of those years, and 10% p.a. over the other seven years. The continuous time-weighted rate",
      "the natural logarithm and r is the continuously compounded rate: This can be generalized to discount rates that vary over time: instead of a constant discount rate \"r,\" one uses a function of time \"r\"(\"t\"). In that case the discount factor, and thus the present value, of a cash flow at time \"T\" is given by the integral of the continuously compounded rate \"r\"(\"t\"): Indeed, a key reason for using continuous compounding is to simplify the analysis of varying discount rates and to allow one to use the tools of calculus. Further, for interest accrued and capitalized overnight (hence compounded"
    ]
  ],
  [
    "The GARCH(1,1) model can capture volatility clustering and mean-reverting behavior in financial asset return time-series.",
    [
      "financial time series go against simple random walk models and have led to the use of GARCH models and mean-reverting stochastic volatility models in financial forecasting and derivatives pricing. The ARCH (Engle, 1982) and GARCH (Bollerslev, 1986) models aim to more accurately describe the phenomenon of volatility clustering and related effects such as kurtosis. The main idea behind these two models is that volatility is dependent upon past realizations of the asset process and related volatility process. This is a more precise formulation of the intuition that asset volatility tends to revert to some mean rather than remaining constant or",
      "order to describe the volatility clustering effect of the return process of an asset, the GARCH model can be used. In the GARCH model, innovation (formula_33) is assumed that formula_34, where formula_35 and where the series formula_36 are modeled by and where formula_38 and formula_39. However, the assumption of formula_35 is often rejected empirically. For that reason, new GARCH models with stable or tempered stable distributed innovation have been developed. GARCH models with formula_1-stable innovations have been introduced. Subsequently, GARCH Models with tempered stable innovations have been developed. Objections against the use of stable distributions in Financial models are given",
      "condition that the industry factor returns sum to zero in each time period. Thus the model is estimated as subject to where the sum is over industry factors. Here m(t) is the market return. Explicitly identifying the market factor then permitted Torre to estimate the variance of this factor using a leveraged GARCH(1,1) model due to Robert Engle and Tim Bollerslev Here and w, a, b1 and b2 are parameters fit from long time series estimations using maximum likelihood methods. This model provides a rapid update of market variance which is incorporated into the update of F, resulting in a"
    ]
  ],
  [
    "In the estimation of a GARCH-M model with daily corporate bond percentage returns data, the value of the GARCH-in-mean parameter estimate would be expected to be fit from long time series estimations using maximum likelihood methods.",
    [
      "normal variable or come from a generalized error distribution. The formulation for formula_46 allows the sign and the magnitude of formula_45 to have separate effects on the volatility. This is particularly useful in an asset pricing context. Since formula_48 may be negative, there are no sign restrictions for the parameters. The GARCH-in-mean (GARCH-M) model adds a heteroskedasticity term into the mean equation. It has the specification: formula_49 The residual formula_50 is defined as: formula_51 The Quadratic GARCH (QGARCH) model by Sentana (1995) is used to model asymmetric effects of positive and negative shocks. In the example of a GARCH(1,1) model,",
      "condition that the industry factor returns sum to zero in each time period. Thus the model is estimated as subject to where the sum is over industry factors. Here m(t) is the market return. Explicitly identifying the market factor then permitted Torre to estimate the variance of this factor using a leveraged GARCH(1,1) model due to Robert Engle and Tim Bollerslev Here and w, a, b1 and b2 are parameters fit from long time series estimations using maximum likelihood methods. This model provides a rapid update of market variance which is incorporated into the update of F, resulting in a",
      "and formula_46 are the current forward price and volatility, whereas formula_47 and formula_48 are two correlated Wiener processes (i.e. Brownian motions) with correlation coefficient formula_49. The constant parameters formula_50 are such that formula_51. The main feature of the SABR model is to be able to reproduce the smile effect of the volatility smile. The Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is another popular model for estimating stochastic volatility. It assumes that the randomness of the variance process varies with the variance, as opposed to the square root of the variance as in the Heston model. The standard GARCH(1,1) model has"
    ]
  ],
  [
    "Bootstrapping would be preferred to pure simulation when the simulation model needs to reach a steady state and initial data points need to be iteratively improved using a pseudorandom number generator to schedule events.",
    [
      "to the initial set of pending events. These events, however, schedule additional events, and with time, the distribution of event times approaches its steady state. This is called \"bootstrapping\" the simulation model. In gathering statistics from the running model, it is important to either disregard events that occur before the steady state is reached or to run the simulation for long enough that the bootstrapping behavior is overwhelmed by steady-state behavior. (This use of the term \"bootstrapping\" can be contrasted with its use in both statistics and computing). The simulation typically keeps track of the system's statistics, which quantify the",
      "aspects of interest. In the bank example, it is of interest to track the mean waiting times. In a simulation model, performance metrics are not analytically derived from probability distributions, but rather as averages over replications, that is different runs of the model. Confidence intervals are usually constructed to help assess the quality of the output. Because events are bootstrapped, theoretically a discrete-event simulation could run forever. So the simulation designer must decide when the simulation will end. Typical choices are \"at time t\" or \"after processing n number of events\" or, more generally, \"when statistical measure X reaches the",
      "event simulation represents the operation of a system as a chronological sequence of events. A technique called \"bootstrapping the simulation model\" is used, which bootstraps initial data points using a pseudorandom number generator to schedule an initial set of pending events, which schedule additional events, and with time, the distribution of event times approaches its steady state\u2014the bootstrapping behavior is overwhelmed by steady-state behavior. Bootstrapping is a technique used to iteratively improve a classifier's performance. Seed AI is a hypothesized type of artificial intelligence capable of recursive self-improvement. Having improved itself, it would become better at improving itself, potentially leading"
    ]
  ],
  [
    "of the syntactic consequence definition, where a formula is considered a logical consequence of a set of statements if it is true in all possible worlds where the statements are true.",
    [
      "is violated when subjects can communicate with each other about their treatments, decisions, or experiences, thereby influencing each other\u2019s potential outcomes. If the non-interference assumption does not hold, units no longer have just two potential outcomes (treated and control), but a variety of other potential outcomes that depend on other units\u2019 treatment assignments, which complicates the estimation of the average treatment effect. Estimating spillover effects requires relaxing the non-interference assumption. This is because a unit\u2019s outcomes depend not only on its treatment assignment but also on the treatment assignment of its neighbors. The researcher must posit a set of potential",
      "sentence. The law itself inflicts the penalty in the moment when the violation of the law is complete. this kind of penalty is especially effective in the Church, whose subjects are obliged in conscience to obey her laws. If the crime be secret, the censure is also secret, but it is binding before God and in conscience; if the crime be public the censure is also public; but if the secret censure thus incurred is to be made public, then a judicial examination of the crime is had, and the formal declaration (declaratory sentence) is made that the delinquent has",
      "there is a formal proof in formula_11 of formula_10 from the set formula_12. Syntactic consequence does not depend on any interpretation of the formal system. A formula formula_10 is a semantic consequence within some formal system formula_11 of a set of statements formula_12 if and only if there is no model formula_21 in which all members of formula_12 are true and formula_10 is false. Or, in other words, the set of the interpretations that make all members of formula_12 true is a subset of the set of the interpretations that make formula_10 true. Modal accounts of logical consequence are variations"
    ]
  ],
  [
    " in the field of economics is known as nowcasting, which involves predicting the present, very near future, and very recent past in order to assess the state of an economy in real-time.",
    [
      "Nowcasting (economics) Nowcasting is the prediction of the present, the very near future and the very recent past in economics. The term is a contraction of \"now\" and \"forecasting\" and has been used for a long-time in meteorology. It has recently become popular in economics as standard measures used to assess the state of an economy, e.g. gross domestic product (GDP), are only determined after a long delay, and are even then subject to subsequent revisions. Nowcasting models have been applied in many institutions, in particular Central Banks, and the technique is used routinely to monitor the state of the",
      "economy in real time. While weather forecasters know weather conditions today and only have to predict future weather, economists have to forecast the present and even the recent past. Historically, nowcasting techniques have been based on simplified heuristic approaches. A 2008 paper by Giannone, Reichlin and Small finds that the process of nowcasting can be formalized in a statistical model which produces predictions without the need for informal judgement. The model exploits information from a large quantity of data series at different frequencies and with different publication lags. The idea is that signals about the direction of change in GDP",
      "models. Examples of US macroeconomic series of interest include but are not limited to Consumption, Investment, Real GNP, and Capital Stock. Factors that are involved in the predictability of an economic system include the range of the forecast (is the forecast two years \"out\" or twenty) and the variability of estimates. Mathematical processes for assessing the predictability of macroeconomic trends are still in development. Predictability Predictability is the degree to which a correct prediction or forecast of a system's state can be made either qualitatively or quantitatively. Causal determinism has a strong relationship with predictability. Perfect predictability implies strict determinism,"
    ]
  ],
  [
    "The purpose of the Power Factor ranking system in practical shooting competitions is to reward cartridges with more recoil and to ensure that ammunition used meets a minimum power factor requirement.",
    [
      "Power factor (shooting sports) Power Factor (PF) in practical shooting competitions refers to a ranking system used to reward cartridges with more recoil. Power factor is a measure of the momentum of the bullet (scaled product of the bullet's mass and velocity), which to some degree reflect the recoil impulse from the firearm onto the shooter (see section on limitations). Power factor is used in competitions sanctioned by the International Practical Shooting Confederation (IPSC), United States Practical Shooting Association (USPSA), Bianchi Cup, Steel Challenge and International Defensive Pistol Association (IDPA). The power factor is based on the bullet's momentum as",
      "meet the previous minimum power factor of 125 kgr\u00b7ft/s. To correlate with commonly available ammunition for firearms within a given division, revised power factors were established, effective 01 January 2017. In the Bianchi Cup, all matches requiring the use of centerfire ammunition must meet or exceed a power factor of 120 kgr\u00b7ft/s ( \u2248 2.37 Ns). Power factor (shooting sports) Power Factor (PF) in practical shooting competitions refers to a ranking system used to reward cartridges with more recoil. Power factor is a measure of the momentum of the bullet (scaled product of the bullet's mass and velocity), which to",
      "devices like smartphones and tablets with the WinMSS Electronic Score Sheet (ESS) app or third party scoring systems like Shoot'n Score It or PractiScore. The power factor is the momentum of the fired bullet as it's moving through the air, which contribute to the recoil of the firearm (together with the propellant gases stemming from the amount of gunpowder). Thus, the power factor in a way reflects recoil. The power factor must exceed certain thresholds, and is calculated by measuring the \"bullet speed\" using a chronograph and measuring another of the competitors bullets on a weighing scale to find the"
    ]
  ],
  [
    "Negative residual autocorrelation in the economy indicates that adjacent observations are more likely to lie below the fitted regression line, suggesting a negative relationship between variables.",
    [
      "Subsequent reviews of the literature have similarly found no agreed resolution. Azeredo (2014) showed that traditional pre-1930 consumption measures understate the extent of serial correlation in the U.S. annual real growth rate of per capita consumption of non-durables and services (\"consumption growth\"). Under alternative measures proposed in the study, the serial correlation of consumption growth is found to be positive. This new evidence implies that an important subclass of dynamic general equilibrium models studied by Mehra and Prescott (1985) generates negative equity premium for reasonable risk-aversion levels, thus further exacerbating the equity premium puzzle. Some explanations rely on assumptions about",
      "Negative relationship In statistics, there is a negative relationship or inverse relationship between two variables if higher values of one variable tend to be associated with lower values of the other. A negative relationship between two variables usually implies that the correlation between them is negative, or \u2014 what is in some contexts equivalent \u2014 that the slope in a corresponding graph is negative. A negative correlation between variables is also called anticorrelation or inverse correlation. Negative correlation can be seen geometrically when two normalized random vectors are viewed as points on a sphere, and the correlation between them is",
      "spend a very large amount or as little as low income people spend. Heteroskedastic can also be caused by changes in measurement practices. For example, as statistical offices improve their data, measurement error decreases, so the error term declines over time. This assumption is violated when there is autocorrelation. Autocorrelation can be visualized on a data plot when a given observation is more likely to lie above a fitted line if adjacent observations also lie above the fitted regression line. Autocorrelation is common in time series data where a data series may experience \"inertia.\" If a dependent variable takes a"
    ]
  ],
  [
    "The ARCH(q) model is more likely to be the more parsimonious compared to the GARCH(1,1) model.",
    [
      "order to describe the volatility clustering effect of the return process of an asset, the GARCH model can be used. In the GARCH model, innovation (formula_33) is assumed that formula_34, where formula_35 and where the series formula_36 are modeled by and where formula_38 and formula_39. However, the assumption of formula_35 is often rejected empirically. For that reason, new GARCH models with stable or tempered stable distributed innovation have been developed. GARCH models with formula_1-stable innovations have been introduced. Subsequently, GARCH Models with tempered stable innovations have been developed. Objections against the use of stable distributions in Financial models are given",
      "financial time series go against simple random walk models and have led to the use of GARCH models and mean-reverting stochastic volatility models in financial forecasting and derivatives pricing. The ARCH (Engle, 1982) and GARCH (Bollerslev, 1986) models aim to more accurately describe the phenomenon of volatility clustering and related effects such as kurtosis. The main idea behind these two models is that volatility is dependent upon past realizations of the asset process and related volatility process. This is a more precise formulation of the intuition that asset volatility tends to revert to some mean rather than remaining constant or",
      "model fits well, and when it is high the model doesn't fit well. If formula_70 for some formula_25 then there is a typical model formula_43 for formula_2 such that formula_68 and formula_2 is typical (random) for S. That is, formula_5 is the best-fitting model for x. For more details see and especially and. Within the constraints that the graph goes down at an angle of at least 45 degrees, that it starts at n and ends approximately at formula_40, every graph (up to a formula_78 additive term in argument and value) is realized by the structure function of some data"
    ]
  ],
  [
    "No, the parameters of a triangular or recursive system are not estimated using separate applications of OLS to each equation.",
    [
      "other approaches. In all cases the formula for OLS estimator remains the same: ; the only difference is in how we interpret this result. For mathematicians, OLS is an approximate solution to an overdetermined system of linear equations , where \"\u03b2\" is the unknown. Assuming the system cannot be solved exactly (the number of equations \"n\" is much larger than the number of unknowns \"p\"), we are looking for a solution that could provide the smallest discrepancy between the right- and left- hand sides. In other words, we are looking for the solution that satisfies where ||\u00b7|| is the standard",
      "have finite variances. Under the additional assumption that the errors are normally distributed, OLS is the maximum likelihood estimator. OLS is used in fields as diverse as economics (econometrics), data science, political science, psychology and engineering (control theory and signal processing). Suppose the data consists of \"n\" observations { \"y, x\" }. Each observation \"i\" includes a scalar response \"y\" and a column vector \"x\" of values of \"p\" predictors (regressors) \"x\" for \"j\" = 1, ..., \"p\". In a linear regression model, the response variable, formula_1, is a linear function of the regressors: or in vector form, where \"\u03b2\"",
      "the OLS estimator for \"\u03b2\". The function \"S\"(\"b\") is quadratic in \"b\" with positive-definite Hessian, and therefore this function possesses a unique global minimum at formula_22, which can be given by the explicit formula: The product \"N\"=\"X\" \"X\" is a normal matrix and its inverse, \"Q\"=\"N\", is the \"cofactor matrix\" of \"\u03b2\", closely related to its covariance matrix, \"C\". The matrix (\"X\" \"X\") \"X\"=\"Q\" \"X\" is called the Moore\u2013Penrose pseudoinverse matrix of X. This formulation highlights the point that estimation can be carried out if, and only if, there is no perfect multicollinearity between the explanatory variables (which would cause"
    ]
  ],
  [
    "The values of \u03b1 and \u03b2 that provide the \"best\" fit for the data points in terms of minimizing the sum of squared residuals in the least-squares approach are those that minimize the objective function \"S\"(\u03b2) of squared residuals.",
    [
      "model. The goal is to find estimated values formula_3 and formula_4 for the parameters and which would provide the \"best\" fit in some sense for the data points. As mentioned in the introduction, in this article the \"best\" fit will be understood as in the least-squares approach: a line that minimizes the sum of squared residuals formula_5 (differences between actual and predicted values of the dependent variable \"y\"), each of which is given by, for any candidate parameter values formula_6 and formula_7, In other words, formula_3 and formula_4 solve the following minimization problem: By expanding to get a quadratic expression",
      "parameter values \u03b2 are defined to be those values that minimise the objective function \"S\"(\u03b2) of squared residuals: where the residuals are defined as the differences between the values of the dependent variables (observations) and the model values: and where \"n\" is the overall number of data points. For a least trimmed squares analysis, this objective function is replaced by one constructed in the following way. For a fixed value of \u03b2, let formula_5 denote the set of ordered absolute values of the residuals (in increasing order of absolute value). In this notation, the standard sum of squares function is",
      "measured by its residual, defined as the difference between the actual value of the dependent variable and the value predicted by the model: The least-squares method finds the optimal parameter values by minimizing the sum, formula_7, of squared residuals: An example of a model in two dimensions is that of the straight line. Denoting the y-intercept as formula_9 and the slope as formula_10, the model function is given by formula_11. See linear least squares for a fully worked out example of this model. A data point may consist of more than one independent variable. For example, when fitting a plane"
    ]
  ],
  [
    "The dimension of a vector space is the cardinality (i.e. the number of vectors) of a basis of the vector space over its base field.",
    [
      "Dimension (vector space) In mathematics, the dimension of a vector space \"V\" is the cardinality (i.e. the number of vectors) of a basis of \"V\" over its base field. It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension. For every vector space there exists a basis, and all bases of a vector space have equal cardinality; as a result, the dimension of a vector space is uniquely defined. We say \"V\" is ' if the dimension of \"V\" is finite, and ' if its dimension is infinite. The dimension",
      "replacing the dimension with the character gives the McKay\u2013Thompson series for each element of the Monster group. Dimension (vector space) In mathematics, the dimension of a vector space \"V\" is the cardinality (i.e. the number of vectors) of a basis of \"V\" over its base field. It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension. For every vector space there exists a basis, and all bases of a vector space have equal cardinality; as a result, the dimension of a vector space is uniquely defined. We say \"V\" is",
      "vector space \"V\", then the codimension of \"W\" in \"V\" is the difference between the dimensions: It is the complement of the dimension of \"W,\" in that, with the dimension of \"W,\" it adds up to the dimension of the ambient space \"V:\" Similarly, if \"N\" is a submanifold or subvariety in \"M\", then the codimension of \"N\" in \"M\" is Just as the dimension of a submanifold is the dimension of the tangent bundle (the number of dimensions that you can move \"on\" the submanifold), the codimension is the dimension of the normal bundle (the number of dimensions you"
    ]
  ],
  [
    "The characteristic roots of the MA process are the roots of the characteristic equation of the process (engineering).",
    [
      "or condition of the operating system which is to be measured. We select a specific characteristic because a correlation exists between it and how the system is performing. The characteristic can be the output of the system during any stage of processing or it may be a condition that is the result of the system. For example, it may be the heat energy produced by the furnace or the temperature in the room which has changed because of the heat generated by the furnace. In an elementary school system, the hours a teacher works or the gain in knowledge demonstrated",
      "factor which cannot be avoided: as a matter of fact, a process is always interdependent with other phenomena including other processes. Process (engineering) In engineering, a process is a series of interrelated tasks that, together, transform inputs into outputs. These tasks may be carried out by people, nature or machines using various resources; an engineering process must be considered in the context of the agents carrying out the tasks and the resource attributes involved. Systems engineering normative documents and those related to Maturity Models are typically based on processes, for example, systems engineering processes of the EIA-632 and processes involved",
      "to all of morphogenesis. Cancer can result from disruption of normal morphogenesis, including both tumor formation and tumor metastasis. Mitochondrial dysfunction can result in increased cancer risk due to disturbed morphogen signaling. a. Thompson's book is often cited. An abridged version, comprising 349 pages, remains in print and readily obtainable. An unabridged version, comprising 1116 pages, has also been published. Morphogenesis Morphogenesis (from the Greek \"morph\u00ea\" shape and \"genesis\" creation, literally, \"beginning of the shape\") is the biological process that causes an organism to develop its shape. It is one of three fundamental aspects of developmental biology along with the"
    ]
  ],
  [
    "Deterministic.",
    [
      "derived from a French verb meaning \"to run\" or \"to gallop\". The first written appearance of the term \"random process\" pre-dates \"stochastic process\", which the Oxford English Dictionary also gives as a synonym, and was used in an article by Francis Edgeworth published in 1888. The definition of a stochastic process varies, but a stochastic process is traditionally defined as a collection of random variables indexed by some set. The terms \"random process\" and \"stochastic process\" are considered synonyms and are used interchangeably, without the index set being precisely specified. Both \"collection\", or \"family\" are used while instead of \"index",
      "Stochastic The word stochastic is an adjective in English that describes something that was randomly determined. The word first appeared in English to describe a mathematical object called a stochastic process, but now in mathematics the terms \"stochastic process\" and \"random process\" are considered interchangeable. The word, with its current definition meaning random, came from German, but it originally came . The term \"stochastic\" is used in many different fields, particularly where stochastic or random processes are used to represent systems or phenomena that seem to change in a random way. Examples of such fields include the physical sciences such",
      "variables. Random variable In probability and statistics, a random variable, random quantity, aleatory variable, or stochastic variable is a variable whose possible values are outcomes of a random phenomenon. More specifically, a random variable is defined as a function that maps the outcomes of unpredictable processes to numerical quantities (labels), typically real numbers. In this sense, it is a procedure for assigning a numerical quantity to each physical outcome. Contrary to its name, this procedure itself is neither random nor variable. Rather, the underlying process providing the input to this procedure yields random (possibly non-numerical) output that the procedure maps"
    ]
  ],
  [
    "The Durbin Watson test statistic of 1.53 indicates that there may be positive autocorrelation in the residuals of the regression model.",
    [
      "is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters of the regression model more often than we would if the standard errors were correctly estimated. If the Durbin\u2013Watson statistic indicates the presence of serial correlation of the residuals, this can be remedied by using the Cochrane\u2013Orcutt procedure. The Durbin\u2013Watson statistic, while displayed by many regression analysis programs, is not applicable in certain situations. For instance, when lagged dependent variables are included in the explanatory variables, then it is inappropriate to use this test. Durbin's h-test (see below) or",
      "Durbin\u2013Watson statistic In statistics, the Durbin\u2013Watson statistic is a test statistic used to detect the presence of autocorrelation at lag 1 in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. The small sample distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. Later, John Denis Sargan and",
      "the balanced panel\u2014time periods the individuals were surveyed), \"K\" (number of regressors) and \"N\" (number of individuals in the panel). This test statistic can also be used for testing the null hypothesis of a unit root against stationary alternatives in fixed effects models using another set of bounds (Tables V and VI) tabulated by Alok Bhargava et al. (1982). A version of the statistic suitable for unbalanced panel data is given by Baltagi and Wu (1999). Durbin\u2013Watson statistic In statistics, the Durbin\u2013Watson statistic is a test statistic used to detect the presence of autocorrelation at lag 1 in the residuals"
    ]
  ],
  [
    "Using OLS in the presence of autocorrelation can lead to biased and inconsistent estimates, as the estimator may pick the parameter that makes the resulting errors appear uncorrelated with the regressors, rather than capturing the true causal impact of the variables.",
    [
      "term is uncorrelated with the regressors. The presence of omitted-variable bias violates this particular assumption. The violation causes the OLS estimator to be biased and inconsistent. The direction of the bias depends on the estimators as well as the covariance between the regressors and the omitted variables. A positive covariance of the omitted variable with both a regressor and the dependent variable will lead the OLS estimate of the included regressor's coefficient to be greater than the true value of that coefficient. This effect can be seen by taking the expectation of the parameter, as shown in the previous section.",
      "solves for formula_3 such that formula_4 (when we minimize the sum of squared errors, formula_5, the first-order condition is exactly formula_6.) If the true model is believed to have formula_7 due to any of the reasons listed above\u2014for example, if there is an omitted variable which affects both formula_2 and formula_9 separately\u2014then this OLS procedure will \"not\" yield the causal impact of formula_2 on formula_9. OLS will simply pick the parameter that makes the resulting errors appear uncorrelated with formula_2. Consider for simplicity the single-variable case. Suppose we are considering a regression with one variable and a constant (perhaps no",
      "whether formula_22 is uncorrelated with formula_23, but based on whether another variable formula_24 (or set of variables) is (are) uncorrelated with formula_23. If theory suggests that formula_24 is related to formula_22 (the first stage) but uncorrelated with formula_23 (the exclusion restriction), then IV may identify the causal parameter of interest where OLS fails. Because there are multiple specific ways of using and deriving IV estimators even in just the linear case (IV, 2SLS, GMM), we save further discussion for the Estimation section below. Of course, IV techniques have been developed among a much broader class of non-linear models. General definitions"
    ]
  ],
  [
    "The presence of multicollinearity can lead to biased and inconsistent estimates of the OLS estimator.",
    [
      "same thing as each other. Multicollinearity In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multivariate regression model with collinear predictors can",
      "Multicollinearity In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multivariate regression model with collinear predictors can indicate how well the entire",
      "term is uncorrelated with the regressors. The presence of omitted-variable bias violates this particular assumption. The violation causes the OLS estimator to be biased and inconsistent. The direction of the bias depends on the estimators as well as the covariance between the regressors and the omitted variables. A positive covariance of the omitted variable with both a regressor and the dependent variable will lead the OLS estimate of the included regressor's coefficient to be greater than the true value of that coefficient. This effect can be seen by taking the expectation of the parameter, as shown in the previous section."
    ]
  ],
  [
    "The ACF plot can be used to identify non-stationary time series by observing slower decay in autocorrelation compared to stationary data.",
    [
      "by removing changes in the level of a time series, and so eliminating trend and seasonality. One of the ways for identifying non-stationary times series is the ACF plot. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Stationary process In mathematics and statistics, a stationary process ( a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time. Since stationarity is",
      "from a run sequence plot. The run sequence plot should show constant location and scale. It can also be detected from an autocorrelation plot. Specifically, non-stationarity is often indicated by an autocorrelation plot with very slow decay. Seasonality (or periodicity) can usually be assessed from an autocorrelation plot, a seasonal subseries plot, or a spectral plot. Box and Jenkins recommend the differencing approach to achieve stationarity. However, fitting a curve and subtracting the fitted values from the original data can also be used in the context of Box\u2013Jenkins models. At the model identification stage, the goal is to detect seasonality,",
      "are finite or infinite sequences of data items, where each item has an associated timestamp and the sequence of timestamps is non-decreasing. Elements of a time series are often called ticks. The timestamps are not required to be ascending (merely non-decreasing) because in practice the time resolution of some systems such as financial data sources can be quite low (milliseconds, microseconds or even nanoseconds), so consecutive events may carry equal timestamps. Time series data provides a historical context to the analysis typically associated with complex event processing. This can apply to any vertical industry such as finance and cooperatively with"
    ]
  ],
  [
    "The optimal one-step ahead forecast of a time series data following a random walk process is equal to the last observed value.",
    [
      "in the autoregressive equation along with observed values of \"X\" for all periods prior to the one being predicted, and the output of the equation is the one-step-ahead forecast; this procedure is used to obtain forecasts for each of the out-of-sample observations. To evaluate the quality of \"n\"-step-ahead forecasts, the forecasting procedure in the previous section is employed to obtain the predictions. Given a set of predicted values and a corresponding set of actual values for \"X\" for various time periods, a common evaluation technique is to use the mean squared prediction error; other measures are also available (see forecasting#forecasting",
      "makes efficient use of the available data, as only one observation is omitted at each step For time series data, the training set can only include observations prior to the test set. Therefore no future observations can be used in constructing the forecast. Suppose \"k\" observations are needed to produce a reliable forecast; then the process works as follows: This procedure is sometimes known as a \"rolling forecasting origin\" because the \"origin\" (\"k+i -1)\" at which the forecast is based rolls forward in time. Further, two-step-ahead or in general \"p\"-step-ahead forecasts can be computed by first forecasting the value immediately",
      "is the past data. Although the time series notation has been used here, the average approach can also be used for cross-sectional data (when we are predicting unobserved values; values that are not included in the data set). Then, the prediction for unobserved values is the average of the observed values. Na\u00efve forecasts are the most cost-effective forecasting model, and provide a benchmark against which more sophisticated models can be compared. This forecasting method is only suitable for time series data. Using the na\u00efve approach, forecasts are produced that are equal to the last observed value. This method works quite"
    ]
  ],
  [
    "Orders with lower priority are given lower priority based on the first-come-first-served rule for economy priority.",
    [
      "applications of priority queues. More advanced implementations may support more complicated operations, such as \"pull_lowest_priority_element\", inspecting the first few highest- or lowest-priority elements, clearing the queue, clearing subsets of the queue, performing a batch insert, merging two or more queues into one, incrementing priority of any element, etc. One can imagine a priority queue as a modified queue, but when one would get the next element off the queue, the highest-priority element is retrieved first. Stacks and queues may be modeled as particular kinds of priority queues. As a reminder, here is how stacks and queues behave: In a stack,",
      "requirement but doing this would cause a previously satisfied requirement to become unsatisfied (that is, to be \"injured\"). The priority order on requirements is used to determine which requirement to satisfy in this case. The informal idea is that if a requirement is injured then it will eventually stop being injured after all higher priority requirements have stopped being injured, although not every priority argument has this property. An argument must be made that the overall set \"X\" is r.e. and satisfies all the requirements. Priority arguments can be used to prove many facts about r.e. sets; the requirements used",
      "one based on Lead Time (manufacturing lead time vs delivery lead time): engineer to order (ETO), purchase to order (PTO), make to order (MTO), assemble to order (ATO) and make to stock (MTS). According to this classification different kinds of systems will have different customer order decoupling points (CODP), meaning that work in progress (WIP) cycle stock levels are practically nonexistent regarding operations located after the CODP (except for WIP due to queues). (See Order fulfillment) The concept of production systems can be expanded to the service sector world keeping in mind that services have some fundamental differences in respect"
    ]
  ],
  [
    "The condition under which the asymptotic variance of a two-step M-estimator takes the form as if there were no first-step estimation procedure is when the perturbation in \u03b3 has no impact on the First-Order condition, as sample size tends to infinity.",
    [
      "of the usual M-estimator in which the first step estimation is not necessary. This fact is intuitive because formula_13 is a random object and its variability should influence the estimation of formula_3. However, there exists a special case in which the asymptotic variance of two-step M-estimator takes the form as if there were no first-step estimation procedure. Such special case occurs if: formula_18 where formula_19 is the true value of formula_20 and formula_13 is the probability limit of formula_10. To interpret this condition, first note that under regularity conditions, formula_23 since formula_19 is the maximizer of formula_25. So the condition",
      "above implies that small perturbation in \u03b3 has no impact on the First-Order condition. Thus, in large sample, variability of formula_10 does not affect the argmax of the objective function, which explains invariant property of asymptotic variance. Of course, this result is valid only as the sample size tends to infinity, so the finite-sample property could be quite different. Two-step M-estimators Two-step M-estimators deals with M-estimation problems that require preliminary estimation to obtain the parameter of interest. Two-step M-estimation is different from usual M-estimation problem because asymptotic distribution of the second-step estimator generally depends on the first-step estimator. Accounting for",
      "M-estimator formula_8 is defined as: formula_9 where formula_10 is a parameter that needs to be estimated in the first step. Consistency of two-step M-estimators can be verified by checking consistency conditions for usual M-estimators, although some modification might be necessary. In practice, the important condition to check is identification condition. If formula_11 where formula_12where formula_13 is a non-random vector, then the identification condition is that formula_14 has a unique maximizer over formula_3. Under regularity conditions, two-step M-estimators have asymptotic normality. An important point to note is that asymptotic variance of a two-step M-estimator is generally not the same as that"
    ]
  ],
  [
    "The Dickey-Fuller / Engle-Granger approach may result in spurious regression due to the high explanatory power between two integrated series that are not directly causally related.",
    [
      "are cointegrated. For example, regressing the consumption series for any country (e.g. Fiji) against the GNP for a randomly selected dissimilar country (e.g. Afghanistan) might give a high R-squared relationship (suggesting high explanatory power on Fiji's consumption from Afghanistan's GNP). This is called spurious regression: two integrated series which are not directly causally related may nonetheless show a significant correlation; this phenomenon is called spurious correlation. The three main methods for testing for cointegration are: If formula_1 and formula_2 are non-stationary and cointegrated, then a linear combination of them must be stationary. In other words: where formula_4 is stationary. If",
      "Asymmetric cointegration In economics, testing for an asymmetric cointegration relationship among variables implies distinguishing the positive and the negative effects of the error obtained from the cointegration regression. In order to do that, economists usually use the asymmetric cointegration framework proposed by Enders and Siklos in 2001. Asymmetric cointegration comes from the analysis of multivariate combinations arising from the decomposition of the series into positive and negative values of its cumulative sums; see Lardic and Mignon (2008, p. 484). According to Cook (2006), testing for a potential asymmetric cointegration extends the analysis in further directions to allow for a possibility",
      "is subject to asymptotic properties, i.e. large samples. If the sample size is too small then the results will not be reliable and one should use Auto Regressive Distributed Lags (ARDL). Peter C. B. Phillips and Sam Ouliaris (1990) show that residual-based unit root tests applied to the estimated cointegrating residuals do not have the usual Dickey\u2013Fuller distributions under the null hypothesis of no-cointegration. Because of the spurious regression phenomenon under the null hypothesis, the distribution of these tests have asymptotic distributions that depend on (1) the number of deterministic trend terms and (2) the number of variables with which"
    ]
  ],
  [
    " plays a crucial role in regression estimation and inference by measuring the spread of data points around the regression line and helping to assess the precision of the estimated coefficients.",
    [
      "variance of the resulting estimator formula_9 is It can be shown that choosing the optimal coefficient minimizes the variance of formula_9, and that with this choice, where is the correlation coefficient of formula_2 and formula_4. The greater the value of formula_17, the greater the variance reduction achieved. In the case that formula_18, formula_19, and/or formula_20 are unknown, they can be estimated across the Monte Carlo replicates. This is equivalent to solving a certain least squares system; therefore this technique is also known as regression sampling. When the expectation of the control variable, formula_5, is not known analytically, it is still",
      "recall, is a random variable itself (a function of \"X\", determined up to probability one). As a result, formula_4 itself is a random variable (and is a function of \"X\"). Recall that variance is the expected squared deviation between a random variable (say, \"Y\") and its expected value. The expected value can be thought of as a reasonable prediction of the outcomes of the random experiment (in particular, the expected value is the best constant prediction when predictions are assessed by expected squared prediction error). Thus, one interpretation of variance is that it gives the smallest possible expected squared prediction",
      "the deviation between the random variable and its mean as the Euclidean distance. This results in formula_194 which is the trace of the covariance matrix. Variance In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of (random) numbers are spread out from their average value. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling. Variance is an important tool in the sciences, where statistical"
    ]
  ],
  [
    "One disadvantage of the general to specific or \"LSE\" approach to building econometric models is that it may lead to overfitting and misspecification, as the process of reducing complexity may result in discarding important variables or relationships in the model.",
    [
      "LSE approach to econometrics The LSE approach to econometrics, named for the London School of Economics, involves viewing econometric models as \"reductions\" from some unknown data generation process (DGP). A complex DGP is typically modelled as the starting point and this complexity allows information in the data from the real world but absent in the theory to be drawn upon. The complexity is then reduced by the econometrician by a series of restrictions which are tested. One particular functional form, the error-correction model, is often arrived at when modelling time series. Denis Sargan and David Forbes Hendry (with his general-to-specific",
      "Methodology of econometrics The methodology of econometrics is the study of the range of differing approaches to undertaking econometric analysis. Commonly distinguished differing approaches that have been identified and studied include: In addition to these more clearly defined approaches, Hoover identifies a range of \"heterogeneous\" or \"textbook approaches\" that those less, or even un-, concerned with methodology, tend to follow. Econometrics may use standard statistical models to study economic questions, but most often they are with observational data, rather than in controlled experiments. In this, the design of observational studies in econometrics is similar to the design of studies in",
      "Edward E. Leamer was an early critic of model discovery methodologies. The approach evolved to include: multiple reduction path searches, indicator saturation, COMFAC testing, and cointegrated vector autoregressive structures. Economists often associated with \"Hendry's methodology\" include Clive Granger, Robert F. Engle, S\u00f8ren Johansen, Grayham Mizon, Jennifer Castle, Hans M. Krolzig, Neil Ericsson, and Jurgen Doornik. LSE approach to econometrics The LSE approach to econometrics, named for the London School of Economics, involves viewing econometric models as \"reductions\" from some unknown data generation process (DGP). A complex DGP is typically modelled as the starting point and this complexity allows information in"
    ]
  ],
  [
    "The method used to estimate GARCH models seeks to maximize the likelihood function through Expectation-Maximum algorithms (Wu 1983).",
    [
      "modelling it has some attractive properties such as a greater weight upon more recent observations, but also drawbacks such as an arbitrary decay factor that introduces subjectivity into the estimation. The lag length \"p\" of a GARCH(\"p\", \"q\") process is established in three steps: Nonlinear Asymmetric GARCH(1,1) (NAGARCH) is a model with the specification: For stock returns, parameter formula_35 is usually estimated to be positive; in this case, it reflects a phenomenon commonly referred to as the \"leverage effect\", signifying that negative returns increase future volatility by a larger amount than positive returns of the same magnitude. This model should",
      "that are applied with GCMM and the algorithms converge globally to local maximums under mild conditions(Wu 1983 [11]). The design and properties of these Expectation-Maximum algorithms are discussed in the next section. 3 Model selection can be conducted through Akaike information criteria (Fan 2009 [5]) and cluster methods such as k-means or hierarchy clustering can be used to set the initial parameters of each component. Gaussian mixture models have been used widely in various applications and the Expectation Maximum algorithm has been utilized for estimating their parameters. The convergence properties of such Expectation Maximum algorithms have been discussed in Lei",
      "variables and the estimated covariance matrices of the best fitting model. This is obtained through numerical maximization via expectation\u2013maximization of a \"fit criterion\" as provided by maximum likelihood estimation, quasi-maximum likelihood estimation, weighted least squares or asymptotically distribution-free methods. This is often accomplished by using a specialized SEM analysis program, of which several exist. Having estimated a model, analysts will want to interpret the model. Estimated paths may be tabulated and/or presented graphically as a path model. The impact of variables is assessed using path tracing rules (see path analysis). It is important to examine the \"fit\" of an estimated"
    ]
  ],
  [
    "including relevant lagged values of the dependent variable in a regression analysis can help control for potential confounding variables and avoid false inferences of causality by directly capturing the effect of the lagged dependent variable on the current dependent variable.",
    [
      "both of which control for lagged values of the dependent variable while testing for causal effects of lagged independent variables. Regression analysis controls for other relevant variables by including them as regressors (explanatory variables). This helps to avoid false inferences of causality due to the presence of a third, underlying, variable that influences both the potentially causative variable and the potentially caused variable: its effect on the potentially caused variable is captured by directly including it in the regression, so that effect will not be picked up as an indirect effect through the potentially causative variable of interest. Given the",
      "formula_10 The gamma lag and the rational lag are other infinite distributed lag structures. Distributed lag In statistics and econometrics, a distributed lag model is a model for time series data in which a regression equation is used to predict current values of a dependent variable based on both the current values of an explanatory variable and the lagged (past period) values of this explanatory variable. The starting point for a distributed lag model is an assumed structure of the form or the form where \"y\" is the value at time period \"t\" of the dependent variable \"y\", \"a\" is",
      "reduces the number of observations available. For example, if observations are available at T time periods, then after first differencing, only T-1 lags are usable. Then, if K lags of the dependent variable are used as instruments, only T-K-1 observations are usable in the regression. This creates a trade-off: adding more lags provides more instruments, but reduces the sample size. The Arellano\u2013Bond method circumvents this problem. Consider the static linear unobserved effects model for formula_1 observations and formula_2 time periods: where formula_6 is the dependent variable observed for individual formula_7 at time formula_8 formula_9 is the time-variant formula_10 regressor matrix,"
    ]
  ],
  [
    "Some factors that can lead to a relatively high degree of out-of-sample forecast accuracy include comparing forecasting accuracy to alternative models, performing data scrubbing to remove outliers, and maximizing sharpness while maintaining reliability in forecasts (accuracy).",
    [
      "accuracy). The question of how to interpret the measured forecasting accuracy arises\u2014for example, what is a \"high\" (bad) or a \"low\" (good) value for the mean squared prediction error? There are two possible points of comparison. First, the forecasting accuracy of an alternative model, estimated under different modeling assumptions or different estimation techniques, can be used for comparison purposes. Second, the out-of-sample accuracy measure can be compared to the same measure computed for the in-sample data points (that were used for parameter estimation) for which enough prior data values are available (that is, dropping the first \"p\" data points, for",
      "additional questions such as the following: When forecasting it is important to understand which factors may influence the calculation, and to what extent. A list of some common factors can be seen below: Before forecasting is performed, the data being used must be \"prepared\". If the data contains errors, then the forecast result will be equally flawed. It is therefore vital that all anomalous data be removed. Such a procedure is known as data \"scrubbing\". Scrubbing data involved removing data points known as \"outliers\". Outliers are data that lie outside the normal pattern. They are usually caused by anomalous and",
      "ensemble is reliable, increasing this deviation will increase the usefulness of the forecast. This forecast quality can also be considered in terms of \"sharpness\", or how small the spread of the forecast is. The key aim of a forecaster should be to maximise sharpness, while maintaining reliability. Forecasts at long leads will inevitably not be particularly sharp (have particularly high resolution), for the inevitable (albeit usually small) errors in the initial condition will grow with increasing forecast lead until the expected difference between two model states is as large as the difference between two random states from the forecast model's"
    ]
  ],
  [
    "In dealing with residual autocorrelation in a regression model, possible approaches include using generalized least squares and the Newey-West HAC estimator, as well as considering the appropriate number of lagged error terms in a moving average model based on the autocorrelation function.",
    [
      "present. Without investigating autocorrelation, however, they may still mis-estimate statistics dealing with relationships among variables. In regression analysis, for example, examining the patterns of autocorrelated residuals may give important clues to third factors that may affect the relationships among variables but that have not been included in the regression model. Second, if there are clusters of similar and related societies in the sample, measures of variance will be underestimated, leading to spurious statistical conclusions. for example, exaggerating the statistical significance of correlations. Third, the underestimation of variance makes it difficult to test for replication of results from two different samples,",
      "\"T\" is the sample size and \"R\" is the coefficient of determination. Under the null hypothesis of no autocorrelation, this statistic is asymptotically distributed as formula_87 with \"k\" degrees of freedom. Responses to nonzero autocorrelation include generalized least squares and the Newey\u2013West HAC estimator (Heteroskedasticity and Autocorrelation Consistent). In the estimation of a moving average model (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order \"q\", we have formula_88, for formula_89, and formula_90, for formula_91. Serial dependence is",
      "would be When mean values are subtracted from signals before computing an autocorrelation function, the resulting function is usually called an auto-covariance function. In the following, we will describe properties of one-dimensional autocorrelations only, since most properties are easily transferred from the one-dimensional case to the multi-dimensional cases. These properties hold for wide-sense stationary processes. For data expressed as a discrete sequence, it is frequently necessary to compute the autocorrelation with high computational efficiency. A brute force method based on the signal processing definition formula_60 can be used when the signal size is small. For example, to calculate the autocorrelation"
    ]
  ],
  [
    "A strict-sense stationary process in time series analysis is a stochastic process whose unconditional joint probability distribution does not change when shifted in time.",
    [
      "Stationary process In mathematics and statistics, a stationary process ( a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time. Since stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data is often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due either to the presence of a unit root or of a deterministic trend.",
      "to be strictly stationary, strongly stationary or strict-sense stationary if Since formula_6 does not affect formula_7, formula_8 is not a function of time. White noise is the simplest example of a stationary process. An example of a discrete-time stationary process where the sample space is also discrete (so that the random variable may take one of \"N\" possible values) is a Bernoulli scheme. Other examples of a discrete-time stationary process with continuous sample space include some autoregressive and moving average processes which are both subsets of the autoregressive moving average model. Models with a non-trivial autoregressive component may be either",
      "by removing changes in the level of a time series, and so eliminating trend and seasonality. One of the ways for identifying non-stationary times series is the ACF plot. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Stationary process In mathematics and statistics, a stationary process ( a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time. Since stationarity is"
    ]
  ],
  [
    "The information criterion that is consistent and deals with the trade-off between goodness of fit and simplicity of a model is the Akaike information criterion (AIC). (In making an estimate of the information lost, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model.)",
    [
      "model. Let formula_1 be the maximum value of the likelihood function for the model. Then the AIC value of the model is the following. Given a set of candidate models for the data, the preferred model is the one with the minimum AIC value. Thus, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages overfitting, because increasing the number of parameters in the model almost always improves the goodness of the fit. AIC is founded in information theory.",
      "lost by a given model: the less information a model loses, the higher the quality of that model. (In making an estimate of the information lost, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model.) The Akaike information criterion is named after the statistician Hirotugu Akaike, who formulated it. It now forms the basis of a paradigm for the foundations of statistics; as well, it is widely used for statistical inference. Suppose that we have a statistical model of some data. Let be the number of estimated parameters in the",
      "degrees of freedom, and the constant formula_51 is an unknown constant. To achieve the theoretical model selection consistency for divergent formula_52, the two high dimensional BICs above require the multiplicative factor formula_53. However, in practical use, the high dimensional BIC can take a simpler form: formula_54 where various choices of the multiplicative factor formula_55 can be used. In empirical studies, formula_56 or formula_57 can be used and it is shown to have good empirical performance. Bayesian information criterion In statistics, the Bayesian information criterion (BIC) or Schwarz information criterion (also SIC, SBC, SBIC) is a criterion for model selection among"
    ]
  ],
  [
    "The final transformation property of the Riesz transform with respect to rotations and translations is that it is invariant under translations.",
    [
      "Translation (geometry) In Euclidean geometry, a translation is a geometric transformation that moves every point of a figure or a space by the same distance in a given direction. In Euclidean geometry a transformation is a one-to-one correspondence between two sets of points or a mapping from one plane to another. A translation can be described as a rigid motion: the other rigid motions are rotations, reflections and glide reflections. A translation can also be interpreted as the addition of a constant vector to every point, or as shifting the origin of the coordinate system. A translation operator is an",
      "of the object. The translation vector formula_8 common to all points of the object describes a particular type of displacement of the object, usually called a \"linear\" displacement to distinguish it from displacements involving rotation, called \"angular\" displacements. When considering spacetime, a change of time coordinate is considered to be a translation. For example, the Galilean group and the Poincar\u00e9 group include translations with respect to time. Translation (geometry) In Euclidean geometry, a translation is a geometric transformation that moves every point of a figure or a space by the same distance in a given direction. In Euclidean geometry a",
      "is equivalent to the angular momentum conservation law. See also rotational invariance. Translational symmetry leaves an object invariant under a discrete or continuous group of translations formula_1. The illustration on the right shows four congruent triangles generated by translations along the arrow. If the line of triangles extended to infinity in both directions, they would have a discrete translational symmetry; any translation that mapped one triangle onto another would leave the whole line unchanged. In 2D, a glide reflection symmetry (in 3D it is called a glide plane symmetry, and a transflection in general) means that a reflection in a"
    ]
  ],
  [
    "The purpose of augmenting the Dickey-Fuller test regression is to test for the presence of a unit root in a time series sample, with the alternative hypothesis typically being stationarity or trend-stationarity.",
    [
      "The regression model can be written as where formula_8 is the first difference operator. This model can be estimated and testing for a unit root is equivalent to testing formula_9 (where formula_10). Since the test is done over the residual term rather than raw data, it is not possible to use standard t-distribution to provide critical values. Therefore, this statistic formula_3 has a specific distribution simply known as the Dickey\u2013Fuller table. There are three main versions of the test: 1. Test for a unit root: 2. Test for a unit root with drift: 3. Test for a unit root with",
      "Augmented Dickey\u2013Fuller test In statistics and econometrics, an augmented Dickey\u2013Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is an augmented version of the Dickey\u2013Fuller test for a larger and more complicated set of time series models. The augmented Dickey\u2013Fuller (ADF) statistic, used in the test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some",
      "Dickey\u2013Fuller test In statistics, the Dickey\u2013Fuller test tests the null hypothesis that a unit root is present in an autoregressive model. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is named after the statisticians David Dickey and Wayne Fuller, who developed the test in 1979. A simple AR(1) model is where formula_2 is the variable of interest, formula_3 is the time index, formula_4 is a coefficient, and formula_5 is the error term. A unit root is present if formula_6. The model would be non-stationary in this case."
    ]
  ],
  [
    "The transition matrix representing the probabilities of weather conditions given the weather on the preceding day can be modeled as a Markov chain, with the matrix \"P\" indicating a 100% probability of a sunny day and 0% probability of a rainy day.",
    [
      "constant, \"c\", equals 1, the probabilities of a move to the left at positions \"x\" = \u22122,\u22121,0,1,2 are given by formula_3 respectively. The random walk has a centering effect that weakens as \"c\" increases. Since the probabilities depend only on the current position (value of \"x\") and not on any prior positions, this biased random walk satisfies the definition of a Markov chain. The probabilities of weather conditions (modeled as either rainy or sunny), given the weather on the preceding day, can be represented by a transition matrix: The matrix \"P\" represents the weather model in which a sunny day",
      "the \"sunny\" entry is 100%, and the \"rainy\" entry is 0%: The weather on day 2 can be predicted by: Thus, there is a 90% chance that day 2 will also be sunny. The weather on day 3 can be predicted in the same way: or General rules for day \"n\" are: In this example, predictions for the weather on more distant days are increasingly inaccurate and tend towards a steady state vector. This vector represents the probabilities of sunny and rainy weather on all days, and is independent of the initial weather. The steady state vector is defined as:",
      "the most probable weather state on every day except for the third one was \"rain\". They tell us more than this, however, as they now provide a way to quantify the probabilities of each state at different times. Perhaps most importantly, our value at formula_82 quantifies our knowledge of the state vector at the end of the observation sequence. We can then use this to predict the probability of the various weather states tomorrow as well as the probability of observing an umbrella. The brute-force procedure for the solution of this problem is the generation of all possible formula_90 state"
    ]
  ],
  [
    "approach to dealing with heteroscedasticity is to use White's estimator for Heteroscedasticity-consistent standard errors when the regression errors are independent but have distinct variances.",
    [
      "matrix for an otherwise inconsistent estimator does not give it redemption. Consequently, the virtue of a robust covariance matrix in this setting is unclear.\u201d There are several methods to test for the presence of heteroscedasticity. Although tests for heteroscedasticity between groups can formally be considered as a special case of testing within regression models, some tests have structures specific to this case. These tests consist of a test statistic (a mathematical expression yielding a numerical value as a function of the data), a hypothesis that is going to be tested (the null hypothesis), an alternative hypothesis, and a statement about",
      "For any non-linear model (for instance Logit and Probit models), however, heteroscedasticity has more severe consequences: the maximum likelihood estimates of the parameters will be biased (in an unknown direction), as well as inconsistent (unless the likelihood function is modified to correctly take into account the precise form of heteroscedasticity). As pointed out by Greene, \u201csimply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.\u201d If the regression errors formula_12 are independent, but have distinct variances \u03c3, then formula_13 which can be estimated with formula_14. This provides White's (1980) estimator, often referred to as",
      "is used. One instance in which robust estimation should be considered is when there is a strong suspicion of heteroscedasticity. In the homoscedastic model, it is assumed that the variance of the error term is constant for all values of \"x\". Heteroscedasticity allows the variance to be dependent on \"x\", which is more accurate for many real scenarios. For example, the variance of expenditure is often larger for individuals with higher income than for individuals with lower incomes. Software packages usually default to a homoscedastic model, even though such a model may be less accurate than a heteroscedastic model. One"
    ]
  ],
  [
    "In practice, the standardized residuals from an estimated GARCH model are likely to have fat tails.",
    [
      "can be estimated using Quasi-likelihood, it follows that details of the distribution of the residuals beyond the mean-variance relationship are of relatively minor importance. One issue that is more common with GAMs than with other GLMs is a danger of falsely concluding that data are zero inflated. The difficulty arises when data contain many zeroes that can be modelled by a Poisson or binomial with a very low expected value: the flexibility of the GAM structure will often allow representation of a very low mean over some region of covariate space, but the distribution of standardized residuals will fail to",
      "the model which uses a basis size of 100 for the smooth of formula_89. The specification of distribution and link function uses the `family' objects that are standard when fitting GLMs in R or S. Note that Gaussian random effects can also be added to the linear predictor. These examples are only intended to give a very basic flavour of the way that GAM software is used, for more detail refer to the software documentation for the various packages and the references below. As with any statistical model it is important to check the model assumptions of a GAM. Residual",
      "plots should be examined in the same way as for any GLM. That is deviance residuals (or other standardized residuals) should be examined for patterns that might suggest a substantial violation of the independence or mean-variance assumptions of the model. This will usually involve plotting the standardized residuals against fitted values and covariates to look for mean-variance problems or missing pattern, and may also involve examining Correlograms (ACFs) and/or Variograms of the residuals to check for violation of independence. If the model mean-variance relationship is correct then scaled residuals should have roughly constant variance. Note that since GLMs and GAMs"
    ]
  ],
  [
    "Considerations for word order in AV verb-initial clauses include the frequency of both SVO and VOS orders in narrative texts, the preference for VOS in foregrounded clauses, the predominance of subject-initial clauses regardless of grounding, the restriction to SVO in subordinate clauses, the occurrence of verb-initial clauses for key action sequences, and the specificity effects where VOS may be unacceptable for specific or definite undergoers in AV languages.",
    [
      "Both SVO and VOS orders occur with equal frequency in narrative texts, though VOS is highly preferred in foregrounded clauses. AV clauses are predominantly subject-initial regardless of grounding. In fact, SVO is the only word-order permitted in subordinate clauses. Where verb-initial clauses in AV do occur, however, they typically represent key sequences of action in the storyline. There are also specificity effects in AV verb-initial word order. VOS is acceptable when the non-subject undergoer is non-specific, but sometimes considered unacceptable if the undergoer is specific. The same is true for definite undergoers. However, the effects are not found when the",
      "structure to obey the phonological constraint Weak Start as well. Verb-initial word order In syntax, verb-initial (V1) word order is a word order in which the verb appears before the subject and the object. In the more narrow sense, this term is used specifically to describe the word order of V1 languages (a V1 language being a language where the word order is obligatorily or predominantly verb-initial). V1 clauses only occur in V1 languages and other languages with a dominant V1 order displaying other properties that correlate with verb-initiality and that are crucial to many analyses of V1. V1 languages",
      "Verb-initial word order In syntax, verb-initial (V1) word order is a word order in which the verb appears before the subject and the object. In the more narrow sense, this term is used specifically to describe the word order of V1 languages (a V1 language being a language where the word order is obligatorily or predominantly verb-initial). V1 clauses only occur in V1 languages and other languages with a dominant V1 order displaying other properties that correlate with verb-initiality and that are crucial to many analyses of V1. V1 languages are estimated to make up 12-19% of the world\u2019s languages."
    ]
  ],
  [
    "Frequentist statistics draws a distinction between inference and decision-making in the classical hypothesis testing framework by emphasizing that hypothesis testing is used for inference to determine which hypothesis is true, rather than making a decision based on this information.",
    [
      "is provided by Kass and Raftery (1995): The use of Bayes factors or classical hypothesis testing takes place in the context of inference rather than decision-making under uncertainty. That is, we merely wish to find out which hypothesis is true, rather than actually making a decision on the basis of this information. Frequentist statistics draws a strong distinction between these two because classical hypothesis tests are not coherent in the Bayesian sense. Bayesian procedures, including Bayes factors, are coherent, so there is no need to draw such a distinction. Inference is then simply regarded as a special case of decision-making",
      "Frequentist inference Frequentist inference is a type of statistical inference that draws conclusions from sample data by emphasizing the frequency or proportion of the data. An alternative name is frequentist statistics. This is the inference framework in which the well-established methodologies of statistical hypothesis testing and confidence intervals are based. Other than frequentistic inference, the main alternative approach to statistical inference is Bayesian inference, while another is fiducial inference. While \"Bayesian inference\" is sometimes held to include the approach to inference leading to optimal decisions, a more restricted view is taken here for simplicity. Frequentist inference has been associated with",
      "is a key technique of both frequentist inference and Bayesian inference, although the two types of inference have notable differences. Statistical hypothesis tests define a procedure that controls (fixes) the probability of incorrectly \"deciding\" that a default position (null hypothesis) is incorrect. The procedure is based on how likely it would be for a set of observations to occur if the null hypothesis were true. Note that this probability of making an incorrect decision is \"not\" the probability that the null hypothesis is true, nor whether any specific alternative hypothesis is true. This contrasts with other possible techniques of decision"
    ]
  ],
  [
    "Constraints on the values of the parameters in an ARMA(p,q) model to ensure stationarity include ensuring that all autoregressive parameters lie within the unit circle, and that all moving average parameters are finite.",
    [
      "\"q\". Brockwell & Davis recommend using Akaike information criterion (AIC) for finding \"p\" and \"q\". ARMA models in general can be, after choosing \"p\" and \"q\", fitted by least squares regression to find the values of the parameters which minimize the error term. It is generally considered good practice to find the smallest values of \"p\" and \"q\" which provide an acceptable fit to the data. For a pure AR model the Yule-Walker equations may be used to provide a fit. ARMA is appropriate when a system is a function of a series of unobserved shocks (the MA or moving",
      "a constant, and the random variable formula_4 is white noise. Some constraints are necessary on the values of the parameters so that the model remains stationary. For example, processes in the AR(1) model with |\"\u03c6\"| \u2265 1 are not stationary. The notation MA(\"q\") refers to the moving average model of order \"q\": where the \u03b8, ..., \u03b8 are the parameters of the model, \u03bc is the expectation of formula_6 (often assumed to equal 0), and the formula_4, formula_8... are again, white noise error terms. The notation ARMA(\"p\", \"q\") refers to the model with \"p\" autoregressive terms and \"q\" moving-average terms.",
      "This allows all the polynomials involving the lag operator to appear in a similar form throughout. Thus the ARMA model would be written as Moreover, if we set formula_21 and formula_22, then we get an even more elegant formulation: formula_23 Finding appropriate values of \"p\" and \"q\" in the ARMA(\"p\",\"q\") model can be facilitated by plotting the partial autocorrelation functions for an estimate of \"p\", and likewise using the autocorrelation functions for an estimate of \"q\". Further information can be gleaned by considering the same functions for the residuals of a model fitted with an initial selection of \"p\" and"
    ]
  ],
  [
    "The appropriate critical value for a 2-sided 5% size test of $H_0: \\beta_3 = 1$ is 7.",
    [
      "values in the other, (ii) count the number of values in the other group falling below all those in the one, and (iii) sum these two counts (we require that neither count be zero). The critical values of the total count are, roughly, 7, 10, and 13, i.e. 7 for a two sided 5% level, 10 for a two sided 1% level, and 13 for a two sided 0.1% level. The test loses some accuracy if the samples are quite large (greater than 30) or much different in size (ratio more than 4:3). Tukey's paper describes adjustments for these conditions.",
      "is Thus the within-group mean square value is Step 5: The \"F\"-ratio is The critical value is the number that the test statistic must exceed to reject the test. In this case, \"F\"(2,15) = 3.68 at \"\u03b1\" = 0.05. Since \"F\"=9.3 > 3.68, the results are significant at the 5% significance level. One would reject the null hypothesis, concluding that there is strong evidence that the expected values in the three groups differ. The p-value for this test is 0.002. After performing the \"F\"-test, it is common to carry out some \"post-hoc\" analysis of the group means. In this case,",
      "expected to follow a binomial distribution, the standard binomial test is used to calculate significance. The normal approximation to the binomial distribution can be used for large sample sizes, \"m\" > 25. The left-tail value is computed by Pr(\"W\" \u2264 \"w\"), which is the p-value for the alternative H: \"p\" < 0.50. This alternative means that the \"X\" measurements tend to be higher. The right-tail value is computed by Pr(\"W\" \u2265 \"w\"), which is the p-value for the alternative H: \"p\" > 0.50. This alternative means that the \"Y\" measurements tend to be higher. For a two-sided alternative H the"
    ]
  ],
  [
    "The test statistic for performing a test of whether the VAR(6) can be restricted to a VAR(4) based on the determinants of the variance-covariance matrices of the residuals for each VAR is the chi-squared test statistic in the Breusch-Pagan test.",
    [
      "hypothesis for the trace test is that the number of cointegration vectors is \"r\"=\"r\"*<\"k\", vs. the alternative that \"r\"=\"k\". Testing proceeds sequentially for \"r\"*=1,2,etc. and the first non-rejection of the null is taken as an estimate of \"r\". The null hypothesis for the \"maximum eigenvalue\" test is as for the trace test but the alternative is \"r\"=\"r\"*+1 and, again, testing proceeds sequentially for \"r\"*=1,2,etc., with the first non-rejection used as an estimator for \"r\". Just like a unit root test, there can be a constant term, a trend term, both, or neither in the model. For a general VAR(\"p\") model:",
      "0 and so, given the assumption that their variance does not depend on the independent variables, an estimate of this variance can be obtained from the average of the squared values of the residuals. If the assumption is not held to be true, a simple model might be that the variance is linearly related to independent variables. Such a model can be examined by regressing the squared residuals on the independent variables, using an auxiliary regression equation of the form This is the basis of the Breusch\u2013Pagan test. It is a chi-squared test: the test statistic is distributed \"n\"\u03c7 with",
      "test is a modification of the corresponding likelihood ratio test designed to make the approximation to the formula_6 distribution better (Bartlett, 1937). The test statistics may be written in some sources with logarithms of base 10 as: Bartlett's test In statistics, Bartlett's test (see Snedecor and Cochran, 1989) is used to test if \"k\" samples are from populations with equal variances. Equal variances across populations is called homoscedasticity or homogeneity of variances. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. The Bartlett test can be used to verify that assumption."
    ]
  ],
  [
    "A disadvantage of using pure time-series models in economics compared to structural models is that structural models allow for a deeper understanding of the underlying system and relationships between variables beyond just statistical estimation.",
    [
      "to the Lucas critique of reduced-form macroeconomic policy predictions. The original distinction between structure and reduced-form was between the underlying system and the direct relationship between observables implied by the system. Different combinations of structural parameters can imply the same reduced-form parameters, so structural estimation must go beyond the direct relationship between variables. Many economists now use the term \"reduced form\" to mean statistical estimation without reference to a specific economic model. For example, a regression is often called a reduced-form equation even when no standard economic model would generate it as the reduced form relationship between variables. These conflicting",
      "are many governing factors, and most of them cannot be expressed as numerical time series data, as one would like to have for building mathematical models. The conventional approach here would be to break the system down into parts, isolate the vital parts (dropping the 'trivial' components) for their contributions to the output and solve the simplified system for creating desired models or scenarios. The disadvantage of this method is that real-world scenarios do not behave rationally: more often than not, a simplified model will break down when the contribution of the 'trivial' components becomes significant. Also, importantly, the behaviour",
      "systemic-elements that will always be omitted from any top-down analysis of the economy. Economic model In economics, a model is a theoretical construct representing economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified, often mathematical, framework designed to illustrate complex processes. Frequently, economic models posit structural parameters. A model may have various exogenous variables, and those variables may change to create various responses by economic variables. Methodological uses of models include investigation, theorizing, and fitting theories to the world. In general terms, economic models have two"
    ]
  ],
  [
    "In linear regression analysis, alternative names for the dependent variable include the \"response variable\", \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\" or \"label\".",
    [
      "usually used instead of \"covariate\". Depending on the context, a dependent variable is sometimes called a \"response variable\", \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\" or \"label\". \"Explanatory variable\" is preferred by some authors over \"independent variable\" when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher. If the independent variable is referred to as an \"explanatory variable\" then the term \"response variable\" is preferred by some authors for the dependent variable. \"Explained variable\" is preferred by some authors over \"dependent variable\" when",
      "Linear regression In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear",
      "linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties. Linear regression In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. In linear regression, the relationships are"
    ]
  ],
  [
    "The advantages of the VAR approach to modeling the relationship between variables compared to the estimation of full structural models include overcoming the parameter identification problem and providing a theory-free method to estimate economic relationships.",
    [
      "when \"y\" can impact directly \"y\" and subsequent future values, but not \"y\". Because of the parameter identification problem, ordinary least squares estimation of the structural VAR would yield inconsistent parameter estimates. This problem can be overcome by rewriting the VAR in reduced form. From an economic point of view, if the joint dynamics of a set of variables can be represented by a VAR model, then the structural form is a depiction of the underlying, \"structural\", economic relationships. Two features of the structural form make it the preferred candidate to represent the underlying relations: By premultiplying the structural VAR",
      "are less than 1 in absolute value. An estimated VAR model can be used for forecasting, and the quality of the forecasts can be judged, in ways that are completely analogous to the methods used in univariate autoregressive modelling. Christopher Sims has advocated VAR models, criticizing the claims and performance of earlier modeling in macroeconomic econometrics. He recommended VAR models, which had previously appeared in time series statistics and in system identification, a statistical specialty in control theory. Sims advocated VAR models as providing a theory-free method to estimate economic relationships, thus being an alternative to the \"incredible identification restrictions\"",
      "in structural models. VAR models are also increasingly used in health research for automatic analyses of diary data or sensor data. Vector autoregression Vector autoregression (VAR) is a stochastic process model used to capture the linear interdependencies among multiple time series. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR enter the model in the same way: each variable has an equation explaining its evolution based on its own lagged values, the lagged values of the other model variables, and an error term. VAR modeling does not"
    ]
  ],
  [
    "The regression coefficients are affected by correlations among predictor variables and the spacings of observations on each variable, as the magnitudes of standardized regression coefficients reflect the presence of correlations among predictor variables and the arbitrary spacings of observations.",
    [
      "regression coefficients, whether standardized or not. The reason is that when the predictor variables are correlated among themselves, \u2026 the regression coefficients are affected by the other predictor variables in the model \u2026 The magnitudes of the standardized regression coefficients are affected not only by the presence of correlations among the predictor variables but also by the spacings of the observations on each of these variables. Sometimes these spacings may be quite arbitrary. Hence, it is ordinarily not wise to interpret the magnitudes of standardized regression coefficients as reflecting the comparative importance of the predictor variables.\" In mathematical statistics, a",
      "formula_13, the sum of the squared correlations with the dependent variable. If the predictor variables are correlated among themselves, the inverse of the correlation matrix formula_5 accounts for this. The squared coefficient of multiple correlation can also be computed as the fraction of variance of the dependent variable that is explained by the independent variables, which in turn is 1 minus the unexplained fraction. The unexplained fraction can be computed as the sum of squared residuals\u2014that is, the sum of the squares of the prediction errors\u2014divided by the sum of the squared deviations of the values of the dependent variable",
      "Regression analysis In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables"
    ]
  ],
  [
    "The parameters formula_84 in the enantioselective synthesis model have a direct correspondence with the autocorrelation function, which can be inverted to determine the parameters using the Yule\u2013Walker equations.",
    [
      "model is given by the equation It is based on parameters formula_84 where \"i\" = 1, ..., \"p\". There is a direct correspondence between these parameters and the covariance function of the process, and this correspondence can be inverted to determine the parameters from the autocorrelation function (which is itself obtained from the covariances). This is done using the Yule\u2013Walker equations. The Yule\u2013Walker equations, named for Udny Yule and Gilbert Walker, are the following set of equations. where , yielding equations. Here formula_86 is the autocovariance function of X, formula_56 is the standard deviation of the input noise process, and",
      "\"T\" is the sample size and \"R\" is the coefficient of determination. Under the null hypothesis of no autocorrelation, this statistic is asymptotically distributed as formula_87 with \"k\" degrees of freedom. Responses to nonzero autocorrelation include generalized least squares and the Newey\u2013West HAC estimator (Heteroskedasticity and Autocorrelation Consistent). In the estimation of a moving average model (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order \"q\", we have formula_88, for formula_89, and formula_90, for formula_91. Serial dependence is",
      "interval for the sample autocorrelation function on the sample autocorrelation plot. Most software that can generate the autocorrelation plot can also generate this confidence interval. The sample partial autocorrelation function is generally not helpful for identifying the order of the moving average process. The following table summarizes how one can use the sample autocorrelation function for model identification. Hyndman & Athanasopoulos suggest the following: In practice, the sample autocorrelation and partial autocorrelation functions are random variables and do not give the same picture as the theoretical functions. This makes the model identification more difficult. In particular, mixed models can be"
    ]
  ],
  [
    "In a Bayesian setting, probit models can be computed using Gibbs sampling, while logit models require numerically calculating integrals.",
    [
      "logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics. A good way to understand the key difference between probit and logit models is to assume that the dependent variable is driven by a latent variable z, which is a sum of a linear combination of explanatory variables and a random noise term. We do not observe z but instead observe y which takes the value 0 (when z < 0) or 1 (otherwise). In the logit model we assume that",
      "is The reason for the use of the probit model is that a constant scaling of the input variable to a normal CDF (which can be absorbed through equivalent scaling of all of the parameters) yields a function that is practically identical to the logit function, but probit models are more tractable in some situations than logit models. (In a Bayesian setting in which normally distributed prior distributions are placed on the parameters, the relationship between the normal priors and the normal CDF link function means that a probit model can be computed using Gibbs sampling, while a logit model",
      "of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model. Practical reasons for choosing the probit model over the logistic model would be: Time series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result, standard regression techniques cannot"
    ]
  ],
  [
    "The BEKK formulation allows for more flexibility in modeling time-varying correlations compared to the diagonal VECH approach.",
    [
      "in nature, and applied to methods such as neural networks, decision trees and -nearest neighbors in the 1990s. The use of probabilistic models and Gaussian processes was pioneered and largely developed in the context of geostatistics, where prediction over vector-valued output data is known as cokriging. Geostatistical approaches to multivariate modeling are mostly formulated around the linear model of coregionalization (LMC), a generative approach for developing valid covariance functions that has been used for multivariate regression and in statistics for computer emulation of expensive multivariate computer codes. The regularization and kernel theory literature for vector-valued functions followed in the 2000s.",
      "formula_8 is the matrix to be solved (the linear or linearised forward model) and formula_9 is the covariance matrix of the vector formula_3. This can be similarly done for formula_2: Here formula_13 is taken to be the so-called \"a-priori\" distribution: formula_14 denotes the a-priori values for formula_15 while formula_16 is its covariance matrix. The nice thing about the Gaussian distributions is that only two parameters are needed to describe them and so the whole problem can be converted once again to matrices. Assuming that formula_17 takes the following form: formula_19 may be neglected since, for a given value of formula_2,",
      "that formula_17. When a simple logistic function is employed instead of the normal density function, then the model has the structure of the Bradley-Terry-Luce model (BTL model) (Bradley & Terry, 1952; Luce, 1959). In turn, the Rasch model for dichotomous data (Rasch, 1960/1980) is identical to the BTL model after the person parameter of the Rasch model has been eliminated, as is achieved through statistical conditioning during the process of Conditional Maximum Likelihood estimation. With this in mind, the specification of uniform discriminal dispersions is equivalent to the requirement of parallel Item Characteristic Curves (ICCs) in the Rasch model. Accordingly,"
    ]
  ],
  [
    "The consequences of omitting a relevant variable from a regression equation include potential bias or inconsistency in the estimated regression results due to the omitted variable being captured in the error term, leading to correlation issues with the included regressors (source).",
    [
      "dependent variable \"y\", independent variables \"x\" and \"z\", and error term \"u\". We wish to know the effect of \"x\" itself upon \"y\" (that is, we wish to obtain an estimate of \"b\"). Two conditions must hold true for omitted-variable bias to exist in linear regression: Suppose we omit \"z\" from the regression, and suppose the relation between \"x\" and \"z\" is given by with parameters \"d\", \"f\" and error term \"e\". Substituting the second equation into the first gives If a regression of \"y\" is conducted upon \"x\" only, this last equation is what is estimated, and the regression",
      "solves for formula_3 such that formula_4 (when we minimize the sum of squared errors, formula_5, the first-order condition is exactly formula_6.) If the true model is believed to have formula_7 due to any of the reasons listed above\u2014for example, if there is an omitted variable which affects both formula_2 and formula_9 separately\u2014then this OLS procedure will \"not\" yield the causal impact of formula_2 on formula_9. OLS will simply pick the parameter that makes the resulting errors appear uncorrelated with formula_2. Consider for simplicity the single-variable case. Suppose we are considering a regression with one variable and a constant (perhaps no",
      "is a direct effect (\"x\" \u2192 \"y\"). Just as an experimenter must be careful to employ an experimental design that controls for every confounding factor, so also must the user of multiple regression be careful to control for all confounding factors by including them among the regressors. If a confounding factor is omitted from the regression, its effect is captured in the error term by default, and if the resulting error term is correlated with one (or more) of the included regressors, then the estimated regression may be biased or inconsistent (see omitted variable bias). In addition to regression analysis,"
    ]
  ],
  [
    "analysis can refer to the independent variable as a \"covariate\", \"explanatory variable\", \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\", or \"label\".",
    [
      "usually used instead of \"covariate\". Depending on the context, a dependent variable is sometimes called a \"response variable\", \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\" or \"label\". \"Explanatory variable\" is preferred by some authors over \"independent variable\" when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher. If the independent variable is referred to as an \"explanatory variable\" then the term \"response variable\" is preferred by some authors for the dependent variable. \"Explained variable\" is preferred by some authors over \"dependent variable\" when",
      "variable and \"p\" independent variables. Thus, \"Y\" is the \"i\" observation of the dependent variable, \"X\" is \"i\" observation of the \"j\" independent variable, \"j\" = 1, 2, ..., \"p\". The values \"\u03b2\" represent parameters to be estimated, and \"\u03b5\" is the \"i\" independent identically distributed normal error. In the more general multivariate linear regression, there is one equation of the above form for each of \"m\" > 1 dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other: for all observations indexed as \"i\" = 1, ... , \"n\" and for",
      "Linear regression In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear"
    ]
  ],
  [
    "The standard error of the slope coefficient in OLS estimation is inversely related to the dispersion of observations on the explanatory variable and the square root of the sample size, but directly related to the residual variance.",
    [
      "of the slope coefficient will itself be normally distributed with mean and variance formula_35 where is the variance of the error terms (see Proofs involving ordinary least squares). At the same time the sum of squared residuals is distributed proportionally to with degrees of freedom, and independently from formula_17. This allows us to construct a -value where is the \"standard error\" of the estimator formula_17. This -value has a Student's-distribution with degrees of freedom. Using it we can construct a confidence interval for : at confidence level , where formula_41 is the formula_42 quantile of the distribution. For example, if",
      "by the sample size. This is because as the sample size increases, sample means cluster more closely around the population mean. Therefore, the relationship between the standard error and the standard deviation is such that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size. In other words, the standard error of the mean is a measure of the dispersion of sample means around the population mean. In regression analysis, the term \"standard error\" refers either to the square root of the reduced chi-squared statistic or the standard error",
      "of mass of the data points affects the slope. Description of the statistical properties of estimators from the simple linear regression estimates requires the use of a statistical model. The following is based on assuming the validity of a model under which the estimates are optimal. It is also possible to evaluate the properties under other assumptions, such as inhomogeneity, but this is discussed elsewhere. The estimators formula_16 and formula_17 are unbiased. To formalize this assertion we must define a framework in which these estimators are random variables. We consider the residuals as random variables drawn independently from some distribution"
    ]
  ],
  [
    "Heterophily is the tendency of individuals to collect in diverse groups and is related to the diffusion of innovations theory as discussed by Everett Rogers in his book \"Diffusion of Innovations\".",
    [
      "terms and homophily is the tendency to be attracted to what is similar. Homogamy and endogamy may be a result of cultural practices or personal preference. Endogamy's antithesis, exogamy, is marriage only outside of a particular group. The concept of heterophily has been mentioned pertaining to working environments and the relationships within them. Heterophily is especially prevalent when discussing the diffusion of innovations theory. \"Diffusion of Innovations\" was the book written by Everett Rogers where he first termed heterophily.The diffusion of innovation theory itself is used to explain how new or innovative ideas are spread throughout a system composed of",
      "individuals. Rogers saw heterophily between individuals as \"one of the most distinctive problems in the communication\". This is because he believed homophily to be a more beneficial agent in communication. The general reasoning for this was that people who have more in common with each other are able to communicate more comfortably with each other. Still, Rogers believed that heterophily has such an impact on the diffusion of innovation theory that he stated in his book that \"the very nature of diffusion demands that at least some degree of heterophily be present between the two participants\". Heterophily is also an",
      "Heterophily Heterophily, or \"love of the different\", is the tendency of individuals to collect in diverse groups; it is the opposite of homophily. This phenomenon can be seen in relationships between individuals. As a result, it can be analyzed in the workplace to create a more efficient and innovative workplace. It has also become an area of social network analysis. Most of the early work in heterophily was done in the 1960s by Everett Rogers in his book \"Diffusion Of Innovations\". According to Rogers, \"Heterophily, the mirror opposite of homophily, is defined as the degree to which pairs of individuals"
    ]
  ],
  [
    "for serial correlation, also known as the Breusch-Godfrey test, is a traditional test for the presence of first-order autocorrelation in econometrics.",
    [
      "and +1. Some sources may use the following formula for the autocovariance function: Although this definition has less bias, the (1/\"N\") formulation has some desirable statistical properties and is the form most commonly used in the statistics literature. See pages 20 and 49\u201350 in Chatfield for details. In the same graph one can draw upper and lower bounds for autocorrelation with significance level formula_11: If the autocorrelation is higher (lower) than this upper (lower) bound, the null hypothesis that there is no autocorrelation at and beyond a given lag is rejected at a significance level of formula_11. This test is",
      "a sequence as first, second, etc., then a rank correlation coefficient can be computed. If just the dependent variable is ordinal, ordered probit or ordered logit can be used. If the dependent variable is continuous\u2014either interval level or ratio level, such as a temperature scale or an income scale\u2014then simple regression can be used. If both variables are time series, a particular type of causality known as Granger causality can be tested for, and vector autoregression can be performed to examine the intertemporal linkages between the variables. When neither variable can be regarded as dependent on the other, regression is",
      "and a test statistic is derived from these. The null hypothesis is that there is no serial correlation of any order up to \"p\". The test is more general than the Durbin\u2013Watson statistic (or Durbin's \"h\" statistic), which is only valid for nonstochastic regressors and for testing the possibility of a first-order autoregressive model (e.g. AR(1)) for the regression errors. The BG test has none of these restrictions, and is statistically more powerful than Durbin's \"h\" statistic. Consider a linear regression of any form, for example where the errors might follow an AR(\"p\") autoregressive scheme, as follows: The simple regression"
    ]
  ],
  [
    "The distribution of the test statistic under the null hypothesis in a Bera-Jarque test is a chi-square distribution with two degrees of freedom.",
    [
      "Null distribution In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. For example, in an F-test, the null distribution is an F-distribution. Null distribution is a tool scientists often use when conducting experiments. The null distribution is the distribution of two sets of data under a null hypothesis. If the results of the two sets of data are not outside the parameters of the expected results, then the null hypothesis is said to be true. The null hypothesis is often a part of an experiment. The null hypothesis",
      "The null hypothesis is that two variances are the same \u2013 so the proposed grouping is not meaningful. In the table below, the symbols used are defined at the bottom of the table. Many other tests can be found in . Proofs exist that the test statistics are appropriate. Test statistic A test statistic is a statistic (a quantity derived from the sample) used in statistical hypothesis testing. A hypothesis test is typically specified in terms of a test statistic, considered as a numerical summary of a data-set that reduces the data to one value that can be used to",
      "a single number, such as the average or the correlation coefficient, that summarizes the characteristics of the data, in a way relevant to a particular inquiry. As such, the test statistic follows a distribution determined by the function used to define that test statistic and the distribution of the input observational data. For the important case in which the data are hypothesized to follow the normal distribution, depending on the nature of the test statistic and thus the underlying hypothesis of the test statistic, different null hypothesis tests have been developed. Some such tests are z-test for normal distribution, t-test"
    ]
  ],
  [
    "A consistent estimator in statistics is defined as a rule for computing estimates of a parameter \"\u03b8\" that, as the number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to \"\u03b8\".",
    [
      "Consistent estimator In statistics, a consistent estimator or asymptotically consistent estimator is an estimator\u2014a rule for computing estimates of a parameter \"\u03b8\"\u2014having the property that as the number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to \"\u03b8\". This means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to \"\u03b8\" converges to one. In practice one constructs an estimator as a function of an available sample of size \"n\", and then imagines",
      "Consistency (statistics) In statistics, consistency of procedures, such as computing confidence intervals or conducting hypothesis tests, is a desired property of their behaviour as the number of items in the data set to which they are applied increases indefinitely. In particular, consistency requires that the outcome of the procedure with unlimited data should identify the underlying truth. Use of the term in statistics derives from Sir Ronald Fisher in 1922. Use of the terms \"consistency\" and \"consistent\" in statistics is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications",
      "being able to keep collecting data and expanding the sample \"ad infinitum\". In this way one would obtain a sequence of estimates indexed by \"n\", and consistency is a property of what occurs as the sample size \u201cgrows to infinity\u201d. If the sequence of estimates can be mathematically shown to converge in probability to the true value \"\u03b8\", it is called a consistent estimator; otherwise the estimator is said to be inconsistent. Consistency as defined here is sometimes referred to as \"weak consistency\". When we replace convergence in probability with almost sure convergence, then the estimator is said to be"
    ]
  ],
  [
    "displays typical nonlinear characteristics such as the occasional presence of aberrant observations and the plausible existence of regimes with different dynamic behavior in returns and volatility.",
    [
      "mean and the conditional variance of such series \u2013 or, in other words, the return and risk of financial assets. As documented in detail below, financial time series display typical nonlinear characteristics. Important examples of those features are the occasional presence of (sequences of) aberrant observations and the plausible existence of regimes within which returns and volatility display different dynamic behaviour. We therefore choose to consider only nonlinear models in substantial detail, in contrast to Mills (1999), where linear models are also considered. Financial theory does not provide many motivations for nonlinear models, but we believe that the data themselves",
      "and model structures. Part IX extends the concepts in chapter three to multivariate time series. Part X examines common aspects across time series.\" Franses' most familiar work is \"Nonlinear Time Series Models in Empirical Finance,\" published in 1998, and co-authored by his former PhD student Dick van Dijk. In its introduction, the books aim and content is summarized: \"This book deals with the empirical analysis of financial time series with an explicit focus on, first, describing the data in order to obtain insights into their dynamic patterns and, second, out-of-sample forecasting. We restrict attention to modelling and forecasting the conditional",
      "dis-aggregated form over a time-series compared to lower frequency methods of data collection, it contains various unique characteristics that alter the way the data are understood and analyzed. Robert Fry Engle III categorizes these distinct characteristics as irregular temporal spacing, discreteness, diurnal patterns, and temporal dependence. High frequency data employs the collection of a large sum of data over a time series, and as such the frequency of single data collection tends to be spaced out in irregular patterns over time. This is especially clear in financial market analysis, where transactions may occur in sequence, or after a prolonged period"
    ]
  ],
  [
    "The assumptions required to show the consistency, unbiasedness, and efficiency of the OLS estimator include exogeneity of regressors, homoscedasticity, and serially uncorrelated errors.",
    [
      "point in the set and the corresponding point on the regression surface \u2013 the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a simple linear regression, in which there is a single regressor on the right side of the regression equation. The OLS estimator is consistent when the regressors are exogenous, and optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated . Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors",
      "only in the class of linear unbiased estimators, which is quite restrictive. Depending on the distribution of the error terms \"\u03b5\", other, non-linear estimators may provide better results than OLS. The properties listed so far are all valid regardless of the underlying distribution of the error terms. However, if you are willing to assume that the \"normality assumption\" holds (that is, that ), then additional properties of the OLS estimators can be stated. The estimator formula_49 is normally distributed, with mean and variance as given before: where \"Q\" is the cofactor matrix. This estimator reaches the Cram\u00e9r\u2013Rao bound for the",
      "matrix of stacked formula_5 values observed in the data. If the sample errors have equal variance \u03c3 and are uncorrelated, then the least-squares estimate of \u03b2 is BLUE (best linear unbiased estimator), and its variance is easily estimated with where formula_7 are regression residuals. When the assumptions of formula_8 are violated, the OLS estimator loses its desirable properties. Indeed, where formula_10. While the OLS point estimator remains unbiased, it is not \"best\" in the sense of having minimum mean square error, and the OLS variance estimator formula_11 does not provide a consistent estimate of the variance of the OLS estimates."
    ]
  ],
  [
    "If the fixed effects estimator is believed to be correlated with one of the independent variables in panel data analysis, instrumental variables or GMM techniques such as the Arellano-Bond estimator must be used as alternative estimation techniques.",
    [
      "may occur. The fixed effect estimator and the first differences estimator both rely on the assumption of strict exogeneity. Hence, if formula_7 is believed to be correlated with one of the independent variables, an alternative estimation technique must be used. Instrumental variables or GMM techniques are commonly used in this situation, such as the Arellano\u2013Bond estimator. Panel data In statistics and econometrics, panel data or longitudinal data are multi-dimensional data involving measurements over time. Panel data contain observations of multiple phenomena obtained over multiple time periods for the same firms or individuals. Time series and cross-sectional data can be thought",
      "unbiased and consistent estimates of parameters even when time constant attributes are present, but random effects will be more efficient. Fixed effects is a feasible generalised least squares technique which is asymptotically more efficient than Pooled OLS when time constant attributes are present. Random effects adjusts for the serial correlation which is induced by unobserved time constant attributes. Panel analysis Panel (data) analysis is a statistical method, widely used in social science, epidemiology, and econometrics to analyze two-dimensional (typically cross sectional and longitudinal) panel data. The data are usually collected over time and over the same individuals and then a",
      "factors. The group means could be modeled as fixed or random effects for each grouping. In a fixed effects model each group mean is a group-specific fixed quantity. In panel data where longitudinal observations exist for the same subject, fixed effects represent the subject-specific means. In panel data analysis the term fixed effects estimator (also known as the within estimator) is used to refer to an estimator for the coefficients in the regression model including those fixed effects (one time-invariant intercept for each subject). Such models assist in controlling for unobserved heterogeneity when this heterogeneity is constant over time. This"
    ]
  ],
  [
    "Given the strong arguments and confidence shown by people on both sides of the economy debate, the two possible answers for the question of the new treatment's effectiveness are equally likely.",
    [
      "be the best advertising strategy? Would they want to refute the arguments or reaffirm their claims? (Szybill and Heslin, 1973) Szybillo and Heslin (1973) sought to test five hypotheses: 1) to reaffirm or refute the arguments is better than not addressing, 2) refuting the same counterarguments will be more effective than supportive defense, 3) refutational defense is better than supportive defense, 4) over a long period, refutational defense is more effective than supportive defenses, and 5) high credibility sources are more effective than low credibility. The controversial topic in the study was: \"Inflatable air bags should be installed as passive",
      "been suggested that the use of the technique is discrediting political polls themselves. The effect is reduced, or even eliminated, if ample credible information is provided to people. Amos Tversky and Daniel Kahneman explored how different phrasing affected participants' responses to a choice in a hypothetical life and death situation in 1981. Participants were asked to choose between two treatments for 600 people affected by a deadly disease. Treatment A was predicted to result in 400 deaths, whereas treatment B had a 33% chance that no one would die but a 66% chance that everyone would die. This choice was",
      "particular considerations more applicable and therefore more relevant to the judgment process. Chong and Druckman suggest framing research has mainly focused on two types of frames: equivalency and emphasis frames. Equivalency frames offer \"different, but logically equivalent phrases,\" which cause individuals to alter their preferences. Equivalency frames are often worded in terms of \"gains\" versus \"losses.\" For example, Kahneman and Tversky asked participants to choose between two \"gain-framed\" policy responses to a hypothetical disease outbreak expected to kill 600 people. Response A would save 200 people while Response B had a one-third probability of saving everyone, but a two-thirds probability"
    ]
  ],
  [
    "The price of a house is best described in terms of numbers by considering the structural characteristics of the house, characteristics of the locality/neighbourhood, and environmental characteristics.",
    [
      "application of the Hedonic Pricing Method, the first assumption made is the value of a house is affected by a particular combination of characteristics that it possesses given that properties with better qualities demand higher prices as compared to properties with lower qualities. This is the Hedonic Pricing Function. The price of a house will thus be affected by the structural characteristics formula_1 of the house itself, characteristics of the locality/neighbourhood formula_2, and environmental characteristics formula_3. Structural Characteristics could be anything from size of the house, to the number of rooms, type of flooring, etc. Neighbourhood attributes include variables like",
      "approach to value is the Cost Approach to value. The Cost Approach to value is most useful in determining insurable value, and cost to construct a new structure or building. For example, single apartment buildings of a given quality tend to sell at a particular price per apartment. In many of those cases, the sales comparison approach may be more applicable. On the other hand, a multiple-building apartment complex would usually be valued by the income approach, as that would follow how most buyers would value it. As another example, single-family houses are most commonly valued with the greatest weighting",
      "when calculating the cost of housing. The low rent costs for a room in a single family home, or an illegal garage conversion, or a college dormitory are generally excluded from the calculation, no matter how many people in an area live in such situations. Because of this study methodology, median housing costs tend to be slightly inflated. Costs are generally considered on a cash (not accrual) basis. Thus a person making the last payment on a large home mortgage might live in officially unaffordable housing one month, and very affordable housing the following month, when the mortgage is paid"
    ]
  ],
  [
    "The eigenvalues used in the Johansen \"trace\" test for a null hypothesis of 2 cointegrating vectors in a system containing 4 variables are computed for the correlation matrix, and the number of eigenvalues greater than 1 determines the number of factors to include in the model.",
    [
      "we knew formula_4, we could just test it for stationarity with something like a Dickey\u2013Fuller test, Phillips\u2013Perron test and be done. But because we don't know formula_4, we must estimate this first, generally by using ordinary least squares, and then run our stationarity test on the estimated formula_4 series, often denoted formula_8. A second regression is then run on the first differenced variables from the first regression, and the lagged residuals formula_9 is included as a regressor. The Johansen test is a test for cointegration that allows for more than one cointegrating relationship, unlike the Engle\u2013Granger method, but this test",
      "partial, all other procedures rely on the analysis of eigenvalues. The \"eigenvalue\" of a factor represents the amount of variance of the variables accounted for by that factor. The lower the eigenvalue, the less that factor contributes to the explanation of variances in the variables. A short description of each of the nine procedures mentioned above is provided below. Compute the eigenvalues for the correlation matrix and determine how many of these eigenvalues are greater than 1. This number is the number of factors to include in the model. A disadvantage of this procedure is that it is quite arbitrary",
      "of eigenvalues formula_20 or singular values formula_25 vs. formula_32. The point formula_33 at which this break occurs should not be confused with a \"dimension\" formula_34 of the underlying deterministic dynamics (Vautard and Ghil, 1989). A Monte-Carlo test (Allen and Smith, 1996; Allen and Robertson, 1996; Groth and Ghil, 2015) can be applied to ascertain the statistical significance of the oscillatory pairs detected by SSA. The entire time series or parts of it that correspond to trends, oscillatory modes or noise can be reconstructed by using linear combinations of the PCs and EOFs, which provide the reconstructed components (RCs) formula_35: here"
    ]
  ],
  [
    "The variance decomposition analysis of VARs indicates the amount of information each variable contributes to the other variables in the autoregression.",
    [
      "Variance decomposition of forecast errors In econometrics and other applications of multivariate time series analysis, a variance decomposition or forecast error variance decomposition (FEVD) is used to aid in the interpretation of a vector autoregression (VAR) model once it has been fitted. The variance decomposition indicates the amount of information each variable contributes to the other variables in the autoregression. It determines how much of the forecast error variance of each of the variables can be explained by exogenous shocks to the other variables. For the VAR (p) of form This can be changed to a VAR(1) structure by writing",
      "Use of VaR in this context, as well as a worthwhile critique on board governance practices as it relates to investment management oversight in general can be found in \"Best Practices in Governance.\" The VaR of formula_1 at the confidence level formula_2 is the smallest number formula_3 such that the probability that formula_4 does not exceed formula_3 is at least formula_6. Mathematically, formula_7 is the formula_8-quantile of formula_9, i.e., This is the most general definition of VaR and the two identities are equivalent (indeed, for any random variable formula_1 its cumulative distribution function formula_12 is well defined). However this formula",
      "different assets). Within any portfolio it is also possible to isolate specific position that might better hedge the portfolio to reduce, and minimise, the VaR. An example of market-maker employed strategies for trading linear interest rate derivatives and interest rate swaps portfolios is cited. VaR can be estimated either parametrically (for example, variance-covariance VaR or delta-gamma VaR) or nonparametrically (for examples, historical simulation VaR or resampled VaR). Nonparametric methods of VaR estimation are discussed in Markovich and Novak. A comparison of a number of strategies for VaR prediction is given in Kuester et al. A McKinsey report published in May"
    ]
  ],
  [
    "The trader can most validly calculate a 10-day forecast of volatility for use in a value at risk model when engaging in volatility arbitrage by computing historical daily returns for the underlying over a given past sample, adjusting the forecast based on factors such as current volatility levels and upcoming events that may impact volatility (Volatility arbitrage; option valuation techniques).",
    [
      "Management used a volatility arbitrage approach. To engage in volatility arbitrage, a trader must first forecast the underlying's future realized volatility. This is typically done by computing the historical daily returns for the underlying for a given past sample such as 252 days (the typical number of trading days in a year for the US stock market). The trader may also use other factors, such as whether the period was unusually volatile, or if there are going to be unusual events in the near future, to adjust his forecast. For instance, if the current 252-day volatility for the returns on",
      "higher than the forecast realized volatility formula_3, for the underlying. In the first case, the trader buys the option and hedges with the underlying to make a delta neutral portfolio. In the second case, the trader sells the option and then hedges the position. Over the holding period, the trader will realize a profit on the trade if the underlying's realized volatility is closer to his forecast than it is to the market's forecast (i.e. the implied volatility). The profit is extracted from the trade through the continuous re-hedging required to keep the portfolio delta-neutral. Volatility arbitrage In finance, volatility",
      "a stock is computed to be 15%, but it is known that an important patent dispute will likely be settled in the next year and will affect the stock, the trader may decide that the appropriate forecast volatility for the stock is 18%. As described in option valuation techniques, there are a number of factors that are used to determine the theoretical value of an option. However, in practice, the only two inputs to the model that change during the day are the price of the underlying and the volatility. Therefore, the theoretical price of an option can be expressed"
    ]
  ],
  [
    "If the value of $R^2$ for an estimated regression model is exactly zero, it can be inferred that there is no linear relationship between the explanatory variables and the response variable, indicating that the variability of y about its mean value is not explained by the regressors in the model.",
    [
      "error term. The quantities formula_25 are unknown coefficients, whose values are estimated by least squares. The coefficient of determination \"R\" is a measure of the global fit of the model. Specifically, \"R\" is an element of [0, 1] and represents the proportion of variability in \"Y\" that may be attributed to some linear combination of the regressors (explanatory variables) in \"X\". \"R\" is often interpreted as the proportion of response variation \"explained\" by the regressors in the model. Thus, \"R\" = 1 indicates that the fitted model explains all variability in formula_14, while \"R\" = 0 indicates no 'linear' relationship",
      "of a model. In regression, the \"R\" coefficient of determination is a statistical measure of how well the regression predictions approximate the real data points. An \"R\" of 1 indicates that the regression predictions perfectly fit the data. Values of \"R\" outside the range 0 to 1 can occur when the model fits the data worse than a horizontal hyperplane. This would occur when the wrong model was chosen, or nonsensical constraints were applied by mistake. If equation 1 of Kv\u00e5lseth is used (this is the equation used most often), \"R\" can be less than zero. If equation 2 of",
      "slope (as well as imprecision). The greater the variance in the \"x\" measurement, the closer the estimated slope must approach zero instead of the true value. It may seem counter-intuitive that noise in the predictor variable \"x\" induces a bias, but noise in the outcome variable \"y\" does not. Recall that linear regression is not symmetric: the line of best fit for predicting \"y\" from \"x\" (the usual linear regression) is not the same as the line of best fit for predicting \"x\" from \"y\". The case that the \"x\" variable arises randomly is known as the structural model or"
    ]
  ],
  [
    "A white noise process has a mean of zero, a constant variance, and autocovariances that only depend on the time lag and have a non-zero value only for a lag of zero.",
    [
      "has a standard abbreviation: AWGN. White noise is the generalized mean-square derivative of the Wiener process or Brownian motion. A generalization to random elements on infinite dimensional spaces, such as random fields, is the white noise measure. White noise is commonly used in the production of electronic music, usually either directly or as an input for a filter to create other types of noise signal. It is used extensively in audio synthesis, typically to recreate percussive instruments such as cymbals or snare drums which have high noise content in their frequency domain. A simple example of white noise is a",
      "noise\" in the theory of continuous-time signals, one must replace the concept of a \"random vector\" by a continuous-time random signal; that is, a random process that generates a function formula_15 of a real-valued parameter formula_16. Such a process is said to be white noise in the strongest sense if the value formula_17 for any time formula_16 is a random variable that is statistically independent of its entire history before formula_16. A weaker definition requires independence only between the values formula_20 and formula_21 at every pair of distinct times formula_22 and formula_23. An even weaker definition requires only that such",
      "will also have a non-zero expected value formula_5; and the power spectrum \"P\" will be flat only over the non-zero frequencies. A discrete-time stochastic process formula_6 is a generalization of random vectors with a finite number of components to infinitely many components. A discrete-time stochastic process formula_6 is called white noise if its mean does not depend on the time formula_8 and is equal to zero, i.e. formula_9 and if the autocorrelation function formula_10 only depends on formula_8 but not on formula_12 and has a nonzero value only for formula_13, i.e. formula_14. In order to define the notion of \"white"
    ]
  ],
  [
    "The tests in the Box-Jenkins approach show if the identified model is appropriate based on the residuals, as the residuals should satisfy the assumptions of a stationary univariate process for the model to be considered good for the data.",
    [
      "and Davis, 1991) for the mathematical details. Model diagnostics for Box\u2013Jenkins models is similar to model validation for non-linear least squares fitting. That is, the error term \"A\" is assumed to follow the assumptions for a stationary univariate process. The residuals should be white noise (or independent when their distributions are normal) drawings from a fixed distribution with a constant mean and variance. If the Box\u2013Jenkins model is a good model for the data, the residuals should satisfy these assumptions. If these assumptions are not satisfied, one needs to fit a more appropriate model. That is, go back to the",
      "model identification step and try to develop a better model. Hopefully the analysis of the residuals can provide some clues as to a more appropriate model. One way to assess if the residuals from the Box\u2013Jenkins model follow the assumptions is to generate statistical graphics (including an autocorrelation plot) of the residuals. One could also look at the value of the Box\u2013Ljung statistic. Box\u2013Jenkins method In time series analysis, the Box\u2013Jenkins method, named after the statisticians George Box and Gwilym Jenkins, applies autoregressive moving average (ARMA) or autoregressive integrated moving average (ARIMA) models to find the best fit of a",
      "particularly difficult to identify. Although experience is helpful, developing good models using these sample plots can involve much trial and error. Estimating the parameters for Box\u2013Jenkins models involves numerically approximating the solutions of nonlinear equations. For this reason, it is common to use statistical software designed to handle to the approach \u2013 virtually all modern statistical packages feature this capability. The main approaches to fitting Box\u2013Jenkins models are nonlinear least squares and maximum likelihood estimation. Maximum likelihood estimation is generally the preferred technique. The likelihood equations for the full Box\u2013Jenkins model are complicated and are not included here. See (Brockwell"
    ]
  ],
  [
    "In regression analysis, normality in residuals can be achieved by adjusting the model to better account for the data tendencies.",
    [
      "see the above Normality tests section. In regression analysis, lack of normality in residuals simply indicates that the model postulated is inadequate in accounting for the tendency in the data and needs to be augmented; in other words, normality in residuals can always be achieved given a properly constructed model. In computer simulations, especially in applications of the Monte-Carlo method, it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a can be generated as , where \"Z\" is standard normal. All these algorithms rely on the availability",
      "other independent variables are held fixed). If linearity fails to hold, even approximately, it is sometimes possible to transform either the independent or dependent variables in the regression model to improve the linearity. Another assumption of linear regression is that the variance be the same for each possible expected value (this is known as homoscedasticity). Univariate normality is not needed for least squares estimates of the regression parameters to be meaningful (see Gauss\u2013Markov theorem). However confidence intervals and hypothesis tests will have better statistical properties if the variables exhibit multivariate normality. This can be assessed empirically by plotting the fitted",
      "normal distribution. (If there are more solutions to the above equation, then we take the one with the smallest solution for s; if there is no solution, then we put formula_12 .) Definition: Let formula_13 be a sample of regression data with p-dimensional formula_14. For each vector formula_15, we obtain residuals formula_16 by solving the equation of scale above, where formula_1 satisfy R1 and R2. The S-estimator formula_18 is defined by formula_19 and the final scale estimator formula_20 is then formula_21 . S-estimator The goal of S-estimators is to have a simple high-breakdown regression estimator, which share the flexibility and"
    ]
  ],
  [
    "is to replace the unobserved effect \"c\" with the linear projection of it onto the explanatory variables in all time periods, providing a way to estimate the linear unobserved effects under Fixed Effect assumptions.",
    [
      "Chamberlain's approach to unobserved effects models In a linear panel data setting, it can be desirable to estimate the magnitude of the Fixed Effects, as they provide measures of the unobserved components. For instance, in wage equation regressions, Fixed Effects capture ability measures that are constant over time, such as motivation. Chamberlain's approach to unobserved effects models is a way of estimating the linear unobserved effects, under Fixed Effect (rather than Random Effects) assumptions, in the following unobserved effects model \"y = xb + c + u \" (1) where \"c\" is the unobserved effect and \"x\" contains only time-varying",
      "average of all \"x\" across all T time periods, more specifically \"c = d + x\u03bb + e\" (4) It can be shown that the Chamberlain method is a generalization of Mundlak's model. The Chamberlain method has been popular in empirical work, ranging from studies trying to estimate the causal returns to union members to studies investigating growth convergence. Chamberlain's approach to unobserved effects models In a linear panel data setting, it can be desirable to estimate the magnitude of the Fixed Effects, as they provide measures of the unobserved components. For instance, in wage equation regressions, Fixed Effects capture",
      "explanatory variables. Rather than differencing out the unobserved effect \"c\", Chamberlain proposed to replace it with the linear projection of it onto the explanatory variables in all time periods. Specifically, this leads to the following equation \"c = d + x \u03bb + x \u03bb + ... + x \u03bb + e \" (2) where the conditional distribution of \"c\" given \"x\" is unspecified, as is standard in Fixed Effects models. Combining equations (1) and (2) then gives rise to the following model. \"y = d + xb + x \u03bb + ... + x (b+\u03bb ) + ... +"
    ]
  ],
  [
    "The OLS estimator is identical to the maximum likelihood estimator under the normality assumption for the error terms.",
    [
      "regression line to be a weighted average of the lines passing through the combination of any two points in the dataset. Although this way of calculation is more computationally expensive, it provides a better intuition on OLS. The OLS estimator is identical to the maximum likelihood estimator (MLE) under the normality assumption for the error terms. This normality assumption has historical importance, as it provided the basis for the early work in linear regression analysis by Yule and Pearson. From the properties of MLE, we can infer that the OLS estimator is asymptotically efficient (in the sense of attaining the",
      "only in the class of linear unbiased estimators, which is quite restrictive. Depending on the distribution of the error terms \"\u03b5\", other, non-linear estimators may provide better results than OLS. The properties listed so far are all valid regardless of the underlying distribution of the error terms. However, if you are willing to assume that the \"normality assumption\" holds (that is, that ), then additional properties of the OLS estimators can be stated. The estimator formula_49 is normally distributed, with mean and variance as given before: where \"Q\" is the cofactor matrix. This estimator reaches the Cram\u00e9r\u2013Rao bound for the",
      "have finite variances. Under the additional assumption that the errors are normally distributed, OLS is the maximum likelihood estimator. OLS is used in fields as diverse as economics (econometrics), data science, political science, psychology and engineering (control theory and signal processing). Suppose the data consists of \"n\" observations { \"y, x\" }. Each observation \"i\" includes a scalar response \"y\" and a column vector \"x\" of values of \"p\" predictors (regressors) \"x\" for \"j\" = 1, ..., \"p\". In a linear regression model, the response variable, formula_1, is a linear function of the regressors: or in vector form, where \"\u03b2\""
    ]
  ],
  [
    "If the standard tools for time-series analysis find no evidence of structure in the economy data, it does not necessarily imply that the data are non-parametric as non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming any particular structure.",
    [
      "Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form. Assessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a \"typical\" set of data. The question of whether the model describes well the properties of the system between data points",
      "can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure. Methods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate. A time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data",
      "treated as random variables. An econometric model then is a set of joint probability distributions to which the true joint probability distribution of the variables under study is supposed to belong. In the case in which the elements of this set can be indexed by a finite number of real-valued \"parameters\", the model is called a parametric model; otherwise it is a nonparametric or semiparametric model. A large part of econometrics is the study of methods for selecting models, estimating them, and carrying out inference on them. The most common econometric models are structural, in that they convey causal and"
    ]
  ],
  [
    "No, $x_t$ and $y_t$ are not required to both be stationary in order for them to be cointegrated. (source: Error correction model)",
    [
      "demonstrated in a simple macroeconomic setting. Suppose, consumption formula_10 and disposable income formula_11 are macroeconomic time series that are related in the long run (see Permanent income hypothesis). Specifically, let average propensity to consume be 90%, that is, in the long run formula_12. From the econometrician's point of view, this long run relationship (aka cointegration) exists if errors from the regression formula_13 are a stationary series, although formula_11 and formula_10 are non-stationary. Suppose also that if formula_11 suddenly changes by formula_17, then formula_10 changes by formula_19, that is, marginal propensity to consume equals 50%. Our last assumption is that the",
      "stationary if: Two stochastic processes formula_1 and formula_46 are called jointly wide-sense stationary if they are both wide-sense stationary and their cross-correlation function formula_52 depends only on the time difference formula_32. This may be summarized as follows: The terminology used for types of stationarity other than strict stationarity can be rather mixed. Some examples follow. One way to make some time series stationary is to compute the differences between consecutive observations. This is known as differencing. Transformations such as logarithms can help to stabilize the variance of a time series. Differencing can help stabilize the mean of a time series",
      "the analytes for the stationary phase. A distribution constant, \"K\" can be defined as where \"a\" and \"a\" are the equilibrium activities in the stationary and mobile phases respectively. It can be shown that the rate of migration, , is related to the distribution constant by \"f\" is a factor which depends on the volumes of the two phases. Thus, the higher the affinity of the solute for the stationary phase, the slower the migration rate. There is a wide variety of chromatographic techniques, depending on the nature of the stationary and mobile phases. When the stationary phase is solid,"
    ]
  ],
  [
    "The term used to describe a dependent variable whose values are not observable outside a certain range but where the corresponding values of the independent variables are still available is known as a \"censored dependent variable.\"",
    [
      "the quantities treated as \"dependent variables\" may not be statistically dependent. If the dependent variable is referred to as an \"explained variable\" then the term \"predictor variable\" is preferred by some authors for the independent variable. Variables may also be referred to by their form: continuous, binary/dichotomous, nominal categorical, and ordinal categorical, among others. An example is provided by the analysis of trend in sea level by . Here the dependent variable (and variable of most interest) was the annual mean sea level at a given location for which a series of yearly values were available. The primary independent variable",
      "term formula_7 is known as the \"error\" and contains the variability of the dependent variable not explained by the independent variable. With multiple independent variables, the model is formula_8, where \"n\" is the number of independent variables. In simulation, the dependent variable is changed in response to changes in the independent variables. Depending on the context, an independent variable is sometimes called a \"predictor variable\", regressor, covariate, \"controlled variable\", \"manipulated variable\", \"explanatory variable\", exposure variable (see reliability theory), \"risk factor\" (see medical statistics), \"feature\" (in machine learning and pattern recognition) or \"input variable.\" In econometrics, the term \"control variable\" is",
      "Dependent and independent variables In mathematical modeling, statistical modeling and experimental sciences, the values of dependent variables depend on the values of independent variables. The dependent variables represent the output or outcome whose variation is being studied. The independent variables, also known in a statistical context as regressors, represent inputs or causes, that is, potential reasons for variation. In an experiment, any variable that the experimenter manipulates can be called an independent variable. Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables"
    ]
  ],
  [
    "High-stakes tests in the medical field are typically used to determine if a nurse is proficient in tasks such as inserting an I.V. line through authentic assessments or performance tests.",
    [
      "in attempts to increase teacher accountability. In common usage, a high-stakes test is any test that has major consequences or is the basis of a major decision. Under a more precise definition, a high-stakes test is any test that: High-stakes testing is not synonymous with high-pressure testing. An American high school student might feel pressure to perform well on the SAT-I college aptitude exam. However, SAT scores do not directly determine admission to any college or university, and there is no clear line drawn between those who pass and those who fail, so it is not formally considered a high-stakes",
      "a medical nurse determines whether the nurse can insert an I.V. line by watching the nurse actually do this task. These assessments are called \"authentic assessments\" or \"performance tests\". Some high-stakes tests may be standardized tests (in which all examinees take the same test under reasonably equal conditions), with the expectation that standardization affords all examinees a fair and equal opportunity to pass. Some high-stakes tests are non-standardized, such as a theater audition. As with other tests, high-stakes tests may be criterion-referenced or norm-referenced. For example, a written driver's license examination typically is criterion-referenced, with an unlimited number of potential",
      "determine the ideal cut score or to keep the test results consistent between groups taking the test at different times. High-stakes tests, despite their extensive usage for determination of academic and non-academic proficiency, are subject to criticism for various reasons. Example concerns include the following: High-stakes testing A high-stakes test is a test with important consequences for the test taker. Passing has important benefits, such as a high school diploma, a scholarship, or a license to practice a profession. Failing has important disadvantages, such as being forced to take remedial classes until the test can be passed, not being allowed"
    ]
  ],
  [
    "The dimensions of the vector of residuals \"u\" in the classical linear regression model are constrained to lie in a space of smaller dimension than the number of components in the vector, which is determined by the number of degrees of freedom for error or residual degrees of freedom.",
    [
      "the vectors of residuals are constrained to lie in a space of smaller dimension than the number of components in the vector. That smaller dimension is the number of degrees of freedom for error, also called residual degrees of freedom. Perhaps the simplest example is this. Suppose are random variables each with expected value \u03bc, and let be the \"sample mean.\" Then the quantities are residuals that may be considered estimates of the errors \"X\" \u2212 \u03bc. The sum of the residuals (unlike the sum of the errors) is necessarily 0. If one knows the values of any \"n\" \u2212",
      "\"L\" norm in the \"n\"-dimensional Euclidean space R. The predicted quantity \"X\u03b2\" is just a certain linear combination of the vectors of regressors. Thus, the residual vector will have the smallest length when \"y\" is projected orthogonally onto the linear subspace spanned by the columns of \"X\". The OLS estimator formula_33 in this case can be interpreted as the coefficients of vector decomposition of along the basis of \"X\". In other words, the gradient equations at the minimum can be written as: A geometrical interpretation of these equations is that the vector of residuals, formula_35 is orthogonal to the column",
      "dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.) Although formally formula_28 is a single parameter that has dimension , it is sometimes regarded as comprising separate parameters. For example, with the univariate Gaussian distribution, formula_13 is a single parameter with dimension 2, but it is sometimes regarded as comprising 2 separate parameters\u2014the mean and the standard deviation. A statistical model is \"nonparametric\" if the parameter set formula_9 is infinite dimensional. A statistical"
    ]
  ],
  [
    "A total of 203 parameters are required to be estimated for all equations of a standard form, unrestricted, tri-variate VAR(4), ignoring the intercepts.",
    [
      "the appropriate sample size where parameters such as expected standard deviations or expected differences in values between groups are unknown or very hard to estimate. All the parameters in the equation are in fact the degrees of freedom of the number of their concepts, and hence, their numbers are subtracted by 1 before insertion into the equation. The equation is: where: For example, if a study using laboratory animals is planned with four treatment groups (\"T\"=3), with eight animals per group, making 32 animals total (\"N\"=31), without any further stratification (\"B\"=0), then \"E\" would equal 28, which is above the",
      "is known as an \"intercept-only\" parameter if the \"j\"th row of all the formula_48 are equal to formula_49 for formula_43, i.e., formula_51 equals an intercept only. Intercept-only parameters are thus modelled as simply as possible, as a scalar. The unknown parameters, formula_52, are typically estimated by the method of maximum likelihood. All the regression coefficients may be put into a matrix as follows: With even more generally, one can allow the value of a variable formula_36 to have a different value for each formula_18. For example, if each linear predictor is for a different time point then one might have",
      "model with a constant, \"k\" variables and \"p\" lags. In a matrix notation, this gives: The covariance matrix of the parameters can be estimated as Vector autoregression models often involve the estimation of many parameters. For example, with seven variables and four lags, each matrix of coefficients for a given lag length is 7 by 7, and the vector of constants has 7 elements, so a total of 49\u00d74 + 7 = 203 parameters are estimated, substantially lowering the degrees of freedom of the regression (the number of data points minus the number of parameters to be estimated). This can"
    ]
  ],
  [
    "The unrestricted residual sum of squares in the regression model with 200 observations split in half for sub-sample regressions is equal to the sum of the squares of the residuals, which measures the discrepancy between the data and the estimation model.",
    [
      "article on total sum of squares for an application of this broad principle to inferential statistics. Theorem. Given a linear regression model formula_5 \"including a constant\" formula_6, based on a sample formula_7 containing \"n\" observations, the total sum of squares formula_8 can be partitioned as follows into the explained sum of squares (ESS) and the residual sum of squares (RSS): where this equation is equivalent to each of the following forms: The requirement that the model includes a constant or equivalently that the design matrix contains a column of ones ensures that formula_12. The proof can also be expressed in",
      "Residual sum of squares In statistics, the residual sum of squares (RSS), also known as the sum of squared residuals (SSR) or the sum of squared errors of prediction (SSE), is the sum of the squares of residuals (deviations predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection. In general, total sum of squares = explained sum of squares + residual sum",
      "Explained sum of squares In statistics, the explained sum of squares (ESS), alternatively known as the model sum of squares or sum of squares due to regression (\"SSR\" \u2013 not to be confused with the residual sum of squares RSS or sum of squares of errors), is a quantity used in describing how well a model, often a regression model, represents the data being modelled. In particular, the explained sum of squares measures how much variation there is in the modelled values and this is compared to the total sum of squares, which measures how much variation there is in"
    ]
  ],
  [
    "Yes, the null hypothesis that a GARCH(2,2) model can be restricted to a process with a constant conditional variance can be tested using the likelihood ratio test approach with the probability distribution of the test statistic approximated using Wilks' theorem. (Likelihood-ratio test; Wilks' theorem)",
    [
      "known as a log-likelihood ratio statistic, and the probability distribution of this test statistic, assuming that the null model is true, can be approximated using Wilks' theorem. In the case of distinguishing between two models, each of which has no unknown parameters, use of the likelihood ratio test can be justified by the Neyman\u2013Pearson lemma, which demonstrates that such a test has the highest power among all competitors. A statistical model is often a parametrized family of probability density functions or probability mass functions formula_1. A simple-vs.-simple hypothesis test has completely specified models under both the null and alternative hypotheses,",
      "Wilks' theorem In statistics, the Wilks' theorem offers an asymptotic distribution of the log-likelihood ratio statistic, which can be used as a test statistic for performing the Likelihood-ratio test. If the distribution of the likelihood ratio corresponding to a particular null and alternative hypothesis can be explicitly determined then it can directly be used to form decision regions (to accept/reject the null hypothesis). In most cases, however, the exact distribution of the likelihood ratio corresponding to specific hypotheses is very difficult to determine. A convenient result by Samuel S. Wilks, says that as the sample size formula_1 approaches formula_2, the",
      "the one-dimensional formula_7, the asymptotic distribution for the test will be formula_45, the formula_4 distribution with one degree of freedom. For the general contingency table, we can write the log-likelihood ratio statistic as Wilks' theorem In statistics, the Wilks' theorem offers an asymptotic distribution of the log-likelihood ratio statistic, which can be used as a test statistic for performing the Likelihood-ratio test. If the distribution of the likelihood ratio corresponding to a particular null and alternative hypothesis can be explicitly determined then it can directly be used to form decision regions (to accept/reject the null hypothesis). In most cases, however,"
    ]
  ],
  [
    "One technique that is NOT a plausible remedy for near multicollinearity is orthogonalizing the explanatory variables.",
    [
      "the matrix XX will not be invertible. Perfect multicollinearity is fairly common when working with raw datasets, which frequently contain redundant information. Once redundancies are identified and removed, however, nearly multicollinear variables often remain due to correlations inherent in the system being studied. In such a case, instead of the above equation holding, we have that equation in modified form with an error term formula_16: In this case, there is no exact linear relationship among the variables, but the formula_18 variables are nearly perfectly multicollinear if the variance of formula_16 is small for some set of values for the formula_20's.",
      "are accorded different names and perhaps employ different numeric measurement scales but are highly correlated with each other, then they suffer from redundancy. One of the features of multicollinearity is that the standard errors of the affected coefficients tend to be large. In that case, the test of the hypothesis that the coefficient is equal to zero may lead to a failure to reject a false null hypothesis of no effect of the explanator, a type II error. Another issue with multicollinearity is that small changes to the input data can lead to large changes in the model, even resulting",
      "errors in the related independent variables. More importantly, the usual use of regression is to take coefficients from the model and then apply them to other data. Since multicollinearity causes imprecise estimates of coefficient values, the resulting out-of-sample predictions will also be imprecise. And if the pattern of multicollinearity in the new data differs from that in the data that was fitted, such extrapolation may introduce large errors in the predictions. Note that one technique that does not work in offsetting the effects of multicollinearity is orthogonalizing the explanatory variables (linearly transforming them so that the transformed variables are uncorrelated"
    ]
  ],
  [
    "The formula for the autoregressive process with a lag of 2 is AR(2): X_t = \u03c6_1*X_(t-1) + \u03c6_2*X_(t-2) + \u03b5_t.",
    [
      "to understand the properties of the AR(1) model cast in an equivalent form. In this form, the AR(1) model, with process parameter formula_75 is given by: By putting this in the form formula_79, and then expanding the series for formula_80, one can show that: The partial autocorrelation of an AR(p) process is zero at lag p + 1 and greater, so the appropriate maximum lag is the one beyond which the partial autocorrelations are all zero. There are many ways to estimate the coefficients, such as the ordinary least squares procedure or method of moments (through Yule\u2013Walker equations). The AR(\"p\")",
      "process), i.e. the series terms. These formula_1 are split into a stochastic piece formula_3 and a time-dependent standard deviation formula_4 characterizing the typical size of the terms so that The random variable formula_3 is a strong white noise process. The series formula_7 is modeled by An ARCH(\"q\") model can be estimated using ordinary least squares. A methodology to test for the lag length of ARCH errors using the Lagrange multiplier test was proposed by Engle (1982). This procedure is as follows: If an autoregressive moving average model (ARMA) model is assumed for the error variance, the model is a generalized",
      "commonly referred to as the autocovariance matrix associated with the random vectors formula_9 and formula_10. If formula_2 is a weakly stationary (WSS) process, then the following are true: and where formula_15 is the lag time, or the amount of time by which the signal has been shifted. The autocovariance function of a WSS process is therefore given by: which is equivalent to When normalizing the autocovariance formula_17 of a weakly stationary process with its variance formula_18, one obtains the autocorrelation coefficient with formula_20. For a weakly stationary process, the value of the autocovariance function at zero is equal to the"
    ]
  ],
  [
    "One method for testing autocorrelation up to third order in a time series dataset is to analyze the correlogram, drawing upper and lower bounds for autocorrelation with a significance level formula and rejecting the null hypothesis if the autocorrelation is higher or lower than these bounds at a given lag (Chatfield, pages 20, 49-50).",
    [
      "Autocorrelation Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals. Unit root processes, trend",
      "and +1. Some sources may use the following formula for the autocovariance function: Although this definition has less bias, the (1/\"N\") formulation has some desirable statistical properties and is the form most commonly used in the statistics literature. See pages 20 and 49\u201350 in Chatfield for details. In the same graph one can draw upper and lower bounds for autocorrelation with significance level formula_11: If the autocorrelation is higher (lower) than this upper (lower) bound, the null hypothesis that there is no autocorrelation at and beyond a given lag is rejected at a significance level of formula_11. This test is",
      "there is statistical dependence between all pairs of values at the same lag formula_20. Autocorrelation Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing"
    ]
  ],
  [
    "The problem of errors and residuals in a regression model can be avoided by conducting checks of goodness of fit, such as analyzing the pattern of residuals and performing hypothesis testing, with the significance of conducting an F-test being to determine if the estimated regression equation has explanatory power beyond simply predicting the sample mean of the response variable.",
    [
      "formula_103 is formula_104. Thus formula_97 is formula_106, formula_3 is formula_108, and formula_103 is formula_110. The solution is Once a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing. Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters. Interpretations of these diagnostic tests rest heavily on the model assumptions. Although examination of the residuals can be",
      "the model by dividing the sum of squares of the model minus the degrees of freedom, which is just the number of parameters. Then the F value can be calculated by dividing the mean square of the model by the mean square of the error, and we can then determine significance (which is why you want the mean squares to begin with.). However, because of the behavior of the process of regression, the \"distributions\" of residuals at different data points (of the input variable) may vary \"even if\" the errors themselves are identically distributed. Concretely, in a linear regression where",
      "that of formula_79: which allows construct confidence intervals for mean response formula_87 to be constructed: Two hypothesis tests are particularly widely used. First, one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean (if not, it is said to have no explanatory power). The null hypothesis of no explanatory value of the estimated regression is tested using an F-test. If the calculated F-value is found to be large enough to exceed its critical value for the pre-chosen level of significance, the null hypothesis is"
    ]
  ],
  [
    "Cross equation restrictions can be used for identification in simultaneous equations models by imposing constraints on the coefficients of the equations, allowing for the determination of unique parameter values that satisfy multiple equations simultaneously.",
    [
      "to cross we simultaneously require Now if the perturbation formula_49 has formula_50 parameters formula_51 we may in general vary these numbers to satisfy these two equations. If we choose the values of formula_54 to formula_55 then both of the equations above have one single free parameter. In general it is not possible to find one formula_56 such that both of the equations are satisfied. However, if we allow another parameter to be free, both of these two equations will now be controlled by the same two parameters And generally there will be two such values of them for which the",
      "restrictions on the \"A\" and \"B\", the coefficients of \"A\" and \"B\" cannot be identified from data on \"y\" and \"z\": each row of the structural model is just a linear relation between \"y\" and \"z\" with unknown coefficients. (This is again the parameter identification problem.) The \"M\" reduced form equations (the rows of the matrix equation \"y\" = \u03a0 \"z\" above) can be identified from the data because each of them contains only one endogenous variable. Reduced form In statistics, and particularly in econometrics, the reduced form of a system of equations is the result of solving the system",
      "coefficients in matrix formula_1 are restricted to be equal to zero, or as the generalization of the general linear model where the regressors on the right-hand-side are allowed to be different in each equation. The SUR model can be further generalized into the simultaneous equations model, where the right-hand side regressors are allowed to be the endogenous variables as well. Suppose there are \"m\" regression equations Here \"i\" represents the equation number, is the time period and we are taking the transpose of the formula_3 column vector. The number of observations \"R\" is assumed to be large, so that in"
    ]
  ],
  [
    "User requirements are more detailed and specific, focusing on what the ideal system needs to perform, while system requirements are a generalization of user requirements, providing the overall requirements for the design of a system or sub-system.",
    [
      "step in the verification process, the requirements of the system are collected by analyzing the needs of the user(s). This phase is concerned with establishing what the ideal system has to perform. However it does not determine how the software will be designed or built. Usually, the users are interviewed and a document called the user requirements document is generated. The user requirements document will typically describe the system's functional, interface, performance, data, security, etc. requirements as expected by the user. It is used by business analysts to communicate their understanding of the system to the users. The users carefully",
      "the term of System requirements, is a generalisation of this first definition, giving the requirements to be met in the design of a system or sub-system. Typically an organisation starts with a set of Business requirements and then derives the System requirements from there. Often manufacturers of games will provide the consumer with a set of requirements that are different from those that are needed to run a software. These requirements are usually called the Recommended Requirements. These requirements are almost always of a significantly higher level than the minimum requirements, and represent the ideal situation in which to run",
      "of the system and its context. User stories are often confused with system requirements. A requirement is a formal description of need; a user story is an informal description of a feature. In 1998 Alistair Cockburn visited the Chrysler C3 project in Detroit and coined the phrase \"A user story is a promise for a conversation.\" With Extreme Programming (XP), user stories were a part of the planning game. In 2001, Ron Jeffries proposed a \"Three Cs\" formula for user story creation: User stories are written by or for users or customers to influence the functionality of the system being"
    ]
  ],
  [
    "A regression model is most appropriate for evaluating the factors affecting an investor's choice between equity funds, bond funds, and cash investments based on economy profile.",
    [
      "profile. The advisor then recommends appropriate investments. The different asset class definitions are widely debated, but four common divisions are stocks, bonds, real estate and commodities. The exercise of allocating funds among these assets (and among individual securities within each asset class) is what investment management firms are paid for. Asset classes exhibit different market dynamics, and different interaction effects; thus, the allocation of money among asset classes will have a significant effect on the performance of the fund. Some research suggests that allocation among asset classes has more predictive power than the choice of individual holdings in determining portfolio",
      "the investor uses: Investor profile An investor profile or style defines an individual's preferences in investment decisions, for example: The style / profile is determined by An investor's style is shaped by his or her sense of balance between risk and return. Some investors can tolerate greater risk to have greater return. Some investors want less risk and are content with a reasonable return. An investor profile is shaped by the area that an investor chooses to focus on; such as: An investor profile is also shaped by the strategy the investor uses: An investor profile is also shaped by",
      "Investor profile An investor profile or style defines an individual's preferences in investment decisions, for example: The style / profile is determined by An investor's style is shaped by his or her sense of balance between risk and return. Some investors can tolerate greater risk to have greater return. Some investors want less risk and are content with a reasonable return. An investor profile is shaped by the area that an investor chooses to focus on; such as: An investor profile is also shaped by the strategy the investor uses: An investor profile is also shaped by the valuation method"
    ]
  ],
  [
    "Variance reduction techniques such as control variates can be used to reduce standard errors in a Monte Carlo study with a small number of replications by exploiting information about the errors in estimates of known quantities to reduce the error of an estimate of an unknown quantity.",
    [
      "variance of the resulting estimator formula_9 is It can be shown that choosing the optimal coefficient minimizes the variance of formula_9, and that with this choice, where is the correlation coefficient of formula_2 and formula_4. The greater the value of formula_17, the greater the variance reduction achieved. In the case that formula_18, formula_19, and/or formula_20 are unknown, they can be estimated across the Monte Carlo replicates. This is equivalent to solving a certain least squares system; therefore this technique is also known as regression sampling. When the expectation of the control variable, formula_5, is not known analytically, it is still",
      "value formula_32 and combine the two into a new estimate Using formula_34 realizations and an estimated optimal coefficient formula_35 we obtain the following results The variance was significantly reduced after using the control variates technique. (The exact result is formula_36.) Control variates The control variates method is a variance reduction technique used in Monte Carlo methods. It exploits information about the errors in estimates of known quantities to reduce the error of an estimate of an unknown quantity. Let the unknown parameter of interest be formula_1, and assume we have a statistic formula_2 such that the expected value of \"m\"",
      "behaviour. For problems where it is possible to incorporate prior information into the reliability algorithm, it is often more efficient to use other variance reduction techniques such as importance sampling. It has been shown that subset simulation is more efficient than traditional Monte Carlo simulation, but less efficient than Line Sampling, when applied to a fracture mechanics test problem . Let X be a vector of random variables and Y = h(X) be a scalar (output) response quantity of interest for which the failure probability formula_1 is to be determined. Each evaluation of h(.) is expensive and so it should"
    ]
  ],
  [
    "One disadvantage of the random effects approach to estimating a panel model is that it assumes individual-specific effects are uncorrelated with the independent variables, which may not always hold true in practice.",
    [
      "are indices for individuals and time. The error formula_2 is very important in this analysis. Assumptions about the error term determine whether we speak of fixed effects or random effects. In a fixed effects model, formula_2 is assumed to vary non-stochastically over formula_4 or formula_5 making the fixed effects model analogous to a dummy variable model in one dimension. In a random effects model, formula_2 is assumed to vary stochastically over formula_4 or formula_5 requiring special treatment of the error variance matrix. Panel data analysis has three more-or-less independent approaches: The selection between these methods depends upon the objective of",
      "unbiased and consistent estimates of parameters even when time constant attributes are present, but random effects will be more efficient. Fixed effects is a feasible generalised least squares technique which is asymptotically more efficient than Pooled OLS when time constant attributes are present. Random effects adjusts for the serial correlation which is induced by unobserved time constant attributes. Panel analysis Panel (data) analysis is a statistical method, widely used in social science, epidemiology, and econometrics to analyze two-dimensional (typically cross sectional and longitudinal) panel data. The data are usually collected over time and over the same individuals and then a",
      "heterogeneity can be removed from the data through differencing, for example by taking a first difference which will remove any time invariant components of the model. There are two common assumptions made about the individual specific effect, the random effects assumption and the fixed effects assumption. The random effects assumption (made in a random effects model) is that the individual-specific effects are uncorrelated with the independent variables. The fixed effect assumption is that the individual-specific effects are correlated with the independent variables. If the random effects assumption holds, the random effects model is more efficient than the fixed effects model."
    ]
  ],
  [
    "Autocorrelated residuals in regression analysis could result from the presence of clusters of similar and related societies in the sample, leading to underestimated measures of variance and spurious statistical conclusions.",
    [
      "present. Without investigating autocorrelation, however, they may still mis-estimate statistics dealing with relationships among variables. In regression analysis, for example, examining the patterns of autocorrelated residuals may give important clues to third factors that may affect the relationships among variables but that have not been included in the regression model. Second, if there are clusters of similar and related societies in the sample, measures of variance will be underestimated, leading to spurious statistical conclusions. for example, exaggerating the statistical significance of correlations. Third, the underestimation of variance makes it difficult to test for replication of results from two different samples,",
      "and phylogenetic autoccorrelation when conducting regression analyses with the Standard Cross-Cultural Sample.\" The use of autocorrelation tests in exploratory data analysis is illustrated, showing how all variables in a given study can be evaluated for nonindependence of cases in terms of distance, language, and cultural complexity. The methods for estimating these autocorrelation effects are then explained and illustrated for ordinary least squares regression using again the Moran I significance measure of autocorrelation. When autocorrelation is present, it can often be removed to get unbiased estimates of regression coefficients and their variances by constructing a respecified dependent variable that is \"lagged\"",
      "valuable clues in uncovering causal relationships among variables, a non-zero estimated correlation between two variables is not, on its own, evidence that changing the value of one variable would result in changes in the values of other variables. For example, the practice of carrying matches (or a lighter) is correlated with incidence of lung cancer, but carrying matches does not cause cancer (in the standard sense of \"cause\"). In case of a single regressor, fitted by least squares, \"R\" is the square of the Pearson product-moment correlation coefficient relating the regressor and the response variable. More generally, \"R\" is the"
    ]
  ],
  [
    "Model order determination can also be performed using chi-square analyses to compare the goodness of fit between different models, with the model that fits the data significantly better being preferred.",
    [
      "to the original model is computed which is commonly referred to as a reduced order model. Reduced order models are useful in settings where it is often unfeasible to perform numerical simulations using the complete full order model. This can be due to limitations in computational resources or the requirements of the simulations setting, for instance real-time simulation settings or many-query settings in which a large number of simulations needs to be performed. Examples of Real-time simulation settings include control systems in electronics and visualization of model results while examples for a many-query setting can include optimization problems and design",
      "for the two models being compared. This value is then compared to the chi-square critical value at their difference in degrees of freedom. If the chi-square difference is smaller than the chi-square critical value, the new model fits the data significantly better and is the preferred model. Else, if the chi-square difference is larger than the critical value, the less parsimonious model is preferred. Once the model of best fit is determined, the highest-order interaction is examined by conducting chi-square analyses at different levels of one of the variables. To conduct chi-square analyses, one needs to break the model down",
      "and is not the subject of the current article. The remaining listed methods fall into the category of projection-based reduction. Projection-based reduction relies on the projection of either the model equations or the solution onto a basis of reduced dimensionality compared to the original solution space. Methods that also fall into this class but are perhaps less commonly found are: Model order reduction finds application within all fields involving mathematical modelling and many reviews exist for the topics of electronics, fluid- and structural mechanics. Current Problems in fluid mechanics involve large dynamical systems representing many effects on many different scales."
    ]
  ],
  [
    "A series must have a constant mean, variance, and autocovariance function that is independent of time to be classifiable as a weakly stationary process.",
    [
      "to be strictly stationary, strongly stationary or strict-sense stationary if Since formula_6 does not affect formula_7, formula_8 is not a function of time. White noise is the simplest example of a stationary process. An example of a discrete-time stationary process where the sample space is also discrete (so that the random variable may take one of \"N\" possible values) is a Bernoulli scheme. Other examples of a discrete-time stationary process with continuous sample space include some autoregressive and moving average processes which are both subsets of the autoregressive moving average model. Models with a non-trivial autoregressive component may be either",
      "of a stationary process for which any single realisation has an apparently noise-free structure, let formula_9 have a uniform distribution on formula_16 and define the time series formula_1 by Then formula_1 is strictly stationary. In , the distribution of formula_20 samples of the stochastic process must be equal to the distribution of the samples shifted in time \"for all\" formula_20. N-th order stationarity is a weaker form of stationarity where this is only requested for all formula_20 up to a certain order formula_23. A random process formula_1 is said to be N-th order stationary if: A weaker form of stationarity",
      "by removing changes in the level of a time series, and so eliminating trend and seasonality. One of the ways for identifying non-stationary times series is the ACF plot. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Stationary process In mathematics and statistics, a stationary process ( a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time. Since stationarity is"
    ]
  ],
  [
    "This equation represents a fixed effects model.",
    [
      "of sentences), it is called a model of the sentence or theory. Model theory has close ties to algebra and universal algebra. Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. A more comprehensive type of mathematical model uses a linguistic version of category theory to model a given situation. Akin to entity-relationship models, custom categories or sketches can be directly translated into database schemas. The difference is that logic",
      "Type (model theory) In model theory and related areas of mathematics, a type is an object that, loosely speaking, describes how a (real or possible) element or elements in a mathematical structure might behave. More precisely, it is a set of first-order formulas in a language \"L\" with free variables \"x\", \"x\",\u2026, \"x\" that are true of a sequence of elements of an \"L\"-structure formula_1. Depending on the context, types can be complete or partial and they may use a fixed set of constants, \"A\", from the structure formula_1. The question of which types represent actual elements of formula_1 leads",
      "\u03b1 of \"V\", the set \"V\" is defined as follows. The class \"V\" is defined to be the union of all sets \"V\". It is also possible to relativize this entire construction to some transitive model \"M\" of ZF (or sometimes a fragment thereof). The Boolean-valued model \"M\" is obtained by applying the above construction \"inside\" \"M\". The restriction to transitive models is not serious, as the Mostowski collapsing theorem implies that every \"reasonable\" (well-founded, extensional) model is isomorphic to a transitive one. (If the model \"M\" is not transitive things get messier, as \"M\"'s interpretation of what it means"
    ]
  ],
  [
    "The key property of the \"t\" statistic in the context of the Dickey-Fuller test is that it will asymptotically have one of the Dickey-Fuller distributions.",
    [
      "the augmented Dickey\u2013Fuller test, the test \"t\"-statistic will asymptotically have one of the Dickey\u2013Fuller distributions (depending on the test setting). Most frequently, \"t\" statistics are used in Student's \"t\"-tests, a form of statistical hypothesis testing, and in the computation of certain confidence intervals. The key property of the \"t\" statistic is that it is a pivotal quantity \u2013 while defined in terms of the sample mean, its sampling distribution does not depend on the population parameters, and thus it can be used regardless of what these may be. One can also divide a residual by the sample standard deviation: to",
      "the blocks, \"t\" denotes the 1 \u2212 \u03b1/2 quantile of the t-distribution with \"bk\" \u2212 \"b\" \u2212 \"t\" + 1 degrees of freedom. \"T\" was the original statistic proposed by James Durbin, which would have an approximate null distribution of formula_8 (that is, chi-squared with formula_9 degrees of freedom). The \"T\" statistic has slightly more accurate critical regions, so it is now the preferred statistic. The \"T\" statistic is the two-way analysis of variance statistic computed on the ranks \"R\"(\"X\"). Cochran's Q test is applied for the special case of a binary response variable (i.e., one that can have only",
      "T-statistic In statistics, the \"t\"-statistic is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error. It is used in hypothesis testing via Student's t-test. For example, it is used in estimating the population mean from a sampling distribution of sample means if the population standard deviation is unknown. Let formula_1 be an estimator of parameter \"\u03b2\" in some statistical model. Then a \"t\"-statistic for this parameter is any quantity of the form where \"\u03b2\" is a non-random, known constant which may or may not match the actual unknown parameter"
    ]
  ],
  [
    "A 5% significance level in hypothesis testing means that if the p-value is less than 0.05, the observed data is sufficiently inconsistent with the null hypothesis to reject it.",
    [
      "5%). The \"p\"-value is widely used in statistical hypothesis testing, specifically in null hypothesis significance testing. In this method, as part of experimental design, before performing the experiment, one first chooses a model (the null hypothesis) and a threshold value for \"p\", called the significance level of the test, traditionally 5% or 1% and denoted as \"\u03b1\". If the \"p\"-value is less than the chosen significance level (\"\u03b1\"), that suggests that the observed data is sufficiently inconsistent with the null hypothesis that the null hypothesis may be rejected. However, that does not prove that the tested hypothesis is true. When",
      "set by exhaustively searching\u2014perhaps for combinations of variables that might show a correlation, and perhaps for groups of cases or observations that show differences in their mean or in their breakdown by some other variable. Conventional tests of statistical significance are based on the probability that a particular result would arise if chance alone were at work, and necessarily accept some risk of mistaken conclusions of a certain type (mistaken rejections of the null hypothesis). This level of risk is called the \"significance\". When large numbers of tests are performed, some produce false results of this type, hence 5% of",
      "Statistical significance In statistical hypothesis testing, a result has statistical significance when it is very unlikely to have occurred given the null hypothesis. More precisely, a study's defined significance level, \"\u03b1,\" is the probability of the study rejecting the null hypothesis, given that it were true; and the \"p\"-value of a result, \"p\", is the probability of obtaining a result at least as extreme, given that the null hypothesis were true. The result is statistically significant, by the standards of the study, when \"p < \u03b1\". The significance level for a study is chosen before data collection, and typically set"
    ]
  ],
  [
    "The Engle-Yoo (EY) procedure overcomes the weakness of the Dickey-Fuller/Engle-Granger approach in not yielding the causal impact of variables when there is an omitted variable that affects both variables separately.",
    [
      "be substantial error. Measured variables that load onto a factor not included in the model can falsely load on factors that are included, altering true factor loadings . This can result in rotated solutions in which two factors are combined into a single factor, obscuring the true factor structure. There are a number of procedures designed to determine the optimal number of factors to retain in EFA. These include Kaiser's (1960) eigenvalue-greater-than-one rule (or K1 rule), Cattell's (1966) scree plot, With the exception of Revelle and Rocklin's (1979) very simple structure criterion, model comparison techniques, and Velicer's (1976) minimum average",
      "solves for formula_3 such that formula_4 (when we minimize the sum of squared errors, formula_5, the first-order condition is exactly formula_6.) If the true model is believed to have formula_7 due to any of the reasons listed above\u2014for example, if there is an omitted variable which affects both formula_2 and formula_9 separately\u2014then this OLS procedure will \"not\" yield the causal impact of formula_2 on formula_9. OLS will simply pick the parameter that makes the resulting errors appear uncorrelated with formula_2. Consider for simplicity the single-variable case. Suppose we are considering a regression with one variable and a constant (perhaps no",
      "individual values. R is minimum when the eigenvalues are equal to each other. Therefore, the higher the absolute difference between the two eigenvalues, which is equivalent to a higher absolute difference between the two principal curvatures of D, the higher the value of R. It follows that, for some threshold eigenvalue ratio formula_41, if R for a candidate keypoint is larger than formula_42, that keypoint is poorly localized and hence rejected. The new approach uses formula_43. This processing step for suppressing responses at edges is a transfer of a corresponding approach in the Harris operator for corner detection. The difference"
    ]
  ],
  [
    "The maximum lag \"q\" in the moving-average model for time series analysis is defined as formula_6.",
    [
      "as the maximum lag \"q\". Sometimes the ACF and partial autocorrelation function (PACF) will suggest that an MA model would be a better model choice and sometimes both AR and MA terms should be used in the same model (see Box\u2013Jenkins method#Identify p and q). Moving-average model In time series analysis, the moving-average model (MA model), also known as moving-average process, is a common approach for modeling univariate time series. The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term. Together with the autoregressive (AR) model, the",
      "moving average model with exogenous inputs (NARMAX model) can represent a wide class of nonlinear systems, and is defined as where \"y\"(\"k\"), \"u\"(\"k\") and \"e\"(\"k\") are the system output, input, and noise sequences respectively; formula_4, formula_5, and formula_6 are the maximum lags for the system output, input and noise; F[\u2022] is some nonlinear function, d is a time delay typically set to \"d\" = 1.The model is essentially an expansion of past inputs, outputs and noise terms. Because the noise is modelled explicitly, unbiased estimates of the system model can be obtained in the presence of unobserved highly correlated and",
      "Lag operator In time series analysis, the lag operator (L) or backshift operator (B) operates on an element of a time series to produce the previous element. For example, given some time series then or similarly :formula_4 for all formula_3 where some authors or equivalently where \"L\" is the lag operator. Sometimes the symbol \"B\" for backshift is used instead. Note that the lag operator can be raised to arbitrary integer powers so that and Also polynomials of the lag operator can be used, and this is a common notation for ARMA (autoregressive moving average) models. For example, specifies an"
    ]
  ],
  [
    "The two-stage least squares method is commonly used for over-identified systems of simultaneous equations.",
    [
      "number of excluded exogenous variables is greater or equal to the number of included endogenous variables\u201d. The \"rank condition\" of identifiability is that , where \u03a0 is a matrix which is obtained from \u03a0 by crossing out those columns which correspond to the excluded endogenous variables, and those rows which correspond to the included exogenous variables. The simplest and the most common estimation method for the simultaneous equations model is the so-called two-stage least squares method, developed independently by and . It is an equation-by-equation technique, where the endogenous regressors on the right-hand side of each equation are being instrumented",
      "Commission work on simultaneous equations estimation centered on Koopman and Hood's (1953) algorithms from the economics of transportation and optimal routing, with maximum likelihood estimation, and closed form algebraic calculations, as iterative solution search techniques were limited in the days before computers. Anderson and Rubin (1949, 1950) developed the limited information maximum likelihood estimator for the parameters of a single structural equation, which indirectly included the two-stage least squares estimator and its asymptotic distribution (Anderson, 2005) and Farebrother (1999). Two-stage least squares was originally proposed as a method of estimating the parameters of a single structural equation in a system",
      "Simultaneous equations model Simultaneous equation models are a type of statistical model in the form of a set of linear simultaneous equations. They are often used in econometrics. One can estimate these models equation by equation; however, estimation methods that exploit the system of equations, such as generalized method of moments (GMM) and instrumental variables estimation (IV) tend to be more efficient. Suppose there are \"m\" regression equations of the form where \"i\" is the equation number, and is the observation index. In these equations \"x\" is the \"k\u00d7\"1 vector of exogenous variables, \"y\" is the dependent variable, \"y\" is"
    ]
  ],
  [
    "A Type II error occurs when a null hypothesis that is actually false is not rejected (Type III error).",
    [
      "general it may be ambiguous. Type errors and undeclared variable errors are sometimes considered to be syntax errors when they are detected at compile-time (which is usually the case when compiling strongly-typed languages), though it is common to classify these kinds of error as semantic errors instead. As an example, the Python code contains a type error because it adds a string literal to an integer literal. Type errors of this kind can be detected at compile-time: They can be detected during parsing (phrase analysis) if the compiler uses separate rules that allow \"integerLiteral + integerLiteral\" but not \"stringLiteral +",
      "Type III error In statistical hypothesis testing, there are various notions of so-called type III errors (or errors of the third kind), and sometimes type IV errors or higher, by analogy with the type I and type II errors of Jerzy Neyman and Egon Pearson. Fundamentally, Type III errors occur when researchers provide the right answer to the wrong question. Since the paired notions of type I errors (or \"false positives\") and type II errors (or \"false negatives\") that were introduced by Neyman and Pearson are now widely used, their choice of terminology (\"errors of the first kind\" and \"errors",
      "error-detection code, then the received coded data block is rejected and a re-transmission is requested by the receiver, similar to ARQ. In a more sophisticated form, Type II HARQ, the message originator alternates between message bits along with error detecting parity bits and only FEC parity bits. When the first transmission is received error free, the FEC parity bits are never sent. Also, two consecutive transmissions can be combined for error correction if neither is error free. To understand the difference between Type I and Type II Hybrid ARQ, consider the size of ED and FEC added information: error detection"
    ]
  ],
  [
    "The most appropriate 95% confidence interval for the intercept term of the model is [0.5, 1.5].",
    [
      "then the confidence level is 95%. Similarly, the confidence interval for the intercept coefficient is given by at confidence level (1 \u2212 \"\u03b3\"), where The confidence intervals for and give us the general idea where these regression coefficients are most likely to be. For example, in the Okun's law regression shown here the point estimates are The 95% confidence intervals for these estimates are In order to represent this information graphically, in the form of the confidence bands around the regression line, one has to proceed carefully and account for the joint distribution of the estimators. It can be shown",
      "the whole population having the same intention on the survey might be 30% to 50%. From the same data one may calculate a 90% confidence interval, which in this case might be 37% to 43%. A major factor determining the length of a confidence interval is the size of the sample used in the estimation procedure, for example, the number of people taking part in a survey. Various interpretations of a confidence interval can be given (taking the 90% confidence interval as an example in the following). In each of the above, the following applies: If the true value of",
      "where formula_21 denotes the 1-alpha quantile of a \"t\"-distribution with m degrees of freedom. It may also be of interest to derive a 95% upper confidence bound for the median air lead level. Such a bound for mu is given by formula_22. Consequently, a 95% upper confidence bound for the median air lead is given by formula_23. Now suppose we want to predict the air lead level at a particular area within the laboratory. A 95% upper prediction limit for the log-transformed lead level is given by formula_24. A two-sided prediction interval can be similarly computed. The meaning and interpretation"
    ]
  ],
  [
    "A stationary process is one whose statistical properties, such as mean, variance, and moments, do not change over time.",
    [
      "Stationary ergodic process In probability theory, a stationary ergodic process is a stochastic process which exhibits both stationarity and ergodicity. In essence this implies that the random process will not change its statistical properties with time and that its statistical properties (such as the theoretical mean and variance of the process) can be deduced from a single, sufficiently long sample (realization) of the process. Stationarity is the property of a random process which guarantees that its statistical properties, such as the mean value, its moments and variance, will not change over time. A stationary process is one whose probability distribution",
      "stochastic process with the above definition of stationarity is sometimes said to be strictly stationary, but there are other forms of stationarity. One example is when a discrete-time or continuous-time stochastic process formula_11 is said to be stationary in the wide sense, then the process formula_11 has a finite second moment for all formula_55 and the covariance of the two random variables formula_15 and formula_137 depends only on the number formula_138 for all formula_55. Khinchin introduced the related concept of stationarity in the wide sense, which has other names including covariance stationarity or stationarity in the broad sense. A filtration",
      "a measured process which is the superposition of two underlying processes, each with its own statistical properties. Although the measured process may be stationary in the long term, it is not appropriate to consider the sampled distribution to be the reflection of a single (ergodic) process: The ensemble average is meaningless. Also see ergodic theory and ergodic process. Stationary ergodic process In probability theory, a stationary ergodic process is a stochastic process which exhibits both stationarity and ergodicity. In essence this implies that the random process will not change its statistical properties with time and that its statistical properties (such"
    ]
  ],
  [
    "Properties of a VAR model are usually summarized using structural analysis techniques such as Granger causality, impulse responses, and forecast error variance decompositions.",
    [
      "hurt the accuracy of the parameter estimates and hence of the forecasts given by the model. Properties of the VAR model are usually summarized using structural analysis using Granger causality, impulse responses, and forecast error variance decompositions. Consider the first-order case (i.e., with only one lag), with equation of evolution for evolving (state) vector formula_31 and vector formula_32 of shocks. To find, say, the effect of the \"j\"-th element of the vector of shocks upon the \"i\"-th element of the state vector 2 periods later, which is a particular impulse response, first write the above equation of evolution one period",
      "in structural models. VAR models are also increasingly used in health research for automatic analyses of diary data or sensor data. Vector autoregression Vector autoregression (VAR) is a stochastic process model used to capture the linear interdependencies among multiple time series. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR enter the model in the same way: each variable has an equation explaining its evolution based on its own lagged values, the lagged values of the other model variables, and an error term. VAR modeling does not",
      "Vector autoregression Vector autoregression (VAR) is a stochastic process model used to capture the linear interdependencies among multiple time series. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR enter the model in the same way: each variable has an equation explaining its evolution based on its own lagged values, the lagged values of the other model variables, and an error term. VAR modeling does not require as much knowledge about the forces influencing a variable as do structural models with simultaneous equations: The only prior knowledge"
    ]
  ],
  [
    "Based on the autocorrelation estimates obtained using 250 data points, the coefficients that are statistically significant at the 5% level can be determined by comparing the autocorrelation values to the upper and lower bounds for autocorrelation with significance level formula_11.",
    [
      "\"T\" is the sample size and \"R\" is the coefficient of determination. Under the null hypothesis of no autocorrelation, this statistic is asymptotically distributed as formula_87 with \"k\" degrees of freedom. Responses to nonzero autocorrelation include generalized least squares and the Newey\u2013West HAC estimator (Heteroskedasticity and Autocorrelation Consistent). In the estimation of a moving average model (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order \"q\", we have formula_88, for formula_89, and formula_90, for formula_91. Serial dependence is",
      "formula_3: Next, if formula_5 is Pearson's coefficient of correlation for segment formula_6, the scaled correlation across the entire signals formula_7 is computed as In a detailed analysis, Nikoli\u0107 et al. showed that the degree to which the contributions of the slow components will be attenuated depends on three factors, the choice of the scale, the amplitude ratios between the slow and the fast component, and the differences in their oscillation frequencies. The larger the differences in oscillation frequencies, the more efficiently will the contributions of the slow components be removed from the computed correlation coefficient. Similarly, the smaller the power",
      "and +1. Some sources may use the following formula for the autocovariance function: Although this definition has less bias, the (1/\"N\") formulation has some desirable statistical properties and is the form most commonly used in the statistics literature. See pages 20 and 49\u201350 in Chatfield for details. In the same graph one can draw upper and lower bounds for autocorrelation with significance level formula_11: If the autocorrelation is higher (lower) than this upper (lower) bound, the null hypothesis that there is no autocorrelation at and beyond a given lag is rejected at a significance level of formula_11. This test is"
    ]
  ],
  [
    "statistic to examine how well the model fits the observed data.",
    [
      "Goodness of fit The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Such measures can be used in statistical hypothesis testing, e.g. to test for normality of residuals, to test whether two samples are drawn from identical distributions (see Kolmogorov\u2013Smirnov test), or whether outcome frequencies follow a specified distribution (see Pearson's chi-squared test). In the analysis of variance, one of the components into which the variance is partitioned may be a",
      "have been recommended at least since the 1981 edition of the popular statistics textbook by Robert R. Sokal and F. James Rohlf. Goodness of fit The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Such measures can be used in statistical hypothesis testing, e.g. to test for normality of residuals, to test whether two samples are drawn from identical distributions (see Kolmogorov\u2013Smirnov test), or whether outcome frequencies follow a specified distribution",
      "lack-of-fit sum of squares. In assessing whether a given distribution is suited to a data-set, the following tests and their underlying measures of fit can be used: In regression analysis, the following topics relate to goodness of fit: The following are examples that arise in the context of categorical data. Pearson's chi-squared test uses a measure of goodness of fit which is the sum of differences between observed and expected outcome frequencies (that is, counts of observations), each squared and divided by the expectation: where: The expected frequency is calculated by: where: The resulting value can be compared with a"
    ]
  ],
  [
    "The use of antithetic variates in a Monte Carlo experiment contributes to variance reduction by balancing unusually large or small output values computed from one path with values computed from the antithetic path, resulting in a reduction in variance.",
    [
      "risk-management decisions. This state of affairs can be mitigated by variance reduction techniques. A simple technique is, for every sample path obtained, to take its antithetic path \u2014 that is given a path formula_15 to also take formula_19. Since the variables formula_13 and formula_21 form an antithetic pair, a large value of one is accompanied by a small value of the other. This suggests that an unusually large or small output computed from the first path may be balanced by the value computed from the antithetic path, resulting in a reduction in variance. Not only does this reduce the number",
      "value formula_32 and combine the two into a new estimate Using formula_34 realizations and an estimated optimal coefficient formula_35 we obtain the following results The variance was significantly reduced after using the control variates technique. (The exact result is formula_36.) Control variates The control variates method is a variance reduction technique used in Monte Carlo methods. It exploits information about the errors in estimates of known quantities to reduce the error of an estimate of an unknown quantity. Let the unknown parameter of interest be formula_1, and assume we have a statistic formula_2 such that the expected value of \"m\"",
      "variance of the resulting estimator formula_9 is It can be shown that choosing the optimal coefficient minimizes the variance of formula_9, and that with this choice, where is the correlation coefficient of formula_2 and formula_4. The greater the value of formula_17, the greater the variance reduction achieved. In the case that formula_18, formula_19, and/or formula_20 are unknown, they can be estimated across the Monte Carlo replicates. This is equivalent to solving a certain least squares system; therefore this technique is also known as regression sampling. When the expectation of the control variable, formula_5, is not known analytically, it is still"
    ]
  ],
  [
    "Alternative forecast accuracy measures can be compared and interpreted by evaluating the forecasting accuracy of different models under various assumptions or estimation techniques, comparing out-of-sample accuracy measures to in-sample measures, and calculating skill scores over a large sample of forecast-observation pairs to determine the most accurate forecast.",
    [
      "accuracy). The question of how to interpret the measured forecasting accuracy arises\u2014for example, what is a \"high\" (bad) or a \"low\" (good) value for the mean squared prediction error? There are two possible points of comparison. First, the forecasting accuracy of an alternative model, estimated under different modeling assumptions or different estimation techniques, can be used for comparison purposes. Second, the out-of-sample accuracy measure can be compared to the same measure computed for the in-sample data points (that were used for parameter estimation) for which enough prior data values are available (that is, dropping the first \"p\" data points, for",
      "that compares the forecast performance of a particular forecast prediction to that of a reference, benchmark prediction\u2014a formulation called a 'Skill Score'. Forecast skill metric and score calculations should be made over a large enough sample of forecast-observation pairs to be statistically robust. A sample of predictions for a single predictand (eg, temperature at one location, or a single stock value) typically includes forecasts made on a number of different dates. A sample could also pool forecast-observation pairs across space, for a prediction made on a single date, as in the forecast of a weather event that is verified at",
      "estimation approaches and models suggest that there is no \u201cbest approach\u201d and that the relative accuracy of one approach or model in comparison to another depends strongly on the context . This implies that different organizations benefit from different estimation approaches. Findings (summarized in,) that may support the selection of estimation approach based on the expected accuracy of an approach include: The most robust finding, in many forecasting domains, is that combination of estimates from independent sources, preferable applying different approaches, will on average improve the estimation accuracy. It is important to be aware of the limitations of each traditional"
    ]
  ],
  [
    "EGARCH models address the criticism that GARCH models cannot account for leverage effects by allowing the sign and magnitude of certain parameters to have separate effects on volatility, capturing the asymmetric impact of positive and negative shocks on future volatility (Nelson & Cao, 1991).",
    [
      "modelling it has some attractive properties such as a greater weight upon more recent observations, but also drawbacks such as an arbitrary decay factor that introduces subjectivity into the estimation. The lag length \"p\" of a GARCH(\"p\", \"q\") process is established in three steps: Nonlinear Asymmetric GARCH(1,1) (NAGARCH) is a model with the specification: For stock returns, parameter formula_35 is usually estimated to be positive; in this case, it reflects a phenomenon commonly referred to as the \"leverage effect\", signifying that negative returns increase future volatility by a larger amount than positive returns of the same magnitude. This model should",
      "normal variable or come from a generalized error distribution. The formulation for formula_46 allows the sign and the magnitude of formula_45 to have separate effects on the volatility. This is particularly useful in an asset pricing context. Since formula_48 may be negative, there are no sign restrictions for the parameters. The GARCH-in-mean (GARCH-M) model adds a heteroskedasticity term into the mean equation. It has the specification: formula_49 The residual formula_50 is defined as: formula_51 The Quadratic GARCH (QGARCH) model by Sentana (1995) is used to model asymmetric effects of positive and negative shocks. In the example of a GARCH(1,1) model,",
      "not be confused with the NARCH model, together with the NGARCH extension, introduced by Higgins and Bera in 1992. Integrated Generalized Autoregressive Conditional heteroskedasticity (IGARCH) is a restricted version of the GARCH model, where the persistent parameters sum up to one, and imports a unit root in the GARCH process. The condition for this is formula_36. The exponential generalized autoregressive conditional heteroskedastic (EGARCH) model by Nelson & Cao (1991) is another form of the GARCH model. Formally, an EGARCH(p,q): formula_37 where formula_38, formula_39 is the conditional variance, formula_40, formula_41, formula_42, formula_43 and formula_44 are coefficients. formula_45 may be a standard"
    ]
  ],
  [
    "The finding of only 3 fund managers outperforming the market out of 100 tested firms suggests evidence of stock market inefficiency.",
    [
      "whether in most cases such extensive efforts will generate sufficiently superior selections to justify their cost. To that very limited extent I'm on the side of the \"efficient market\" school of thought now generally accepted by the professors.\" Graham stated that the average manager of institutional funds could not obtain better results than stock market indexes, since \"that would mean that the stock market experts as a whole could beat themselves \u2014 a logical contradiction.\" Regarding portfolio formation, Graham suggested that investors use \"a highly simplified\" approach that applies one or two criteria to security prices \"to assure that full",
      "reasoning, saying \"From a nonempirical base of axioms you never get empirical results.\" In 1970, Fama published a review of both the theory and the evidence for the hypothesis. The paper extended and refined the theory, included the definitions for three forms of financial market efficiency: weak, semi-strong and strong (see below). It has been argued that the stock market is \"micro efficient\" but not \"macro efficient\". The main proponent of this view was Samuelson, who asserted that the EMH is much better suited for individual stocks than it is for the aggregate stock market. Research based on regression and",
      "the efficient-market hypothesis (EMH) as the fundamental premise that justifies the creation of the index funds. The hypothesis implies that fund managers and stock analysts are constantly looking for securities that may out-perform the market; and that this competition is so effective that any new information about the fortune of a company will rapidly be incorporated into stock prices. It is postulated therefore that it is very difficult to tell ahead of time which stocks will out-perform the market. By creating an index fund that mirrors the whole market the inefficiencies of stock selection are avoided. In particular, the EMH"
    ]
  ],
  [
    "The advantage of using panel data over pure cross-sectional or pure time-series modeling is that it allows for the examination of changes in variables over time and differences in variables between subjects.",
    [
      "panel data would be the wide format where one row represents one observational unit for \"all\" points in time (for the example, the wide format would have only two (left example) or three (right example) rows of data with additional columns for each time-varying variable (income, age). A panel has the form where formula_2 is the individual dimension and formula_3 is the time dimension. A general panel data regression model is written as formula_4 Different assumptions can be made on the precise structure of this general model. Two important models are the fixed effects model and the random effects model.",
      "time. Panel data differs from pooled cross-sectional data across time, because it deals with the observations on the same subjects in different times whereas the latter observes different subjects in different time periods. Panel analysis uses panel data to examine changes in variables over time and differences in variables between the subjects. In a rolling cross-section, both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly. For example, a political poll may decide to interview 1000 individuals. It first selects these individuals randomly from the entire",
      "inferences that may yield better empirical performance than do purely observational studies. Data sets to which econometric analyses are applied can be classified as time-series data, cross-sectional data, panel data, and multidimensional panel data. Time-series data sets contain observations over time; for example, inflation over the course of several years. Cross-sectional data sets contain observations at a single point in time; for example, many individuals' incomes in a given year. Panel data sets contain both time-series and cross-sectional observations. Multi-dimensional panel data sets contain observations across time, cross-sectionally, and across some third dimension. For example, the Survey of Professional Forecasters"
    ]
  ],
  [
    "The interpretation of the null hypothesis when applying the Engle-Granger test to the residuals of a potentially cointegrating regression is that \"x\" does not Granger-cause \"y.\"",
    [
      "null hypothesis is no explanatory power jointly added by the \"x\"'s). In the notation of the above augmented regression, \"p\" is the shortest, and \"q\" is the longest, lag length for which the lagged value of \"x\" is significant. The null hypothesis that \"x\" does not Granger-cause \"y\" is accepted if and only if no lagged values of \"x\" are retained in the regression. Multivariate Granger causality analysis is usually performed by fitting a vector autoregressive model (VAR) to the time series. In particular, let formula_14 for formula_15 be a formula_16-dimensional multivariate time series. Granger causality is performed by fitting",
      "is subject to asymptotic properties, i.e. large samples. If the sample size is too small then the results will not be reliable and one should use Auto Regressive Distributed Lags (ARDL). Peter C. B. Phillips and Sam Ouliaris (1990) show that residual-based unit root tests applied to the estimated cointegrating residuals do not have the usual Dickey\u2013Fuller distributions under the null hypothesis of no-cointegration. Because of the spurious regression phenomenon under the null hypothesis, the distribution of these tests have asymptotic distributions that depend on (1) the number of deterministic trend terms and (2) the number of variables with which",
      "it may be found that neither variable Granger-causes the other, or that each of the two variables Granger-causes the other. Let \"y\" and \"x\" be stationary time series. To test the null hypothesis that \"x\" does not Granger-cause \"y\", one first finds the proper lagged values of \"y\" to include in a univariate autoregression of \"y\": Next, the autoregression is augmented by including lagged values of \"x\": One retains in this regression all lagged values of \"x\" that are individually significant according to their t-statistics, provided that collectively they add explanatory power to the regression according to an F-test (whose"
    ]
  ],
  [
    "The partial autocorrelation function (PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags, while the autocorrelation function (ACF) does not control for other lags.",
    [
      "Partial autocorrelation function In time series analysis, the partial autocorrelation function (PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags. This function plays an important role in data analysis aimed at identifying the extent of the lag in an autoregressive model. The use of this function was introduced as part of the Box\u2013Jenkins approach to time series modelling, whereby plotting the partial autocorrelative functions one could determine the appropriate lags",
      "relies on the assumption that the record length is at least moderately large (say \"n\">30) and that the underlying process has finite second moment. Partial autocorrelation function In time series analysis, the partial autocorrelation function (PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags. This function plays an important role in data analysis aimed at identifying the extent of the lag in an autoregressive model. The use of this function",
      "Davis, 2009). These algorithms derive from the exact theoretical relation between the partial autocorrelation function and the autocorrelation function. Partial autocorrelation plots (Box and Jenkins, Chapter 3.2, 2008) are a commonly used tool for identifying the order of an autoregressive model. The partial autocorrelation of an AR(\"p\") process is zero at lag \"p\" + 1 and greater. If the sample autocorrelation plot indicates that an AR model may be appropriate, then the sample partial autocorrelation plot is examined to help identify the order. One looks for the point on the plot where the partial autocorrelations for all higher lags are"
    ]
  ],
  [
    "Huxley argued that protoplasm is the substance of life and established it as incompatible with a vitalistic theory of life.",
    [
      "materialism of Huxley. In 1880, term protoplast was proposed by Hanstein (1880) for the entire cell, excluding the cell wall, and some authors like Julius von Sachs (1882) preferred that name instead of cell. In 1965, lardy introduced the term \"cytosol\", later redefined to refer to the liquid inside cell. By the time Huxley wrote, a long-standing debate was largely settled over the fundamental unit of life: was it the cell or was it protoplasm? By the late 1860s, the debate was largely settled in favor of protoplasm. The cell was a container for protoplasm, the fundamental and universal material",
      "substance of life. Huxley's principal contribution was to establish protoplasm as incompatible with a vitalistic theory of life. Attempts to investigate the origin of life through the creation of synthetic \"protoplasm\" in the laboratory were not successful. The idea that protoplasm of eukaryotes is simply divisible into a ground substance called \"cytoplasm\" and a structural body called the cell nucleus reflects the more primitive knowledge of cell structure that preceded the development of electron microscopy, when it seemed that cytoplasm was a homogeneous fluid and the existence of most sub-cellular compartments, or how cells maintain their shape, was unknown. Today,",
      "biologist T.H. Huxley wrote in 1868: But what consciousness is, we know not; and how it is that anything so remarkable as a state of consciousness comes about as the result of irritating nervous tissue, is just as unaccountable as the appearance of the Djin when Aladdin rubbed his lamp in the story, or as any other ultimate fact of nature. The philosopher Thomas Nagel argued in 1974: If physicalism is to be defended, the phenomenological features must themselves be given a physical account. But when we examine their subjective character it seems that such a result is impossible. The"
    ]
  ],
  [
    "Edward Gibbon viewed the various modes of worship in the Roman world as contributing to the decay of the empire, particularly citing the influence of Christianity and the adoption of Oriental-style despotism by Roman emperors.",
    [
      "may be very broadly classified into four schools of thought, although the classification is not without overlap: The tradition positing general malaise goes back to Edward Gibbon who argued that the edifice of the Roman Empire had been built on unsound foundations to begin with. According to Gibbon, the fall was - in the final analysis - inevitable. On the other hand, Gibbon had assigned a major portion of the responsibility for the decay to the influence of Christianity, and is often, though perhaps unjustly, seen as the founding father of the school of monocausal explanation. On the other hand,",
      "Oriental notion of a sun-king, and the divine law that Oriental societies accepted. Herodotus's version of history advocated a society where men became free when they consented lawfully to the social contract of their respective city-state. Edward Gibbon suggested that the increasing use of Oriental-style despotism by the Roman emperors was a major factor in the fall of the Roman Empire, particularly from the reign of Elagabalus: As the attention of the new emperor was diverted by the most trifling amusements, he wasted many months in his luxurious progress from Syria to Italy, passed at Nicomedia his first winter after",
      "The eighteenth-century historian Edward Gibbon, interpreting Tacitus, \"Germania\" \u00a740, detected a parallel among the pagan German tribes who worshipped a goddess of the earth (identified by modern scholars with Nerthus) who in Gibbon's interpretation resided at the island of R\u00fcgen, who annually travelled to visit the tribes. However, Gibbon's assertion has since been discredited, given that the canon law of \"Pax Dei\" derives no foundation from pagan customs, but rather from rational principles of Roman Law regarding violence. The Christian concept evolved from the earlier concept of Pax Romana. As early as 697, Adomn\u00e1n of Iona promulgated the C\u00e1in Adomn\u00e1in,"
    ]
  ],
  [
    "The 1755 Lisbon earthquake led Voltaire to reject the idea of a benevolent deity and question the nature of God's involvement in human affairs.",
    [
      "a benevolent deity. The Lisbon disaster provided a counterexample. As Theodor Adorno wrote, \"[t]he earthquake of Lisbon sufficed to cure Voltaire of the theodicy of Leibniz\" (\"Negative Dialectics\" 361). In the later twentieth century, the 1755 earthquake has sometimes been compared to the Holocaust as a catastrophe that transformed European culture and philosophy. Jean-Jacques Rousseau was also influenced by the devastation following the earthquake, whose severity he believed was due to too many people living within the close quarters of the city. Rousseau used the earthquake as an argument against cities as part of his desire for a more naturalistic",
      "a loving Christian God with the seeming indifference of nature in disasters such as Lisbon. The phrase \"what is, is right\" coined by Alexander Pope in his Essay on Man, and Leibniz' affirmation \"we live in the best of all possible worlds\", provoked Voltaire's scorn. He railed against what he perceived as intricate but empty philosophizing which served only to demean humanity and ultimately lead to fatalism. The earthquake further bolstered Voltaire's philosophical pessimism and deism. Due to the prevalence of evil, he argued, there could not possibly exist a benevolent, loving deity who intervened in human affairs to reward",
      "the earthquake and questioning whether a just and compassionate God would seek to punish sins through such cruelty, Voltaire argued that the all-powerful God Leibniz and Pope hypothesized could have prevented the innocent suffering of the sinners, reduced the scale of destruction, or announced his purpose of purifying mankind. He rejected the charge that selfishness and pride had made him rebel against suffering: In the poem, Voltaire rejected belief in \"Providence\" as impossible to defend \u2014 he believed that all living things seemed doomed to live in a cruel world. Voltaire concludes that human beings are weak, ignorant and condemned"
    ]
  ],
  [
    "The bias found in the memoir of Louis XIV and his Court as described by Duke Saint-Simon was influenced by personal grudges and grievances, such as feeling slighted by the king and being critical of his character.",
    [
      "upheaval culminating in the French Revolution to his failure to reform French institutions while the monarchy was still secure. Other scholars counter that there was little reason to reform institutions that largely worked well under Louis. They also maintain that events occurring almost 80 years after his death were not reasonably foreseeable to Louis, and that in any case, his successors had sufficient time to initiate reforms of their own. Louis has often been criticised for his vanity. The memoirist Saint-Simon, who claimed that Louis slighted him, criticised him thusly: There was nothing he liked so much as flattery, or,",
      "a night in small camp beds in St-Germain during the Fronde; the people besieging the royal palace and \"dissidents\" approaching the little king's bed to make sure of his presence, but who give rise to lingering grudges. Pagnol then quotes stories extracted from the \"Memoirs\" of Saint-Simon, depicting the King in all his deceit. The forbidden marriage of Mademoiselle and Lauzun \"Mademoiselle\", Louis XIV's first cousin, possessed considerable properties, in particular the principality of Dombes and the comt\u00e9 of Eu. After having declared that he would not oppose her marriage to the Duke of Lauzun, the King had it cancelled",
      "de France\") of Louis XIV, were demoted in rank. The actual instruction of the young king was not much disturbed however, since it was mostly done by his old and trusted tutor, Andr\u00e9-Hercule de Fleury, Bishop of Fr\u00e9jus, who remained in place. Many of the surviving descriptions of the duke's personality are highly uncomplimentary. They fall under the general categories greed, bad manners, stupidity. For example, Barbier said he \"had a very limited mind, knows nothing, and only likes pleasure and hunting.\" He was described as pretending to like hunting to ingratiate himself with the king. The Regency ended when"
    ]
  ],
  [
    "The Treaty of Versailles impacted Germany after World War I by imposing harsh reparations, territorial losses, and military restrictions on the country, leading to economic hardship and resentment among the German population.",
    [
      "World War I broke out in Europe on August 1, 1914. The conflict dragged on until a truce was declared on November 11, 1918, leading to the controversial, one-sided Treaty of Versailles, which was signed on June 28, 1919. The war's end triggered the abdication of various monarchies and the collapse of five of the last modern empires of Russia, Germany, China, Ottoman Turkey and Austria-Hungary, with the latter splintered into Austria, Hungary, southern Poland (who acquired most of their land in a war with Soviet Russia), Czechoslovakia and Yugoslavia, as well as the unification of Romania with Transylvania and",
      "World War I. After four years of warfare, in which approximately two million German soldiers were killed, a general armistice ended the fighting on 11 November, and German troops returned home. In the German Revolution (November 1918), Emperor Wilhelm II and all German ruling princes abdicated their positions and responsibilities. Germany's new political leadership signed the Treaty of Versailles in 1919. In this treaty, Germany, as part of the Central Powers, accepted defeat by the Allies in one of the bloodiest conflicts of all time. Germans perceived the treaty as humiliating and unjust and it was later seen by historians",
      "Treaty of Versailles The Treaty of Versailles () was the most important of the peace treaties that brought World War I to an end. The Treaty ended the state of war between Germany and the Allied Powers. It was signed on 28 June 1919 in Versailles, exactly five years after the assassination of Archduke Franz Ferdinand which directly led to World War I. The other Central Powers on the German side of World War I signed separate treaties. Although the armistice, signed on 11 November 1918, ended the actual fighting, it took six months of Allied negotiations at the Paris"
    ]
  ],
  [
    "The German General Staff potentially influenced the Russian revolutionaries Lenin and Trotsky by transporting Lenin and his supporters to Russia in a sealed train in April 1917, as well as favoring the opposition Communist Party (Bolsheviks) who were proponents of Russia's withdrawal from the war (Treaty of Brest-Litovsk).",
    [
      "question that the German General Staff had hired Lenin and Trotsky and discredited the Russian revolutionaries. The \"New York Evening Post\" challenged the authenticity of the documents on September 21, 1918 by saying they originated with Santeri Nuorteva, a well-known propagandist who had worked for the Communist government the Bolsheviks had established in Finland. Newspapers debated their authenticity for months. The \"New York Times\" reported the CPI's version of the documents in September and detailed the damaging charges, with the newspaper claiming: that the present heads of the Bolshevist governmentLenin and Trotsky and their associatesare German agents... that the Bolshevist",
      "had. The pro-war Provisional Government was opposed by the self-proclaimed Petrograd Soviet of Workers' and Soldiers' Deputies, dominated by leftist parties. Its Order No. 1 called for an overriding mandate to soldier committees rather than army officers. The Soviet started to form its own paramilitary power, the Red Guards, in March 1917. The continuing war led the German Government to agree to a suggestion that they should favor the opposition Communist Party (Bolsheviks), who were proponents of Russia's withdrawal from the war. Therefore, in April 1917, Germany transported Bolshevik leader Vladimir Lenin and thirty-one supporters in a sealed train from",
      ") and the Red Guards (; ) were recruited through the local social democratic party sections and from the labour unions. The Bolsheviks' and Vladimir Lenin's October Revolution of 7 November 1917 transferred political power in Petrograd to the radical, left-wing socialists. The German government's decision to arrange safe conduct for Lenin and his comrades from exile in Switzerland to Petrograd in April 1917, was a success. An armistice between Germany and the Bolshevik regime came into force on 6 December and peace negotiations began on 22 December 1917 at Brest-Litovsk. November 1917 became another watershed in the 1917\u20131918 rivalry"
    ]
  ],
  [
    "The key factors contributing to Florence's prosperity during the Renaissance period were the great wealth accumulated by merchant cities, the patronage of dominant families, and the migration of Greek scholars and texts following the Conquest of Constantinople, leading to the birth of the Italian Renaissance.",
    [
      "universities. Venice entered in control of the mediterranean trade routes and consolidated a maritime empire after the Sack of Constantinople of the Fourth Crusade and the Venetian-Genoese wars. Furthermore, Frederick of Sicily made Italy the cultural and strategic centre of a large reign that included Imperial Germany and, following the Sixth Crusade, the Kingdom of Jerusalem. These socio-economic factors paved the way for the beginning of the Renaissance in Florence, Tuscany, in the 14th century. Renaissance philosophy, art, science and exploration marked the transition to the modern era and notable figures such as Leonardo, Dante, Giotto, Marco Polo, Machiavelli, Michelangelo,",
      "a number of factors: the great wealth accumulated by merchant cities, the patronage of its dominant families, and the migration of Greek scholars and texts to Italy following the Conquest of Constantinople at the hands of the Ottoman Turks. The Italian Renaissance peaked in the mid-16th century as foreign invasions plunged the region into the turmoil of the Italian Wars. The Medici became the leading family of Florence and fostered and inspired the birth of the Italian Renaissance, along with other families of Italy, such as the Visconti and Sforza of Milan, the Este of Ferrara, and the Gonzaga of",
      "the constitution of the republic of Florence throughout the Italian Renaissance. The city's numerous luxurious palazzi were becoming surrounded by townhouses, built by the ever prospering merchant class. In 1298, one of the leading banking families of Europe, the Bonsignoris, were bankrupted and so the city of Siena lost her status as the banking center of Europe to Florence. The main challengers of the Albizzi family were the Medicis, first under Giovanni de' Medici, later under his son Cosimo di Giovanni de' Medici. The Medici controlled the Medici bank\u2014then Europe's largest bank\u2014and an array of other enterprises in Florence and"
    ]
  ],
  [
    "The stark differences in interactions between the Spaniards and the indigenous peoples described by Christopher Columbus and Bartholomew de las Casas can be attributed to Columbus emphasizing the docility and lack of weapons of the Taino people, while Las Casas highlighted the atrocities committed by some Spaniards towards the indigenous population.",
    [
      "about the treatment and rights of indigenous peoples in the Americas. In 1552, the Dominican friar Bartolom\u00e9 de las Casas published the \"Brev\u00edsima relaci\u00f3n de la destrucci\u00f3n de las Indias\" (\"A Short Account of the Destruction of the Indies\"), an account of atrocities committed by landowners and some officials during the early period of colonization of New Spain, particularly in Hispaniola (today Haiti and the Dominican Republic). Las Casas, son of the merchant Pedro de las Casas who accompanied Columbus on his second voyage, described Columbus's treatment of the natives in his \"History of the Indies\". Their description of Spanish",
      "people could fit into a single canoe. The Taino came in contact with the Caribs, another indigenous tribe, often. The caribs lived mostly in modern day Puerto Rico and northeast Hispaniola and were known to be hostile towards other tribes. The Arawak/Taino people had to defend themselves using bow and arrows with poisoned tips and some war clubs. When Columbus landed on Hispaniola, many Taino leaders wanted protection from the Caribs. Christopher Columbus inadvertently landed on the island during his first voyage across the Atlantic in 1492, where his flagship, the \"Santa Maria\", sank after running aground on December 25.",
      "on the natives' interaction with the Spaniards, underlining their docility and amenability and other points relevant for the prospects of successful future colonization (religion, exchange, notions of property, work capacity). In emphasizing their timidity and lack of weapons, Columbus may have had in mind the long and painful Spanish conquest of the Canary Islands, which had been fiercely resisted by the aboriginal Guanches, and perhaps sought to underline that such difficulties would not likely be encountered in the Indies islands. The existence of the Caribs\u2014the prospect of warlike cannibals would surely be discouraging to colonization\u2014is promptly dismissed by Columbus as"
    ]
  ],
  [
    "The introduction of Scribbling-Machines in the textile industry in Leeds in 1786 led to poor working conditions, including low wages, child labour, and long work days, impacting the employment and living conditions of workers.",
    [
      "low wages, child labour, and 18-hour work days. Richard Arkwright created a textile empire by building a factory system powered by water, which was occasionally raided by the Luddites, weavers put out of business by the mechanization of textile production. In the 1790s, James Watt's steam power was applied to textile production, and by 1839 200,000 children worked in Manchester's cotton mills. Karl Marx, who frequently visited Lancashire, may have been influenced by the conditions of workers in these mills in writing \"Das Kapital\". Anglo-French warfare in the early 1790s restricted access to continental Europe, causing the United States to",
      "mills by 1802. The mechanisation of the spinning process in the early factories was instrumental in the growth of the machine tool industry, enabling the construction of larger cotton mills. Limited companies were developed to construct mills, and the trading floors of the cotton exchange in Manchester, created a vast commercial city. Mills generated employment, drawing workers from largely rural areas and expanding urban populations. They provided incomes for girls and women. Child labour was used in the mills, and the factory system led to organised labour. Poor conditions became the subject of expos\u00e9s, and in England, the Factory Acts",
      "water powered cotton factories (two or three times the size of a cottage) can be traced to 1782. The construction of more mills followed\u2014ten by 1789\u2014facilitating a process of urbanisation and socioeconomic transformation in the region; the population moved away from farming, adopting employment in the factory system. The introduction of the factory system led to an increase of the township's population; from 872 in 1714 to 3,500 in 1801, mostly as a result of an influx of people from Yorkshire and Lancashire looking for employment in the cotton mills. Power looms introduced in the early 19th century put an"
    ]
  ],
  [
    "The evidence in the passage that supports the view of Florence as a cultural and economic powerhouse during the Renaissance includes its dominance in the region through annexations, purchases, and the establishment of Livorno as its harbor (History of Italy).",
    [
      "universities. Venice entered in control of the mediterranean trade routes and consolidated a maritime empire after the Sack of Constantinople of the Fourth Crusade and the Venetian-Genoese wars. Furthermore, Frederick of Sicily made Italy the cultural and strategic centre of a large reign that included Imperial Germany and, following the Sixth Crusade, the Kingdom of Jerusalem. These socio-economic factors paved the way for the beginning of the Renaissance in Florence, Tuscany, in the 14th century. Renaissance philosophy, art, science and exploration marked the transition to the modern era and notable figures such as Leonardo, Dante, Giotto, Marco Polo, Machiavelli, Michelangelo,",
      "the feudal system declined. This spurred the growth of towns and cities in the West and improved the economy of Europe. This, in turn helped begin a cultural movement in the West known as the Renaissance, which began in Italy. Italy was dominated by city-states, many of which were nominally part of the Holy Roman Empire, and were ruled by wealthy aristocrats like the Medicis, or in some cases, by the pope. The Renaissance, originating from Italy, ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. The merchant cities of Florence,",
      "from 45% to 75% of its population in the first year.\" In 1630, Florence and Tuscany were once again ravaged by the plague. Tuscany, especially Florence, is regarded as the birthplace of the Renaissance. Though \"Tuscany\" remained a linguistic, cultural and geographic conception, rather than a political reality, in the 15th century, Florence extended its dominion in Tuscany through the annexation of Arezzo in 1384, the purchase of Pisa in 1405 and the suppression of a local resistance there (1406). Livorno was bought in 1421 and became the harbour of Florence. From the leading city of Florence, the republic was"
    ]
  ],
  [
    "Herbert Spencer advocated for minimal government assistance for the poor, believing in the concept of \"survival of the fittest\" where societies are in a struggle for survival and only the strong should survive.",
    [
      "also integrated into the social and political philosophies of English thinker Herbert Spencer and American philosopher William Graham Sumner. Herbert Spencer, who coined the oft-misattributed term \"survival of the fittest,\" believed that societies were in a struggle for survival, and that groups within society are where they are because of some level of fitness. This struggle is beneficial to human kind, as in the long run the weak will be weeded out and only the strong will survive. This position is often referred to as Social Darwinism, though it is distinct from the eugenics movements with which social darwinism is",
      "thought as the work of Charles Darwin became known among intellectuals. Following Darwin's idea of natural selection, English philosopher Herbert Spencer proposed the idea of social Darwinism. This new concept justified the stratification of the wealthy and poor, and it was in this proposal that Spencer coined the term \"survival of the fittest\". Joining Spencer was Yale professor William Graham Sumner whose book \"What Social Classes Owe to Each Other\" (1884) argued that assistance to the poor actually weakens their ability to survive in society. Sumner argued for a laissez-faire and free-market economy. Few people, however, agreed with the social",
      "work together to preserve society. Perhaps Spencer's greatest obstacle that is being widely discussed in modern sociology is the fact that much of his social philosophy is rooted in the social and historical context of ancient Egypt. He coined the term \"survival of the fittest\" in discussing the simple fact that small tribes or societies tend to be defeated or conquered by larger ones. Of course, many sociologists still use his ideas (knowingly or otherwise) in their analyses, especially due to the recent re-emergence of evolutionary theory. Talcott Parsons began writing in the 1930s and contributed to sociology, political science,"
    ]
  ],
  [
    "The aim of the Carlsbad Resolutions adopted by the Germanic States in 1819 was to suppress liberal and nationalist movements, as well as to prevent revolution and maintain conservative control over the German Confederation.",
    [
      "German Question \"The German Question\" was a debate in the 19th century, especially during the Revolutions of 1848, over the best way to achieve the unification of Germany. From 1815 to 1866, about 37 independent German-speaking states existed within the German Confederation. The ' (\"Greater German solution\") favored unifying all German-speaking peoples under one state, and was promoted by the Austrian Empire and its supporters. The ' (\"Lesser German solution\") sought only to unify the northern German states and did not include Austria; this proposal was favored by the Kingdom of Prussia. The solutions are also referred to by the",
      "Prussia, acquiring Holstein with the important naval harbour of Kiel or controlling the entrance to the Baltic. There was also the national question: both Germany and Denmark wished, characteristically of the nineteenth century, to create and consolidate nationalities from a background of fragmented cultural practices and dialects. Lastly, there was the international question: the rival ambitions of the German powers involved, and beyond them the interests of other European states, notably that of the United Kingdom in preventing the rise of a German sea-power in the north. German had been the language of government in Schleswig and Holstein while more-or-less",
      "for the rising influence of Prussia. After the so-called \"Wars of Liberation\" (\"Befreiungskriege\", the German term for the German part of the War of the Sixth Coalition), many contemporaries had expected a nation-state solution and thus considered the subdivision of Germany as unsatisfactory. Apart from this nationalist component, calls for civic rights influenced political discourse. The Napoleonic \"Code Civil\" had led to the introduction of civic rights in some German states in the early 19th century. Furthermore, some German states had adopted constitutions after the foundation of the German Confederacy. Between 1819 and 1830, the Carlsbad Decrees and other instances"
    ]
  ],
  [
    "Bakunin believed that revolutions must be led by the people directly and rejected the concept of the \"dictatorship of the proletariat\", emphasizing the potential revolutionary role of peasants, the lumpenproletariat, and marginalized individuals in the working class movement.",
    [
      "and Marxism. Bakunin argued\u2014against certain ideas of a number of Marxists\u2014that not all revolutions need be violent. He also strongly rejected Marx's concept of the \"dictatorship of the proletariat\", a concept that vanguardist socialism including Marxism\u2013Leninism would use to justify one-party rule from above by a party claiming to represent the proletariat. Bakunin insisted that revolutions must be led by the people directly while any \"enlightened elite\" must only exert influence by remaining \"invisible...not imposed on anyone...[and] deprived of all official rights and significance\". He held that the state should be immediately abolished because all forms of government eventually lead",
      "Marx the proletariat was the exclusive, leading revolutionary agent while Bakunin entertained the possibility that the peasants and even the lumpenproletariat (-proletarians in rags- the unemployed, common criminals, etc.) could rise to the occasion.\" Bakunin \"considers workers' integration in capital as destructive of more primary revolutionary forces. For Bakunin, the revolutionary archetype is found in a peasant milieu (which is presented as having longstanding insurrectionary traditions, as well as a communist archetype in its current social form\u2014the peasant commune) and amongst educated unemployed youth, assorted marginals from all classes, brigands, robbers, the impoverished masses, and those on the margins of",
      "Bakunin also had a different view as compared to Marx's on the revolutionary potential of the lumpenproletariat and the proletariat. As such, \"[b]oth agreed that the proletariat would play a key role, but for Marx the proletariat was the exclusive, leading revolutionary agent while Bakunin entertained the possibility that the peasants and even the lumpenproletariat (the unemployed, common criminals, etc.) could rise to the occasion\". Bakunin \"considers workers' integration in capital as destructive of more primary revolutionary forces. For Bakunin, the revolutionary archetype is found in a peasant milieu (which is presented as having longstanding insurrectionary traditions, as well as"
    ]
  ],
  [
    "Source 1 is referencing Marxist economic theory in support of the expansion and use of machines in the manufacturing sector.",
    [
      "industries. As such Marx cites the growth of the domestic service industry equated to greater servitude by the exploited classes. The political economist apology for the displacement of workers by machinery asserts that there is a corresponding increase in employment. Marx is quick to cite the example of the silk industry in which an actual decrease of employment appears simultaneously with an increase of existing machinery. On the other hand, an increase in the number of factory workers employed is the result of \"the gradual annexation of neighboring branches of industry\" and \"the building of more factories or the extension",
      "capitalist exploitation without the ability to procure the means of subsistence for survival. Furthermore, Marx argues that the introduction of machinery may increase employment in other industries, yet this expansion \"has nothing in common with the so-called theory of compensation.\" Greater productivity will necessarily generate an expansion of production into peripheral fields that provide raw materials. Conversely, machinery introduced to industries that produce raw materials will lead to an increase in those industries that consume them. The production of greater surplus-value leads to greater wealth of the ruling classes, an increase in the labor-market, and consequently the establishment of new",
      "value, properly understood, a more logically sound basis for political economic reasoning. Foley also discusses the chapter \"On Machinery\", which Ricardo included in his third and final (1821) version of \"Principles\". Here Ricardo famously analysed the impact of the adoption of machinery on the different classes of society, revising his earlier view that mechanization could be expected to be of benefit to each of the classes of the society. The increase in productivity due to mechanization lowers the production costs and thus also the real prices of commodities. Whereas the landowning class and capitalists benefit from the lower prices, workers"
    ]
  ],
  [
    "Marx and Engels are trying to convey the message that the history of all hitherto existing society is the history of class struggles, and that the ultimate goal of communism is to overthrow the bourgeoisie and establish a classless society.",
    [
      "The Communist Manifesto The Communist Manifesto (originally Manifesto of the Communist Party) is an 1848 political pamphlet by the German philosophers Karl Marx and Friedrich Engels. Commissioned by the Communist League and originally published in London (in German as \"Manifest der Kommunistischen Partei\") just as the revolutions of 1848 began to erupt, the \"Manifesto\" was later recognised as one of the world's most influential political documents. It presents an analytical approach to the class struggle (historical and then-present) and the conflicts of capitalism and the capitalist mode of production, rather than a prediction of communism's potential future forms. \"The Communist",
      "Milton\u2019s republicanism, Warren continues, served as \"a useful, if unlikely, bridge\" as Marx and Engels sought to forge a revolutionary international coalition. The Communist Manifesto The Communist Manifesto (originally Manifesto of the Communist Party) is an 1848 political pamphlet by the German philosophers Karl Marx and Friedrich Engels. Commissioned by the Communist League and originally published in London (in German as \"Manifest der Kommunistischen Partei\") just as the revolutions of 1848 began to erupt, the \"Manifesto\" was later recognised as one of the world's most influential political documents. It presents an analytical approach to the class struggle (historical and then-present)",
      "Engels wrote to Marx recommending a further re-draft, in historical prose: Following the second congress of the Communist League, it commissioned Marx to write a final program. Drawing directly upon the ideas in \"Principles of Communism\", Marx delivered a final revision, the \"Manifesto\", in early 1848. Although Marx was the exclusive author of the \"Manifesto's\" manuscript, the ideas were adapted from Engels' earlier drafts, with the result that the \"Manifesto\" was credited to both authors. Beginning with a definition of communism as a political theory for the liberation of the proletariat, Engels provides a brief history of the proletariat as"
    ]
  ],
  [
    "The term \"quid\" was coined by Isabella Beeton in 1851 in her influential manual \"Mrs Beeton's Book of Household Management.\"",
    [
      "well. From 1772, the silver coins had a denomination in function of their value in gold, and had significantly less silver than their face value (rather than being just silver-by-weight) so as to cover coinage expenses, a practice known as token or fiduciary coinage, and a characteristic of modern coinage. This technique was introduced later in England, in 1816, with its adoption of the full gold standard. At the market rates of 1858 10 silver units could be exchanged for 1 gold unit by weight, whereas the face value of silver units was only convertible at 5 to 1. This",
      "Penny (British decimal coin) The British decimal one penny (1p) coin, usually simply known as a penny, is a unit of currency equaling one one-hundredth of a pound sterling. The penny\u2019s symbol is p. Its obverse has featured the profile of Queen Elizabeth II since the coin\u2019s introduction on 15 February 1971, the day British currency was decimalised. Four different portraits of the Queen have been used on the coin, with the latest design by Jody Clark being introduced in 2015. The second and current reverse, featuring a segment of the Royal Shield, was introduced in 2008. The correct plural",
      "\"groat\", although the coin of that name was withdrawn in the 19th century. A sixpenny bit was a \"tanner\", known in Australia as a \"zack\". One shilling was a \"bob\", and a pound a \"nicker\" or a \"quid\". The term \"quid\" is said to originate from the Latin phrase quid pro quo. A pound note was also sometimes called simply \"a note\" (e.g., \"You owe me 50 notes\"). A ten-shilling note was sometimes known as \"half a bar\". A two-shillings-and-sixpence piece, in use until the introduction of decimal currency, was known as \"half a crown\" or \"a half crown\". Crown"
    ]
  ],
  [
    "Copernicus argued for the spherical nature of the cosmos and the earth based on the mathematical model of Eudoxus, the heliocentric theory proposed by Aristarchus of Samos, and the prevailing belief in the circular motions of celestial bodies as described by Plato, Aristotle, and Ptolemy.",
    [
      "than with developing an explanation of the reasons for the motions of the Cosmos. In his \"Timaeus\", Plato described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul. Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the earth. This basic cosmological model prevailed, in various forms, until the 16th century. In the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric",
      "\"The physicist proves the Earth to be round by one means, the astronomer by another: for the latter proves this by means of mathematics, e. g. by the shapes of eclipses, or something of the sort; while the former proves it by means of physics, e. g. by the movement of heavy bodies towards the center, and so forth.\" Lectures in the medieval universities commonly advanced evidence in favor of the idea that the Earth was a sphere. Also, \"On the Sphere of the World\", an influential astronomy textbook of the 13th century, commonly studied by students at Western European",
      "stars, the sun, the moon, and the planets were carried in their orbits by a complicated arrangement of crystalline spheres, all centered around an immobile earth. The Platonic-Aristotelian conviction that celestial motions must be circular persisted stubbornly. It was fundamental to the astronomer Ptolemy's system, which improved on Aristotle's in conforming to the astronomical data by allowing the planets to move in combinations of circles called \"epicycles\". It even survived the Copernican revolution. Copernicus was conservative in his Platonic reverence for the circle as the heavenly pattern. According to Weinberg, Copernicus was motivated to dethrone the earth in favor of"
    ]
  ],
  [
    "During the Roman era, the nobility's fondness for entertainment was taken to its most outrageous degree, with examples such as Emperor Claudius adding crushed pearls to wine and live animals being served as shows of entertainment and richness.",
    [
      "The nobility mostly drank wine. However, wine drunk during feasts was not as strong as what we drink today as it was watered down. But it is a common fact that lots of alcohol was consumed. Even a century later when Beatrix of Aragon came to Upper Hungary she was shocked by the amount of alcohol consumption and by the horrible way in which the festivities were held. She had arrived from civilized Italy so she experienced a culture shock when she took part in the feasts at a Hungarian court for the first time. Duke Stibor definitely had jesters",
      "Jester A jester, court jester, or fool, was historically an entertainer during the medieval and Renaissance eras who was a member of the household of a nobleman or a monarch employed to entertain him and his guests. A jester was also an itinerant performer who entertained common folk at fairs and markets. Jesters are also modern-day entertainers who resemble their historical counterparts. Jesters in medieval times are often thought to have worn brightly coloured clothes and eccentric hats in a motley pattern and their modern counterparts usually mimic this costume. Jesters entertained with a wide variety of skills: principal among",
      "exoticism and high price. Some ancient writers recount Emperor Claudius adding crushed pearls to wine and flecks of gold to peas solely to increase their cost. Others recall live animals being served as shows of entertainment and richness. For instance, at one event mackerels were pickled live in order to showcase their silvery bodies thrashing in vinegar. Overall, Roman aristocrats desired to showcase their wealth and luxury through the presentation of food. Medieval aristocrats also desired to entertain and impress through food. Banquets were usually huge feasts with diverse choices of dishes. Social etiquette dictated that the wealthy and powerful"
    ]
  ],
  [
    "De las Casas believes that the primary motive for the Spanish treatment of indigenous peoples in the Americas was greed for gold and wealth, leading to extreme violence and atrocities committed against the native populations.",
    [
      "about the treatment and rights of indigenous peoples in the Americas. In 1552, the Dominican friar Bartolom\u00e9 de las Casas published the \"Brev\u00edsima relaci\u00f3n de la destrucci\u00f3n de las Indias\" (\"A Short Account of the Destruction of the Indies\"), an account of atrocities committed by landowners and some officials during the early period of colonization of New Spain, particularly in Hispaniola (today Haiti and the Dominican Republic). Las Casas, son of the merchant Pedro de las Casas who accompanied Columbus on his second voyage, described Columbus's treatment of the natives in his \"History of the Indies\". Their description of Spanish",
      "people of the Americas. In his attempt to defend the indigenous people, he argues that they are part of the human race by describing their bodies, skin color, language and culture. In \"A Short Account\", De Las Casas racialized the indigenous people and created a new understanding for them in the context and hierarchy of European ideas of race. His account was largely responsible for the passage of the new Spanish colonial laws known as the New Laws of 1542, which abolished native slavery for the first time in European colonial history and led to the Valladolid debate. This text",
      "anti-colonial stance, on account of the extreme violence inflicted on the indigenous occupants of the islands by the Spaniards. He describes the extensive torture, murder, and mutilation of the Natives, referring to them as \"innocent Sheep\" who were \"assaulted\" by the Spanish colonizers. De las Casas noticed that no matter where he visited, the Spanish were committing the same crimes. On the island of Hispaniola, the Spanish were herding people into a straw building and setting fire to it, burning the occupants alive. In addition, \u201cthey sent the Males to the Mines to dig and bring away the Gold, which"
    ]
  ],
  [
    "Germany did not differ significantly from other European nations in its development, despite the debate surrounding the concept of a \"Sonderweg.\" (Winkler, Fischer)",
    [
      "different course in comparison with the other nations of the West, which had a normal development of their histories. The German historian Heinrich August Winkler wrote about the question of there being a : \"For a long time, educated Germans answered it in the positive, initially by laying claim to a special German mission, then, after the collapse of 1945, by criticizing Germany's deviation from the West. Today, the negative view is predominant. Germany did not, according to the now prevailing opinion, differ from the great European nations to an extent that would justify speaking of a 'unique German path'.",
      "that would justify speaking of a 'unique German path.' And, in any case, no country on earth ever took what can be described as the 'normal path.' Fritz Fischer (1908 \u2013 1999) was best known for his analysis of the causes of World War I. In the early 1960s Fischer published Germany's Aims in the First World War. He put forward the controversial thesis that responsibility for the outbreak of the war rested solely with Imperial Germany. That set off a long debate that reverberates into the 21st century. He has been described by \"The Encyclopedia of Historians and Historical",
      "was much contingency and chance in German history. In addition, there was much debate within the supporters of the \"Sonderweg\" concept as for the reasons for the \"Sonderweg\", and whether or not the \"Sonderweg\" ended in 1945. Was there a Sonderweg? Winkler says: For a long time, educated Germans answered it in the positive, initially by laying claim to a special German mission, then, after the collapse of 1945, by criticizing Germany's deviation from the West. Today, the negative view is predominant. Germany did not, according to the now prevailing opinion, differ from the great European nations to an extent"
    ]
  ],
  [
    "The skill of proficiency in sports, telling jokes, fighting, writing poetry, playing music, drawing, and dancing, as recommended by Castiglione, would be valued least by a European gentleman at the end of the twentieth century.",
    [
      "of Castiglione's ideal Renaissance gentleman was not self-cultivation for its own sake but in order to participate in an active life of public service, as recommended by Cicero. To do this he had to win the respect and friendship of his peers and most importantly of a ruler, or prince, i.e., he had to be a courtier, so as to be able to offer valuable assistance and disinterested advice on how to rule the city. He must be a worthy friend, accomplished\u2014in sports, in telling jokes, in fighting, writing poetry, playing music, drawing, and dancing\u2014but not too much. To his",
      "1507, while Castiglione was supposedly absent on an embassy to England. It addresses the topic, proposed by Federigo Fregoso, of what constitutes an ideal Renaissance gentleman. In the Middle Ages, the perfect gentleman had been a chivalrous knight who distinguished himself by his prowess on the battlefield. Castiglione's book changed that. Now the perfect gentleman had to have a classical education in Greek and Latin letters, as well. The Ciceronian humanist model of the ideal orator (whom Cicero called \"the honest man\"), on which \"The Courtier\" is based, prescribes for the orator an active political life of service to country,",
      "informed by ambition, intrigue, etc. Concerning \"sprezzatura\", Castiglione said: Thus, \"sprezzatura\" was essential to becoming the ideal courtier. \"Sprezzatura\" was a vital quality for a courtier to have. Courtiers essentially had to put on a performance for their peers and those who employed \"sprezzatura\" created the impression that they completely mastered the roles they played. A courtier's \"sprezzatura\" made him seem to be fully at ease in court and like someone who was \"the total master of self, society's rules, and even physical laws, and [his \"sprezzatura\" created] the distinct impression that he [was] unable to err\". However, while the"
    ]
  ],
  [
    "Pico della Mirandola actively participated in the Renaissance cultural movement.",
    [
      "there even a non-political previous generation at all? 1968 movement in Italy The 1968 movement in Italy or Sessantotto was inspired by distaste for traditional Italian society and international protests. In May 1968 all universities, except Bocconi, were occupied. In the same month a hundred artists, including Gio Pomodoro, Arnaldo Pomodoro, Ernesto Treccani and Gianni Dova occupied for 15 days the Palazzo della Triennale. The background of the movement came from the newly transformed economy of Italy. The country had recently increased industrialization and a new modern culture began to develop. The movement has its roots in the strikes and",
      "time later he was welcomed by the parish priest of St. Benedict at Port of Genoa, Don Federico Rebora, and together with a small group he established the \"Comunit\u00e0 di San Benedetto al Porto\". Since then he has committed to pacifism and to the recovery of marginalized people, even advocating for the legalization of soft drugs: in 2006 Gallo was fined for smoking marijuana in the town hall of Genoa to protest against the law on drugs in Italy. He actively supported the movement \"No Dal Molin\" of Vicenza against the construction of a new U.S. military base in Padua.",
      "Valla's contemporaries paid little attention to it, his \"Disputations\" foreshadows what we now call \"philosophy of language\". Both Valla and Pico lived during a great era of Italian intellectual life, starting with Dante, Petrarch and Boccaccio in the fourteenth century and ending with Giordano Bruno, Tommaso Campanella and Galileo in the seventeenth century. In the nineteenth century, Italian philosophers worked out the grand narrative of Italian thought in this earlier period \u2013 the Renaissance \u2013 and afterward: the construction of this story as an artifact of modern Italian politics, especially the Risorgimento and the Fascist regime, is another topic that"
    ]
  ],
  [
    "The study of Plato influenced Western philosophy during the Renaissance and early modern period by inspiring scholars to delve deeper into his ideas and incorporate them into their own philosophical theories.",
    [
      "Eastern and Western mysticism. Meanwhile, Platonism influenced various philosophers. While Aristotle became more influential than Plato in the 13th century, St. Thomas Aquinas's philosophy was still in certain respects fundamentally Platonic. With the Renaissance, scholars became more interested in Plato himself. In 16th-, 17th-, and 19th-century England, Plato's ideas influenced many religious thinkers. Orthodox Protestantism in continental Europe, however, distrusts natural reason and has often been critical of Platonism. An issue in the reception of Plato in early modern Europe was how to deal with the same-sex elements of his corpus. \"Christoplatonism\" is a term used to refer to a",
      "period, at which point Ancient Greece was incorporated into the Roman Empire. It dealt with a wide variety of subjects, including political philosophy, ethics, metaphysics, ontology, logic, biology, rhetoric, and aesthetics. Plato was a Classical Greek philosopher, mathematician and writer of philosophical dialogues. He was the founder of the Academy in Athens which was the first institution of higher learning in the Western world. Inspired by the admonition of his mentor, Socrates, prior to his unjust execution that \"the unexamined life is not worth living\", Plato and his student, the political scientist Aristotle, helped lay the foundations of Western philosophy",
      "of a series of footnotes to Plato\". Clear, unbroken lines of influence lead from ancient Greek and Hellenistic philosophers to Early Islamic philosophy, the European Renaissance and the Age of Enlightenment. Some claim that Greek philosophy was in turn influenced by the older wisdom literature and mythological cosmogonies of the ancient Near East, though this is debated. Martin Litchfield West gives qualified assent to this view by stating that \"contact with oriental cosmology and theology helped to liberate the early Greek philosophers' imagination; it certainly gave them many suggestive ideas. But they taught themselves to reason. Philosophy as we understand"
    ]
  ],
  [
    "Elie Wiesel believed that the current generation of Germans should be held responsible for maintaining the memory of the Holocaust and not allowing it to be forgotten.",
    [
      "we must all pitch in order to protect it.\" Other Holocaust survivors both inside and outside of Israel were shocked about the rising anti-Semitism masked as anti-Israel criticism, with one saying \"It's ok not to agree with the Israeli government, like lots of people do over here. But if they are yelling 'kill all Jews' during protests, you haven't learned anything from the past.\" and another saying \"I am deeply worried about the future of my children and grandchildren. Most of the elderly people are afraid. Everybody hates us. We are being surrounded by danger.\" Nobel laureate Elie Wiesel said",
      "satisfactory explanations for the existence of both evil and an all-powerful, all-good, and infallible God in the universe. These convenient logical arguments could not provide sufficient solace for a Jewish people emerging from the horrors of the Holocaust. Many scholars contend that the enormous tragedy of the Holocaust represents an entirely new category of evil that one could not explain with traditional Jewish theology. The preeminent survivor-novelist Elie Wiesel (1928-2016) raises a variety of unanswerable questions about the Holocaust in his novels, such as the best-selling \"Night\" (1958). Many Jews, whether they were survivors or not, experienced a loss of",
      "exerted by historical memory of the Nazi past. Nolte concluded that there was excessive contemporary interest in the Holocaust because it served the concerns of those descended from the victims of Nazism, and placed them in a \"permanent status of privilege\". Nolte argued that Germans had an unhealthy obsession with guilt for Nazi crimes, and called for an end to this \"obsession\". Nolte's opinion was that there was no moral difference between German self-guilt over the Holocaust, and Nazi claims of Jewish collective guilt for all the world's problems. He called for an end to the maintaining of the memory"
    ]
  ],
  [
    "The Molotov-Ribbentrop Pact allowed Germany and the Soviet Union to freely invade Poland.",
    [
      "so that \"she must be brought to her knees as soon as possible.\" Germany broke the Molotov\u2013Ribbentrop Pact in June 1941 by invading the Soviet Union. During the summer of 1939, after conducting negotiations with both a British-French group and Germany regarding potential military and political agreements, the Soviet Union chose Germany, resulting in an August 19 German\u2013Soviet Commercial Agreement providing for the trade of certain German military and civilian equipment in exchange for Soviet raw materials. Four days later, the countries signed the Molotov\u2013Ribbentrop Pact, which contained secret protocols dividing the states of Northern and Eastern Europe into German",
      "Soviets, and signed the Molotov\u2013Ribbentrop Pact, which allowed the Soviets to invade the Baltic states and parts of Poland and Romania. Germany invaded Poland on 1 September 1939, prompting France and the United Kingdom to declare war on Germany on 3 September, opening the European Theatre of World War II. The Soviet invasion of Poland started on 17 September and Poland fell soon thereafter. On 24 September, the Soviet Union attacked the Baltic countries and later, Finland. The British hoped to land at Narvik and send troops to aid Finland, but their primary objective in the landing was to encircle",
      "annexed the three Baltic states (Estonia, Latvia and Lithuania). The Molotov\u2013Ribbentrop Pact ostensibly provided security to the Soviets in the occupation both of the Baltics and of the north and northeastern regions of Romania (Northern Bukovina and Bessarabia, June\u2013July 1940), although Hitler, in announcing the invasion of the Soviet Union, cited the Soviet annexations of Baltic and Romanian territory as having violated Germany's understanding of the Pact. Moscow partitioned the annexed Romanian territory between the Ukrainian and Moldavian Soviet republics. Adolf Hitler had argued in his autobiography \"Mein Kampf\" (1925) for the necessity of \"Lebensraum\" (\"living space\"): acquiring new territory"
    ]
  ],
  [
    "By the 1800s, empirical reasoning had evolved from Francis Bacon's complex method of induction outlined in Novum Organum to a more straightforward approach based on the central role of reason and the deduction of general propositions from phenomena, as exemplified by the works of Descartes and Newton.",
    [
      "Baconian method The Baconian method is the investigative method developed by Sir Francis Bacon. The method was put forward in Bacon's book \"Novum Organum\" (1620), or 'New Method', and was supposed to replace the methods put forward in Aristotle's \"Organon\". This method was influential upon the development of the scientific method in modern science; but also more generally in the early modern rejection of medieval Aristotelianism. Bacon's method is an example of the application of inductive reasoning. However, Bacon's method of induction is much more complex than the essential inductive process of making generalizations from observations. Bacon's method begins with",
      "allusion to Aristotle's \"Organon\" \u2013 Bacon outlined a new system of logic to improve upon the old philosophical process of syllogism. Bacon's method relied on experimental \"histories\" to eliminate alternative theories. In 1637, Ren\u00e9 Descartes established a new framework for grounding scientific knowledge in his treatise, \"Discourse on Method\", advocating the central role of reason as opposed to sensory experience. By contrast, in 1713, the 2nd edition of Isaac Newton's \"Philosophiae Naturalis Principia Mathematica\" argued that \"... hypotheses ... have no place in experimental philosophy. In this philosophy[,] propositions are deduced from the phenomena and rendered general by induction. \"",
      "true, simply \"corroborate\" the theory. In 1620 in England, Francis Bacon's \"Novum Organum\" alleged that scholasticism's Aristotelian method of deductive inference via syllogistic logic upon traditional categories was impeding society's progress. Admonishing the alleged classic induction for proceeding immediately from \"sense and particulars up to the most general propositions\", then deducing generalizations onto new particulars without empirically verifying them, Bacon stated the \"true and perfect Induction\". In Bacon's inductivist method, a scientist\u2014at the time, a \"natural philosopher\"\u2014ventures an axiom of modest scope, makes many observations, accepts the axiom if it is confirmed and never disconfirmed, then ventures another axiom only"
    ]
  ],
  [
    "Napoleon Bonaparte was most often criticized for his economic policies, including lowering tariffs and increasing government debt.",
    [
      "a series of conflicts throughout Europe and ignored treaties and conventions alike. His role in the Haitian Revolution and decision to reinstate slavery in France's overseas colonies are controversial and affect his reputation. Napoleon institutionalised plunder of conquered territories: French museums contain art stolen by Napoleon's forces from across Europe. Artefacts were brought to the Mus\u00e9e du Louvre for a grand central museum; his example would later serve as inspiration for more notorious imitators. He was compared to Adolf Hitler most famously by the historian Pieter Geyl in 1947 and Claude Ribbe in 2005. David G. Chandler, a foremost historian",
      "protected the pope in Rome. The British grew annoyed at Napoleon's humanitarian intervention in Syria in 1860\u201361. Napoleon lowered the tariffs, which helped in the long run but in the short run angered owners of large estates and the textile and iron industrialists, while leading worried workers to organize. Matters grew worse in the 1860s as Napoleon nearly blundered into war with the United States in 1862, while his takeover of Mexico in 1861\u201367 was a total disaster. The puppet emperor he put on the Mexican throne was overthrown and executed. Finally in the end he went to war with",
      "and textile industries, were unhappy, because he had reduced the tariffs on British products, putting the British products in direct competition with their own. The members of Parliament were particularly unhappy with him for dealing with them only when he needed money. When he had liberalized trade with England, he had not even consulted them. Napoleon's large-scale program of public works, and his expensive foreign policy, had created rapidly mounting government debts; the annual deficit was about 100 million gold-francs, and the cumulative debt had reached nearly 1,000 million gold-francs (1 billion in US readings). The Emperor needed to restore"
    ]
  ],
  [
    "Elie Wiesel believes that remembering the Holocaust is essential for the future because it provides insights into the inhumanity of man and helps to protect Jewish existentialism.",
    [
      "predecessor of the current Council), which our distinguished chairman, Elie Wiesel, submitted to the President, specifically stated: \u201cThe universality of the Holocaust lies in its uniqueness: the event is essentially Jewish, its interpretation is universal.\u201d This statement informs the philosophy of the work of the Council. The projected Holocaust Museum will be concerned with the Jewish tragedy. It is unique in the annals of human history. However, as with all gravely momentous events, the particular illuminates the universal aspects of human existence. It is through contemplation of the Jewish Holocaust that some insights are gained about the inhumanity of man",
      "satisfactory explanations for the existence of both evil and an all-powerful, all-good, and infallible God in the universe. These convenient logical arguments could not provide sufficient solace for a Jewish people emerging from the horrors of the Holocaust. Many scholars contend that the enormous tragedy of the Holocaust represents an entirely new category of evil that one could not explain with traditional Jewish theology. The preeminent survivor-novelist Elie Wiesel (1928-2016) raises a variety of unanswerable questions about the Holocaust in his novels, such as the best-selling \"Night\" (1958). Many Jews, whether they were survivors or not, experienced a loss of",
      "we must all pitch in order to protect it.\" Other Holocaust survivors both inside and outside of Israel were shocked about the rising anti-Semitism masked as anti-Israel criticism, with one saying \"It's ok not to agree with the Israeli government, like lots of people do over here. But if they are yelling 'kill all Jews' during protests, you haven't learned anything from the past.\" and another saying \"I am deeply worried about the future of my children and grandchildren. Most of the elderly people are afraid. Everybody hates us. We are being surrounded by danger.\" Nobel laureate Elie Wiesel said"
    ]
  ],
  [
    "Preventive measures to address future famines and hunger crises could include immediate responses to droughts, long-term planning, and the creation and implementation of pre-famine indicators and measurement systems to assess food security and nutrition levels (Esther Ngumbi, 2017 Somalian drought, World Food Programme, FSAU).",
    [
      "famine conditions, it is government action or inaction that determines its severity, and often even whether or not a famine will occur. The 20th century has examples of governments, as in Collectivization in the Soviet Union or the Great Leap Forward in the People's Republic of China undermining the food security of their own nations. Mass starvation is frequently a weapon of war, as in the blockade of Germany, the Battle of the Atlantic, and the blockade of Japan during World War I and World War II and in the Hunger Plan enacted by Nazi Germany. Governments sometimes have a",
      "in the coming weeks. He also stated that the omission of such an immediate response \"will cost lives, further destroy livelihoods, and could undermine the pursuit of key State-building and peacebuilding initiatives\". On 8 March 2017 United Nations Secretary-General Ant\u00f3nio Guterres urged a massive scale up in international support to avert a famine. While the particular drought can only be dealt with by an immediate response some suggest foresight and preventive, long-term, more cost-efficient and appropriate measures. Esther Ngumbi, researcher at the Department of Entomology and Plant Pathology at Auburn University in Alabama, suggests that Horn of Africa's repetitive cycles",
      "each of which is linked to a planned response to mitigate the crisis and try to prevent a worsening of the situation. International organizations responding to recent food crises created \"ad hoc\" measurements. In 2002, the World Food Programme created a number of \"pre-famine indicators\" for Ethiopia and combined it with measurements of nutrition levels to create recommendations. The Food Security Assessment Unit (FSAU) devised a system for Somalia with four levels: Non-alert (near normal), Alert (requires close attention), Livelihood Crisis (basic social structures under threat) and Humanitarian Emergency (threat of widespread mortality requiring immediate humanitarian assistance). The FSAU system"
    ]
  ],
  [
    "Pangloss managed to survive being hanged, dissected, whipped, and sent to the galleys in Candide by luck and circumstance, similar to Dirk Willems escaping using a rope made out of knotted rags and Mackandal evading capture by the French.",
    [
      "led to his condemnation by the Roman Catholic Church in the Netherlands and subsequent arrest. Willems was held in a residential palace turned into a prison, from which he escaped using a rope made out of knotted rags. Using this, he was able to climb out of the prison onto the frozen moat. A guard noticed his escape and gave chase. Willems was able to traverse the thin ice of a frozen pond, the \"Hondegat\", because of his lighter weight after subsisting on prison rations. However, the pursuing guard broke through the ice and yelled for help as he struggled",
      "such protections were often ignored by white colonists. A passage from Henri Christophe's personal secretary, who lived more than half his life as a slave, describes the crimes committed against the slaves of Saint-Domingue by the French colonists: Thousands of slaves found freedom by fleeing into the mountains, forming communities of maroons and raiding isolated plantations. The most famous was Mackandal, a one-armed slave, originally from Guinea (region), who escaped in 1751. A Vodou Houngan (priest), he united many of the different maroon bands. For the next six years, he staged successful raids while evading capture by the French. He",
      "to Havana, Cuba. The captives were stripped, chained in groups of five, and packed tightly into the slave hold (a deck below the main deck and above the cargo hold) so that one person's head, when lying in rows, was forced upon another person's thigh. In the ship's dark cargo hold, each slave had of headroom during the ten-week voyage. The captives were sometimes brought up on deck and fed rice. Those who tried to starve themselves, as often happened, were whipped and forced to eat. While they were at sea, water supplies ran low, and disease spread through the"
    ]
  ],
  [
    "The Boyle Lectures contributed to the development of the relationship between Christianity and emerging natural philosophy in European society by demonstrating how natural philosophers could use the design apparent in nature to reveal God's involvement with the world.",
    [
      "thesis\" which assumes that modern science emerged from Christian sources and doctrines. Though he acknowledges that modern science emerged in a religious framework, that Christinaity greatly elevated the importance of science by sanctioning and religiously legitimizing it in medieval period, and that Christianity created a favorable social context for it to grow; he argues that direct Christian beliefs or doctrines were not primary source of scientific pursuits by natural philosophers, nor was Christianity, in and of itself, exclusively or directly necessary in developing or practicing modern science. Oxford University historian and theologian John Hedley Brooke wrote that \"when natural philosophers",
      "medieval period, and that Christianity created a favorable social context for it to grow; he argues that direct Christian beliefs or doctrines were not primary sources of scientific pursuits by natural philosophers, nor was Christianity, in and of itself, exclusively or directly necessary in developing or practicing modern science. Oxford University historian and theologian John Hedley Brooke wrote that \"when natural philosophers referred to \"laws\" of nature, they were not glibly choosing that metaphor. Laws were the result of legislation by an intelligent deity. Thus the philosopher Ren\u00e9 Descartes (1596\u20131650) insisted that he was discovering the \"laws that God has",
      "such as \"Disquisition about the Final Causes of Natural Things\" (1688), for instance, he criticised contemporary philosophers - such as Ren\u00e9 Descartes - who denied that the study of nature could reveal much about God. Instead, Boyle argued that natural philosophers could use the design apparently on display in some parts of nature to demonstrate God's involvement with the world. He also attempted to tackle complex theological questions using methods derived from his scientific practices. In \"Some Physico-Theological Considerations about the Possibility of the Resurrection\" (1675), he used a chemical experiment known as the reduction to the pristine state as"
    ]
  ],
  [
    ", by compiling and interpreting thousands of classical proverbs and adages, played a significant role in spreading classical culture and wisdom in the Renaissance period.",
    [
      "the \"Adagia\" could only have happened via the developing intellectual environment in which careful attention to a broader range of classical texts produced a much fuller picture of the literature of antiquity than had been possible, or desired, in medieval Europe. In a period in which \"sententi\u00e6\" were often marked by special fonts and footnotes in printed texts, and in which the ability to use classical wisdom to bolster modern arguments was a critical part of scholarly and even political discourse, it is not surprising that Erasmus' \"Adagia\" was among the most popular volumes of the century. Source: Erasmus, Desiderius.",
      "the philosophical grounds for much of Renaissance art, music, and science. Erasmus, for example, was important in spreading humanist ideas in the north, and was a central figure at the intersection of classical humanism and mounting religious questions. Forms of artistic expression which a century ago would have been banned by the church were now tolerated or even encouraged in certain circles. The velocity of transmission of the Renaissance throughout Europe can also be ascribed to the invention of the printing press. Its power to disseminate information enhanced scientific research, spread political ideas and generally impacted the course of the",
      "on political and moral topics. The work continued to expand right up to the author's death in 1536 (to a final total of 4,151 entries), confirming the fruit of Erasmus' vast reading in ancient literature. Many of the adages have become commonplace in many European languages, and we owe our use of them to Erasmus. Equivalents in English include: The work reflects a typical Renaissance attitude toward classical texts: to wit, that they were fit for appropriation and amplification, as expressions of a timeless wisdom first uncovered by the classical authors. It is also an expression of the contemporary Humanism:"
    ]
  ],
  [
    "The main contrast between Calvinists and Lutherans in terms of their beliefs about original sin is that Lutherans view original sin as the root of all actual sins and believe that God cooperates with everything that happens, while Calvinists believe in total depravity and unconditional election, rejecting the idea of prevenient grace and emphasizing irresistible grace and limited atonement.",
    [
      "to be in contrast to Calvin's monergistic damnation and to Arminius' synergistic salvation; however, Calvinists would take issue with their view being called \"monergistic damnation,\" since they claim to agree with Lutherans and Arminians that mankind alone bears responsibility for their sin and for their rejection of God's world-wide call to repent and be saved. The difference Lutheranism has with Calvinism and Arminianism, then, lies in how they describe the workings of God's will, foreordination, and gracious providence. Lutheranism teaches that God predestines some to salvation but does not predestine others to damnation as God wills that all might be",
      "convict the world of sin . Calvinism's response is found in Limited Atonement. So as a result of the Calvinist understanding of God's sovereignty, one must conclude that God's election does not depend upon any human response, necessitating a belief in (1) both Total Depravity and Unconditional Election, (2) Irresistible Grace rather than Prevenient Grace, and (3) Limited Atonement; if any of these beliefs are rejected, this logic fails. Like Calvinists, Lutherans view the work of salvation as monergistic in which an unconverted or unrepentant person always resists and rejects God and his ways. Even during conversion, the Formula of",
      "made the world, including humanity, perfect, holy and sinless. However, Adam and Eve chose to disobey God, trusting in their own strength, knowledge, and wisdom. Consequently, people are saddled with original sin, born sinful and unable to avoid committing sinful acts. For Lutherans, original sin is the \"chief sin, a root and fountainhead of all actual sins.\" According to Lutherans, God preserves his creation, in doing so cooperates with everything that happens, and guides the universe. While God cooperates with both good and evil deeds, with evil deeds he does so only inasmuch as they are deeds, but not with"
    ]
  ],
  [
    "The Council of Ferrara played a role in the reintroduction of Plato's works in Western scholarship by sparking a renewed interest in Platonic philosophy through Plethon's Platonic revival following the Council of Florence.",
    [
      "opponent of the Byzantine Church, inasmuch as the latter, under Western scholastic influence, relied heavily upon Aristotelian methodology. Plethon's Platonic revival, following the Council of Florence (1438\u20131439), largely accounts for the renewed interest in Platonic philosophy which accompanied the Renaissance. \"Of all the students of Greek in Renaissance Italy, the best-known are the Neoplatonists who studied in and around Florence\" (Hole). Neoplatonism was not just a revival of Plato's ideas, it is all based on Plotinus' created synthesis, which incorporated the works and teachings of Plato, Aristotle, Pythagoras, and other Greek philosophers. The Renaissance in Italy was the revival of",
      "rapidly spread direct knowledge of Plato throughout Western Europe: Their publication ... was an intellectual event of the first magnitude since they established Plato as a newly discovered authority for the Renaissance who could now take precedence over Aristotle, and whose work ... was of sufficient profundity to be set above his rival's. Ficino's translations helped make Renaissance Platonism into 'an attacking progressive force besieging the conservative cultural fortress which defended the Aristotelianism of the Schoolmen ... the firmest support of the established order.' Ficino's commentaries and translations ensured that the Neo-Platonist, allegorical approach to Plato became the norm throughout",
      "the European philosophical tradition is that it consists of a series of footnotes to Plato.\" In addition to being a foundational figure for Western science, philosophy, and mathematics, Plato has also often been cited as one of the founders of Western religion and spirituality. Plato was the innovator of the written dialogue and dialectic forms in philosophy. Plato appears to have been the founder of Western political philosophy, with his \"Republic\", and \"Laws\" among other dialogues, providing some of the earliest extant treatments of political questions from a philosophical perspective. Plato's own most decisive philosophical influences are usually thought to"
    ]
  ],
  [
    "Pangloss's experiences did not change his belief that \"everything always happens for the best.\" (Cecil J. Picard; optimism)",
    [
      "mantra. From this, the adjective \"Panglossian\" describes a person who believes that the world about us is the best possible one. While Leibniz does state this universe is the best possible version of itself, the standard for goodness does not seem clear to many of his critics. To Leibniz, the best universe means a world that is \u201cthe simplest in hypotheses and the richest in phenomena\u201d, in addition to the \u201chappiness of minds\u201d being God's main goal. Voltaire, Bertrand Russell, and other critics seem to equate goodness of the universe to no evil or evil acts whatsoever, presuming that a",
      "in one sentence: He taught us to believe. As a father, he taught us to believe success was possible. Not a single day did he bemoan his undeserved fate. Quite the contrary, he accepted his condition with peace and deepened his relationship with God.\" Former state Representative Jimmy D. Long of Natchitoches, the chairman of the University of Louisiana System board of supervisors, headed the House Education Committee when Picard was chairman of the corresponding Senate committee. He said that the two developed a working and friendly relationship. \"Cecil and I authored a raft of education reform,\" Long said in",
      "optimism. The phrase \"panglossian pessimism\" has been used to describe the pessimistic position that, since this is the best of all possible worlds, it is impossible for anything to get any better. Conversely, philosophical pessimism might be associated with an optimistic long-term view because it implies that no change for the worse is possible. Philosophical optimalism, as defined by Nicholas Rescher, holds that this universe exists because it is better than the alternatives. While this philosophy does not exclude the possibility of a deity, it also doesn't require one, and is compatible with atheism. Psychological optimalism, as defined by the"
    ]
  ],
  [
    "The desired outcome of the Navigation Acts of 1651 passed by the Rump Parliament of England was to increase England's shipping industry, encourage navigation, and regulate international trade and trade with colonies by requiring all trade to be carried in English ships.",
    [
      "to as the Navigation Act of 1650, was hastily passed as a war measure during the English Civil Wars, but it was followed by a more carefully conceived Act the following year. The Navigation Act 1651, long titled \"An Act for increase of Shipping, and Encouragement of the Navigation of this Nation\" was passed on 9 October 1651 by the Rump Parliament led by Oliver Cromwell. It authorized the Commonwealth to regulate England's international trade, as well as the trade with its colonies. It reinforced long-standing principles of national policy that English trade and fisheries should be carried in English",
      "Navigation Acts The Navigation Acts, or more broadly The Acts of Trade and Navigation were a long series of English laws that developed, promoted, and regulated English ships, shipping, trade, and commerce between other countries and with its own colonies. The laws also regulated England's fisheries and restricted foreigners' participation in its colonial trade. While based on earlier precedents, they were first enacted in 1651 under the Commonwealth. The system was reenacted and broadened with the restoration by the Act of 1660, and further developed and tightened by the Navigation Acts of 1663, 1673, and 1696. Upon this basis during",
      "English Parliament in 1651 limited Dutch trade with any of the English colonies in America unless the shipping was done in \"English bottoms\" i.e. English ships. Indeed, any shipping coming into English ports or the ports of English colonies from anywhere in the world was required to be carried in English ships. Furthermore, the Navigation Act forbade all trade with those English colonies that retained connections and sympathy for the royalist cause of Charles I. Acceptance of the terms of the Navigation Act was seen by the Dutch as subordinating Dutch trade to the English trading system. This insulted Dutch"
    ]
  ],
  [
    "William Morris was driven into revolutionary activism by his anger and shame at the injustices within society, and his passion for art and history influenced his socialist beliefs by inspiring his conviction that perfectionism of design and craftsmanship should be available to everyone, as seen in his handbound copy of Karl Marx's \"Das Kapital\" and his involvement in the Socialist League.",
    [
      "William Morris William Morris (24 March 1834 \u2013 3 October 1896) was a British textile designer, poet, novelist, translator, and socialist activist. Associated with the British Arts and Crafts Movement, he was a major contributor to the revival of traditional British textile arts and methods of production. His literary contributions helped to establish the modern fantasy genre, while he played a significant role in propagating the early socialist movement in Britain. Born in Walthamstow, Essex, to a wealthy middle-class family, Morris came under the strong influence of medievalism while studying Classics at Oxford University, there joining the Birmingham Set. After",
      "How can we combat all this luxury and waste? What drove him into revolutionary activism was his anger and shame at the injustices within society. He burned with guilt at the fact that his \"good fortune only\" allowed him to live in beautiful surroundings and to pursue the work he adored. \"Anarchy & Beauty\"'s arts and crafts section featured Morris' own copy of the French edition of Karl Marx's \"Das Kapital\" handbound in a gold-tooled leather binding that MacCarthy describes as \"the ultimate example of Morris's conviction that perfectionism of design and craftsmanship should be available to everyone.\" A number",
      "justice in \"Unto This Last\" (1860). In it, he stated four goals that might be called \"socialist\" although Ruskin did not use the term. Ruskin was not \"an authentic Socialist in any of its various nineteenth-century meanings.\" His only real contact with the Christian Socialists came through the Working Men's College. However, he influenced later socialist thinking, especially William Morris. The painters of the Pre-Raphaelite Brotherhood were influenced and sponsored by Ruskin. The artist William Morris was a leader of the Socialist League founded in December 1884. The Fabian Society was founded in the same year; Sydney and Beatrice Webb"
    ]
  ],
  [
    "De las Casas criticizes the motive of gathering slaves for the mines in the name of exploration (Fray Diego de Landa; Bartolom\u00e9 de las Casas).",
    [
      "Philip II of Spain. One of the stated purposes for writing the account was Las Casas's fear of Spain coming under divine punishment and his concern for the souls of the native peoples. The account was one of the first attempts by a Spanish writer of the colonial era to depict the unfair treatment that the indigenous people endured during the early stages of the Spanish conquest of the Greater Antilles, particularly the island of Hispaniola. Las Casas's point of view can be described as being heavily against some of the Spanish methods of colonization, which, as he described them,",
      "Things of Yucat\u00e1n\"), Fray Diego de Landa writes that Hern\u00e1ndez de C\u00f3rdoba went... \"to gather slaves for the mines, now that in Cuba the population is getting smaller\", although a while later he adds, \"Others say that he left to discover land and that he brought Alaminos as a pilot...\" Bartolom\u00e9 de Las Casas also says that even if the original intent was to kidnap and enslave Indians, at some point the objective was broadened to one of discovery, which justifies Alaminos. The presence of Ant\u00f3n de Alaminos on the expedition is, in effect, one of the arguments against the",
      "cultures that Las Casas learned about through his travels and readings. The history is apologetic because it is written as a defense of the cultural level of the Indians, arguing throughout that indigenous peoples of the Americas were just as civilized as the Roman, Greek and Egyptian civilizations\u2014and more civilized than some European civilizations. It was in essence a comparative ethnography comparing practices and customs of European and American cultures and evaluating them according to whether they were good or bad, seen from a Christian viewpoint. He wrote: \"I have declared and demonstrated openly and concluded, from chapter 22 to"
    ]
  ],
  [
    "Napoleon is a worldwide cultural icon and symbol of military genius and political power due to his reputation as the most competent human being who ever lived, his portrayal in numerous works of fiction, and his enduring legacy as one of the greatest commanders in history.",
    [
      "in charge they would surely win. Napoleon has become a worldwide cultural icon who symbolises military genius and political power. Martin van Creveld described him as \"the most competent human being who ever lived\". Since his death, many towns, streets, ships, and even cartoon characters have been named after him. He has been portrayed in hundreds of films and discussed in hundreds of thousands of books and articles. When met in person, many of his contemporaries were surprised by his apparently unremarkable physical appearance in contrast to his significant deeds and reputation, especially in his youth, when he was consistently",
      "Cultural depictions of Napoleon Napoleon I, Emperor of the French, has become a worldwide cultural icon generally associated with tactical brilliance, ambition and political power. His distinctive features and costume have made him a very recognizable figure in popular culture. He has been portrayed in many works of fiction, his depiction varying greatly with the author's perception of the historical character. In the 1927 film \"Napoleon\", young general Bonaparte is portrayed as a heroic visionary. On the other hand, he has been occasionally reduced to a stock character, depicted as short and bossy, sometimes comically so. SEE ALSO: Napol\u00e9on Bonaparte",
      "continental Europe before its final collapse in 1815. He is considered one of the greatest commanders in history, and his wars and campaigns are studied at military schools worldwide. Napoleon's political and cultural legacy has endured as one of the most celebrated and controversial leaders in human history. He was born Napoleone di Buonaparte () in Corsica to a relatively modest family of Italian origin from minor nobility. He was serving as an artillery officer in the French army when the French Revolution erupted in 1789. He rapidly rose through the ranks of the military, seizing the new opportunities presented"
    ]
  ],
  [
    "The ideas of Renaissance thinkers, such as Copernicus and his heliocentric theory, most likely influenced Galileo to discover Jupiter's moons.",
    [
      "thinkers, like Renaissance thinkers, were interested in pre-Christian Greek and Roman thought. Galileo, born in 1564 in Pisa, was the most famous scientist of his age, and played a major role in the Scientific Revolution. He is often referred to as the \u201cFather of Modern Science.\u201d His achievements include a greatly improved telescope, which he employed in making astronomical observations supporting the heliocentric theory of Copernicus; and also the experiments he carried out leading to his law of falling bodies, which was of key importance in Newton\u2019s synthesis of his own famous three laws of motion. Bonaventura Cavalieri, a Jesuit",
      "wanted, a ticket to Florence.\u201d Westfall describes that \u201c[i]n a word, Galileo had raised himself with one inspired blow from the level of an obscure professor of mathematics at the University of Padua to the status of the most desirable client in Italy.\u201d Following the discovery of the Jupiter's moons, Galileo would then look to discover their periods; due to ensuing competition and even some minimizing the importance of only discovering the moons without knowledge of their period, Galileo's \u201cacknowledged position as the messenger from the heavens was threatened\u201d. Westfall also contends that evidence of Galileo's patterns of observing the",
      "Xi Zezong has claimed that Gan De, an ancient Chinese astronomer, discovered one of Jupiter's moons in 362 with the unaided eye. If accurate, this would predate Galileo's discovery by nearly two millennia. In his 2nd century work the \"Almagest\", the Hellenistic astronomer Claudius Ptolemaeus constructed a geocentric planetary model based on deferents and epicycles to explain Jupiter's motion relative to Earth, giving its orbital period around Earth as 4332.38 days, or 11.86 years. In 1610, Italian polymath Galileo Galilei discovered the four largest moons of Jupiter (now known as the Galilean moons) using a telescope; thought to be the"
    ]
  ],
  [
    "The Lisbon earthquake influenced Voltaire's views on theodicy and deism by causing him to reject the idea of a benevolent deity and question the concept of philosophical optimism (\"Negative Dialectics\" 361).",
    [
      "a benevolent deity. The Lisbon disaster provided a counterexample. As Theodor Adorno wrote, \"[t]he earthquake of Lisbon sufficed to cure Voltaire of the theodicy of Leibniz\" (\"Negative Dialectics\" 361). In the later twentieth century, the 1755 earthquake has sometimes been compared to the Holocaust as a catastrophe that transformed European culture and philosophy. Jean-Jacques Rousseau was also influenced by the devastation following the earthquake, whose severity he believed was due to too many people living within the close quarters of the city. Rousseau used the earthquake as an argument against cities as part of his desire for a more naturalistic",
      "between 10,000 and 60,000 people were killed. One of the most destructive earthquakes in history, the event had a major effect on the cultural consciousness of much of Europe. Voltaire was one of many philosophers, theologians and intellectuals to be deeply affected by the disaster. Catholics attempted to explain it as God's wrath on the sins of the Portuguese, among them Protestant heretics and Jesuit casuists; while Protestants blamed the Portuguese for being Catholic. Polymath Gottfried Wilhelm Leibniz and poet Alexander Pope were both famous for developing a system of thought known as philosophical optimism in an attempt to reconcile",
      "the tragedy. The recovery from the earthquake also led to a rise in the wage premium of construction workers. More significantly, the earthquake became an opportunity to reform the economy and to reduce the economic semi-dependency vis-\u00e0-vis Britain.\" The earthquake and its fallout strongly influenced the intelligentsia of the European Age of Enlightenment. The noted writer-philosopher Voltaire used the earthquake in \"Candide\" and in his \"Po\u00e8me sur le d\u00e9sastre de Lisbonne\" (\"Poem on the Lisbon disaster\"). Voltaire's \"Candide\" attacks the notion that all is for the best in this, \"the best of all possible worlds\", a world closely supervised by"
    ]
  ],
  [
    "\"The White Man's Burden\" by Rudyard Kipling portrays a paternalistic and condescending attitude towards the native peoples under the rule or influence of the British Empire, suggesting that they need the guidance and civilization brought by the white colonizers.",
    [
      "ruling party, follows this tradition of nationalistic Hinduism (Hindutva), and promotes an Indian national identity infused with neo-Vedantic thought influenced by this historical colonial mentality. Critics have claimed that British writer Rudyard Kipling's portrayals of Indian characters in his works generally supported the colonialist view that colonized people were incapable of surviving without the help of Europeans, describing these portrayals as racist. In his famous poem \"The White Man's Burden\", Kipling directly makes this point by romanticizing British colonialism in India and elsewhere in the world. Kipling's poem idolizes Western culture as entirely rational and civilized, while treating non-white cultures",
      "second line of Rudyard Kipling\u2019s 1899 poem on the civilizing mission of the white colonizer, \"The White Man's Burden.\" Written for Queen Victoria's Jubilee in 1897, and revised as a response \"to resistance in the Philippines to the United States' assumption of colonial power\" after the Spanish\u2013American War of 1898., the poem's depiction of whites as having the responsibility to rule the nonwhite peoples of the world with beneficence implies the superiority of the white race. In \"The Best Ye Breed\", this notion of white supremacy is rejected at the socioeconomic level when Crawford and his men reveal to Paul",
      "on the civilizing mission of the colonizer, \"The White Man's Burden.\" Written for Queen Victoria's Jubilee in 1897, and revised as a response \"to resistance in the Philippines to the United States' assumption of colonial power\" after the Spanish\u2013American War of 1898., the poem's depiction of colonial government as a \"burden\" suggests that empires exist not for imperialist gain, but for the development of the unruly and uncivilized colonials. In \"Black Man's Burden\", this tradition of \"white imperial benevolence\" is initially subverted by having black-only fieldworkers foment economic and technological progress in the underdeveloped regions of North Africa, and later,"
    ]
  ],
  [
    "Dudley North's beliefs conflict with the 1651 Navigation Acts as he argued against the assumption of a need for a favorable balance of trade and believed that regulations interfere with the benefits of trade.",
    [
      "shared the institutionalist skepticism of \u201ceconomic laws\u201d that purport to be applicable outside of specific historical institutional contexts. He viewed orthodox theory more as expressions of doctrine rather than empirical observation. For example, he saw the quantity theory of money as a mathematical device convenient for neoclassical doctrine rather than as a hypothesis that emerged from solid empirical observation of economic data. Rather than base economics on introspection and mental states that cannot be empirically verified, he wished to engage in what he viewed as a more science-based approach which necessarily proceeds first from observations about an economy\u2019s actual behavior.",
      "universally in all things that are to be bought and sold.\" Dudley North (1641\u20131691) was a wealthy merchant and landowner who worked for Her Majesty's Treasury and opposed most mercantile policy. His \"Discourses upon trade\" (1691), published anonymously, argued against assuming a need for a favorable balance of trade. Trade, he argued, benefits both sides, promotes specialization, division of labor and wealth for everyone. Regulation of trade interferes with these benefits, he said. David Hume (1711\u20131776) agreed with North's philosophy and denounced mercantilist assumptions. His contributions were set down in \"Political Discourses\" (1752), and later consolidated in his \"Essays, Moral,",
      "measures. History of economic thought The history of economic thought deals with different thinkers and theories in the subject that became political economy and economics, from the ancient world to the present day in the 21st Century. This field encompasses many disparate schools of economic thought. Ancient Greek writers such as the philosopher Aristotle examined ideas about the art of wealth acquisition, and questioned whether property is best left in private or public hands. In the Middle Ages, scholasticists such as Thomas Aquinas argued that it was a moral obligation of businesses to sell goods at a just price In"
    ]
  ],
  [
    "Mikhail Gorbachev argued for the urgent necessity of economic and political reforms in the Soviet Union during the 1980s and 1990s.",
    [
      "\"a swollen state\" and \"a spent society\". Gorbachev made glasnost, perestroika, and democratization the centerpieces of a revolutionary ideology, which sparked a divisive public debate about the political content and policy implications of these concepts. More revolutionary, in the late 1980s, Gorbachev discarded the \"Brezhnev doctrine\", withdrawing Soviet troops from Afghanistan and allowing East European countries in the Soviet bloc to choose their own types of political system. And, most revolutionary, from late 1990 to late 1991, Gorbachev unintentionally and Boris Yeltsin intentionally spurred the disintegration of the Soviet Union, enabling the fifteen union republics to develop their own types",
      "in less than two years. In an attempt to avoid a third short-lived leader, in 1985, the Soviets turned to the next generation and selected Mikhail Gorbachev. Gorbachev made significant changes in the economy and party leadership, called \"perestroika\". His policy of \"glasnost\" freed public access to information after decades of heavy government censorship. Gorbachev also moved to end the Cold War. In 1988, the Soviet Union abandoned its nine-year war in Afghanistan and began to withdraw its forces. In the late 1980s, , which paved the way for Revolutions of 1989. With the tearing down of the Berlin Wall",
      "came to power in 1985. Although the Soviet Union had not accelerated military spending during Reagan's military buildup, their large military expenses, in combination with collectivized agriculture and inefficient planned manufacturing, were a heavy burden for the Soviet economy. Gorbachev was less ideologically rigid than his predecessors, and he believed that the Soviet Union urgently needed economic and political reforms. In 1986, he introduced his twin reforms of perestroika and glasnost, which would change the political and economic conditions of the Soviet Union. Seeking to reduce military expenditures and minimize the possibility of nuclear war, he also sought to re-open"
    ]
  ],
  [
    "revolutionized modern European warfare by introducing mass national conscription, creating larger armies composed of citizens and shifting the concept of war from one fought by professional soldiers to one involving the entire population.",
    [
      "expense)\u2014the actual practice of a \"lev\u00e9e en masse\" was rare before the French Revolution. The \"lev\u00e9e\" was a key development in modern warfare and would lead to steadily larger armies with each successive war, culminating in the enormous conflicts of World War I and World War II during the first half of the 20th century. Lev\u00e9e en masse Lev\u00e9e en masse ( or, in English, \"mass levy\") is a French term used for a policy of mass national conscription, often in the face of invasion. The concept originated during the French Revolutionary Wars, particularly for the period following 16 August",
      "become a responsibility of \"all\". Thus, the \"lev\u00e9e en masse\" was created and understood as a means to defend the nation by the nation. Historically, the \"lev\u00e9e en masse\" heralded the age of the people's war and displaced restricted forms of warfare, such as the cabinet wars (1715\u20131792), when armies of professional soldiers fought without the general participation of the population. The first modern use of \"lev\u00e9e en masse\" occurred during the French Revolutionary Wars. Under the Ancien R\u00e9gime, there had been some conscription (by ballot) to a militia, \"milice\", to supplement the large standing army in times of war.",
      "60 percent deserters. For all the rhetoric, the \"lev\u00e9e en masse\" was not popular; desertion and evasion were high. However, the effort was sufficient to turn the tide of the war, and there was no need for any further conscription until 1797, when a more permanent system of annual intakes was instituted. An effect of the \"lev\u00e9e en masse\" was the creation of a national army in France, made up of citizens, rather than an all-professional army, as was the standard practice of the time. Its main result, protecting French borders against all enemies, surprised and shocked Europe. The \"lev\u00e9e"
    ]
  ],
  [
    "The group that advocated for universal male suffrage in Fourier's circles was the Chartist movement.",
    [
      "century until the Civil Rights Movement gained passage of the Voting Rights Act of 1965 through Congress. As women also began to win the right to vote during the late 19th and early 20th centuries, the goal of universal manhood suffrage was replaced by universal suffrage. Universal manhood suffrage Universal manhood suffrage is a form of voting rights in which all adult males within a political system are allowed to vote, regardless of income, property, religion, race, or any other qualification. It is sometimes summarized by the slogan, \"one man, one vote.\" In 1789, Revolutionary France adopted the Declaration of",
      "his speech a demand for \"universal suffrage in its broadest sense, females as well as males being entitled to vote.\" The delegates approved a passage in their party platform addressing votes for women: \"Neither here, nor in any other part of the world, is the right of suffrage allowed to extend beyond one of the sexes. This universal exclusion of woman ... argues, conclusively, that, not as yet, is there one nation so far emerged from barbarism, and so far practically Christian, as to permit woman to rise up to the one level of the human family.\" At this convention,",
      "better conditions of work, and a repeal of the obnoxious Poor Law Act. The idea of universal male suffrage, an initial goal of the Chartist movement, was to include all males as voters regardless of their social standing. This later evolved into a campaign for universal suffrage. This movement sought to redraw the parliamentary districts within Great Britain and create a salary system for elected officials so workers could afford to represent their constituents without a burden on their families. Many consider Mary Wollstonecraft's \"Vindication of the Rights of Woman\" (1792) to be the source of the reformers' long-running campaign"
    ]
  ],
  [
    "According to Baldassare Castiglione's \"The Book of the Courtier,\" the ideal Renaissance gentleman must have a classical education in Greek and Latin letters, be skilled in various arts and sports, be well-read in the Humanities and classical literature, and be able to offer valuable assistance and advice to a ruler in order to participate in an active life of public service.",
    [
      "1507, while Castiglione was supposedly absent on an embassy to England. It addresses the topic, proposed by Federigo Fregoso, of what constitutes an ideal Renaissance gentleman. In the Middle Ages, the perfect gentleman had been a chivalrous knight who distinguished himself by his prowess on the battlefield. Castiglione's book changed that. Now the perfect gentleman had to have a classical education in Greek and Latin letters, as well. The Ciceronian humanist model of the ideal orator (whom Cicero called \"the honest man\"), on which \"The Courtier\" is based, prescribes for the orator an active political life of service to country,",
      "of Castiglione's ideal Renaissance gentleman was not self-cultivation for its own sake but in order to participate in an active life of public service, as recommended by Cicero. To do this he had to win the respect and friendship of his peers and most importantly of a ruler, or prince, i.e., he had to be a courtier, so as to be able to offer valuable assistance and disinterested advice on how to rule the city. He must be a worthy friend, accomplished\u2014in sports, in telling jokes, in fighting, writing poetry, playing music, drawing, and dancing\u2014but not too much. To his",
      "knight\u2014of Baldassarre Castiglione's \"The Book of the Courtier\" became a model of the ideal virtues of nobility. Castiglione's tale took the form of a discussion among the nobility of the court of the Duke of Urbino, in which the characters determine that the ideal knight should be renowned not only for his bravery and prowess in battle, but also as a skilled dancer, athlete, singer and orator, and he should also be well-read in the Humanities and classical Greek and Latin literature. Later Renaissance literature, such as Miguel de Cervantes's \"Don Quixote\", rejected the code of chivalry as unrealistic idealism."
    ]
  ],
  [
    "Metternich aimed to ensure the continuation of the Habsburg monarchy in Europe by promoting absolute monarchy, practicing balance-of-power diplomacy, and forming alliances with other European powers such as Britain, Prussia, and Russia following the Congress of Vienna in 1815.",
    [
      "Metternich believed that absolute monarchy was the only proper system of government. This notion influenced his anti-revolutionary policy to ensure the continuation of the Habsburg monarchy in Europe. Metternich was a practitioner of balance-of-power diplomacy. His foreign policy aimed to maintain international political equilibrium to preserve the Habsburgs' power and influence in international affairs. Following the Napoleonic Wars, Metternich was the chief architect of the Congress of Vienna in 1815. The Austrian Empire was the main beneficiary from the Congress of Vienna and it established an alliance with Britain, Prussia, and Russia forming the Quadruple Alliance. The Austrian Empire also",
      "Metternich pursued a peace for Europe, based on restored monarchical principle and on solidarity among the monarchs of Europe. The French Revolution of 1789 and the subsequent Napoleonic invasion and rule of much of Europe had implanted new liberal revolutionary ideas that were never to be eliminated. At the same time, nationalism was rising over much of the world. The Habsburg Empire was a complex political entity, with many ethnic groups and languages co-existing, which threatened its survival. Metternich expected to lead an alliance against France, pressing only enough to depose Napoleon, who had shown a complete unwillingness to accept",
      "the end, Francis accepted the revised proposals, albeit with several alterations and restrictions. Metternich's primary focus remained on preserving unity among the Great Powers of Europe and hence his own power as mediator. He was also concerned by liberal-minded Ioannis Kapodistrias' increasing influence over Tsar Alexander and the continual threat of Russia annexing large areas of the declining Ottoman Empire (the so-called \"Eastern Question\"). As he had earlier envisaged, by April 1818 Britain had drawn up, and Metternich pushed through, proposals to have a Congress at Aachen, then a Prussian frontier town, six months later. Meanwhile, Metternich was advised to"
    ]
  ],
  [
    "Yes, Herbert Spencer was an advocate of Social Darwinism in his nineteenth-century political philosophy, as he believed that societies were in a struggle for survival and that only the strong would survive. (History of creationism)",
    [
      "also integrated into the social and political philosophies of English thinker Herbert Spencer and American philosopher William Graham Sumner. Herbert Spencer, who coined the oft-misattributed term \"survival of the fittest,\" believed that societies were in a struggle for survival, and that groups within society are where they are because of some level of fitness. This struggle is beneficial to human kind, as in the long run the weak will be weeded out and only the strong will survive. This position is often referred to as Social Darwinism, though it is distinct from the eugenics movements with which social darwinism is",
      "Herbert Spencer Herbert Spencer (27 April 1820 \u2013 8 December 1903) was an English philosopher, biologist, anthropologist, sociologist, and prominent classical liberal political theorist of the Victorian era. Spencer developed an all-embracing conception of evolution as the progressive development of the physical world, biological organisms, the human mind, and human culture and societies. As a polymath, he contributed to a wide range of subjects, including ethics, religion, anthropology, economics, political theory, philosophy, literature, astronomy, biology, sociology, and psychology. During his lifetime he achieved tremendous authority, mainly in English-speaking academia. \"The only other English philosopher to have achieved anything like such",
      "English philosopher who developed ideas about the unifying concept of evolution across the natural and social sciences. Spencer is the first to develop a theory of cultural evolution and is considered by some to be the father of Social Darwinism. It is also he and not Darwin who coined the phrase \"survival of the fittest\". Much of the positivist ideas of progress that dominated the social science philosophy of Spencer and subsequent Social Darwinists has been criticized by present-day sociologists, but such ideas continue to be one of the major critiques made by creationists against evolution in general, even though"
    ]
  ],
  [
    "The deregulation of the grain industry worsening bad harvests was NOT one of the problems in France that caused the French Revolution.",
    [
      "crisis had become a political crisis as well, and the French Revolution loomed just beyond the horizon. Causes of the French Revolution The causes of the French Revolution can be attributed to several intertwining factors: All these factors created a revolutionary atmosphere and a tricky situation for Louis XVI. In order to resolve the crisis, the king summoned the Estates-General in May 1789 and, as it came to an impasse, the representatives of the Third Estates formed a National Assembly, against the wishes of the king, signaling the outbreak of the French Revolution. The essence of the revolutionary situation which",
      "event can be relatable or caused a bourgeois revolution (179-180). The second section highlights the 1789 effects of Frances social revolutionary crisis (181). The peasant revolts were the key to revolution, and the key to change in France (181). The dominant classes were divided from the very beginning over what kind of king should represent the French state (182). This division established the creation of the Estates General, which was only known as only as historical precedent. No real change came of the creation of this assembly because the 3rd estate had only 1 vote, causing the other 2 estates;",
      "global conflicts that extended from the Caribbean to the Middle East. Historians widely regard the Revolution as one of the most important events in human history. The causes of the French Revolution are complex and are still debated among historians. Following the Seven Years' War and the American Revolution, the French government was deeply in debt. It attempted to restore its financial status through unpopular taxation schemes, which were heavily regressive. Leading up to the Revolution, years of bad harvests worsened by deregulation of the grain industry also inflamed popular resentment of the privileges enjoyed by the aristocracy and the"
    ]
  ],
  [
    "The most important effect of the political revolutions of 1848, according to A. J. P. Taylor, was the removal of old feudal structures and the creation of independent national states.",
    [
      "as the Revolutions of 1848. It remains the most widespread revolutionary wave in European history, but within a year, reactionary forces had regained control, and the revolutions collapsed. The revolutions were essentially bourgeois-democratic in nature with the aim of removing the old feudal structures and the creation of independent national states. The revolutionary wave began in France in February, and immediately spread to most of Europe and parts of Latin America. Over 50 countries were affected, but with no coordination or cooperation among the revolutionaries in different countries. Six factors were involved: widespread dissatisfaction with political leadership; demands for more",
      "of the governed, as it had been put as early as the Declaration of Independence, was epoch-marking\". The American Revolution had its impact on the French Revolution and later movements in Europe. Leopold von Ranke, a leading German historian, in 1848 argued that American republicanism played a crucial role in the development of European liberalism: Historians widely regard the French Revolution as one of the most important events in history. The Revolution is often seen as marking the \"dawn of the modern era\", and its convulsions are widely associated with \"the triumph of liberalism\". Three years into the French Revolution,",
      "Revolutions of 1848 The Revolutions of 1848, known in some countries as the Spring of Nations, People's Spring, Springtime of the Peoples, or the Year of Revolution, were a series of political upheavals throughout Europe in 1848. It remains the most widespread revolutionary wave in European history. The revolutions were essentially democratic and liberal in nature, with the aim of removing the old monarchical structures and creating independent nation states. The revolutions spread across Europe after an initial revolution began in France in February. Over 50 countries were affected, but with no significant coordination or cooperation among their respective revolutionaries."
    ]
  ],
  [
    "The changing political climate in the 1920s led to a shift towards passive despair, impacting public housing plans in Britain as the mood changed and the influence of the \"Reds\" within the Labour Party decreased.",
    [
      "aspirations of upward mobility and made possible the fastest rate of growth in working-class owner-occupation during the 20th century. The boom was largely financed by the savings ordinary Britons put into their building societies. Starting in the 1920s favourable tax policies encouraged substantial investment in the societies, creating huge reserves for lending. Beginning in 1927, the societies encouraged borrowing through gradual liberalization of mortgage terms. Housing was a critical shortage in the post-war era. Air raids had destroyed half a million housing units; repairs and maintenance on undamaged homes had been postponed. 3,000,000 new dwellings were needed. The government aimed",
      "Formerly a Liberal Party stronghold, the industrial districts switched to Labour by 1922, with a base among the Irish Catholic working class districts. Women were especially active solidarity on housing issues. However, the \"Reds\" operated within the Labour Party and had little influence in Parliament; the mood changed to passive despair by the late 1920s. David Lloyd George became prime minister in December 1916 and immediately transformed the British war effort, taking firm control of both military and domestic policy. In rapid succession in spring 1918 came a series of military and political crises. The Germans, having moved troops from",
      "moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. The rapid expansion of housing was a major success story of the interwar years, 1919\u20131939, standing in sharp contrast to the United States, where the construction of new housing practically collapsed after 1929. The total housing stock in England and Wales was 7,600,000 in 1911; 8,000,000 in 1921; 9,400,000 in 1931; and 11,300,000 in"
    ]
  ],
  [
    "A.J. P. Taylor believed that 1848 was the decisive year for German and European history because it marked the beginning of aggressive German nationalism and imperialism, leading to the rise of Hitler and subsequent conflicts.",
    [
      "been written. He argued that: \"The history of the Germans is a history of extremes. It contains everything except moderation, and in the course of a thousand years the Germans have experienced everything except normality.\" The \"German problem\" in Taylor's eyes has two sides: how can Europe be protected against repeated German aggression and how can the Germans secure a peaceful form of political existence? Taylor also notes the repeated failure of the German left to choose a democratic Germany when the choice was between democracy and unity. In his discussion of the revolutions of 1848 Taylor said: 1848 was",
      "dreams of conquest and the German people needing Hitler to fulfil their dreams of subjugation of their neighbours. In particular, he accused the Germans of waging an endless Drang nach Osten against their Slavic neighbours since the days of Charlemagne. For Taylor, Nazi racial imperialism was a continuation of policies pursued by every German ruler. \"The Course of German History\" was a best-seller in both the United Kingdom and the United States; it was the success of this book that made Taylor's reputation in the United States. Its success also marked the beginning of the breach between Taylor and his",
      "and Czechs presaged the great international crises of 1938\u201339, and called the 1848 revolution \"a touchstone of German mentality and a decisive element in East-European politics\" In his lecture, Namier described the 1848 revolution as \"the early manifestations of aggressive nationalism, especially of German nationalism which derives from the much belauded Frankfort Parliament rather than from Bismarck and \"Prussianism\". Namier concluded \"had not Hitler and his associates blindly accepted the legend which latter-day liberals, German and foreign had spun around 1848, they might well have found a great deal to extol in the of the Frankfort Assembly\". Taylor wrote in"
    ]
  ],
  [
    ", the Edict of Nantes aimed to promote civil unity and religious tolerance by granting Calvinist Protestants substantial rights in the predominantly Catholic nation of France.",
    [
      "Edict of Nantes The Edict of Nantes (French: \"\u00e9dit de Nantes\"), signed in April 1598 by King Henry IV of France, granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the nation, which was still considered essentially Catholic at the time. In the edict, Henry aimed primarily to promote civil unity. The edict separated civil from religious unity, treated some Protestants for the first time as more than mere schismatics and heretics, and opened a path for secularism and tolerance. In offering general freedom of conscience to individuals, the edict offered many specific concessions to the",
      "followed by most modern historians is the Huguenot refugee \u00c9lie Benoist's \"Histoire de l'\u00e9dit de Nantes\", 3 vols. (Delft, 1693\u201395). E.G. L\u00e9onard devotes a chapter to the Edict of Nantes in his \"Histoire g\u00e9n\u00e9ral du protestantisme\", 2 vols. (Paris) 1961:II:312\u201389. Edict of Nantes The Edict of Nantes (French: \"\u00e9dit de Nantes\"), signed in April 1598 by King Henry IV of France, granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the nation, which was still considered essentially Catholic at the time. In the edict, Henry aimed primarily to promote civil unity. The edict separated civil from",
      "religion did not sit well with Louis XIV's vision of perfected autocracy: \"Bending all else to his will, Louis XIV resented the presence of heretics among his subjects.\" The Edict of Nantes had been issued on 13 April 1598 by Henry IV of France. It had granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the predominantly Catholic state. Through the Edict, Henry had aimed to promote civil unity. The Edict treated some Protestants with tolerance and opened a path for secularism. It offered general freedom of conscience to individuals and many specific concessions to the"
    ]
  ],
  [
    "Huxley's view on the substance of life, establishing protoplasm as incompatible with vitalistic theories, shaped nineteenth-century ideology by promoting a materialistic understanding of life and emphasizing human agency in determining destiny and ethics.",
    [
      "substance of life. Huxley's principal contribution was to establish protoplasm as incompatible with a vitalistic theory of life. Attempts to investigate the origin of life through the creation of synthetic \"protoplasm\" in the laboratory were not successful. The idea that protoplasm of eukaryotes is simply divisible into a ground substance called \"cytoplasm\" and a structural body called the cell nucleus reflects the more primitive knowledge of cell structure that preceded the development of electron microscopy, when it seemed that cytoplasm was a homogeneous fluid and the existence of most sub-cellular compartments, or how cells maintain their shape, was unknown. Today,",
      "biological science to the public, with a focus on three aspects in particular. For a modern survey of the idea of progress in evolution see Nitecki and Dawkins. Huxley's humanism came from his appreciation that mankind was in charge of its own destiny (at least in principle), and this raised the need for a sense of direction and a system of ethics. His grandfather T. H. Huxley, when faced with similar problems, had promoted agnosticism, but Julian chose humanism as being more directed to supplying a basis for ethics. Julian's thinking went along these lines: \"The critical point in the",
      "materialism of Huxley. In 1880, term protoplast was proposed by Hanstein (1880) for the entire cell, excluding the cell wall, and some authors like Julius von Sachs (1882) preferred that name instead of cell. In 1965, lardy introduced the term \"cytosol\", later redefined to refer to the liquid inside cell. By the time Huxley wrote, a long-standing debate was largely settled over the fundamental unit of life: was it the cell or was it protoplasm? By the late 1860s, the debate was largely settled in favor of protoplasm. The cell was a container for protoplasm, the fundamental and universal material"
    ]
  ],
  [
    "The primary goal of the Chartist movement was to campaign for political reform, including universal suffrage and the implementation of the secret ballot, as outlined in the People's Charter of 1838.",
    [
      "social movements and special-interest associations. Chartism was the first mass movement of the growing working-class in the world. It campaigned for political reform between 1838 and 1848 with the People's Charter of 1838 as its manifesto \u2013 this called for universal suffrage and the implementation of the secret ballot, amongst other things. The term \"social movements\" was introduced in 1848 by the German Sociologist Lorenz von Stein in his book \"Socialist and Communist Movements since the Third French Revolution (1848)\" in which he introduced the term \"social movement\" into scholarly discussions - actually depicting in this way political movements fighting",
      "launch of Chartism. If, as the movement came together, there were different priorities amongst local leaders, the Charter and the Star soon created a national, and largely united, campaign of national protest. John Bates, an activist, recalled: The movement organised a National Convention in London in early 1839 to facilitate the presentation of the first petition. Delegates used the term MC, Member of Convention, to identify themselves; the convention undoubtedly saw itself as an alternative parliament. In June 1839, the petition, signed by 1.3 million working people, was presented to the House of Commons, but MPs voted, by a large",
      "Chartism Chartism was a working-class movement for political reform in Britain that existed from 1838 to 1857. It took its name from the People's Charter of 1838 and was a national protest movement, with particular strongholds of support in Northern England, the East Midlands, the Staffordshire Potteries, the Black Country, and the South Wales Valleys. Support for the movement was at its highest in 1839, 1842, and 1848, when petitions signed by millions of working people were presented to the House of Commons. The strategy employed was to use the scale of support which these petitions and the accompanying mass"
    ]
  ],
  [
    "The French Committee of Public Safety authorized the Lev\u00e9e en Masse, which required all able-bodied men aged 18 to 25 to be conscripted for the extraordinary manufacture of arms in 1793.",
    [
      "constitution of France, the French Constitution of 1793. It was ratified by public referendum, but never put into force. On 13 July the assassination of Jean-Paul Marat \u2013 a Jacobin leader and journalist \u2013 resulted in a further increase in Jacobin political influence. Georges Danton, the leader of the August 1792 uprising against the king, was removed from the committee. On July 27, 1793, Robespierre became part of the Committee of Public Safety. On 23 August, the National Convention decreed the lev\u00e9e en masse, \"The young men shall fight; the married man shall forge arms and transport provisions; the women",
      "France declared war on Spain and adding further strains on the French armies' limited manpower. By some accounts, only about half this number appears to have been actually raised, bringing the army strength up to about 645,000 in mid-1793, and the military situation continued to deteriorate, particularly when Mainz fell on 23 July 1793. In response to this desperate situation, at war with European states, and insurrection, Paris petitioners and the f\u00e9d\u00e9r\u00e9s demanded that the Convention enact a Lev\u00e9e en Masse. In response, Convention member Bertrand Bar\u00e8re asked the Convention to \u201cdecree the solemn declaration that the French people was",
      "Lev\u00e9e en masse Lev\u00e9e en masse ( or, in English, \"mass levy\") is a French term used for a policy of mass national conscription, often in the face of invasion. The concept originated during the French Revolutionary Wars, particularly for the period following 16 August 1793, when able-bodied men aged 18 to 25 were conscripted. It formed an integral part of the creation of national identity, making it distinct from forms of conscription which had existed before this date. The term is also applied to other historical examples of mass conscription. The term \"lev\u00e9e en masse\" denotes a short-term requisition"
    ]
  ],
  [
    "The Northern European Renaissance movement most similar to the humanism attributed to Erasmus is the Protestant Reformation, as both were influenced by a desire for individualism, anti-clericalism, and a return to ancient sources of wisdom.",
    [
      "more or less simultaneously in many parts of Europe and America. Developing during the Enlightenment era, Renaissance humanism as an intellectual movement spread across Europe. The basic training of the humanist was to speak well and write (typically, in the form of a letter). The term \"umanista\" comes from the latter part of the 15th century. The people were associated with the \"studia humanitatis\", a novel curriculum that was competing with the \"quadrivium\" and scholastic logic. Renaissance humanism took a close study of the Latin and Greek classical texts, and was antagonistic to the values of scholasticism with its emphasis",
      "didactic ritual, Erasmus laid the groundwork for Luther. Humanism's intellectual anti-clericalism would profoundly influence Luther. The increasingly well-educated middle sectors of Northern Germany, namely the educated community and city dwellers would turn to Luther's rethinking of religion to conceptualize their discontent according to the cultural medium of the era. The great rise of the burghers, the desire to run their new businesses free of institutional barriers or outmoded cultural practices, contributed to the appeal of humanist individualism. To many, papal institutions were rigid, especially regarding their views on just price and usury. In the North, burghers and monarchs were united",
      "to a Christianity its students in the main wished to supplement, not contradict, through their patient excavation of the sources of ancient God-inspired wisdom\" Renaissance humanism Renaissance humanism is the study of classical antiquity, at first in Italy and then spreading across Western Europe in the 14th, 15th, and 16th centuries. The term \"humanism\" is contemporary to that period, while \"Renaissance humanism\" is a retronym used to distinguish it from later humanist developments. Renaissance humanism was a response to the utilitarian approach and what came to be depicted as the \"narrow pedantry\" associated with medieval scholasticism. Humanists sought to create"
    ]
  ],
  [
    "The Treaty of Nanjing allowed British subjects residing in China for mercantile purposes to live and conduct business in designated treaty ports, but they were not allowed to travel to the interior of China or trade there.",
    [
      "allowed to buy property in the treaty ports and reside there with their families, they were not allowed to travel to the interior of China or trade there. The treaty also granted extraterritorial privileges to British subjects and most Favored Nation status to Great Britain, which meant that the latter would enjoy any privilege granted to other powers. In China, the treaty is widely regarded as an imperialist one, which paved the way for the subjugation of China to Western imperialism. The treaty consolidated the \"opening\" of China to foreign trade in the wake of the First Opium War and",
      "the Yangtze Patrol. As a result of the \"unequal treaties\" imposed on China by Great Britain and other European powers after the First Opium War (18391842) and Second Opium War (18561860), China was opened to foreign trade at a number of locations known as \"treaty ports\" where foreigners were permitted to live and conduct business. Also, created by the treaties was the doctrine of extraterritoriality, a system whereby citizens of foreign countries living in China were subject to the laws of their home country. Most favoured nation treatment under the treaties assured other countries of the privileges afforded Great Britain,",
      "trade. Following the signature of the 1842 Treaty of Nanking, British subjects are \"allowed to reside, for the purpose of carrying on their mercantile pursuits, without molestation or restraint\" at Canton, Shanghai, Amoy (Xiamen), Ningpo (Ningbo) and Foochow (Fuzhou). In addition, Article V of the Treaty specifically abolishes the Canton system, allowing British merchants, and eventually all foreign merchants, to deal with whomever they please in the newly-opened ports. In 1859 Canton's trade moved to a new site on the reclaimed sandbank of Shaming Island, a short distance west of the former factories. By then much of the foreign trade"
    ]
  ],
  [
    "The Paris Exposition of 1889 helped anthropology to grow increasingly distinct from natural history and crystallize into its modern form by the end of the 19th century.",
    [
      "racism, which first formulation may be found in Arthur de Gobineau's \"An Essay on the Inequality of Human Races\" (1853\u201355). In 1931, the Colonial Exhibition in Paris still displayed Kanaks from New Caledonia in the \"indigenous village\"; it received 24 million visitors in six months, thus demonstrating the popularity of such \"human zoos\". Anthropology grew increasingly distinct from natural history and by the end of the nineteenth century the discipline began to crystallize into its modern form\u2014by 1935, for example, it was possible for T.K. Penniman to write a history of the discipline entitled \"A Hundred Years of Anthropology\". At",
      "prove in the same movement the validity of scientific racism, which first formulation may be found in Arthur de Gobineau's \"An Essay on the Inequality of Human Races\" (1853\u201355). In 1931, the Colonial Exhibition in Paris still displayed Kanaks from New Caledonia in the \"indigenous village\"; it received 24 million visitors in six months, thus demonstrating the popularity of such \"human zoos\". Anthropology grew increasingly distinct from natural history and by the end of the 19th century the discipline began to crystallize into its modern form - by 1935, for example, it was possible for T.K. Penniman to write a",
      "which they did. This part of History is named Anthropology.\" Many scholars consider modern anthropology as an outgrowth of the Age of Enlightenment (1715\u201389), a period when Europeans attempted to study human behavior systematically, the known varieties of which had been increasing since the fifteenth century as a result of the first European colonization wave. The traditions of jurisprudence, history, philology, and sociology then evolved into something more closely resembling the modern views of these disciplines and informed the development of the social sciences, of which anthropology was a part. It took Immanuel Kant (1724-1804) 25 years to write one"
    ]
  ],
  [
    "No, Herbert Spencer believed that government intervention to alleviate poverty does not necessarily go against the principle of individual freedom and self-reliance, as long as certain conditions were met, such as the matter being of primary social importance, proven to be practicable, and not diminishing self-reliance (Arnold Toynbee).",
    [
      "the government, the discouragement given to voluntary self-improvement, and the disregard of the \"laws of life.\" The reforms, he said, were tantamount to \"socialism\", which he said was about the same as \"slavery\" in terms of limiting human freedom. Spencer vehemently attacked the widespread enthusiasm for annexation of colonies and imperial expansion, which subverted all he had predicted about evolutionary progress from 'militant' to 'industrial' societies and states. Spencer anticipated many of the analytical standpoints of later libertarian theorists such as Friedrich Hayek, especially in his \"law of equal liberty\", his insistence on the limits to predictive knowledge, his model",
      "philosophy ever written.\" Spencer argued that the state was not an \"essential\" institution and that it would \"decay\" as voluntary market organisation would replace the coercive aspects of the state. He also argued that the individual had a \"right to ignore the state.\" As a result of this perspective, Spencer was harshly critical of patriotism. In response to being told that British troops were in danger during the Second Afghan War (1878-1880) he replied: \"When men hire themselves out to shoot other men to order, asking nothing about the justice of their cause, I don't care if they are shot",
      "is this: We have not abandoned our old belief in liberty, justice, and Self-help, but we say that under certain conditions the people cannot help themselves, and that then they should be helped by the State representing directly the whole people. In giving this State help, we make three conditions: first, the matter must be one of primary social importance; next, it must be proved to be practicable; thirdly, the State interference must not diminish self-reliance. Even if the chance should arise of removing a great social evil, nothing must be done to weaken those habits of individual self-reliance and"
    ]
  ],
  [
    "Pico della Mirandola viewed the dignity of human beings as superior to that of other creatures, emphasizing the capacity for intellectual achievement and moral transformation. (Oration on the Dignity of Man)",
    [
      "capacity and human perspective. Arriving in a place near Florence, he taught the amazing capacity of human achievement. \"Pico himself had a massive intellect and studied everything there was to be studied in the university curriculum of the Renaissance; the \"Oration\" in part is meant to be a preface to a massive compendium of all the intellectual achievements of humanity, a compendium that never appeared because of Pico's early death.\" Pico della Mirandola intended to speak in front of an invited audience of scholars and clerics of the dignity of the liberal arts and about the glory of angels. Of",
      "Mirandola emphasizes the Cherubim and that by embodying the values of the Cherub, one can be equally prepared for \"the fire of the Seraphim and the judgement of the Thrones\". This deviation into the hierarchy of angels makes sense when Pico della Mirandola makes his point that a philosopher \"is a creature of Heaven and not of earth\" because they are capable of obtaining any one of the statuses. In the \"Oration\", Pico justified the importance of the human quest for knowledge within a Neoplatonic framework. He writes that after God had created all creatures, He conceived of the desire",
      "written in August 5, 1487 but it was not issued until the following December. In a letter to Lorenzo dated August 27, 1489, Pico affirms among other things some of his thesis refer purely to profane matters and were never intended for general reading, but for private debate among the learned. In the \"Oration\" he writes that \"human vocation is a mystical vocation that has to be realized following a three stage way, which comprehends necessarily moral transformation, intellectual research and final perfection in the identity with the absolute reality. This paradigm is universal, because it can be retraced in"
    ]
  ],
  [
    "The consequences of the Treaty of Versailles on Germany after World War I included territorial losses, military restrictions, economic hardships, and a sense of humiliation and injustice among the German population.",
    [
      "World War I broke out in Europe on August 1, 1914. The conflict dragged on until a truce was declared on November 11, 1918, leading to the controversial, one-sided Treaty of Versailles, which was signed on June 28, 1919. The war's end triggered the abdication of various monarchies and the collapse of five of the last modern empires of Russia, Germany, China, Ottoman Turkey and Austria-Hungary, with the latter splintered into Austria, Hungary, southern Poland (who acquired most of their land in a war with Soviet Russia), Czechoslovakia and Yugoslavia, as well as the unification of Romania with Transylvania and",
      "World War I. After four years of warfare, in which approximately two million German soldiers were killed, a general armistice ended the fighting on 11 November, and German troops returned home. In the German Revolution (November 1918), Emperor Wilhelm II and all German ruling princes abdicated their positions and responsibilities. Germany's new political leadership signed the Treaty of Versailles in 1919. In this treaty, Germany, as part of the Central Powers, accepted defeat by the Allies in one of the bloodiest conflicts of all time. Germans perceived the treaty as humiliating and unjust and it was later seen by historians",
      "Treaty of Versailles The Treaty of Versailles () was the most important of the peace treaties that brought World War I to an end. The Treaty ended the state of war between Germany and the Allied Powers. It was signed on 28 June 1919 in Versailles, exactly five years after the assassination of Archduke Franz Ferdinand which directly led to World War I. The other Central Powers on the German side of World War I signed separate treaties. Although the armistice, signed on 11 November 1918, ended the actual fighting, it took six months of Allied negotiations at the Paris"
    ]
  ],
  [
    "The crucial factor in the Europeans' success in the New World was their economic desire for new resources and markets, which drove their colonization efforts and expansion.",
    [
      "or less complex), where and as a common trait among them, there was a ruling class that held power and determined the destinies of peoples and territories under its control. These elites were those that the Spaniards discovered and conquered in the New World. It was these Spanish conquerors, using European terminology, who correlated the identity of classes of the pre-Hispanic elites, alongside with the royalty or with the nobility of Europe at the time, according to appropriate categories, e.g., emperor, king, etc. The thoughts of the more notable among them give useful insights on how the first European settlers",
      "there were other European demands the New World could also satisfy, which contributed to its growing involvement in the Western-dominated world economy. While Spanish America seemed to fulfill dreams of mineral wealth, Brazil became the first major plantation colony in 1532, organized to produce a tropical crop \u2013 sugar \u2013 in great demand and short supply in Europe. The other major powers, England, France and the Netherlands, soon thereafter hoped to establish profitable colonies of their own. Presented with new opportunities, Europeans disenchanted by the rigid social structures of feudalism emigrated to the abundant virginal lands of the colonial frontier.",
      "powers, the economic desire for new resources and markets, and a \"civilizing mission\" ethos. Many of the colonies established during this era gained independence during the era of decolonization that followed World War II. The qualifier \"new\" is used to differentiate modern imperialism from earlier imperial activity, such as the so-called first wave of European colonization between the 15th and early-19th centuries. In the first wave of colonization, European powers conquered and colonized the Americas and Siberia; they then later established more outposts in Africa and Asia. The American Revolution (1775\u201383) and the collapse of the Spanish Empire in Latin"
    ]
  ],
  [
    "Accusations of cannibalism served as a justification for the Spanish conquest of indigenous groups in the New World by exempting cannibals from Queen Isabella's prohibition on enslaving the indigenous.",
    [
      "settlers' cannibalism in 1609 in the Jamestown Colony under famine conditions. In Spain's overseas expansion to the New World, the practice of cannibalism was reported by Christopher Columbus in the Caribbean islands, and the Caribs were greatly feared because of their supposed practice of it. Queen Isabel of Castile had forbade the Spaniards to enslave the indigenous, but if they were \"guilty\" of cannibalism, they could be enslaved. The accusation of cannibalism became a pretext for attacks on indigenous groups and justification for the Spanish conquest. In Yucat\u00e1n, shipwrecked Spaniard Jer\u00f3nimo de Aguilar, who later became a translator for Hern\u00e1n",
      "New Spain\" (written by 1568, published 1632) contains several accounts of cannibalism among the people the conquistadors encountered during their warring expedition to Tenochtitlan. D\u00edaz's testimony is corroborated by other Spanish historians who wrote about the conquest. In \"History of Tlaxcala\" (written by 1585), Diego Mu\u00f1oz Camargo (c. 1529 \u2013 1599) states that: Accounts of the Aztec Empire as a \"Cannibal Kingdom\", Marvin Harris's expression, have been commonplace from Bernal D\u00edaz to Harris, William H. Prescott and Michael Harner. Harner has accused his colleagues, especially those in Mexico, of downplaying the evidence of Aztec cannibalism. Ortiz de Montellano presents evidence",
      "helped characterize indigenous peoples as \"uncivilized\", \"primitive\", or even \"inhuman.\" These assertions promoted the use of military force as a means of \"civilizing\" and \"pacifying\" the \"savages\". The Spanish conquest of the Aztec Empire and its earlier conquests in the Caribbean where there were widespread reports of cannibalism, justifying the conquest. Cannibals were exempt from Queen Isabella's prohibition on enslaving the indigenous. Another example of the sensationalism of cannibalism and its connection to imperialism occurred during Japan's 1874 expedition to Taiwan. As Eskildsen describes, there was an exaggeration of cannibalism by Taiwanese indigenous peoples in Japan's popular media such as"
    ]
  ],
  [
    "The law enabling the right of defense being overturned was the change instituted by Robespierre during the radical phase of the revolution that was least beneficial to the defense of France.",
    [
      "Terror: \"\"No, we will not step backward, our zeal will only be smothered in the tomb; either the revolution will triumph or we will all die.\"\" The law enabling right of defense was overturned the next day. As 1794 progressed, Robespierre began to speak out against over zealous factions: he believed both pro-Terror and indulgent positions were dangerous to the well-being of the Revolution. Robespierre saw danger in members of the Revolution, like Billaud-Varenne, Collot d'Herbois, and Marc-Guillaume Alexis Vadier who had been too committed to attacks on Church property or had been too vigorous in their pursuit of revolutionary",
      "Nevertheless, Robespierre remains controversial to this day. Apart from one Metro station in Montreuil (a Paris suburb) and several streets named after him in about twenty towns, there are no memorials or monuments to him in France. By making himself the embodiment of virtue and of total commitment, he took control of the Revolution in its most radical and bloody phase: the Jacobin republic. His goal in the Terror was to use the guillotine to create what he called a \"republic of virtue\", wherein terror and virtue would be imposed at the same time. He argued, \"Terror is nothing more",
      "secure in France by making it universal. The extreme Left had the same objects, but it held that a war for those objects could not safely be entrusted to the king and his ministers. Victory would revive the power of the crown; defeat would be the undoing of the Revolution. Robespierre also argued against the Girondins' objective of using war as a means of exporting revolution, on the grounds that \"nobody likes armed missionaries\". Hence Robespierre and those who thought with him desired peace. The French nation generally had never approved of the Austrian alliance, and regarded the Habsburgs as"
    ]
  ],
  [
    "The main interest of Dr. John Wallis's group in London in the mid-17th century was in experimental science and natural philosophy.",
    [
      "question. A College (or Society) of Antiquaries was founded in London in \"c\".1586, to debate matters of antiquarian interest. Members included William Camden, Sir Robert Cotton, John Stow, William Lambarde, Richard Carew and others. This body existed till 1604, when it fell under suspicion of being political in its aims, and was abolished by King James I. Papers read at their meetings are preserved in Cotton's collections, and were printed by Thomas Hearne in 1720 under the title \"A Collection of Curious Discourses\", a second edition appearing in 1771. In 1707 a number of English antiquaries began to hold regular",
      "account of John Wallis asserts there was a group convened by Theodore Haak. This group is now often called simply the \u201c1645 group\u201d. George Ent, Francis Glisson and Charles Scarborough joined around 1647. Wallis mentions also John Wilkins, Samuel Foster and Christopher Merret. The group broke up around 1648. There are in fact two accounts by Wallis, the first from 1678, and the second (unpublished at the time) from 1697. The first version (\"A Defence of the Royal Society\") was produced to contradict William Holder, with whom Wallis was in dispute over his work in speech therapy. The second version",
      "first medical publication, the \"Diatribae Duae\" of 1659. Bathurst was active in the intellectual ferment of the time, and very well connected. In the account given by John Wallis of the precursor groups to the Royal Society of London, Bathurst is mentioned as one of the Oxford experimentalists who gathered from 1648\u20139. Also in that group were Willis, William Petty and Seth Ward. The group expanded in the 1650s when it gathered around John Wilkins of Wadham College, close however to Oliver Cromwell, and then included also Jonathan Goddard, Thomas Millington, Laurence Rooke, and Christopher Wren. Later Robert Boyle joined."
    ]
  ],
  [
    "De Gaulle exclaimed \"Vive le Que\u0301bec libre! Long live, long live, long live French Canada!\" from the balcony, echoed by the crowd with approval and excitement.",
    [
      "wrote that de Gaulle's statement was planned, and that he used it when the opportunity presented itself. De Gaulle stepped out onto the balcony to give a short address to the assembled masses which was also broadcast live on radio. In his speech he commented that his drive down the banks of the Saint Lawrence River, lined as it had been with cheering crowds, reminded him of his triumphant return to Paris after the liberation from Nazi Germany. The speech appeared to conclude with the words (\"Long live Montreal! Long live Quebec!\"), but he then added, (\"Long live free Quebec!",
      "Long live, long live, long live French Canada! And long live France!\"), whereupon the crowd roared with approval, especially after hearing, . De Gaulle particularly emphasized the use of the word , as he leaned into the microphones and enunciated it more slowly and loudly than other elements of his speech. This statement, coming from the French head of state, was considered a serious breach of diplomatic protocol. It emboldened the Quebec sovereignty movement, and produced tensions between the leadership of the two countries. The crowd's reaction to De Gaulle's phrase was emotional, and has been described as frenzied, but",
      "trip to Algeria, de Gaulle calculatedly made an ambiguous and broad emotional appeal to all the inhabitants, declaring, \"Je vous ai compris\" (\"I have understood you\"). De Gaulle raised the hopes of the \"pied-noir\" and the professional military, disaffected by the indecisiveness of previous governments, with his exclamation of \"Vive l'Alg\u00e9rie fran\u00e7aise\" (\"Long live French Algeria\") to cheering crowds in Mostaganem. At the same time, he proposed economic, social, and political reforms to improve the situation of the Muslims. Nonetheless, de Gaulle later admitted to having harbored deep pessimism about the outcome of the Algerian situation even then. Meanwhile, he"
    ]
  ],
  [
    "The author fails to acknowledge Napoleon's anti-Semitic actions, such as passing the Infamous Decree, while praising his intellect and accomplishments.",
    [
      "megalomaniac goals. He says Napoleon's reputation is exaggerated. French scholar Jean Tulard provided an influential of his image as a savior. Louis Bergeron has praised the numerous changes he made to French society, especially regarding the law as well as education. His greatest failure was the Russian invasion. Many historians have blamed Napoleon's poor planning, but Russian scholars instead emphasize the Russian response, noting the notorious winter weather was just as hard on the defenders. The large and growing historiography in French, English, Russian, Spanish and other languages has been summarized and evaluated by numerous scholars. Napoleon's use of propaganda",
      "and vividly describe certain events of Samoan culture in which he found to be significant. In the text, Tuiteleleapaga states that his text is not a publication that contains any scientific arguments or dissertations. The book is treasured among the people of both Samoa and American Samoa, especially the village of Leone. Napoleon's private life was very colorful. He was very charismatic but was also very rigid. He had a firm belief that God is real and that God was the first cause of everything. Napoleon attributed his successes to God and blamed his failures on nature. He never attended",
      "no way acted against the Jews until the early 19th century, when he passed a series of three decrees, one of which became known as the Infamous Decree. Some, such as author Franz Kobler, attribute Napoleon\u2019s change in attitude to Napoleon\u2019s new attachment to France and his newfound desire to protect the interests of the French people. When he was the hero of the Jews, he still was an \"ardent patriot\" of his home island of Corsica. In France, quite early in the Nineteenth Century, Jewish moneylenders were accused of usury, in Alsace, as well as of abusing other rights,"
    ]
  ],
  [
    "Bethmann-Hollweg's telegram is often referred to as Germany's \"blank check\" in relation to Austria-Hungary and Serbia because it promised unconditional support for Austria-Hungary's actions against Serbia, including the possibility of war (German entry into World War I; needed to consult with Chancellor Theobald von Bethmann-Hollweg who he was quite sure would have a similar view).",
    [
      "Chancellor von Bethmann-Hollweg and Zimmermann, the Under Secretary of State. Austria-Hungary had received the so-called 'blank check' promising German support for an Austro-Hungarian attack on Serbia. Count Sz\u0151gy\u00e9ny-Marich's action during this critical month has been much debated by historians, some arguing that he did not fully grasp all the intrinsic details in the conversations he entertained with German leaders, in particular that he exaggerated the German support, and that his reports to Vienna therefore were misleading. Strained by the burdens of the July Crisis, Count Sz\u0151gy\u00e9ny-Marich was succeeded as Ambassador by Prince von Hohenlohe-Schillingsf\u00fcrst on 4 August, his replacement having",
      "for any war once the reserves were called to duty. In July, 1914, Germany gave Austria a \"blank cheque\" in handling its punishment of Serbia regarding the assassination of the heir to the Austrian throne. It meant that Germany would support whatever decision Austria made. Austria decided on war with Serbia, which quickly led to escalation with Russia. Chancellor Bethmann Hollweg on July 6 told the Austrian ambassador in Berlin: Shortly after the war began, the German foreign office issued a statement justifying the Blank Check as necessary for the preservation of Austria, and the Teutonic (German) race in central",
      "needed to consult with Chancellor Theobald von Bethmann-Hollweg who he was quite sure would have a similar view. After his meeting, Sz\u00f6gy\u00e9ny reported to Vienna that Wilhelm \"would regret it if we [Austria-Hungary] let this present chance, which was so favourable for us, go by without utilising it\". This so-called \"blank cheque\" of German support up to and including war was to be the main determining factor in Austrian policy in July 1914. At another meeting held on 5 July, this one at Potsdam palace, German Chancellor Theobald von Bethmann-Hollweg, the Foreign Ministry's State Secretary Arthur Zimmermann, the Minister of"
    ]
  ],
  [
    "Copernicus differed from traditional Aristotelian natural philosophers by moving the Earth from the center of the universe and placing the Sun at the center of the celestial spheres, while also asserting that the Earth is another planet revolving around the fixed sun once a year and turning on its axis once a day.",
    [
      "to be properly applicable to the motion of celestial bodies, although some, such as Bartholomeus Amicus, thought analogically in terms of force and resistance. By the end of the Middle Ages it was the common opinion among philosophers that the celestial bodies were moved by external intelligences, or angels, and not by some kind of an internal mover. Although Nicolaus Copernicus (1473\u20131543) transformed Ptolemaic astronomy and Aristotelian cosmology by moving the Earth from the center of the universe, he retained both the traditional model of the celestial spheres and the medieval Aristotelian views of the causes of its motion. Copernicus",
      "his philosophical treatises. They evince complete independence of traditional doctrines, though they are based on symbolism of numbers, on combinations of letters, and on abstract speculations rather than observation. The earth is a star like other stars, is not the centre of the universe, is not at rest, nor are its poles fixed. The celestial bodies are not strictly spherical, nor are their orbits circular. The difference between theory and appearance is explained by relative motion. Had Copernicus been aware of these assertions he would probably have been encouraged by them to publish his own monumental work. Like Nicole Oresme,",
      "was published during the year of his death, though he had arrived at his theory several decades earlier. The book marks the beginning of the shift away from a geocentric (and anthropocentric) universe with the Earth at its center. Copernicus held that the Earth is another planet revolving around the fixed sun once a year, and turning on its axis once a day. But while Copernicus put the Sun at the center of the celestial spheres, he did not put it at the exact center of the universe, but near it. Copernicus' system used only uniform circular motions, correcting what"
    ]
  ],
  [
    "The concessions offered to British subjects in the Treaty of Nanjing included opening all of China to British merchant companies, legalizing the opium trade, exempting foreign imports from internal transit duties, regulating the coolie trade, allowing a British ambassador to reside in Beijing, and establishing that the English-language version of treaties took precedence over the Chinese language.",
    [
      "demanded the Qing authorities renegotiate the Treaty of Nanking (signed in 1842), citing their most favoured nation status. The British demands included opening all of China to British merchant companies, legalising the opium trade, exempting foreign imports from internal transit duties, suppression of piracy, regulation of the coolie trade, permission for a British ambassador to reside in Beijing and for the English-language version of all treaties to take precedence over the Chinese language. To give Chinese merchant vessels operating around treaty ports the same privileges accorded to British ships by the Treaty of Nanking, British authorities granted these vessels British",
      "Concessions in China Concessions in China were a group of concessions within China, governed and occupied by foreign powers, that are frequently associated with colonialism. Most had extraterritoriality and were enclaves inside key cities that became treaty ports. Other than other minor extraterritorial regions, these concessions no longer exist. Imperial China granted the concessions during the latter Qing Dynasty period (16441911), as a result of the series of \"Unequal Treaties\". They began in 1842's Treaty of Nanjing with the United Kingdom. Under each treaty, China was usually obligated to open more treaty ports for trade and lease out more territory",
      "British Concession (Shanghai) The British Concession or Settlement was a foreign enclave (a \"concession\") in Shanghai within the Qing Empire which existed from around 1845 until its unification with the city's American area to form the Shanghai International Settlement in 1863. The British occupied Shanghai during the First Opium War and it was opened to foreign trade by the terms of the Treaty of Nanking. The British settlement was established by the 1845 Land Regulations, undertaken on the initiative of the intendant Gong Mujiu. On 20 November 1846, a formal concession was established; this was expanded on 27 November 1848."
    ]
  ],
  [
    "The earth is spherical based on the evenly curving surface and the differences in time that occur across the world in a uniform fashion, as observed from the rotation of a sphere.",
    [
      "that the earth is spherical because of the evenly curving surface and the differences in time that was constant and proportional. In laymen terms, the earth must be spherical because they change in time-zones across the world occur in a uniform fashion, as with the rotation of a sphere. The observation of eclipses further confirmed these findings because everyone on earth could see a lunar eclipse, for example, but it would be at different hours. The Almagest also suggest that the earth is at the center of the universe. The basis on which this is found is in the fact",
      "field of the earth is neither perfect nor uniform. A flattened ellipsoid is typically used as the idealized earth, but even if the earth were perfectly spherical, the strength of gravity would not be the same everywhere, because density (and therefore mass) varies throughout the planet. This is due to magma distributions, mountain ranges, deep sea trenches, and so on. If that perfect sphere were then covered in water, the water would not be the same height everywhere. Instead, the water level would be higher or lower depending on the particular strength of gravity in that location. Spherical harmonics are",
      "is the following: In geodesy, the science of measuring Earth's size and shape, a geodesic (from Greek \"geo\", Earth, and \"daiein\", to divide) is the shortest route between two points on the Earth's surface. Approximately, such a route is a segment of a great circle, such as a line of longitude or the equator. These paths are certainly not straight, simply because they must follow the curvature of the Earth's surface. But they are as straight as is possible subject to this constraint. The properties of geodesics differ from those of straight lines. For example, on a plane, parallel lines"
    ]
  ],
  [
    "Jean-Baptiste Colbert helped Louis XIV reorganize France and make it into a world power.",
    [
      "and colony founded in 1642). Louis XIV, known as the \"Sun King\", reigned over France from 1643 until 1715 although his strongest period of personal rule did not begin until 1661 after the death of his Italian chief minister Cardinal Mazarin. Louis believed in the divine right of kings, which asserts that a monarch is above everyone except God, and is therefore not answerable to the will of his people, the aristocracy, or the Church. Louis continued his predecessors' work of creating a centralized state governed from Paris, sought to eliminate remnants of feudalism in France, and subjugated and weakened",
      "English settlers and their respective Indigenous allies, today called King William's War by Americans. Louis XIV of France had emerged from the Franco-Dutch War in 1678 as the most powerful monarch in Europe, an absolute ruler who had won numerous military victories. Using a combination of aggression, annexation, and quasi-legal means, Louis XIV set about extending his gains to stabilize and strengthen France's frontiers, culminating in the brief War of the Reunions (1683\u201384). The Truce of Ratisbon guaranteed France's new borders for twenty years, but Louis XIV's subsequent actions \u2013 notably his Edict of Fontainebleau (the revocation of the Edict",
      "Louis XIV of France with a Sovereign Council that included intendant Jean Talon. The population grew slowly under French rule, thus remained relatively low as growth was largely achieved through natural births, rather than by immigration. To encourage population growth and to redress the severe imbalance between single men and women, King Louis XIV sponsored the passage of approximately 800 young French women (known as \"les filles du roi\") to the colony. Most of the French were farmers (\"Canadiens\" or \"Habitants\"), and the rate of population growth among the settlers themselves was very high. Authorities in New France became more"
    ]
  ],
  [
    "Italy's connections to its history and traditions were most \"fully snapped\" during the Italian Renaissance.",
    [
      "Renaissance. Italy in the Middle Ages The history of the Italian peninsula during the medieval period can be roughly defined as the time between the collapse of the Western Roman Empire and the Italian Renaissance. Late Antiquity in Italy lingered on into the 7th century under the Ostrogothic Kingdom and the Byzantine Empire under the Justinian dynasty, the Byzantine Papacy until the mid 8th century. The \"Middle Ages\" proper begin as the Byzantine Empire was weakening under the pressure of the Muslim conquests, and the Exarchate of Ravenna finally fell under Lombard rule in 751. Lombard rule ended with the",
      "For more than 2,000 years Italy experienced migrations, invasions and was divided into many independent states until 1861 when it became a nation-state. Due to this comparatively late unification, and the historical autonomy of the regions that comprise the Italian peninsula, many traditions and customs that are now recognized as distinctly Italian can be identified by their regions of origin. Despite the political and social isolation of these regions, Italy's contributions to the cultural and historical heritage of Europe and the world remain immense. The famous elements of Italian culture are its art, music, style, and iconic food. Italy was",
      "Italy in the Middle Ages The history of the Italian peninsula during the medieval period can be roughly defined as the time between the collapse of the Western Roman Empire and the Italian Renaissance. Late Antiquity in Italy lingered on into the 7th century under the Ostrogothic Kingdom and the Byzantine Empire under the Justinian dynasty, the Byzantine Papacy until the mid 8th century. The \"Middle Ages\" proper begin as the Byzantine Empire was weakening under the pressure of the Muslim conquests, and the Exarchate of Ravenna finally fell under Lombard rule in 751. Lombard rule ended with the invasion"
    ]
  ],
  [
    "The main theme of The Communist Manifesto is the liberation of the proletariat and the analysis of class struggle and conflicts within capitalism.",
    [
      "Engels wrote to Marx recommending a further re-draft, in historical prose: Following the second congress of the Communist League, it commissioned Marx to write a final program. Drawing directly upon the ideas in \"Principles of Communism\", Marx delivered a final revision, the \"Manifesto\", in early 1848. Although Marx was the exclusive author of the \"Manifesto's\" manuscript, the ideas were adapted from Engels' earlier drafts, with the result that the \"Manifesto\" was credited to both authors. Beginning with a definition of communism as a political theory for the liberation of the proletariat, Engels provides a brief history of the proletariat as",
      "as the \"Manifesto's \" first section, its second half was rephrased as the \"Manifesto's\" second section, its penultimate question-answer pair was significantly expanded to become the \"Manifesto's\" third section, and its final question-answer pair was rephrased as the \"Manifesto's\" fourth and final brief section. Due to their brevity, all questions are reproduced verbatim from the English translation given below. The answers are significantly longer and therefore summarized here, except for the first, which is the shortest. Principles of Communism Principles of Communism (German: \"Grunds\u00e4tze des Kommunismus\") is a brief 1847 work written by Friedrich Engels, the co-founder of Marxism. It",
      "The Communist Manifesto The Communist Manifesto (originally Manifesto of the Communist Party) is an 1848 political pamphlet by the German philosophers Karl Marx and Friedrich Engels. Commissioned by the Communist League and originally published in London (in German as \"Manifest der Kommunistischen Partei\") just as the revolutions of 1848 began to erupt, the \"Manifesto\" was later recognised as one of the world's most influential political documents. It presents an analytical approach to the class struggle (historical and then-present) and the conflicts of capitalism and the capitalist mode of production, rather than a prediction of communism's potential future forms. \"The Communist"
    ]
  ],
  [
    "The greatest weakness and regret of King Louis XIV's rule, according to the memoir of Duke Saint-Simon, was his failure to reform French institutions while the monarchy was still secure.",
    [
      "Louis XIV also revoked the Edict of Nantes, forcing thousands of Huguenots into exile. Under Louis XV, Louis XIV's great-grandson, France lost New France and most of its Indian possessions after its defeat in the Seven Years' War, which ended in 1763. Its European territory kept growing, however, with notable acquisitions such as Lorraine (1766) and Corsica (1770). An unpopular king, Louis XV's weak rule, his ill-advised financial, political and military decisions as well as the debauchery of his court discredited the monarchy, which arguably paved the way for the French Revolution 15 years after his death. Louis XVI, Louis",
      "and colony founded in 1642). Louis XIV, known as the \"Sun King\", reigned over France from 1643 until 1715 although his strongest period of personal rule did not begin until 1661 after the death of his Italian chief minister Cardinal Mazarin. Louis believed in the divine right of kings, which asserts that a monarch is above everyone except God, and is therefore not answerable to the will of his people, the aristocracy, or the Church. Louis continued his predecessors' work of creating a centralized state governed from Paris, sought to eliminate remnants of feudalism in France, and subjugated and weakened",
      "upheaval culminating in the French Revolution to his failure to reform French institutions while the monarchy was still secure. Other scholars counter that there was little reason to reform institutions that largely worked well under Louis. They also maintain that events occurring almost 80 years after his death were not reasonably foreseeable to Louis, and that in any case, his successors had sufficient time to initiate reforms of their own. Louis has often been criticised for his vanity. The memoirist Saint-Simon, who claimed that Louis slighted him, criticised him thusly: There was nothing he liked so much as flattery, or,"
    ]
  ],
  [
    "The Molotov-Ribbentrop Pact paved the way for future cooperation between the German Reich and the Soviet Union in 1939.",
    [
      "Molotov\u2013Ribbentrop Pact negotiations The Molotov\u2013Ribbentrop Pact was an August 23, 1939, agreement between the Soviet Union and Nazi Germany colloquially named after Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. The treaty renounced warfare between the two countries. In addition to stipulations of non-aggression, the treaty included a secret protocol dividing several eastern European countries between the parties. Before the treaty's signing, the Soviet Union conducted negotiations with the United Kingdom and France regarding a potential \"Tripartite\" alliance. Long-running talks between the Soviet Union and Germany over a potential economic pact expanded to include the military",
      "claimed power. This agreement paved the way for future cooperation between the two. The German Government, represented by Dr Walther Rathenau, Minister of State, and the Government of the Russian Socialist Federal Soviet Republic, represented by M. Tchitcherin, People's Commissary, have agreed upon the following provisions: Article 1 The two Governments are agreed that the arrangements arrived at between the German \"Reich\" and the Russian Socialist Federal Soviet Republic, with regard to questions dating from the period of war between Germany and Russia, shall be definitely settled upon the following basis: [a] The German \"Reich\" and the Russian Socialist Federal",
      "1939, the Soviet Union discussed the entry of a political and military pact with contingents representing France and Britain, while also discussing a potential deal with Germany. In early August, Germany and the Soviet Union finalized the terms for an economic agreement, but the Soviets delayed executing that agreement until the terms of the political agreement with Germany were finalized. On August 19, Germany and the Soviet Union signed a trade agreement providing for the trade of certain German military and civilian equipment in exchange for Soviet raw materials. The agreement provided that Germany would accept 200 million Reichmarks in"
    ]
  ],
  [
    "reflects Kipling's colonialist views by portraying European superiority over non-white cultures as a moral obligation to civilize and uplift them through imperial domination and exploitation.",
    [
      "The philosophic perspective of \"The Black Man's Burden [A Reply to Rudyard Kipling]\" (1920), by the social critic Hubert Harrison, describes moral degradation as a consequence of being a colonized coloured man and of being a white colonizer. Moreover, since the late 20th-century contexts of post-imperial decolonisation and of the developing world, the phrase \"The white man's burden\" communicates the false good-intentions of Western neo-colonialism for the non-white world: civilisation by colonial domination. The White Man's Burden The White Man's Burden: The United States and the Philippine Islands (1899), by Rudyard Kipling, is a poem about the Philippine\u2013American War (1899\u20131902),",
      "and celebrates British colonialism as a mission of civilisation that would \u2014 eventually \u2014 benefit the colonised natives. In the early 20th century, in addition \"To the Person Sitting in Darkness\" (1901), Mark Twain's factual satire of the civilizing mission proposed, justified, and defended in \"The White Man's Burden'\" (1899), it was Kipling's jingoism that provoked contemporary poetic parodies that expressed anti-imperialist moral outrage, by critically addressing the white-supremacy racism that is basic to colonial empire; among the literary responses to Kipling are: \"The Brown Man's Burden\" (February 1899), by the British politician Henry Labouch\u00e8re; \"The Black Man's Burden: A",
      "Kipling positively represents colonial imperialism as the moral burden of the white race, who are divinely destined to civilise the brutish, non-white Other who inhabits the barbarous parts of the world; to wit, the seventh and eighth lines of the first stanza misrepresent the Filipinos as \"new-caught, sullen peoples, half-devil and half-child.\" Despite the chauvinistic nationalism that supported Western imperialism in the 19th century, public moral opposition to Kipling's racialist misrepresentation of the colonial exploitation of labour in \"The White Man's Burden\" produced the satirical essay \"To the Person Sitting in Darkness\" (1901), by Mark Twain, which catalogues the Western"
    ]
  ],
  [
    "The October Revolution in Russia in 1917 reflected the ideas and predictions outlined in Karl Marx and Friedrich Engels' Communist Manifesto by establishing the world's first socialist state explicitly along Marxist lines.",
    [
      "definition of communism and popularized the term in their famous pamphlet \"The Communist Manifesto\". The 1917 October Revolution in Russia set the conditions for the rise to state power of Vladimir Lenin's Bolsheviks, which was the first time any avowedly communist party reached that position. The revolution transferred power to the All-Russian Congress of Soviets, in which the Bolsheviks had a majority. The event generated a great deal of practical and theoretical debate within the Marxist movement. Marx predicted that socialism and communism would be built upon foundations laid by the most advanced capitalist development. However, Russia was one of",
      "in theory; Marxist works such as the \"Manifesto\" or \"Das Kapital\" were read primarily by party theoreticians. On the other hand, small, dedicated militant parties and Marxist sects in the West took pride in knowing the theory; Hobsbawm says \"This was the milieu in which 'the clearness of a comrade could be gauged invariably from the number of earmarks on his Manifesto. Following the October Revolution of 1917 that swept the Vladimir Lenin-led Bolsheviks to power in Russia, the world's first socialist state was founded explicitly along Marxist lines. The Soviet Union, which Bolshevik Russia would become a part of,",
      "(as occurred in the French Revolution, 1789). Although Russia's political economy principally was agrarian and semi-feudal, the task of democratic revolution therefore fell to the urban, industrial working class as the only social class capable of effecting land reform and democratization, in view that the Russian propertied classes would attempt to suppress any revolution, in town and country. In April 1917, Lenin published the April Theses, the political strategy of the October Revolution (7\u20138 November 1917), which proposed that the Russian revolution was not an isolated national event, but a fundamentally international event\u2014the first world socialist revolution. Thus Lenin's practical"
    ]
  ],
  [
    "Marx and Engels viewed the history of societies as a continuous struggle between different economic classes, with class struggles being the driving force behind historical change.",
    [
      "but as a class. It is upon this realization that the working class reaches class consciousness). Marx believed that all past history is a struggle between hostile and competing economic classes in state of change. Marx and Friedrich Engels collaborated to produce a range of publications based on capitalism, class struggles and socialist movements. These theories and ideologies can be found within three published works: The first publication \"Communist Manifesto\" (1848) argues that \u2018the history of all hitherto existing societies is the history of class struggle\u2019. As class struggle is the engine room of history, to understand the course of",
      "class struggle, summarised in the initial line introducing \"The Communist Manifesto\" (1848): \"The history of all hitherto existing society is the history of class struggles\". Friedrich Engels (28 November 1820 \u2013 5 August 1895) was a German political philosopher who together with Marx co-developed communist theory. Marx and Engels first met in September 1844. Discovering that they had similar views of philosophy and socialism, they collaborated and wrote works such as \"Die heilige Familie\" (\"The Holy Family\"). After Marx was deported from France in January 1845, they moved to Belgium, which then permitted greater freedom of expression than other European",
      "Unlike those Marx described as utopian socialists, Marx determined that \"[t]he history of all hitherto existing society is the history of class struggles\". While utopian socialists believed it was possible to work within or reform capitalist society, Marx confronted the question of the economic and political power of the capitalist class, expressed in their ownership of the means of producing wealth (factories, banks, commerce \u2013 in a word, \"Capital\"). Marx and Engels formulated theories regarding the practical way of achieving and running a socialist system, which they saw as only being achieved by those who produce the wealth in society,"
    ]
  ],
  [
    "Before 1850, the government typically reacted to complaints and protests by workers in the manufacturing sector by prosecuting and imprisoning or sentencing them to hard labour, as their contracts barred them from leaving their assigned estates, showing bias towards plantation owners.",
    [
      "the damning report, conditions for workers worsened in the subsequent years. Indentured workers who attempted to complain about their living and working conditions were often prosecuted and imprisoned or sentenced to hard labour, as the terms of their contracts barred from leaving their assigned estates. Magistrates in Saint Vincentian courts were often biased towards plantation owners, as they belonged to the same social class. A group of Indian male labourers protested by marching to Kingstown from their estate in 1861. This is first known instance of an organized protest by the indentured workers in Saint Vincent. The workers demanded that",
      "these measures, a crisis erupted in 1921. Reacting to massive lay-offs and pay cuts in the steel industry, the mine- and metalworkers' union (BMIAV) started a prolonged strike. The government initially refrained from intervening in the labour dispute. However, the strike movement started to take on revolutionary characteristics. On 1 March, the strikers occupied the factory in Differdange. Giving way to the fears of business-owners and under pressure from the French and Belgian ambassadors, \u00c9mile Reuter abolished the factory councils by decree of 11 March 1921. The government appealed for French troops to intervene, who managed to establish order in",
      "Bagley. Workers writing in the \"Voice\" were sharply critical of the character and effects of the Industrial Revolution. Their concerns mirrored those of the broader American labor movement in the 1840s, a period in which worker unrest was directed towards the loss of status and economic independence experienced under the new economic system. It was, as the historian Norman Ware noted, \"this \"social revolution\" that primarily affected the industrial worker in this period, and against which his protests were made. At the center of this revolution was a dramatic loss of control over economic life, as workers were made to"
    ]
  ],
  [
    "Pico della Mirandola believed that humankind was unique and worthy of admiration because humans have the potential to embody the values of the Cherubim and ascend to higher spiritual states, reflecting the divinity of God.",
    [
      "Mirandola emphasizes the Cherubim and that by embodying the values of the Cherub, one can be equally prepared for \"the fire of the Seraphim and the judgement of the Thrones\". This deviation into the hierarchy of angels makes sense when Pico della Mirandola makes his point that a philosopher \"is a creature of Heaven and not of earth\" because they are capable of obtaining any one of the statuses. In the \"Oration\", Pico justified the importance of the human quest for knowledge within a Neoplatonic framework. He writes that after God had created all creatures, He conceived of the desire",
      "capacity and human perspective. Arriving in a place near Florence, he taught the amazing capacity of human achievement. \"Pico himself had a massive intellect and studied everything there was to be studied in the university curriculum of the Renaissance; the \"Oration\" in part is meant to be a preface to a massive compendium of all the intellectual achievements of humanity, a compendium that never appeared because of Pico's early death.\" Pico della Mirandola intended to speak in front of an invited audience of scholars and clerics of the dignity of the liberal arts and about the glory of angels. Of",
      "that all of creation constitutes a symbolic reflection of the divinity of God, Pico's philosophies had a profound influence on the arts, helping to elevate writers and painters from their medieval role as mere artisans to the Renaissance ideal of the artist as genius. The \"Oration\" also served as an introduction to Pico's 900 theses, which he believed to provide a complete and sufficient basis for the discovery of all knowledge, and hence a model for mankind's ascent of the chain of being. The 900 Theses are a good example of humanist syncretism, because Pico combined Platonism, Neoplatonism, Aristotelianism, Hermeticism"
    ]
  ],
  [
    "The text most directly influenced by the cultural diffusion described by Bertrand Russell, leading to the rediscovery of Plato's original writings post the fall of Constantinople in 1453 is Marsilio Ficino's publication of the first complete translation of Plato's dialogues in 1484, which sparked debates and philosophical writings in the 15th century Renaissance.",
    [
      "never been without readers since the time they were written. Plato's thought is often compared with that of his most famous student, Aristotle, whose reputation during the Western Middle Ages so completely eclipsed that of Plato that the Scholastic philosophers referred to Aristotle as \"the Philosopher\". However, in the Byzantine Empire, the study of Plato continued. The only Platonic work known to western scholarship was Timaeus, until translations were made at a time post the fall of Constantinople, which occurred during 1453, George Gemistos Plethon brought Plato's original writings from Constantinople in the century of its fall. It is believed",
      "channels: All mediaeval thought up to the twelfth century was Neoplatonic rather than Aristotelian; and such popular authors of the Middle Ages as Augustine, Boethius, and the Pseudo-Dionysius carried Christian Neoplatonism to England as they did to all other parts of Western Europe. From the Twelfth Century, the works of Aristotle became increasingly available and his philosophy came to dominate late medieval Scholasticism. Plato's dialogues were preserved in the Byzantine Empire and Latin translations of individual dialogues began to appear in Italy early in the Renaissance. Marsilio Ficino (1433 \u2013 1499) published the first complete translation in 1484 and this",
      "also Plato affected the Renaissance by causing debates over man\u2019s place in the universe, the immortality of the soul, and the ability of man to improve himself through virtue. The flourishing of philosophical writings in the 15th century revealed the impact of Greek philosophy and science on the Renaissance. The resonance of these changes lasted through the centuries following the Renaissance not only in the writing of humanists, but also in the education and values of Europe and western society even to the present day. Deno Geanakopoulos in his work on the contribution of Byzantine scholars to Renaissance has summarised"
    ]
  ],
  [
    "Queen Victoria's busy social calendar reveals the role and expectations of the English monarchy in the nineteenth century as being heavily focused on public appearances, social engagements, and maintaining a visible presence in society.",
    [
      "landmark study, \"Prostitution\", William Acton reported that the police estimated there were 8,600 in London alone in 1857. by \" (1938) online, 608pp of primary sources Victorian era In the history of the United Kingdom, the Victorian era was the period of Queen Victoria's reign, from 20 June 1837 until her death on 22 January 1901. The era followed the Georgian period and preceded the Edwardian period, and its later half overlaps with the first part of the \"Belle \u00c9poque\" era of Continental Europe. In terms of moral sensibilities and political reforms, this period began with the passage of the",
      "the strictest sense, the Victorian era covers the duration of Victoria's reign as Queen of the United Kingdom of Great Britain and Ireland, from her accession on 20 June 1837\u2014after the death of her uncle, William IV\u2014until her death on 22 January 1901, after which she was succeeded by her eldest son, Edward VII. Her reign lasted for 63 years and seven months, a longer period than any of her predecessors. The term 'Victorian' was in contemporaneous usage to describe the era. The era has also been understood in a more extensive sense as a period that possessed sensibilities and",
      "was the period of Queen Victoria's rule between 1837 and 1901 which signified the height of the British Industrial Revolution and the apex of the British Empire. Scholars debate whether the Victorian period\u2014as defined by a variety of sensibilities and political concerns that have come to be associated with the Victorians\u2014actually begins with the passage of the Reform Act 1832. The era was preceded by the Regency era and succeeded by the Edwardian period. Victoria became queen in 1837 at age 18. Her long reign saw Britain reach the zenith of its economic and political power, with the introduction of"
    ]
  ],
  [
    "The Molotov-Ribbentrop Pact of 1939 included a non-aggression agreement between Germany and the Soviet Union, as well as a secret protocol dividing several eastern European countries between the two parties.",
    [
      "Molotov\u2013Ribbentrop Pact negotiations The Molotov\u2013Ribbentrop Pact was an August 23, 1939, agreement between the Soviet Union and Nazi Germany colloquially named after Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. The treaty renounced warfare between the two countries. In addition to stipulations of non-aggression, the treaty included a secret protocol dividing several eastern European countries between the parties. Before the treaty's signing, the Soviet Union conducted negotiations with the United Kingdom and France regarding a potential \"Tripartite\" alliance. Long-running talks between the Soviet Union and Germany over a potential economic pact expanded to include the military",
      "and France because he was not encumbered with the baggage of collective security and could more easily negotiate with Germany. Geoffrey Roberts argued that Litvinov's dismissal helped the Soviets with British-French talks, because Litvinov doubted or maybe even opposed such discussions. Molotov\u2013Ribbentrop Pact negotiations The Molotov\u2013Ribbentrop Pact was an August 23, 1939, agreement between the Soviet Union and Nazi Germany colloquially named after Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. The treaty renounced warfare between the two countries. In addition to stipulations of non-aggression, the treaty included a secret protocol dividing several eastern European countries",
      "German and Soviet officials concluded the 1939 German\u2013Soviet Commercial Agreement, an economic mutual understanding that exchanged Soviet Union raw materials with Germany in exchange for weapons, military technology and civilian machinery. Two days later, the Soviets suspended the tripartite military talks. On 24 August, the Soviet Union and Germany signed the political and military deal that accompanied the trade agreement, the Molotov\u2013Ribbentrop Pact. This pact was an agreement of mutual non-aggression that contained secret protocols dividing the states of northern and eastern Europe into German and Soviet spheres of influence. The Soviet sphere initially included Latvia, Estonia and Finland. Germany"
    ]
  ],
  [
    "The changing attitudes towards post-war policies in Britain after the First World War led to the implementation of affordable council housing schemes, such as Lloyd George's Housing Act 1919, which allowed people to move out of Victorian inner-city slums and promoted working-class owner-occupation.",
    [
      "Britain today. When the loss of Britain's architectural heritage reached its height at the rate of one house every five days in 1955, few were particularly interested or bothered. In the immediate aftermath of World War II, to the British public still suffering from the deprivations of food rationing and restriction on building work the destruction of these great redundant houses was of little interest. From 1914 onwards there had been a huge exodus away from a life in domestic service; having experienced the less restricted and better paid life away from the great estates, few were anxious to return",
      "also people from all over the empire, together and this mixing was seen as a great leveller which would only accelerate social change after the war. The social reforms of the previous century continued into the twentieth with the Labour Party being formed in 1900, but this did not achieve major success until the 1922 general election. Lloyd George said after the World War that \"the nation was now in a molten state\", and his Housing Act 1919 would lead to affordable council housing which allowed people to move out of Victorian inner-city slums. The slums, though, remained for several",
      "aspirations of upward mobility and made possible the fastest rate of growth in working-class owner-occupation during the 20th century. The boom was largely financed by the savings ordinary Britons put into their building societies. Starting in the 1920s favourable tax policies encouraged substantial investment in the societies, creating huge reserves for lending. Beginning in 1927, the societies encouraged borrowing through gradual liberalization of mortgage terms. Housing was a critical shortage in the post-war era. Air raids had destroyed half a million housing units; repairs and maintenance on undamaged homes had been postponed. 3,000,000 new dwellings were needed. The government aimed"
    ]
  ],
  [
    "The first moons of a planet other than Earth to be discovered by Galileo Galilei in 1610 were the four Galilean moons of Jupiter.",
    [
      "the Solar System. On 25 August 1609, Galileo Galilei demonstrated his first telescope to a group of Venetian merchants, and in early January 1610, Galileo observed four dim objects near Jupiter, which he mistook for stars. However, after a few days of observation, Galileo realized that these \"stars\" were in fact orbiting Jupiter. These four objects (later named the Galilean moons in honor of their discoverer) were the first celestial bodies observed to orbit something other than the Earth or Sun. Galileo continued to observe these moons over the next eighteen months, and by the middle of 1611 he had",
      "Jupiter in 1610, which are now collectively known as the Galilean moons, in his honor. This discovery was the first known observation of satellites orbiting another planet. He also found that our Moon had craters and observed, and correctly explained, sunspots, and that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these facts demonstrated incompatibility with the Ptolemaic model, which could not explain the phenomenon and would even contradict it. With the moons it demonstrated that the earth does not have to have everything orbiting it and that other parts of the solar system could",
      "he was able to see celestial bodies more distinctly than was ever possible before. This allowed Galilei to observe in either December 1609 or January 1610 what came to be known as the Galilean moons. On January 7, 1610, Galileo wrote a letter containing the first mention of Jupiter's moons. At the time, he saw only three of them, and he believed them to be fixed stars near Jupiter. He continued to observe these celestial orbs from January 8 to March 2, 1610. In these observations, he discovered a fourth body, and also observed that the four were not fixed"
    ]
  ],
  [
    "Isabella Beeton believed that a strong and stable family unit was essential for the happiness, comfort, and well-being of a family.",
    [
      "what is needed to make family life perform its essential functions.\" \"The modern world needs people with a complex identity who are intellectually autonomous and prepared to cope with uncertainty; who are able to tolerate ambiguity and not be driven by fear into a rigid, single-solution approach to problems, who are rational, foresightful and who look for facts; who can draw inferences and can control their behavior in the light of foreseen consequences, who are altruistic and enjoy doing for others, and who understand social forces and trends.\" \"A successful mother sets her children free and becomes free herself in",
      "society as its meets women's needs: \"The strength and stability of families determines the vitality and moral life of society; thus, as the family goes, so goes the nation\". The family, as well as standing \"at the center of this world\" and \"representing the building block of society,\" also teaches children \"moral values\" that will benefit them and society as they grow to become moral citizens. Schlafly stated unequivocally that \"the future of our nation depends on children who grow up to be good citizens, and the best way of achieving that goal is to have emotionally stable, intact families\".",
      "no longer arranged for economic, social or political gain, and children are no longer expected to contribute to family income. Instead, people choose mates based on love. This increased role of love indicates a societal shift toward favoring emotional fulfilment and relationships within a family, and this shift necessarily weakens the institution of the family. Margaret Mead considers the family as a main safeguard to continuing human progress. Observing, \"Human beings have learned, laboriously, to be human\", she adds: \"we hold our present form of humanity on trust, [and] it is possible to lose it\" ... \"It is not without"
    ]
  ],
  [
    "Voltaire's views on tolerance differed from other philosophers of his time, as he specifically emphasized the importance of religious tolerance in his works such as the \"Trait\u00e9 sur la Tolerance\" and the \"Dictionaire Philosophique\".",
    [
      "the way of human progress. Voltaire's deism is best summarized in his \"Trait\u00e9 sur la Tolerance\", the \"Dictionaire Philosophique\", and \"Lettres Philosophiques\". His conviction was that if God did not exist, it would be necessary to invent him and his conviction fits nicely with the contemporary view of psychology in explaining the need for religion even in an enlightened world. Voltaire attacked faith in a Christian God and the superstitions in the teachings of the Catholic Church, raising an element of doubt over many old practices of the Judeo-Christian tradition. He attempted to convince his readers that there were certain",
      "to have said, \"I detest what you write, but I would give my life to make it possible for you to continue to write.\" Nevertheless, scholars believe there must have again been misinterpretation, as the letter does not seem to contain any such quote. Voltaire's first major philosophical work in his battle against \"\"l'inf\u00e2me\"\" was the \"Trait\u00e9 sur la tol\u00e9rance\" (\"Treatise on Tolerance\"), exposing the Calas affair, along with the tolerance exercised by other faiths and in other eras (for example, by the Jews, the Romans, the Greeks and the Chinese). Then, in his \"Dictionnaire philosophique\", containing such articles as",
      "that hotter climates create hot-tempered people and colder climates aloof people, whereas the mild climate of France is ideal for political systems. This theory may possibly have been influenced by similar sentiment expressed in \"Germania\", an ethnographic writing by Tacitus, a writer frequently studied by Montesquieu. Voltaire (1694\u20131778) came to embody the Enlightenment with his criticisms of Church dogma and French institutions, his defence of civil liberties and his support of social reform. The civil liberties for which he fought were those of the right to a free trial and freedom of religion. He is best remembered for his aphorisms"
    ]
  ],
  [
    "Stalin's policy of purges and political repression was NOT implemented to address the need for industrial development and modernization as advocated by Lenin in 1928.",
    [
      "class alone, Lenin argued that a socialist revolution did not necessarily need to be led by or composed of the working class alone, instead contending that a revolution needed to be led by the oppressed classes of society, which in the case of Russia was the peasant class. Stalinism, while not an ideology \"per se\", refers to Stalin's thoughts and policies. Stalin's introduction of the concept \"Socialism in One Country\" in 1924 was a major turning point in Soviet ideological discourse. The Soviet Union did not need a socialist world revolution to construct a socialist society, Stalin claimed. Four years",
      "swept away by the 1917 Revolution. Vladimir Lenin and his New Economic Policy were both failures tainted by Western thought. Duranty felt that Stalin scrapped the New Economic Policy because he had no political competition. The famine in Ukraine demonstrated the lack of organized opposition to Stalin, because his position was never truly threatened by the catastrophe; Stalin's purges surely contributed to this political vacuum. Stalin succeeded in doing what Lenin could only attempt to do, i.e., he \"re-established a dictator of the imperial idea and put himself in charge\" by means of intimidation. \"Stalin didn't look upon himself as",
      "and not as described by Marx in the 19th century. Lenin's pragmatic explanation was the following: \"Our poverty is so great that we cannot, at one stroke, restore full-scale factory, state, socialist production\". To overcome the lack of educated Russians who could operate and administrate industry, Lenin advocated the development of a technical intelligentsia who would propel the industrial development of Russia to self-sufficiency. The principal obstacles to Russian economic development and modernisation were great material poverty and the lack of modern technology, which conditions orthodox Marxism considered unfavourable to communist revolution; and that agricultural Russia was right for developing"
    ]
  ],
  [
    "The sentiments expressed by the Leeds woolen workers in 1786 foreshadowed the historical trend in the mid-19th century of mill owners exploiting labor, using child labor, and leading to organized labor movements due to poor working conditions.",
    [
      "of packhorse routes to emerging woollen markets in Yorkshire, the inhabitants of Milnrow adopted the domestic system, supplementing their income by fellmongering and producing flannel in their weavers' cottages. Coal mining and metalworking also flourished in the Early Modern period, and the farmers, colliers and weavers formed a \"close-knit population of independent-minded workers\". The hamlets of Butterworth coalesced around the commercial and ecclesiastical centre in Milnrow as demand for the area's flannel grew. In the 19th century, the Industrial Revolution supplanted domestic woollen industries and converted the area into a mill town, with cotton spinning as the principal industry. Mass-produced",
      "mills by 1802. The mechanisation of the spinning process in the early factories was instrumental in the growth of the machine tool industry, enabling the construction of larger cotton mills. Limited companies were developed to construct mills, and the trading floors of the cotton exchange in Manchester, created a vast commercial city. Mills generated employment, drawing workers from largely rural areas and expanding urban populations. They provided incomes for girls and women. Child labour was used in the mills, and the factory system led to organised labour. Poor conditions became the subject of expos\u00e9s, and in England, the Factory Acts",
      "the textile industry, mills were built using water then steam power. A cotton mill and weaving shed for calicoes were built by the river in the late 18th century. Later woolcombing and worsted spinning were introduced. By the mid 19th century 500 inhabitants were employed in two worsted-mills, a paper-mill, and other mills. A tannery was established in the 19th century. At this time the opening of the new Leeds Road and Bradford Road greatly increased access for trade. Many houses were built from the middle of the 19th century onwards, including the first row of terraces by the newly"
    ]
  ],
  [
    "The primary industries that drove the economy of Renaissance Florence were the wool and silk industries, as well as the emerging financial industry centered around banking and international trade.",
    [
      "which flowered for much of their reign. Tuscany became a more cohesive and unified state during these years, rather than simply the dominion of a dominating city, Florence. As time went on, Tuscany was transformed in a number of ways, not always positively. Most importantly, the economy underwent a fundamental change in character. The wool industry was decimated during these later years, though the silk industry was, to some extent, able to replace it. Nonetheless, industry, which had shaped and sustained Florence since the Middle Ages, began to decline throughout the 17th century. Investment in business became less lucrative and",
      "modern commercial infrastructure developed, with double-entry book-keeping, joint stock companies, an international banking system, a systematized foreign exchange market, insurance, and government debt. Florence became the centre of this financial industry and the gold florin became the main currency of international trade. The new mercantile governing class, who gained their position through financial skill, adapted to their purposes the feudal aristocratic model that had dominated Europe in the Middle Ages. A feature of the High Middle Ages in Northern Italy was the rise of the urban communes which had broken from the control by bishops and local counts. In much",
      "the Florentine entrepreneurs. Some of these industries even rivalled the traditional industrial districts in Emilia-Romagna and Veneto due to high profits and productivity. In the fourth quarter of 2015, manufacturing increased by 2.4% and exports increased by 7.2%. Leading sectors included mechanical engineering, fashion, pharmaceutics, food and wine. During 2015, permanent employment contracts increased by 48.8 percent, boosted by nationwide tax break. Tourism is the most significant industry in central Florence. From April to October, tourists outnumber local population. Tickets to the Uffizi and Accademia museums are regularly sold out and large groups regularly fill the basilicas of Santa Croce"
    ]
  ],
  [
    "The urgency behind Gorbachev's belief in the necessity of perestroika for the socialist society in the Soviet Union was due to being \"under extraordinary pressure at home, particularly on the economy\" and the need to prop up the centrally planned economy.",
    [
      "Perestroika Perestroika (; ) was a political movement for reformation within the Communist Party of the Soviet Union during the 1980s and 1990s and is widely associated with Soviet leader Mikhail Gorbachev and his glasnost (meaning \"openness\") policy reform. The literal meaning of perestroika is \"restructuring,\" referring to the restructuring of the Soviet political and economic system. Perestroika is sometimes argued to be a significant cause of the dissolution of the Soviet Union, the revolutions of 1989 in Eastern Europe, and the end of the Cold War. Perestroika allowed more independent actions from various ministries and introduced some market-like reforms.",
      "other countries [that had] achieved their independence,\" and who was \"under extraordinary pressure at home, particularly on the economy.\" Perestroika Perestroika (; ) was a political movement for reformation within the Communist Party of the Soviet Union during the 1980s and 1990s and is widely associated with Soviet leader Mikhail Gorbachev and his glasnost (meaning \"openness\") policy reform. The literal meaning of perestroika is \"restructuring,\" referring to the restructuring of the Soviet political and economic system. Perestroika is sometimes argued to be a significant cause of the dissolution of the Soviet Union, the revolutions of 1989 in Eastern Europe, and",
      "of reform was to prop up the centrally planned economy\u2014not to transition to market socialism. Speaking in late summer 1985 to the secretaries for economic affairs of the central committees of the East European communist parties, Gorbachev said: \"Many of you see the solution to your problems in resorting to market mechanisms in place of direct planning. Some of you look at the market as a lifesaver for your economies. But, comrades, you should not think about lifesavers but about the ship, and the ship is socialism.\" Gorbachev initiated his new policy of \"perestroika\" (literally \"restructuring\" in Russian) and its"
    ]
  ],
  [
    "resulted in the transformation of Russian peasants into independent communal proprietors, allowing them to buy land from landlords and initiating the dissolution of the \"mir\" system, leading to the beginnings of a market economy.",
    [
      "being solved instantaneously by imperial decree. It contained complicated problems, deeply affecting the economic, social and political future of the nation. Alexander had to choose between the different measures recommended to him and decide if the serfs would become agricultural laborers dependent economically and administratively on the landlords or if the serfs would be transformed into a class of independent communal proprietors. The emperor gave his support to the latter project, and the Russian peasantry became one of the last groups of peasants in Europe to shake off serfdom. The architects of the emancipation manifesto were Alexander's brother Konstantin, Yakov",
      "business. The Manifesto prescribed that peasants would be able to buy the land from the landlords. Household serfs were the least affected: they gained only their freedom and no land. In Georgia the emancipation took place later, in 1864, and on much better terms for the nobles than in Russia. The serfs were emancipated in 1861, following a speech given by Tsar Alexander II on 30 March 1856. State owned serfs, i.e. the serfs living on Imperial lands, were emancipated later in 1866. Prior to 1861 Russia had two main categories of peasants: with only those owned privately considered to",
      "the \"mir\" would dissolve into individual peasant land owners and the beginnings of a market economy. Alexander II, unlike his father, was willing to deal with this problem. Moving on from a petition from the Lithuanian provinces, a committee \"for ameliorating the condition of the peasants\" was founded and the principles of the abolition considered. The main point at issue was whether the serfs should remain dependent on the landlords, or whether they should be transformed into a class of independent communal proprietors. The land-owners initially pushed for granting the peasants freedom but not any land. The tsar and his"
    ]
  ],
  [
    "The German confederated states most feared radical and liberal publications according to the Carlsbad Resolutions of 1819.",
    [
      "experienced similar turnover among immigrant publications, especially from opinion press, which published little news and focused instead on editorial commentary. By the end of the 19th century, there were over 800 German-language publications in the United States. German immigration was on the decline, however, and with subsequent generations integrating into English-speaking society, the German language press began to struggle. The periodicals that managed to survive in immigrant communities faced an additional challenge with anti-German sentiment during World War I and with the Espionage and Sedition Acts, which authorized censorship of foreign language newspapers. Prohibition also had a destabilizing impact on",
      "the press and other liberal rights were most often uttered in connection with the demand for a united Germany, even though many revolutionaries-to-be had different opinions about whether a republic or a constitutional monarchy would be the best solution for Germany. The German Confederation \"(Deutscher Bund\" 1815\u20131866) was a loose federation of 35 monarchical states and four republican free cities, with a Federal Assembly in Frankfurt. They began to remove internal customs barriers during the Industrial Revolution, and the German Customs Union (\"Zollverein\") was formed among the majority of the states in 1834. In 1840 Hoffmann wrote a song about",
      "for the rising influence of Prussia. After the so-called \"Wars of Liberation\" (\"Befreiungskriege\", the German term for the German part of the War of the Sixth Coalition), many contemporaries had expected a nation-state solution and thus considered the subdivision of Germany as unsatisfactory. Apart from this nationalist component, calls for civic rights influenced political discourse. The Napoleonic \"Code Civil\" had led to the introduction of civic rights in some German states in the early 19th century. Furthermore, some German states had adopted constitutions after the foundation of the German Confederacy. Between 1819 and 1830, the Carlsbad Decrees and other instances"
    ]
  ],
  [
    "exclaimed \"Vive la France!\" upon hearing news of the armistice.",
    [
      "P\u00e9tain was forming a government with a view to signing an armistice and it seemed likely that the French fleet might be seized by the Germans. In a speech to Parliament, Churchill repeated that the Armistice of 22 June 1940 was a betrayal of the Allied agreement not to make a separate peace. Churchill said \"What is the value of that? Ask half a dozen countries; what is the value of such a solemn assurance? ... Finally, the armistice could be voided at any time on any pretext of non-observance ...\" The French fleet had seen little fighting during the",
      "who appeared on the balcony. Clemenceau exclaimed \"Vive la France!\"\u2014the crowd echoed him. At 11:00 a.m., the first peace-gunshot was fired from Fort Mont-Val\u00e9rien, which told the population of Paris that the armistice was concluded, but the population were already aware of it from official circles and newspapers. Although the information about the imminent ceasefire had spread among the forces at the front in the hours before, fighting in many sections of the front continued right until the appointed hour. At 11 a.m. there was some spontaneous fraternization between the two sides. But in general, reactions were muted. A British",
      "and never stopped believing that France could achieve total victory. There were skeptics, however, that believed that Clemenceau, like other war time leaders, would have a short time in office. It was said, \"Like everyone else \u2026 Clemenceau will not last long- only long enough to clean up [the war].\" As the military situation worsened in early 1918, Clemenceau continued to support the policy of total war \u2013 \"We present ourselves before you with the single thought of total war\" \u2013 and the policy of \"la guerre jusqu'au bout\" (war until the end). His speech of 8 March advocating this"
    ]
  ],
  [
    "The greatest influence on Lenin in his desire to industrialize his new nation was the need to address external and internal threats to the young Soviet Union.",
    [
      "Union. All these tensions had the potential to destroy the young Soviet Union and forced Joseph Stalin to introduce rapid industrialization of heavy industry so that the Soviet Union could address external and internal threats if needed. The central aspect of the first Soviet five-year plan was the rapid industrialization of the Soviet Union from October 1928 to December 1932, which was thought to be the most crucial time for Russian industrialization. Lenin himself before the time of his death, knew the importance of building a transitionary state to communism and was quoted saying \"Modern industry is the key to",
      "Petrograd, Lenin declared that the revolution in Russia was not over, but had only begun and that the next step was for the workers' soviets to take full state authority. He issued a thesis outlining the Bolshevik's party programme, including rejection of any legitimacy in the provisional government and advocacy for state power to be given to the peasant and working class through the soviets. The Bolsheviks became the most influential force in the soviets and on 7 November the capitol of the provisional government was stormed by Bolshevik Red Guards in what afterwards known as the \"Great October Socialist",
      "economic growth through the market-oriented New Economic Policy. Several non-Russian nations secured independence after 1917, but three re-united with Russia through the formation of the Soviet Union in 1922. In increasingly poor health, Lenin died at his dacha in Gorki, with Joseph Stalin succeeding him as the pre-eminent figure in the Soviet government. Widely considered one of the most significant and influential figures of the 20th century, Lenin was the posthumous subject of a pervasive personality cult within the Soviet Union until its dissolution in 1991. He became an ideological figurehead behind Marxism\u2013Leninism and thus a prominent influence over the"
    ]
  ],
  [
    "1534, the Act of Supremacy established the king as the supreme head of the Church of England, known as Royal Supremacy.",
    [
      "Acts of Supremacy The Acts of Supremacy are two acts of the Parliament of England passed in 1534 and 1559 which established King Henry VIII of England and subsequent monarchs as the supreme head of the Church of England. Prior to 1534, the supreme head of the English Church was the Pope of the Roman Catholic Church. The first Act of Supremacy was passed on 3 November 1534 (26 Hen. VIII c. 1) by the Parliament of England. It granted King Henry VIII of England and subsequent monarchs Royal Supremacy, such that he was declared the supreme head of the",
      "Church's assets in England and declared the Church of England as the established church with himself as its head. The Act of Supremacy 1534 confirmed the King's status as having supremacy over the church and required the peers to swear an oath recognising Henry's supremacy. Henry's daughter, Mary I, attempted to restore the English Church's allegiance to the Pope and repealed the Act of Supremacy in 1555. Elizabeth I ascended to the throne in 1558 and the next year Parliament passed the Act of Supremacy 1558 that restored the original act. To placate critics, the Oath of Supremacy which peers",
      "Church of England. Royal Supremacy is specifically used to describe the legal sovereignty of the civil laws over the laws of the Church in England. The act declared that the king was \"the only supreme head on Earth of the Church of England\" and that the English crown shall enjoy \"all honours, dignities, preeminences, jurisdictions, privileges, authorities, immunities, profits, and commodities to the said dignity.\" The wording of the act made clear that Parliament was not granting the king the title (thereby suggesting that they had the right to withdraw it later); rather, it was acknowledging an established fact. In"
    ]
  ],
  [
    "The philosopher whose optimistic views are critiqued through the character of Pangloss in Voltaire's Candide is Leibniz.",
    [
      "philosophy of awe and desperation in the face of the infinite by claiming that infinity should be celebrated. While Pascal advocated for making man's rational aspirations more humble, Leibniz was optimistic about the capacity of human reason to further extend itself. This idea was famously mocked by Voltaire in his satirical novel \"Candide\" as baseless optimism of the sort exemplified by the beliefs of one of its characters Dr. Pangloss, which are the opposite of his fellow traveller Martin's pessimism and emphasis on free will. The optimistic position is also called Panglossianism and became an adjective for excessive, even stupendous,",
      "aphorisms on how to live in what he saw as a world filled with deception, duplicity and disillusionment. Voltaire was the first European to be labeled as a pessimist due to his critique of Alexander Pope's optimistic \"An Essay on Man\", and Leibniz' affirmation that \"we live in the best of all possible worlds.\" Voltaire's novel Candide is an extended criticism of theistic optimism and his Poem on the Lisbon Disaster is especially pessimistic about the state of mankind and the nature of God. Though himself a Deist, Voltaire argued against the existence of a compassionate personal God through his",
      "This form of pessimism is not an emotional disposition as the term commonly connotes. Instead, it is a philosophy or worldview that directly challenges the notion of progress and what may be considered the faith-based claims of optimism. Philosophical pessimists are often existential nihilists believing that life has no intrinsic meaning or value. Their responses to this condition, however, are widely varied and often life-affirming. The term pessimism derives from the Latin word \"pessimus\" meaning 'the worst'. It was first used by Jesuit critics of Voltaire's 1759 novel 'Candide, ou l'Optimisme'. Voltaire was satirizing the philosophy of Leibniz who maintained"
    ]
  ],
  [
    "Balboa's motivations are most similar to those of Cort\u00e9s as they both sought out new lands and experiences in the New World. (Balboa passage: \"At length the Indians assured them, that from the top of the next mountain they should discover the ocean which was the object of their wishes.\")",
    [
      "probably spending most of his time in Spain's southern ports of Cadiz, Palos, Sanlucar, and Seville. He finally left for Hispaniola in 1504 and became a colonist. Cort\u00e9s reached Hispaniola in a ship commanded by Alonso Quintero, who tried to deceive his superiors and reach the New World before them in order to secure personal advantages. Quintero's mutinous conduct may have served as a model for Cort\u00e9s in his subsequent career. The history of the conquistadores is rife with accounts of rivalry, jockeying for positions, mutiny, and betrayal. Upon his arrival in 1504 in Santo Domingo, the capital of Hispaniola,",
      "there described: Balboa's finding of the Pacific and Cort\u00e9s's first view of the Valley of Mexico (1519). The Balboa passage: \"At length the Indians assured them, that from the top of the next mountain they should discover the ocean which was the object of their wishes. When, with infinite toil, they had climbed up the greater part of the steep ascent, Balboa commanded his men to halt, and advanced alone to the summit, that he might be the first who should enjoy a spectacle which he had so long desired. As soon as he beheld the South Sea stretching in",
      "Diaz: On meeting a group of inhabitants from Cempoala who gave Cortes and his men food and invited them to their village: Cort\u00e9s was the Spanish conquistador whose expedition to Mexico in 1519 led to the fall of the Aztecs, and led to the conquering of vast sections of Mexico on behalf of the Crown of Castile. Cort\u00e9s wrote of Aztec sacrifice on numerous occasions, one of which in his \"Letters,\" he states: The Anonymous Conquistador was an unknown travel companion of Cort\u00e9s who wrote \"Narrative of Some Things of New Spain and of the Great City of Temestitan\" which"
    ]
  ],
  [
    "Anabaptism was destroyed for supporting adult baptism and pacifism around the same time as Calvinism.",
    [
      "for handicapped or elderly people, and educational institutions on all levels. For example, American Congregationalists founded Harvard (1636), Yale (1701), and about a dozen other colleges. Princeton was a Presbyterian foundation. Calvinism Calvinism (also called the Reformed tradition, Reformed Christianity, Reformed Protestantism, or the Reformed faith) is a major branch of Protestantism that follows the theological tradition and forms of Christian practice set down by John Calvin and other Reformation-era theologians. Calvinists broke from the Roman Catholic Church in the 16th century. Calvinists differ from Lutherans on the real presence of Christ in the Eucharist, theories of worship, and the",
      "very radical and equalitarian; they believed that the apocalypse was very near. They refused to live the old way, and began new communities, creating considerable chaos. A prominent Dutch Anabaptist was Menno Simons, who initiated the Mennonite church. The movement was allowed in the north, but never grew to a large scale. The third wave of the Reformation, that ultimately proved to be permanent, was Calvinism. It arrived in the Netherlands in the 1540s, attracting both the elite and the common population, especially in Flanders. The Catholic Spanish responded with harsh persecution and introduced the Inquisition of the Netherlands. Calvinists",
      "in churches by Huguenot Calvinists had begun in 1560; unlike in the Low Countries they were often physically resisted and repulsed by Roman Catholic crowds, but were to continue throughout the French Wars of Religion. In Anglican England much destruction had already taken place in an organized fashion under orders from the government, while in Northern Europe, groups of Calvinists marched through churches and removed images, a move which \"provoked reactive riots by Lutheran mobs\" in Germany and \"antagonized the neighbouring Eastern Orthodox\" in the Baltic region. In Germany, Switzerland and England, conversion to Protestantism had been enforced on the"
    ]
  ],
  [
    "system, the enslavement of the native population was officially abolished by the Spanish crown.",
    [
      "slavery is illegal throughout the Americas, some indigenous peoples are still enslaved today. The \"encomienda\" system was an agreement between the Council of the Indies and the Spanish crown to exchange education and protection from warring tribes for the use of the land owned by the \"caciques\", lords, or encomenderos and the promise of seasonal labour. Intermittently, the colonists needed to purge these \"anaborios\" (native mercenaries). From the earliest days on the Caribbean islands they settled, the Spanish encomenderos precipitated many revolts and hostilities, both Native American and Spanish in origin, through their harsh treatment. One of the first localities",
      "of the Indians\", the Spanish greatly restricted the power of the encomienda system, which effectively caused abuse by the \"encomenderos\", and officially abolished the enslavement of the native population. The statutes of 1573, within the \"Ordinances Concerning Discoveries,\" forbade unauthorized operations against independent Indian peoples. It required appointment of a \"protector de indios\", an ecclesiastical representative who acted as the protector of the Indians and represented them in formal litigation. Later in the 16th century, in the viceroyalties of New Spain and Peru, thousands of indigenous people were forced to hard work as underground miners in the mines of Potosi,",
      "the policy in Peru, shortly after the 1535 Spanish conquest, Spanish recipients rebelled against the crown, killing the viceroy, Don Blasco N\u00fa\u00f1ez Vela. In Mexico, viceroy Don Antonio de Mendoza decided against implementing the reform, citing local circumstances and the potential for a similar conqueror rebellion. To the crown he said, \"I obey crown authority but do not comply with this order.\" The \"encomienda\" system was ended legally in 1720, when the crown attempted to abolish the institution. The \"encomenderos\" were then required to pay remaining \"encomienda\" laborers for their work. The \"encomiendas\" became very corrupt and harsh. In the"
    ]
  ],
  [
    "Locke believed that the right of revolution was justified by the government's violation of the natural rights of life, liberty, and property of its citizens.",
    [
      "relationships. Locke declared that under natural law, all people have the right to life, liberty, and estate; under the social contract, the people could instigate a revolution against the government when it acted against the interests of citizens, to replace the government with one that served the interests of citizens. In some cases, Locke deemed revolution an obligation. The right of revolution thus essentially acted as a safeguard against tyranny. Locke affirmed an explicit right to revolution in \"Two Treatises of Government\": \u201c\"whenever the Legislators endeavor to take away, and destroy the Property of the People\", or to reduce them",
      "by John Locke in \"Two Treatises of Government\" as part of his social contract theory. Locke declared that under natural law, all people have the right to life, liberty, and estate; under the social contract, the people could instigate a revolution against the government when it acted against the interests of citizens, to replace the government with one that served the interests of citizens. In some cases, Locke deemed revolution an obligation. The right of revolution thus essentially acted as a safeguard against tyranny. Locke affirmed an explicit right to revolution in \"Two Treatises of Government\": \u201c\"whenever the Legislators endeavor",
      "it. While Hobbes argued for near-absolute authority, Locke argued for inviolate freedom under law in his \"Second Treatise of Government\". Locke argued that a government's legitimacy comes from the citizens' delegation to the government of their absolute right of violence (reserving the inalienable right of self-defense or \"self-preservation\"), along with elements of other rights (e.g. property will be liable to taxation) as necessary to achieve the goal of security through granting the state a monopoly of violence, whereby the government, as an impartial judge, may use the collective force of the populace to administer and enforce the law, rather than"
    ]
  ],
  [
    "The need for redemption is alluded to in the excerpts from John Calvin's \"Institutes of Christian Religion\" by emphasizing the inheritance of a sinful nature and guilt from Adam's fall, with redemption through Jesus Christ as the only remedy.",
    [
      "Not only do individuals inherit a sinful nature due to Adam's fall, but since he was the federal head and representative of the human race, all whom he represented inherit the guilt of his sin by imputation. Redemption by Jesus Christ is the only remedy. John Calvin defined original sin in his \"Institutes of the Christian Religion\" as follows: The Council of Trent (1545\u20131563), while not pronouncing on points disputed among Catholic theologians, condemned the teaching that in baptism the whole of what belongs to the essence of sin is not taken away, but is only cancelled or not imputed,",
      "original sin. Why does he do that? By love. The whole of the work of redemption begins with God\u2019s love: \u201cGod so loved the world that he gave his only Son\u201d (Jn 3:16). John Calvin was one of the first systematic theologians of the Reformation. As such, he wanted to solve the problem of Christ's atonement in a way that he saw as just to the Scriptures and Church Fathers, rejecting the need for condign merit. His solution was that Christ's death on the cross paid not a general penalty for humanity's sins, but a specific penalty for the sins",
      "believer should expect a continual struggle against sin. Several chapters are then devoted to the subject of justification by faith alone. He defined justification as \"the acceptance by which God regards us as righteous whom he has received into grace.\" In this definition, it is clear that it is God who initiates and carries through the action and that people play no role; God is completely sovereign in salvation. According to Alister McGrath, Calvin provided a solution to the Reformation problem of how justification relates to sanctification. Calvin suggested that both came out of union with Christ. McGrath notes that"
    ]
  ],
  [
    "Pangloss survived being hanged and dissected by answering the questions properly and pointing out hidden dangers to other prisoners.",
    [
      "false weights: The length of time which one managed to survive upon the stake is reported as quite varied, from a few seconds or minutes to a few hours or 1 to 3 days. The Dutch overlords at Batavia, present day Jakarta, seem to have been particularly proficient in prolonging the lifetime of the impaled, one witnessing a man surviving 6 days on the stake, another hearing from local surgeons that some could survive 8 or more days. A critical determinant for survival length seems to be precisely \"how\" the stake was inserted: If it went into the \"interior\" parts,",
      "saved from execution by answering the questions properly returned to their cells and passed along what the commission was asking. A leftist prisoner \"who had at one time attended a seminary quickly grasped the theological significance of the questions\" and \"spent the night of August 30 sending morse code messages to other cells\" by knocking on the prison walls. He pointed \"out the hidden dangers.\" The questioners wanted to know if prisoners' fathers prayed, fasted, and read the Qur'an because the sons of devout men could be called apostates. If they had not been raised in proper Muslim homes first",
      "scourging that often preceded crucifixion, eventual dehydration, or animal predation. A theory attributed to Pierre Barbet holds that, when the whole body weight was supported by the stretched arms, the typical cause of death was asphyxiation. He wrote that the condemned would have severe difficulty inhaling, due to hyper-expansion of the chest muscles and lungs. The condemned would therefore have to draw himself up by the arms, leading to exhaustion, or have his feet supported by tying or by a wood block. When no longer able to lift himself, the condemned would die within a few minutes. Some scholars, including"
    ]
  ],
  [
    "Erasmus made Humanism more accessible to a wider audience by creating manuals that were quickly introduced into schools and by laying the groundwork for the religious thoughts of the Reformation, thus exerting a lasting influence on scientific life in Germany.",
    [
      "simplest, clearest, and most suitable form for his knowledge; therefore his manuals, even if they were not always original, were quickly introduced into schools and kept their place for more than a century. Knowledge had for him no purpose of its own; it existed only for the service of moral and religious education, and so the teacher of Germany prepared the way for the religious thoughts of the Reformation. He is the father of Christian humanism, which has exerted a lasting influence upon scientific life in Germany. (But it is Erasmus who is called, \"The Prince of the Humanists\".) His",
      "didactic ritual, Erasmus laid the groundwork for Luther. Humanism's intellectual anti-clericalism would profoundly influence Luther. The increasingly well-educated middle sectors of Northern Germany, namely the educated community and city dwellers would turn to Luther's rethinking of religion to conceptualize their discontent according to the cultural medium of the era. The great rise of the burghers, the desire to run their new businesses free of institutional barriers or outmoded cultural practices, contributed to the appeal of humanist individualism. To many, papal institutions were rigid, especially regarding their views on just price and usury. In the North, burghers and monarchs were united",
      "on political and moral topics. The work continued to expand right up to the author's death in 1536 (to a final total of 4,151 entries), confirming the fruit of Erasmus' vast reading in ancient literature. Many of the adages have become commonplace in many European languages, and we owe our use of them to Erasmus. Equivalents in English include: The work reflects a typical Renaissance attitude toward classical texts: to wit, that they were fit for appropriation and amplification, as expressions of a timeless wisdom first uncovered by the classical authors. It is also an expression of the contemporary Humanism:"
    ]
  ],
  [
    "Erasmus contributed to making the \"book of antiquity\" accessible to a wider audience through his collection of Latin proverbs and adages in \"Adagia,\" which became one of the most popular volumes of the century due to its careful attention to a broader range of classical texts.",
    [
      "And while Cicero was considered the \"patron saint of the Renaissance era\" \u2013 a title that would become fundamentally problematic for a number of Christians because of his pagan beliefs \u2013 Erasmus came to be known as the \"prince of humanists\" esteemed by many of his contemporaries as a \"man born to bring back literature.\" Erasmus' stylistic form of writing was often compared to the standards set forth by Cicero, particularly influenced by his \"De Inventione\"; however, Erasmus' primary goal for \"Christian Prince\" and all of his works, as he argued, was to be seen as \"a Christian rather than",
      "the \"Adagia\" could only have happened via the developing intellectual environment in which careful attention to a broader range of classical texts produced a much fuller picture of the literature of antiquity than had been possible, or desired, in medieval Europe. In a period in which \"sententi\u00e6\" were often marked by special fonts and footnotes in printed texts, and in which the ability to use classical wisdom to bolster modern arguments was a critical part of scholarly and even political discourse, it is not surprising that Erasmus' \"Adagia\" was among the most popular volumes of the century. Source: Erasmus, Desiderius.",
      "and those of general human interest. By the 1530s, the writings of Erasmus accounted for 10 to 20 percent of all book sales in Europe. He is credited with coining the adage, \"In the land of the blind, the one-eyed man is king.\" With the collaboration of Publio Fausto Andrelini, he formed a Paremiography (collection) of Latin proverbs and adages, commonly titled \"Adagia\". Erasmus is also generally credited with originating the phrase \"Pandora's box\", arising through an error in his translation of Hesiod's \"Pandora\" in which he confused \"pithos\" (storage jar) with \"pyxis\" (box). His more serious writings begin early"
    ]
  ],
  [
    "John Locke argued that the purpose of society and its legitimate government is to protect personal and property rights, and that individuals may dissolve governments that do not fulfill this purpose.",
    [
      "self-interest to create and interact in society. Locke said that if there were no God and no divine law, the result would be moral anarchy: every individual \"could have no law but his own will, no end but himself. He would be a god to himself, and the satisfaction of his own will the sole measure and end of all his actions.\" The \"Radical Enlightenment\" promoted the concept of separating church and state, an idea that is often credited to English philosopher John Locke (1632\u20131704). According to his principle of the social contract, Locke said that the government lacked authority",
      "the purpose of government is to protect personal and property rights; that people may dissolve governments that do not do so; and that representative government is the best form to protect rights. The United States Declaration of Independence was inspired by Locke in its statement: \"[T]o secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed. That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it\". Nevertheless scholar Ellen Meiksins Wood says that \"there are doctrines of individualism",
      "it. While Hobbes argued for near-absolute authority, Locke argued for inviolate freedom under law in his \"Second Treatise of Government\". Locke argued that a government's legitimacy comes from the citizens' delegation to the government of their absolute right of violence (reserving the inalienable right of self-defense or \"self-preservation\"), along with elements of other rights (e.g. property will be liable to taxation) as necessary to achieve the goal of security through granting the state a monopoly of violence, whereby the government, as an impartial judge, may use the collective force of the populace to administer and enforce the law, rather than"
    ]
  ],
  [
    "Louis Pasteur.",
    [
      "first Paris-Bordeaux race took place on 10\u201312 June 1895, and the first race from Paris to Monte-Carlo in 1911. Scientists in Paris played a leading role in many of major scientific developments of the period, particularly in bacteriology and physics. Louis Pasteur (1822-1895) was a pioneer in vaccination, microbacterial fermentation and pasteurization. He developed the first vaccines against anthrax (1881) and rabies (1885), and the process for stopping bacterial growth in milk and wine. He founded the Pasteur Institute in 1888 to carry on his work, and his tomb is located at the Institute. The physicist Henri Becquerel (1852-1908), while",
      "scientific subjects. Marie Sk\u0142odowska-Curie worked in France, winning the Nobel Prize for Physics in 1903, and the Nobel Prize for Chemistry in 1911. Physicist Gabriel Lippmann invented integral imaging, still in use today. In 1890, Vincent van Gogh died. It was during the 1890s that his paintings achieved the admiration that had eluded them during Van Gogh's life, first among other artists, then gradually among the public. Reactions against the ideals of the Impressionists characterized visual arts in Paris during the \"Belle \u00c9poque\". Among the post-Impressionist movements in Paris were the Nabis, the Salon de la Rose + Croix, the",
      "read a paper in front of the International Congress on Psychiatry in Amsterdam spurring the headline \"The First Born Not Necessarily the Most Brilliant.\" The paper dispelled the belief that birth order influenced intelligence. The focus of Rabinovitch's medical research was the use of electricity as an anesthetic, termed \"electrical anesthesia\". A cable from Paris was reported in 1908, \"The Paris authorities which will permit her to apply the system on a large scale in local insane asylums... Luis Parisot, a prominent scientist said Miss Rabinovitch's discovery is destined to exert a profound influence in the practice of both surgery"
    ]
  ],
  [
    "The treatment of the natives by the Spanish during colonization led to a catastrophic impact on the indigenous population in the Americas, resulting in the decimation of their numbers through violence, enslavement, and the spread of European diseases.",
    [
      "so they had relatively little cultural impact on the societies they engaged. In the Western Hemisphere, the European colonization involved the emigration of large numbers of settlers, soldiers and administrators intent on owning land and exploiting the apparently primitive (as perceived by Old World standards) indigenous peoples of the Americas. The result was that the colonization of the New World was catastrophic: native peoples were no match for European technology, ruthlessness, or their diseases which decimated the indigenous population. Spanish treatment of the indigenous populations caused a fierce debate, the Valladolid Controversy, over whether Indians possessed souls and if so,",
      "America and had arrived at the Plata basin. Colonist violence towards indigenous peoples exacerbated the loss of lives. European colonists perpetrated massacres on the indigenous groups and enslaved them. According to the U.S. Bureau of the Census (1894), the North American Indian Wars of the 19th century cost the lives of about 19,000 Europeans and 30,000 Native Americans. The first indigenous group encountered by Columbus, the 250,000 Ta\u00ednos of Hispaniola, represented the dominant culture in the Greater Antilles and the Bahamas. Within thirty years about 70% of the Ta\u00ednos had died. They had no immunity to European diseases, so outbreaks",
      "The conquerors and colonists of Latin America also had a major impact on the population of Latin America. The Spanish conquistadors committed savage acts of violence against the natives. According to Bartolom\u00e9 de las Casas, the Europeans worked the native population to death, separated the men and the women so they could not reproduce, and hunted down and killed any natives who escaped with dogs. Las Casas claimed that the Spaniards made the natives work day and night in mines and would \"test the sharpness of their blades\" on the natives. Las Casas estimated that around three million natives died"
    ]
  ],
  [
    "The German General Staff did hire Lenin and Trotsky as agents and attempted to discredit the Russian revolutionaries.",
    [
      "question that the German General Staff had hired Lenin and Trotsky and discredited the Russian revolutionaries. The \"New York Evening Post\" challenged the authenticity of the documents on September 21, 1918 by saying they originated with Santeri Nuorteva, a well-known propagandist who had worked for the Communist government the Bolsheviks had established in Finland. Newspapers debated their authenticity for months. The \"New York Times\" reported the CPI's version of the documents in September and detailed the damaging charges, with the newspaper claiming: that the present heads of the Bolshevist governmentLenin and Trotsky and their associatesare German agents... that the Bolshevist",
      "revolt was arranged for by the German Great General Staff and financed by the German Imperial Bank and other German financial institutions... that the Treaty of Brest-Litovsk was a betrayal of the Russian people by German agents, Lenin and Trotsky; that a German-picked \"commander\" was chosen to defend Petrograd against the Germans; that German officers have been secretly received by the Bolshevist government as military advisers, as spies upon the embassies of Russia's allies, as officers in the Russian army, and as directors of the Bolshevist military, foreign and domestic policy... that the present Bolshevist government is not a Russian",
      "by planning the sabotage of a major Russian warship. This success gave him more credibility, once again, in the eyes of the Germans. In April 1917, in a plan strategized together with Parvus, German intelligence sent Vladimir Lenin and a group of 30 of his revolutionary associates from Switzerland through Germany and Sweden in a train car under supervision of Swiss socialist Fritz Platten. Leon Trotsky has responded to these allegations in Volume 2 Chapter 4 of his \"History of the Russian Revolution\". As the depth of Parvus' arrangements with the Imperial Government became known, the revelations ruined relations with"
    ]
  ],
  [
    "Lenin argues in his speech on industrialization and socialist construction that the compromise of allowing capitalist elements in the economy under the New Economic Policy needs to be fixed in order to fully achieve socialism.",
    [
      "to a close, Lenin proposed the New Economic Policy (NEP), a system of state capitalism that started the process of industrialization and post-war recovery. The NEP ended a brief period of intense rationing called \"war communism\" and began a period of a market economy under Communist dictation. The Bolsheviks believed at this time that Russia, being among the most economically undeveloped and socially backward countries in Europe, had not yet reached the necessary conditions of development for socialism to become a practical pursuit and that this would have to wait for such conditions to arrive under capitalist development as had",
      "Gotha Programme\". Lenin claims that socialism will not be perfect since, as Marx said, it has emerged from the womb of capitalism and which is in every respect stamped with the birthmarks of the old society. This society, socialism, will be unable to provide people with total equality, precisely because it is still marked by capitalism. He also explains the difference between the old society and the new as: Lenin states that such a society is indeed socialism as it realizes the two principles of socialism \"he who does not work, neither shall he eat\" and \"an equal amount of",
      "lectures to the issue of the historical roots of Leninism as a form of Marxism. According to Stalin, the Leninism is a product of the conditions of imperialism in which it was formed as a guiding ideology of The Bolshevik Party. Stalin lists three contradictions to which imperialism brings into the capitalist climate: It is this way that imperialism agitates the already present contradictions within the capitalist countries. This builds on Lenin's writings on the nature of imperialism that are most prominent in 1917's \"Imperialism, the Highest Form of Capitalism\". Stalin opens this lecture by referring to the period of"
    ]
  ],
  [
    "Voltaire supported the demand for an enlightened absolute monarch to impose reforms based on reason and justice.",
    [
      "view, based on a pantheistic interpretation of Plato. This induced a revolution in curiosity about nature in general and scientific advance, which opened the gates for technical and economic advance. Furthermore, the individual potential was seen as a never-ending quest for being God-like, paving the way for a view of Man based on unlimited perfection and progress. In the Enlightenment, French historian and philosopher Voltaire (1694\u20131778) was a major proponent. At first Voltaire's thought was informed by the idea of progress coupled with rationalism. His subsequent notion of the historical idea of progress saw science and reason as the driving",
      "\"not only morally wrong and economically inefficient, but also politically unwise.\" As these notions gained more adherents, Britain was forced to end its participation in the slave trade. The leaders of the Enlightenment were not especially democratic, as they more often look to absolute monarchs as the key to imposing reforms designed by the intellectuals. Voltaire despised democracy and said the absolute monarch must be enlightened and must act as dictated by reason and justice \u2013 in other words, be a \"philosopher-king\". In several nations, rulers welcomed leaders of the Enlightenment at court and asked them to help design laws",
      "in the French people. As a result, he decided to weed out those he believed could never possess this virtue. The result was a continual push towards Terror. The Convention used this as justification for the course of action to \u201ccrush the enemies of the revolution, ... let the laws be executed, \u2026 and let liberty be saved.\u201d These members of the Enlightenment movement greatly influenced revolutionary leaders; however, cautions from other Enlightenment thinkers were blatantly ignored. Voltaire\u2019s warnings were often overlooked, though some of his ideas were used for justification of the Revolution and the start of the Terror."
    ]
  ],
  [
    "Articles 10 to 22 of the Declaration of the Rights of Man and of the Citizen focused on safety and property.",
    [
      "Declaration of the Rights of Man and of the Citizen The Declaration of the Rights of Man and of the Citizen (), set by France's National Constituent Assembly in 1789, is a human civil rights document from the French Revolution. The Declaration was drafted by the Abb\u00e9 Siey\u00e8s and the Marquis de Lafayette, in consultation with Thomas Jefferson. Influenced by the doctrine of \"natural right\", the rights of man are held to be universal: valid at all times and in every place, pertaining to human nature itself. It became the basis for a nation of free individuals protected equally by",
      "are liberty, equality, safety, property, social security, and resistance to oppression. Articles 2 to 9 treat of liberty and equality and define these two terms. Articles 10 to 22 treat of safety and property. Article 23 declares a right to elementary instruction. Article 24 treat of public relief. Articles 25 to 30 treat of social security. Articles 31 and 32 treat of resistance to oppression. Article 33, the last one, declares the right of the people to review, reform and modify the constitution. The territory of the French Republic keeps the preexisting 85 departments. The departments themselves are divided into",
      "attention to a set of feminist concerns that collectively reflected and influenced the aims of many French Revolution activists. The Declaration of the Rights of Man and of the Citizen was adopted in 1789 by the National Constituent Assembly (\"Assembl\u00e9e nationale constituante\"), during the French revolution . Prepared and proposed by the marquis de Lafayette, the declaration asserted that all men \"are born and remain free and equal in rights\" and that these rights were universal. The Declaration of the Rights of Man and of the Citizen became a key human rights document and a classic formulation of the rights"
    ]
  ],
  [
    "The French government addressed health and hunger problems in the seventeenth century by implementing rationing, creating food charts and tickets for bread, meat, butter, and cooking oil, but the system was mismanaged, leading to malnourishment, black markets, and hostility towards state management of the food supply.",
    [
      "bread was often consumed three times a day by the people of France. According to Brace, bread was referred to as the basic dietary item for the masses, and it was also used as a foundation for soup. In fact, bread was so important that harvest, interruption of commerce by wars, heavy flour exploration, and prices and supply were all watched and controlled by the French Government. Among the underprivileged, constant fear of famine was always prevalent. From 1725 to 1789, there was fourteen years of bad yields to blame for low grain supply. In Bordeaux, during 1708-1789, thirty-three bad",
      "meat, 20 percent of the produce, and 80 percent of the Champagne. Supply problems quickly affected French stores which lacked most items. Faced with these difficulties in everyday life, the government answered by rationing, and creating food charts and tickets which were to be exchanged for bread, meat, butter and cooking oil. The rationing system was stringent but badly mismanaged, leading to malnourishment, black markets, and hostility to state management of the food supply. The official ration provided starvation level diets of 1,300 or fewer calories a day, supplemented by home gardens and, especially, black market purchases. Hunger prevailed, especially",
      "dramatically. The 13 decades from 1335 to 1450 spawned a series of economic catastrophes, with bad harvests, famines, plagues and wars that overwhelmed four generations of Frenchmen. The population had expanded, making the food supply more precarious. The bubonic plague (\"Black Death\") hit Western Europe in 1347, killing a third of the population, and it was echoed by several smaller plagues at 15 year intervals. The French and English armies during the Hundred Years War marched back and forth across the land; they did not massacre civilians, but they drained the food supply, disrupted agriculture and trade, and left disease"
    ]
  ],
  [
    "Kipling can be said to be offering Americans the message that it is their duty to assume colonial control and annexation of the Filipino people and their country, in line with the belief in the civilizing mission of the white colonizer.",
    [
      "military atrocities of revenge committed against the Chinese people for their anti-colonial Boxer Rebellion (1899\u20131901) against abusive European businessmen and Christian missionaries. Politically, Kipling proffered the poem to New York governor Theodore Roosevelt (1899\u20131900) to help him persuade anti-imperialist Americans to accept the territorial annexation of the Philippine Islands to the United States. In September 1898, Kipling's literary reputation in the U.S. allowed his promotion of American empire to governor Roosevelt: As Victorian imperial poetry, \"The White Man's Burden\" thematically corresponds to Kipling's belief that the British Empire (1583\u20131945) was the Englishman's \"Divine Burden to reign God's Empire on Earth\";",
      "The White Man's Burden The White Man's Burden: The United States and the Philippine Islands (1899), by Rudyard Kipling, is a poem about the Philippine\u2013American War (1899\u20131902), which exhorts the U.S. to assume colonial control of the Filipino people and their country. Kipling originally wrote the poem to celebrate the Diamond Jubilee of Queen Victoria (22 June 1897), but it was replaced with the sombre poem \"Recessional\" (1897), also a Kipling work about empire. He rewrote \"The White Man's Burden\" to encourage American colonization and annexation of the Philippine Islands, a Pacific Ocean archipelago conquered in the three-month Spanish\u2013American War",
      "second line of Rudyard Kipling\u2019s 1899 poem on the civilizing mission of the white colonizer, \"The White Man's Burden.\" Written for Queen Victoria's Jubilee in 1897, and revised as a response \"to resistance in the Philippines to the United States' assumption of colonial power\" after the Spanish\u2013American War of 1898., the poem's depiction of whites as having the responsibility to rule the nonwhite peoples of the world with beneficence implies the superiority of the white race. In \"The Best Ye Breed\", this notion of white supremacy is rejected at the socioeconomic level when Crawford and his men reveal to Paul"
    ]
  ],
  [
    "The Soviet Union agreed to the Molotov-Ribbentrop Pact with Germany in 1939 in order to avoid being encumbered with the baggage of collective security and to more easily negotiate with Germany.",
    [
      "Molotov\u2013Ribbentrop Pact negotiations The Molotov\u2013Ribbentrop Pact was an August 23, 1939, agreement between the Soviet Union and Nazi Germany colloquially named after Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. The treaty renounced warfare between the two countries. In addition to stipulations of non-aggression, the treaty included a secret protocol dividing several eastern European countries between the parties. Before the treaty's signing, the Soviet Union conducted negotiations with the United Kingdom and France regarding a potential \"Tripartite\" alliance. Long-running talks between the Soviet Union and Germany over a potential economic pact expanded to include the military",
      "and France because he was not encumbered with the baggage of collective security and could more easily negotiate with Germany. Geoffrey Roberts argued that Litvinov's dismissal helped the Soviets with British-French talks, because Litvinov doubted or maybe even opposed such discussions. Molotov\u2013Ribbentrop Pact negotiations The Molotov\u2013Ribbentrop Pact was an August 23, 1939, agreement between the Soviet Union and Nazi Germany colloquially named after Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. The treaty renounced warfare between the two countries. In addition to stipulations of non-aggression, the treaty included a secret protocol dividing several eastern European countries",
      "us out, as happened in the last war. The Molotov\u2013Ribbentrop Pact signed in August 1939 was a non-aggression agreement between Nazi Germany and the Soviet Union. It contained a secret protocol aiming to return Central Europe to the pre\u2013World War I \"status quo\" by dividing it between Germany and the Soviet Union. Finland, Estonia, Latvia and Lithuania would return to the Soviet control, while Poland and Romania would be divided. The Eastern Front was also made possible by the German-Soviet Commercial Agreement (1941) in which the Soviet Union gave Nazi Germany the resources necessary to launch military operations in Eastern"
    ]
  ],
  [
    "Germany and Austria-Hungary planned to break up Pan-Slavic agitation and influence other countries in the region by dissolving Austria-Hungary to prevent Germany from acquiring substantial influence in the future and by creating a ring of states friendly to the Allies to contain Austria and Hungary.",
    [
      "German expansion, reliant on Russia. Germany was seen as a threat due to its Germanization policies, and slow but steady expansion of influence over the Slavic lands. Compared to Pan-Slavism, seen as subservient to the Russian interests, instead of a Russian dominance over all the Slavs advocated by Pan-Slavism it aimed at a more balanced federation of Slavic states, which was hoped to emerge from a reformed Austria-Hungary. It has also been described as a final evolution of Austro-Slavism. Outside of Austria it aimed at reconciliation between Poles and Russians, with Russian neo-slavists declaring their support to recreation of independent",
      "decided to dissolve the Central-European great power, Austria-Hungary, a strong German supporter and fast developing region, was to prevent Germany from acquiring substantial influence in the future. The Western powers' main priority was to prevent a resurgence of the German Reich and they therefore decided that her allies in the region, Austria and Hungary, should be \"contained\" by a ring of states friendly to the Allies, each of which would be bigger than either Austria or Hungary. Compared to the Habsburg Kingdom of Hungary, post-Trianon Hungary had 60% less population and its political and economic footprint in the region was",
      "Russia, and Russia competing with the newly renamed Austro-Hungarian Empire for an increased role in the Balkans at the expense of the Ottoman Empire, the foundations were in place for creating the diplomatic alliances that would lead to World War I. Russia subsequently stood aside as Austria was evicted from the Italian and German states. The Habsburgs therefore gave in to Hungarian demands for autonomy and refounded their state as the Austro-Hungarian Empire. Austrian officials worried that Russia was adopting a pan-Slavist policy designed to unite all Slavonic-speaking peoples under the Tsar's leadership. This lead them to pursue an anti-Slavic"
    ]
  ],
  [
    "Klemens von Metternich led a political reaction characterized by his resignation and exile to London, as well as the appointment of liberal ministers such as Kolowrat, in response to the Chartists, the French Revolution, and other liberal movements that sparked nationalist sentiments within the Austrian Empire.",
    [
      "of 26. After a successful revolution in France in February 1848 toppled King Louis Philippe I and established a Second French Republic, revolutionary fervor broke out across Europe. In Vienna, Austrian Chancellor Klemens von Metternich resigned his post and leave in exile to London while Emperor Ferdinand I was forced to abdicate the throne in favor of his nephew, Franz Joseph. Across the Austrian Empire, nationalist sentiments among Austria's various ethnic groups led to the revolutions in Austria to take several different forms. Liberal sentiments prevailed extensively among the German Austrians, which were further complicated by the simultaneous events in",
      "of Habsburg-Lorraine and their ministers: Archduke Louis was brother of late Emperor Francis I, Archduke Karl was brother of Ferdinand I, Metternich was responsible for foreign affairs, while Kolowrat was responsible for domestic policy and finances. While Metternich became the symbol of reaction and conservatism, Kolowrat was considered liberal. The State Conference's activities were overshadowed by hostilities between Chancellor Metternich and his rival Kolowrat that finally led to the outbreak of the Revolution of 1848. Metternich was forced to resign and fled to England, while Kolowrat was appointed first Minister-President of Austria on March 20. However, he only the office",
      "asked: This appeal by the emperor encouraged the citizens of Emden to reject the authority of the Count. He had obviously lost his authority, and this triggered the revolution. On 18 March 1595, many people gathered in the Great Church in Emden. Menso Alting warmed them up with a sermon. Then Gerhard Bolardus, a member of the college of forty, held an impassioned speech, in which he called for the overthrow of the Count. Weapons were handed out, and the crowd moved to the town hall. They occupied the city walls and various strategic locations in the city. Following the"
    ]
  ],
  [
    "reacted to the legacy of the Treaty of Nanjing by recognizing the rights of the missionaries only because of the superiority of Western naval and military power, leading to protests from the Boxers.",
    [
      "the Boxers, and issued edicts in their defence, causing protests from foreign powers. In spring 1900, the Boxer movement spread rapidly north from Shandong into the countryside near Beijing. Boxers burned Christian churches, killed Chinese Christians and intimidated Chinese officials who stood in their way. American Minister Edwin H. Conger cabled Washington, \"the whole country is swarming with hungry, discontented, hopeless idlers.\" On 30 May the diplomats, led by British Minister Claude Maxwell MacDonald, requested that foreign soldiers come to Beijing to defend the legations. The Chinese government reluctantly acquiesced, and the next day a multinational force of 435 navy",
      "recognized the rights of the missionaries only because of the superiority of Western naval and military power. Many Chinese associated the missionaries with Western imperialism and resented them, especially the educated classes who feared changes that might threaten their position. As the foreign and missionary presence in China grew, so also did Chinese resentment of foreigners. The Boxers were a peasant mass movement, stimulated by drought and floods in the north China countryside. The Qing dynasty took the side of the Boxers, besieged the foreigners in Beijing in the Siege of the International Legations and was invaded by a coalition",
      "the Boxer Protocol. The Boxers, a peasant movement, had attacked and killed foreign missionaries, nationals and Chinese Christians across northern China in 1899 and 1900. The Qing government and Imperial Army supported the Boxers and under the Manchu general Ronglu, besieged foreign diplomats and civilians taking refuge in the Legation Quarter in Peking. After failing in its initial attempt to relieve the Legation Quarter, in August 1900 the Allied force marched to Peking from Tianjin, defeated the Qing Imperial Army's Wuwei Corps in several engagements, and brought an end to the Boxer Rebellion and the siege. The members of the"
    ]
  ],
  [
    "Stalin's attempt to fix the problem of inadequate industrialization and agricultural development ultimately led to success, as the rapid growth achieved through his Five Year Plans helped the Soviet Union reach the industrial level of capitalist countries in the West.",
    [
      "obedience, mass murder and disappearance, censoring of communications and absence of justice was only reinforced by his successor, Joseph Stalin. Stalin too sought to rapidly industrialize the Soviet Union in a way that was perhaps unrealistic given the aggregate skill level and capital of the population. Acknowledging this inadequacy, Stalin ordered that resources slotted for consumption be redirected to production or exported as a temporary sacrifice on the part of the population for the sake of rapid growth. The model was successful initially, with ideology and nationalism promoting morale despite shortages in resources such as food and construction materials for",
      "saw the need to quickly accumulate capital for the vast industrialization programme introduced with the Five Year Plans starting in 1928. The Bolsheviks hoped that the USSR's industrial base would reach the level of capitalist countries in the West, to avoid losing a future war. (Stalin proclaimed, \"Either we do it, or we shall be crushed.\") Stalin proposed that the grain crisis was caused by \"kulaks\" - relatively wealthy farmers who hoarded grain and participated in speculation of agricultural produce. The peasant farms were too small to support the massive agricultural demands of the Soviet Union's push for rapid industrialization,",
      "shortage in 1928, Stalin reversed himself and proposed a program of rapid industrialization and forced collectivization because he believed that the NEP was not working fast enough. Stalin felt that in the new situation the policies of his former foes\u2013Trotsky, Zinoviev, and Kamenev\u2014were the right ones.Bukharin was worried by the prospect of Stalin's plan, which he feared would lead to \u201cmilitary-feudal exploitation\u201d of the peasantry. Bukharin did want the Soviet Union to achieve industrialization but he preferred the more moderate approach of offering the peasants the opportunity to become prosperous, which would lead to greater grain production for sale abroad."
    ]
  ],
  [
    "Marx and Engels would view the French Revolution as significant based on their belief in the history of class struggles.",
    [
      "oppressed nationalities, and the methodology of history from below. Friedrich Engels' most important historical contribution was \"Der deutsche Bauernkrieg\" (\"The German Peasants' War\"), which analysed social warfare in early Protestant Germany in terms of emerging capitalist classes. \"The German Peasants' War\" indicate the Marxist interest in history from below and class analysis, and attempts a dialectical analysis. Engels' short treatise \"The Condition of the Working Class in England in 1844\" (1870s) was salient in creating the socialist impetus in British politics. Marx's most important works on social and political history include \"The Eighteenth Brumaire of Louis Napoleon\", \"The Communist Manifesto\",",
      "but as a class. It is upon this realization that the working class reaches class consciousness). Marx believed that all past history is a struggle between hostile and competing economic classes in state of change. Marx and Friedrich Engels collaborated to produce a range of publications based on capitalism, class struggles and socialist movements. These theories and ideologies can be found within three published works: The first publication \"Communist Manifesto\" (1848) argues that \u2018the history of all hitherto existing societies is the history of class struggle\u2019. As class struggle is the engine room of history, to understand the course of",
      "and driven by conflicts and contradictions. As Engels clarified: \"The history of all hitherto existing society is the history of class struggles. Freeman and slave, patrician and plebeian, lord and serf, guild-master and journeyman, in a word, oppressor and oppressed, stood in constant opposition to one another, carried on uninterrupted, now hidden, now open fight, a fight that each time ended, either in a revolutionary reconstitution of society at large, or in the common ruin of the contending classes\". Marx considered class conflicts as the driving force of human history since these recurring conflicts have manifested themselves as distinct transitional"
    ]
  ],
  [
    "The influence of Paine's Quaker upbringing, which valued plain speaking and skepticism, likely contributed to his statement criticizing persecution in the last sentence of \"The Age of Reason\".",
    [
      "He contends that Paine draws on the Puritan tradition in which \"theology was wedded to politics and politics to the progress of the kingdom of God\". One reason Paine may have been drawn to this style is because he may have briefly been a Methodist preacher, although this suspicion cannot be verified. \"The Age of Reason\" provoked a hostile reaction from most readers and critics, although the intensity of that hostility varied by locality. There were four major factors for this animosity: Paine denied that the Bible was a sacred, inspired text; he argued that Christianity was a human invention;",
      "plagues and desolation throughout the world.\" Thomas Paine, a Deistic American political theorist and pamphleteer, wrote in his three-part work \"The Age of Reason\" (published in 1794, 1795, and 1807), \"The study of theology, as it stands in Christian churches, is the study of nothing; it is founded on nothing; it rests on no principles; it proceeds by no authorities; it has no data; it can demonstrate nothing; and it admits of no conclusion. Not anything can be studied as a science, without our being in possession of the principles upon which it is founded; and as this is the",
      "a scholar of deism, puts it: \"the age of reason could perhaps more eloquently and adequately be called the age of ridicule, for it was ridicule, not reason, that endangered the Church.\" Significantly, Watson's \"Apology\" directly chastises Paine for his mocking tone: Paine's Quaker upbringing predisposed him to deistic thinking at the same time that it positioned him firmly within the tradition of religious Dissent. Paine acknowledged that he was indebted to his Quaker background for his skepticism, but the Quakers' esteem for plain speaking, a value expressed both explicitly and implicitly in \"The Age of Reason\", influenced his writing"
    ]
  ],
  [
    "The Edict of Nantes granted the Calvinist Protestants in France substantial rights, including freedom of conscience and specific concessions to promote civil unity.",
    [
      "Edict of Nantes The Edict of Nantes (French: \"\u00e9dit de Nantes\"), signed in April 1598 by King Henry IV of France, granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the nation, which was still considered essentially Catholic at the time. In the edict, Henry aimed primarily to promote civil unity. The edict separated civil from religious unity, treated some Protestants for the first time as more than mere schismatics and heretics, and opened a path for secularism and tolerance. In offering general freedom of conscience to individuals, the edict offered many specific concessions to the",
      "followed by most modern historians is the Huguenot refugee \u00c9lie Benoist's \"Histoire de l'\u00e9dit de Nantes\", 3 vols. (Delft, 1693\u201395). E.G. L\u00e9onard devotes a chapter to the Edict of Nantes in his \"Histoire g\u00e9n\u00e9ral du protestantisme\", 2 vols. (Paris) 1961:II:312\u201389. Edict of Nantes The Edict of Nantes (French: \"\u00e9dit de Nantes\"), signed in April 1598 by King Henry IV of France, granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the nation, which was still considered essentially Catholic at the time. In the edict, Henry aimed primarily to promote civil unity. The edict separated civil from",
      "religion did not sit well with Louis XIV's vision of perfected autocracy: \"Bending all else to his will, Louis XIV resented the presence of heretics among his subjects.\" The Edict of Nantes had been issued on 13 April 1598 by Henry IV of France. It had granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the predominantly Catholic state. Through the Edict, Henry had aimed to promote civil unity. The Edict treated some Protestants with tolerance and opened a path for secularism. It offered general freedom of conscience to individuals and many specific concessions to the"
    ]
  ],
  [
    "The economic policy driving the conquistadores' search for gold was the exploitation of indigenous labor to extract precious metals for the benefit of the Spanish colonial economy.",
    [
      "from the colony of Castilla del Oro (Tierra Firme, present-day Panama) under its governor Pedrarias D\u00e1vila, where they were apparently surplus to requirements; they complained that \"they hadn't done a single thing worth the telling\". From Bernal D\u00edaz del Castillo's narrative, it appears possible to deduce \u2014 possibly against the narrator's own pretences because he would prefer to keep this hidden \u2014 that the original goal of the project was to capture Indians as slaves to increase or replace the manpower available to work the agricultural land or the mines of Cuba, and so that the Spaniards resident on the",
      "rule was that a miner could not leave the mine until he paid his debts. If a man died then his children had to work in the mines to pay his debts, so eventually they were in a circle, and rarely came back home. The Spanish conquistadors also utilized the same labor system to supply the workforce they needed for the silver mines, which was the basis of their economy in the colonial period. Under the leadership of Viceroy Francisco de Toledo, who was dispatched to Peru in 1569, the mit'a system greatly expanded as Toledo sought to increase silver",
      "Economic history of Nicaragua The first Spanish explorers of Nicaragua found a well-developed agrarian society in the central highlands and Pacific lowlands. The rich volcanic soils produced a wide array of products, including beans, peppers, corn, cocoa, and cassava (manioc). Agricultural land was held communally, and each community had a central marketplace for trading and distributing food. The arrival of the Spanish in the early 16th century destroyed, for all intents and purposes, the indigenous agricultural system. The early conquistadors were interested primarily in gold; European diseases and forced work in the gold mines decimated the native population. Some small"
    ]
  ],
  [
    "The conflict involving Sadoul, Pascal, and the French expeditionary corps in Ukraine during the Russian Civil War is best described in the book \"Southern Front of the Russian Civil War.\"",
    [
      "Group: Sadoul, who fought for leadership against Pascal, denounced the latter to the Cheka as a Menshevik and a Catholic dissident. In late 1918, a French expeditionary corps commanded by Louis Franchet d'Esp\u00e8rey entered the Ukraine and attempted to contain Bolshevik penetration \"(see Southern Front of the Russian Civil War)\". The Russian government dispatched Sadoul, by then an officer in the Red Army, on a mission to spread anti-war and mutinous propaganda (including \"Vive la R\u00e9publique des Soviets!\") among the French troops. The French commanders recorded that the effect of such work was pervasive and infuriating, leading them to capture",
      "close contacts with the Bolsheviks, pledging them his support against the Central Powers during the crisis of 1917\u20131918. He was unable to prevent Bolshevist Russia from signing the Treaty of Brest-Litovsk, which took her out of the war, but, having established close contacts with Leon Trotsky and other communist leaders, became a communist himself. Opting not to return to France during the Russian Civil War, Sadoul co-founded the French Communist Group in Russia, fighting for control of it against Pierre Pascal and Henri Guilbeaux. Helping to set up the Red Army, he was sent to the Ukraine, where he instigated",
      "while publishing eulogistic portraits of the Russian Bolsheviks. Sadoul's work for the Red Army and the Comintern was branded a treasonous act in France, particularly after revelations about his activities in the Ukraine. In January 1919, a source quoted by \"Le Petit Parisien\" also noted that Sadoul had no redeeming contribution to the repatriation of French hostages in Russia. Together with another expatriate, the Marquis Delafarre, he acted as a Red Army recruiter among the captured French soldiers. In July, Christian Rakovsky, head of the Ukrainian Council of People's Commissars, assigned Sadoul to negotiate an exchange of prisoners with the"
    ]
  ],
  [
    "were convinced of the necessity of freeing the serfs due to the fear of a large-scale revolt by the serfs, evolving cultural sensibilities, and the economic, social, and political implications of serfdom in Russia.",
    [
      "Great, businesses began to send serfs into those areas in an attempt to harvest their large amount of untapped natural resources Tsar Alexander I wanted to reform the system but was thwarted in this ambition. New laws allowed all classes (except the serfs) to own land, a privilege that had previously been confined to the nobility. Russian serfdom was finally abolished in the emancipation reform of 1861 by Tsar Alexander II. Scholars have proposed multiple overlapping reasons to account for the abolition, including fear of a large-scale revolt by the serfs, the government's financial needs, evolving cultural sensibilities, and the",
      "being solved instantaneously by imperial decree. It contained complicated problems, deeply affecting the economic, social and political future of the nation. Alexander had to choose between the different measures recommended to him and decide if the serfs would become agricultural laborers dependent economically and administratively on the landlords or if the serfs would be transformed into a class of independent communal proprietors. The emperor gave his support to the latter project, and the Russian peasantry became one of the last groups of peasants in Europe to shake off serfdom. The architects of the emancipation manifesto were Alexander's brother Konstantin, Yakov",
      "the \"mir\" would dissolve into individual peasant land owners and the beginnings of a market economy. Alexander II, unlike his father, was willing to deal with this problem. Moving on from a petition from the Lithuanian provinces, a committee \"for ameliorating the condition of the peasants\" was founded and the principles of the abolition considered. The main point at issue was whether the serfs should remain dependent on the landlords, or whether they should be transformed into a class of independent communal proprietors. The land-owners initially pushed for granting the peasants freedom but not any land. The tsar and his"
    ]
  ],
  [
    "The Edict of Nantes granted Calvinist Protestants in France the right to hold religious services in certain towns, control and fortify eight cities, have equal civil rights with Catholics, and established special courts to try Huguenot offenders.",
    [
      "Edict of Nantes The Edict of Nantes (French: \"\u00e9dit de Nantes\"), signed in April 1598 by King Henry IV of France, granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the nation, which was still considered essentially Catholic at the time. In the edict, Henry aimed primarily to promote civil unity. The edict separated civil from religious unity, treated some Protestants for the first time as more than mere schismatics and heretics, and opened a path for secularism and tolerance. In offering general freedom of conscience to individuals, the edict offered many specific concessions to the",
      "followed by most modern historians is the Huguenot refugee \u00c9lie Benoist's \"Histoire de l'\u00e9dit de Nantes\", 3 vols. (Delft, 1693\u201395). E.G. L\u00e9onard devotes a chapter to the Edict of Nantes in his \"Histoire g\u00e9n\u00e9ral du protestantisme\", 2 vols. (Paris) 1961:II:312\u201389. Edict of Nantes The Edict of Nantes (French: \"\u00e9dit de Nantes\"), signed in April 1598 by King Henry IV of France, granted the Calvinist Protestants of France (also known as Huguenots) substantial rights in the nation, which was still considered essentially Catholic at the time. In the edict, Henry aimed primarily to promote civil unity. The edict separated civil from",
      "1598, which guaranteed religious liberties to the Protestants, thereby effectively ending the civil war. The main provisions of the Edict of Nantes were as follows: a) Huguenots were allowed to hold religious services in certain towns in each province, b) They were allowed to control and fortify eight cities (including La Rochelle and Montauban), c) Special courts were established to try Huguenot offenders, d) Huguenots were to have equal civil rights with the Catholics. When in 1620 the Huguenots proclaimed a constitution for the 'Republic of the Reformed Churches of France', the chief minister Cardinal Richelieu (1585\u20131642) invoked the entire"
    ]
  ],
  [
    "John Calvin supports the belief in double predestination, where God has preordained some individuals to eternal life and others to eternal damnation.",
    [
      "categories do not comprise Calvinism in its entirety. They simply encapsulate its central, definitive doctrines. The notion that God has foreordained who will be saved is generally called predestination. The concept of predestination peculiar to Calvinism, \"double-predestination\", (in conjunction with limited atonement) is the most controversial expression of the doctrine. According to Reformed theology, the \"good news\" of the gospel of Christ is that God has freely granted the gift of salvation to those the Holy Spirit causes to believe; what he freely grants to some (the \"elect\" individuals), he withholds from others (the \"reprobate\" individuals). Calvin sought to provide",
      "while Martin Bucer suggested that justification causes (moral) regeneration, Calvin argued that \"both justification and regeneration are the results of the believer's union with Christ through faith.\" Near the end of the \"Institutes\", Calvin describes and defends the doctrine of predestination, a doctrine advanced by Augustine in opposition to the teachings of Pelagius. Fellow theologians who followed the Augustinian tradition on this point included Thomas Aquinas and Martin Luther, though Calvin's formulation of the doctrine went further than the tradition that went before him. The principle, in Calvin's words, is that \"All are not created on equal terms, but some",
      "matter, any one of the other Five Points of Calvinism. The central truth proclaimed by Calvinism, Calvinism that is faithful to its heritage, is the absolute sovereignty of God.\" Calvin expressly taught that it is God's sovereign decision to determine whether an individual is saved or damned. He writes \"By predestination we mean the eternal decree of God, by which he determined with himself whatever he wished to happen with regard to every man. All are not created on equal terms, but some are preordained to eternal life, others to eternal damnation; and, accordingly, as each has been created for"
    ]
  ],
  [
    "Voltaire was participating in the Enlightenment cultural movement with his writings on tolerance and religious views.",
    [
      "the way of human progress. Voltaire's deism is best summarized in his \"Trait\u00e9 sur la Tolerance\", the \"Dictionaire Philosophique\", and \"Lettres Philosophiques\". His conviction was that if God did not exist, it would be necessary to invent him and his conviction fits nicely with the contemporary view of psychology in explaining the need for religion even in an enlightened world. Voltaire attacked faith in a Christian God and the superstitions in the teachings of the Catholic Church, raising an element of doubt over many old practices of the Judeo-Christian tradition. He attempted to convince his readers that there were certain",
      "matter of faith, but of reason.\" Voltaire held mixed views of the Abrahamic religions but had a favourable view of Hinduism. In a 1763 essay, Voltaire supported the toleration of other religions and ethnicities: \"It does not require great art, or magnificently trained eloquence, to prove that Christians should tolerate each other. I, however, am going further: I say that we should regard all men as our brothers. What? The Turk my brother? The Chinaman my brother? The Jew? The Siam? Yes, without doubt; are we not all children of the same father and creatures of the same God?\" In",
      "shortcomings, Voltaire was a forerunner of liberal pluralism in his approach to history and non-European cultures. Voltaire wrote, \"We have slandered the Chinese because their metaphysics is not the same as ours ... This great misunderstanding about Chinese rituals has come about because we have judged their usages by ours, for we carry the prejudices of our contentious spirit to the end of the world.\" In speaking of Persia, he condemned Europe's \"ignorant audacity\" and \"ignorant credulity\". When writing about India, he declares, \"It is time for us to give up the shameful habit of slandering all sects and insulting"
    ]
  ],
  [
    "A government loses its legitimacy when it exceeds the boundaries set by the people and the governed have the right to abolish it and start anew. (John Locke, \"Second Treatise of Government\")",
    [
      "matters like applications of the law. Doing so would undermine its generality, and therefore damage its legitimacy. Thus, government must remain a separate institution from the sovereign body. When the government exceeds the boundaries set in place by the people, it is the mission of the people to abolish such government, and begin anew. Rousseau claims that the size of the territory to be governed often decides the nature of the government. Since a government is only as strong as the people, and this strength is absolute, the larger the territory, the more strength the government must be able to",
      "or ever pledged his faith to them. In his \"Second Treatise of Government\", John Locke takes the position that government authority arises from the consent of the governed, and not through the accidental birth of rulers. L.K. Samuels asserts in his \"Rulers' Paradox\" that since the citizenry is the holder of all rights, governmental bodies derive their authority to govern society via elections of government officials. In that vein, Samuels maintains that citizens can only give rights which they have. The Rulers' Paradox comes into play when governmental bodies exercise rights that the citizens do not hold or could not",
      "Chinese political philosophy, since the historical period of the Zhou Dynasty (1046\u2013256 BC), the political legitimacy of a ruler and government was derived from the Mandate of Heaven, and unjust rulers who lost said mandate therefore lost the right to rule the people. In moral philosophy, the term \"legitimacy\" is often positively interpreted as the normative status conferred by a governed people upon their governors' institutions, offices, and actions, based upon the belief that their government's actions are appropriate uses of power by a legally constituted government. The Enlightenment-era British social philosopher John Locke (1632\u20131704) said that political legitimacy derives"
    ]
  ],
  [
    "The full incorporation of the Americas into the United States of America.",
    [
      "Colonial history of the United States The colonial history of the United States covers the history of European colonization of the Americas from the start of colonization in the early 16th century until their incorporation into the United States of America. In the late 16th century, England, France, Spain, and the Netherlands launched major colonization programs in eastern North America. Small early attempts sometimes disappeared, such as the English Lost Colony of Roanoke. Everywhere, the death rate was very high among the first arrivals. Nevertheless, successful colonies were established within several decades. European settlers came from a variety of social",
      "the 17th century. Long-term European exploration of the Americas commenced after the 1492 expedition of Christopher Columbus, and the 1497 expedition of John Cabot is credited with discovering continental North America for Europeans. European exploration of the North America continued in the 16th century, and the area now known as Pennsylvania was mapped by the French and labeled \"L'arcadia\", or \"wooded coast\", during Giovanni da Verrazzano's voyage in 1524. Even before large-scale European settlement, the Native American tribes in Pennsylvania engaged in trade with Europeans, and the fur trade was a major motivation for the European colonization of North America.",
      "in the Iberian Peninsula. Hispanic America also contrasts with Latin America, which includes not only Hispanic America, but also Brazil, as well as the former French colonies in the Western Hemisphere (areas that are now in either the United States of America or Canada are usually excluded). The Spanish conquest of the Americas began in 1492, and ultimately was part of a larger historical process of world discovery, through which various European powers incorporated a considerable amount of territory and peoples in the Americas, Asia, and Africa between the 15th and 20th centuries. Hispanic America became the main part of"
    ]
  ],
  [
    "The Act of Supremacy in 1534 confirmed the King's status as having supremacy over the church and established the Church of England with the monarch as its head.",
    [
      "Church's assets in England and declared the Church of England as the established church with himself as its head. The Act of Supremacy 1534 confirmed the King's status as having supremacy over the church and required the peers to swear an oath recognising Henry's supremacy. Henry's daughter, Mary I, attempted to restore the English Church's allegiance to the Pope and repealed the Act of Supremacy in 1555. Elizabeth I ascended to the throne in 1558 and the next year Parliament passed the Act of Supremacy 1558 that restored the original act. To placate critics, the Oath of Supremacy which peers",
      "Act of Supremacy 1558 The Act of Supremacy (1 Eliz 1 c 1), also referred to as the Act of Supremacy 1559, is an act of the Parliament of England, passed under the auspices of Elizabeth I. It replaced the original Act of Supremacy 1534 issued by Elizabeth's father, Henry VIII, which arrogated ecclesiastical authority to the monarchy, and which had been repealed by Mary I. Along with the Act of Uniformity 1558 it made up what is generally referred to as the Elizabethan Religious Settlement. The Act was passed in 1559 but is dated 1558; this is because until",
      "ought not to be the monarch. Another act (1 Eliz.1 c.6) dealt with sedition. Act of Supremacy 1558 The Act of Supremacy (1 Eliz 1 c 1), also referred to as the Act of Supremacy 1559, is an act of the Parliament of England, passed under the auspices of Elizabeth I. It replaced the original Act of Supremacy 1534 issued by Elizabeth's father, Henry VIII, which arrogated ecclesiastical authority to the monarchy, and which had been repealed by Mary I. Along with the Act of Uniformity 1558 it made up what is generally referred to as the Elizabethan Religious Settlement."
    ]
  ],
  [
    "The ideas expressed by Thomas Paine in \"The Age of Reason\" align with the characteristics of Enlightenment intellectuals by advocating for rationality, progress, human perfectibility, and challenging traditional religious beliefs. (Paine's emphasis on reason and progress mirrors the Enlightenment's focus on rationality and progress as means of societal advancement.)",
    [
      "secular image of utopia,\" emphasizing the possibilities of \"progress\" and \"human perfectibility\" that could be achieved by humankind, without God's aid. Although Paine liked to say that he read very little, his writings belie this statement; \"The Age of Reason\" has intellectual roots in the traditions of David Hume, Spinoza, and Voltaire. Since Hume had already made many of the same \"moral attacks upon Christianity\" that Paine popularized in \"The Age of Reason\", scholars have concluded that Paine probably read Hume's works on religion or had at least heard about them through the Joseph Johnson circle. Paine would have been",
      "diversity.\" The Age of Enlightenment refers to the 18th century in European philosophy, and is often thought of as part of a period which includes the Age of Reason. The term also more specifically refers to a historical intellectual movement, \"The Enlightenment\". This movement advocated rationality as a means to establish an authoritative system of aesthetics, ethics, and logic. The intellectual leaders of this movement regarded themselves as a courageous elite, and regarded their purpose as one of leading the world toward progress and out of a long period of doubtful tradition, full of irrationality, superstition, and tyranny, which they",
      "images and arguments half-formed, Paine encourages his readers to complete them independently. The most distinctive element of Paine's style in \"The Age of Reason\" is its \"vulgarity\". In the eighteenth century \"vulgarity\" was associated with the middling and lower classes and not with obscenity; thus, when Paine celebrates his \"vulgar\" style and his critics attack it, the dispute is over class accessibility, not profanity. For example, Paine describes the Fall this way: The irreverent tone that Paine combined with this vulgar style set his work apart from its predecessors. It took \"deism out of the hands of the aristocracy and"
    ]
  ],
  [
    "The philosophy favored by the schools founded by the Medici family was a combination of humanism and Renaissance ideals.",
    [
      "in 1494, was one of the most prosperous and respected institutions in Europe, and the Medici family was considered the wealthiest in Europe for a time. From this base, they acquired political power initially in Florence and later in wider Italy and Europe. They were among the earliest businesses to use the general ledger system of accounting through the development of the double-entry bookkeeping system for tracking credits and debits. The Medici family bankrolled the invention of Piano and Opera, funded the construction of Saint Peter Basilica and Santa Maria del Fiore, and patronized Leonardo, Michelangelo, Machiavelli and Galileo. They",
      "and cultivation as much as their military campaigns and their economic welfare. Though they encouraged higher learning by giving protection to the various colleges and universities that were established (especially by Catholic religious orders), they also kept a very strict surveillance on all aspects of scholarship. They certainly did not like being picked on by the Inquisition, which could make them look bad with the Pope in Rome. Though philosophy continued to be mainly viewed as the \"hand-maid\" of theology, some intellectuals had an interest in cautiously branching out along some pathways of their own. Though the philosophical contributioins of",
      "Renaissance of the 8th/9th century was fed by Church missionaries travelling from Ireland, most notably John Scotus Eriugena, a Neoplatonic philosopher. The modern university system has roots in the European medieval university, which was created in Italy and evolved from Catholic Cathedral schools for the clergy during the High Middle Ages. Thomas Aquinas, an academic philosopher and the father of Thomism, was immensely influential in Catholic Europe; he placed a great emphasis on reason and argumentation, and was one of the first to use the new translation of Aristotle's metaphysical and epistemological writing. Philosophers from the Middle Ages include the"
    ]
  ],
  [
    "The concatenation of universal events brought Candide and Martin together on the ship as they floated about the ocean on a raft after their ship sank (\"Bon Voyage\").",
    [
      "had both Cunegonde and the Old Lady tied up in sacks and carried to a boat in the harbor. He tells Candide that the women have sailed for Europe, and Candide eagerly purchases a leaky ship from the Governor and dashes off. As the Governor and his suite watch from his terrace, the ship with Candide and Martin casts off and almost immediately sinks (\"Bon Voyage\"). Candide and Martin have been rescued from the ship, and are floating about the ocean on a raft. Martin is devoured by a shark, but Dr. Pangloss miraculously reappears. Candide is overjoyed to find",
      "define events under five conditions: Other problems exist within Kim\u2019s theory, as he never specified what properties were (e.g. universals, tropes, natural classes, etc.). In addition, it is not specified if properties are few or abundant. The following is Kim\u2019s response to the above. There is also a major debate about the essentiality of a constitutive object. There are two major questions involved in this: If one event occurs, could it have occurred in the same manner if it were another person, and could it occur in the same manner if it would have occurred at a different time? Kim",
      "confident of success \"(Septet: \"En mon bon droit j\u2019ai confiance\")\". Marcel calls for assistance from the Huguenot soldiers in the tavern on the right and Saint-Bris to the Catholic students in the tavern on the left and a near-riot ensues. Only the arrival of the Queen, on horseback, stems the chaos. Raoul realises that Valentine has saved him and that his suspicions of her were unfounded. However, now she is married to his enemy and indeed at that moment an illuminated barge appears on the river with wedding guests serenading the newly wedded couple \"(Wedding chorus:\"Au banquet o\u00f9 le ciel"
    ]
  ],
  [
    "The October Revolution in Russia, which took place with an armed insurrection in Petrograd on 7 November (25 October, O.S.) 1917, led to the immediate context for Lenin's \"Call to Power\" in October 1917.",
    [
      "October Revolution The October Revolution (), officially known in Soviet literature as the Great October Socialist Revolution (, '), and commonly referred to as Red October, the October Uprising, the Bolshevik Revolution, or the Bolshevik Coup, was a revolution in Russia led by the Bolsheviks and Vladimir Lenin that was instrumental in the larger Russian Revolution of 1917. It took place with an armed insurrection in Petrograd on 7 November (25 October, O.S.) 1917. It followed and capitalized on the February Revolution of the same year, which overthrew the Tsarist autocracy and resulted in a provisional government after a transfer",
      "mid-September 1918: The campaign of mass repressions officially started as retribution for the assassination (17 August 1918) of Petrograd Cheka leader Moisei Uritsky by Leonid Kannegisser and for the attempted assassination (30 August 1918) of Lenin by Fanni Kaplan. While recovering from his wounds, Lenin instructed: \"It is necessary \u2013 secretly and \"urgently\" to prepare the terror\". Even before the assassinations, Lenin had sent telegrams \"to introduce mass terror\" in Nizhny Novgorod in response to a suspected civilian uprising there, and to \"crush\" landowners in Penza who resisted, sometimes violently, the requisitioning of their grain by military detachments: The Bolshevik",
      "definition of communism and popularized the term in their famous pamphlet \"The Communist Manifesto\". The 1917 October Revolution in Russia set the conditions for the rise to state power of Vladimir Lenin's Bolsheviks, which was the first time any avowedly communist party reached that position. The revolution transferred power to the All-Russian Congress of Soviets, in which the Bolsheviks had a majority. The event generated a great deal of practical and theoretical debate within the Marxist movement. Marx predicted that socialism and communism would be built upon foundations laid by the most advanced capitalist development. However, Russia was one of"
    ]
  ],
  [
    "France experimented with utilizing the ideas of Fourier and others like him to create national workshops in 1848.",
    [
      "itself \"in the middle of the 19th century (where) literally hundreds of communes (phalansteries) were founded on fourierist principles in France, N. America, Mexico, S. America, Algeria, Yugoslavia, \"etc\"\". Robert Owen (1771\u20131858) advocated the transformation of society into small, local collectives without such elaborate systems of social organization. Owen managed mills for many years. He transformed life in the village of New Lanark with ideas and opportunities which were at least a hundred years ahead of their time. Child labor and corporal punishment were abolished, and villagers were provided with decent homes, schools and evening classes, free health-care, and affordable",
      "Fourierism Fourierism is the systematic set of economic, political, and social beliefs first espoused by French intellectual Charles Fourier (1772\u20131837). Based upon a belief in the inevitability of communal associations of people who worked and lived together as part of the human future, Fourier's committed supporters referred to his doctrines as Associationism. Political contemporaries and subsequent scholarship has identified Fourier's set of ideas as a form of utopian socialisma phrase which retains mild pejorative overtones. Never tested in practice at any scale in Fourier's lifetime, Fourierism enjoyed a brief boom in the United States of America during the middle of",
      "the pages of his newspaper of that day, \"The New Yorker,\" throughout 1840 and 1841, and offering Brisbane a column in his successor publication, the \"New York Tribune,\" from the time of its establishment in March 1842. A groundswell of enthusiasm resulted from coverage of the ideas of Association in the \"New York Tribune,\" and from 1843 to 1845 more than 30 Fourierian \"Phalanxes\" were established in a number of northern and midwestern states. In 1844, Brook Farm, already an established Transcendentalist agrarian community in Massachusetts, formally declared itself a Fourierist community based on Brisbane's teachings. A national movement of"
    ]
  ],
  [
    "Galileo's discovery of the moons orbiting Jupiter challenged the beliefs of ancient philosophers like Aristotle by demonstrating that not all celestial bodies revolve around the Earth, contradicting the Aristotelian idea of a geocentric universe.",
    [
      "Against classical authority: The \"Discourse\" opens with a review of the opinions on comets of Aristotle, Anaxagoras, Democritus, Hippocrates of Chios and Seneca the Younger. By showing how one contradicts the other, and indeed how Aristotle contradicts even himself, Galileo sought \"to inculcate a certain skepticism and distrust of dogmatic authority, to encourage observation and mathematical analysis in preference to philosophical speculation, and to emphasise the vast extent of the unknown in comparison with the little men had gained as certain knowledge.\" Against the assumption that parallax can measure all visible objects: He cites phenomena such as haloes, rainbows and",
      "Tycho Brahe, Galileo, Descartes and Newton, it became generally accepted that Aristotelian physics was neither correct nor viable. Despite this, it survived as a scholastic pursuit well into the seventeenth century, until universities amended their curricula. In Europe, Aristotle's theory was first convincingly discredited by Galileo's studies. Using a telescope, Galileo observed that the Moon was not entirely smooth, but had craters and mountains, contradicting the Aristotelian idea of the incorruptibly perfect smooth Moon. Galileo also criticized this notion theoretically; a perfectly smooth Moon would reflect light unevenly like a shiny billiard ball, so that the edges of the moon's",
      "Jupiter in 1610, which are now collectively known as the Galilean moons, in his honor. This discovery was the first known observation of satellites orbiting another planet. He also found that our Moon had craters and observed, and correctly explained, sunspots, and that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these facts demonstrated incompatibility with the Ptolemaic model, which could not explain the phenomenon and would even contradict it. With the moons it demonstrated that the earth does not have to have everything orbiting it and that other parts of the solar system could"
    ]
  ],
  [
    "Operation Barbarossa violated Article II of the Molotov-Ribbentrop Pact, which stated that \"no political, military, economic or other considerations may serve as an excuse or justification for the aggression referred to in Article II.\"",
    [
      "issue. Nazi Germany terminated the Molotov\u2013Ribbentrop Pact with its invasion of the Soviet Union in Operation Barbarossa on June 22, 1941. After the launch of the invasion, the territories gained by the Soviet Union due to the Molotov\u2013Ribbentrop Pact were lost in a matter of weeks. In the three weeks following the Pact's breaking, attempting to defend against large German advances, the Soviet Union suffered 750,000 casualties, and lost 10,000 tanks and 4,000 aircraft. Within six months, the Soviet military had suffered 4.3 million casualties and the Germans had captured three million Soviet prisoners, two million of which would die",
      "\"no political, military, economic or other considerations may serve as an excuse or justification for the aggression referred to in Article II.\" And while the annex to Article III lists conceivable reasons for intervention in a neighboring state, it also stipulates that \"the High Contracting Parties further agree to recognize that the present convention \"can never legitimate any violations of International Law that may be implied in the circumstances comprised in the above list\".\" On August 24, 1939, the Soviet Union and Nazi Germany signed the Molotov\u2013Ribbentrop pact, which contained a secret protocol dividing the states of Northern and Eastern",
      "Molotov\u2013Ribbentrop Pact (Article 4 of the Secret Annex to the Treaty), Nazi Germany agreed that it had no interest in Bessarabia, effectively yielding it to the Soviet Union. On June 26, 1940, after the Fall of France, Romania's ally, Romania received an ultimatum from the Soviet Union demanding the evacuation of the Romanian military and administration from Bessarabia and from the northern part of Bukovina, with an implied threat of invasion in the event of noncompliance. Under pressure from Moscow and Berlin, the Romanian administration and the army were forced to retreat from Bessarabia and from Northern Bukovina to avoid"
    ]
  ],
  [
    "The treatment of indigenous peoples by the Spanish during the early period of colonization of the Americas impacted European settlers by providing them with land and resources through exploitation and subjugation of the native populations.",
    [
      "so they had relatively little cultural impact on the societies they engaged. In the Western Hemisphere, the European colonization involved the emigration of large numbers of settlers, soldiers and administrators intent on owning land and exploiting the apparently primitive (as perceived by Old World standards) indigenous peoples of the Americas. The result was that the colonization of the New World was catastrophic: native peoples were no match for European technology, ruthlessness, or their diseases which decimated the indigenous population. Spanish treatment of the indigenous populations caused a fierce debate, the Valladolid Controversy, over whether Indians possessed souls and if so,",
      "about the treatment and rights of indigenous peoples in the Americas. In 1552, the Dominican friar Bartolom\u00e9 de las Casas published the \"Brev\u00edsima relaci\u00f3n de la destrucci\u00f3n de las Indias\" (\"A Short Account of the Destruction of the Indies\"), an account of atrocities committed by landowners and some officials during the early period of colonization of New Spain, particularly in Hispaniola (today Haiti and the Dominican Republic). Las Casas, son of the merchant Pedro de las Casas who accompanied Columbus on his second voyage, described Columbus's treatment of the natives in his \"History of the Indies\". Their description of Spanish",
      "instead estimated that over the centuries of European colonization about 2 million to 15 million American indigenous people were the victims of what he calls \"democide.\" \"Even if these figures are remotely true\", writes Rummel, \"then this still make this subjugation of the Americas one of the bloodier, centuries long, democides in world history.\" Spain and Portugal sought the utilization of foreign and indigenous peoples during post colonial contact with the New World. The Portuguese and Spanish use of slavery in Latin America was seen as a lucrative business which ultimately led to internal and external development in gaining economic"
    ]
  ],
  [
    "The recently emancipated serfs of Russia were given the opportunity to buy the land from their former landlords.",
    [
      "business. The Manifesto prescribed that peasants would be able to buy the land from the landlords. Household serfs were the least affected: they gained only their freedom and no land. In Georgia the emancipation took place later, in 1864, and on much better terms for the nobles than in Russia. The serfs were emancipated in 1861, following a speech given by Tsar Alexander II on 30 March 1856. State owned serfs, i.e. the serfs living on Imperial lands, were emancipated later in 1866. Prior to 1861 Russia had two main categories of peasants: with only those owned privately considered to",
      "process, referred to as \"second serfdom\" or \"export-led serfdom\", persisted until the mid-19th century and became very repressive and substantially limited serfs' rights. Before the 1861 abolition of serfdom in Russia, a landowner's estate was often measured by the number of \"souls\" he owned, a practice made famous by Gogol's 1842 novel \"Dead Souls\". Many of these countries abolished serfdom during the Napoleonic invasions of the early 19th century. Serfdom remained in force in most of Russia until the Emancipation reform of 1861, enacted on February 19, 1861, though in the Russian-controlled Baltic provinces it had been abolished at the",
      "the 17th century. Serfdom only existed in central and southern areas of the Russian Empire. It was never established in the North, in the Urals, and in Siberia. According to the \"Encyclopedia of Human Rights\": Russia's over 23 million privately held serfs were freed from their lords by an edict of Alexander II in 1861. The owners were compensated through taxes on the freed serfs. State serfs were emancipated in 1866. Serfdom Serfdom is the status of many peasants under feudalism, specifically relating to manorialism. It was a condition of debt bondage, which developed primarily during the High Middle Ages"
    ]
  ],
  [
    "interprets the state of religious belief in ancient Rome as contributing to the empire's decline, as the adoption of Christianity led to a loss of civic virtue and a focus on the afterlife rather than the present.",
    [
      "Christianity and then of the Roman State Church, and the history of Europe, and discusses the decline of the Roman Empire among other things. Gibbon\u2019s work remains a great literary achievement and a very readable introduction to the period, but considerable progress has since been made in history and archaeology, and his interpretations no longer represent current academic knowledge or thought. Gibbon offers an explanation for the fall of the Roman Empire, a task made difficult by a lack of comprehensive written sources, though he was not the only historian to attempt it. According to Gibbon, the Roman Empire succumbed",
      "British historian Edward Gibbon argued in \"The History of the Decline and Fall of the Roman Empire\" (1776) that the Romans had become decadent, they had lost civic virtue. Gibbon said that the adoption of Christianity, meant belief in a better life after death, and therefore made people lazy and indifferent to the present. \"From the eighteenth century onward\", Glen W. Bowersock has remarked, \"we have been obsessed with the fall: it has been valued as an archetype for every perceived decline, and, hence, as a symbol for our own fears.\" It remains one of the greatest historical questions, and",
      "to barbarian invasions in large part due to the gradual loss of civic virtue among its citizens. He began an ongoing controversy about the role of Christianity, but he gave great weight to other causes of internal decline and to attacks from outside the Empire. Like other Enlightenment thinkers and British citizens of the age steeped in institutional anti-Catholicism, Gibbon held in contempt the Middle Ages as a priest-ridden, superstitious Dark Age. It was not until his own era, the \"Age of Reason\", with its emphasis on rational thought, it was believed, that human history could resume its progress. Gibbon's"
    ]
  ],
  [
    "The author laments the loss of Roman cultural traditions and the complex cultural transformation following the fall of the Western Roman Empire in relation to the Kingdom of Italy and Rome.",
    [
      "of the formerly imperial territories. Some historians thus refer to the reorganizations of Italy and abolition of the old and separate Western Roman administrative units, such as the Praetorian prefecture of Italy, during the sixth century as the \"true\" fall of the Western Roman Empire. Roman cultural traditions continued throughout the territory of the Western Empire for long after its disappearance, and a recent school of interpretation argues that the great political changes can more accurately be described as a complex cultural transformation, rather than a fall. After the fall of the Western Roman Empire, the Germanic kingdoms, often referred",
      "Empire, which had ruled modern-day Italy, France, Spain, Portugal and England for centuries, collapsed due to a combination of economic decline, and drastically reduced military strength which allowed invasion by barbarian tribes originating in southern Scandinavia and modern-day northern Germany. Historical opinion is divided as to the reasons for the fall of Rome, but the societal collapse encompassed both the gradual disintegration of the political, economic, military, and other social institutions of Rome as well as the barbarian invasions of Western Europe. In England, several Germanic tribes invaded, including the Angles and Saxons. In Gaul (modern-day France, Belgium and parts",
      "through and beyond the period of lost political control, the process has been described as a complex cultural transformation, rather than a fall. Fall of the Western Roman Empire The Fall of the Western Roman Empire (also called Fall of the Roman Empire or Fall of Rome) was the process of decline in the Western Roman Empire in which the Empire failed to enforce its rule, and its vast territory was divided into several successor polities. The Roman Empire lost the strengths that had allowed it to exercise effective control over its Western provinces; modern historians mention factors including the"
    ]
  ],
  [
    "The Committee of Public Safety mobilized the French people during the Lev\u00e9e en Masse in 1793 by implementing mass national conscription for able-bodied men aged 18 to 25 to defend the nation.",
    [
      "become a responsibility of \"all\". Thus, the \"lev\u00e9e en masse\" was created and understood as a means to defend the nation by the nation. Historically, the \"lev\u00e9e en masse\" heralded the age of the people's war and displaced restricted forms of warfare, such as the cabinet wars (1715\u20131792), when armies of professional soldiers fought without the general participation of the population. The first modern use of \"lev\u00e9e en masse\" occurred during the French Revolutionary Wars. Under the Ancien R\u00e9gime, there had been some conscription (by ballot) to a militia, \"milice\", to supplement the large standing army in times of war.",
      "Lev\u00e9e en masse Lev\u00e9e en masse ( or, in English, \"mass levy\") is a French term used for a policy of mass national conscription, often in the face of invasion. The concept originated during the French Revolutionary Wars, particularly for the period following 16 August 1793, when able-bodied men aged 18 to 25 were conscripted. It formed an integral part of the creation of national identity, making it distinct from forms of conscription which had existed before this date. The term is also applied to other historical examples of mass conscription. The term \"lev\u00e9e en masse\" denotes a short-term requisition",
      "of the Jacobin government, was to pour the entire nation's resources into an unprecedented war effort\u2014this was the advent of the lev\u00e9e en masse. The following decree of the National Convention on August 23, 1793 demonstrates the immensity of the French war effort, when the French front line forces grew to some 800,000 with a total of 1.5 million in all services\u2014the first time an army in excess of a million had been mobilized in Western history: The wars merged into the Napoleonic Wars of the First French Empire from ca. 1803. Over the coming two decades of almost constant"
    ]
  ],
  [
    "The historical transition that had been made complete with the negotiations with the Tories coming to an end was the achievement of full independence for Mauritius from the United Kingdom.",
    [
      "we at this time would have unleashed ourselves a war against Germany, would have moved forces to Europe, then England could have entered the alliance with Germany without any delay... And not only England. We could have been face to face with the entire capitalist world\". Meltyukhov believes that \"the question about the new period of the completion of war build-up was solved on May 24, 1941, at the secret conference of military-political leadership at the Kremlin. Now accessible sources show that the full concentration and the development of the Red Army on Soviet Western districts was to be completed",
      "was 'surprised' and 'disappointed' the UK couldn't reach a deal. However, later in the week a consensus deal was finalised. Varadkar stated he had received guarantees from the UK there would be no hard border between Ireland and Northern Ireland. He later said he and his cabinet had 'achieved all we set out to achieve' during the talks before quoting former British PM Winston Churchill by saying 'This is not the end, this is the end of the beginning\". An \"Irish Times\" poll taken during these days showed Varadkar with a 53% approval rating, the highest for any Taoiseach since",
      "an all-party coalition government to pursue negotiations with the British on independence. The final Constitutional Conference was held in London in September 1965 and was principally concerned with the debate between those Mauritian political leaders favouring independence and those preferring some form of continued association with the United Kingdom. On 24 September 1965, the final day of the conference, the Secretary of State for the Colonies, the Rt. Hon. Anthony Greenwood MP, who was the minister in the United Kingdom Government with responsibility for Mauritius, announced that the United Kingdom Government intended that Mauritius would proceed to full independence. Mauritius"
    ]
  ],
  [
    "Based on his ideals outlined in \"The Book of the Courtier,\" Castiglione would most likely want to institute reforms that promote education in the social arts for both men and women, encourage ambition and risk taking, and allow for radical innovation in political leadership.",
    [
      "character, in strong contrast to the traditional Christian uses of those terms, but more keeping with the original pre-Christian Greek and Roman concepts from which they derived. He encouraged ambition and risk taking. So in another break with tradition, he treated not only stability, but also radical innovation, as possible aims of a prince in a political community. Managing major reforms can show off a Prince's virtue and give him glory. He clearly felt Italy needed major reform in his time, and this opinion of his time is widely shared. Machiavelli's descriptions encourage leaders to attempt to control their fortune",
      "wants to write something useful to those who understand, he thought it more fitting \"to go directly to the effectual truth (\"\"verit\u00e0 effettuale\"\") of the thing than to the imagination of it\". This section is one where Machiavelli\u2019s pragmatic ideal can be seen most clearly. The prince should, ideally, be virtuous, but he should be willing and able to abandon those virtues if it becomes necessary. Concerning the behavior of a prince toward his subjects, Machiavelli announces that he will depart from what other writers say, and writes: Since there are many possible qualities that a prince can be said",
      "was \"Il Cortegiano\" or \"The Courtier\" by 16th-century Italian humanist Baldassare Castiglione. This enormously popular work stated that men and women should be educated in the social arts. His influence made it acceptable for women to engage in the visual, musical, and literary arts. Thanks to Castiglione, this was the first period of renaissance history in which noblewomen were able to study painting. Sofonisba Anguissola was the most successful of these minor aristocrats who first benefited from humanist education and then went on to recognition as painters. Artists who were not noblewomen were affected by the rise in humanism as"
    ]
  ],
  [
    "The method of inquiry elucidated by Francis Bacon in his work \"Novum Organum\" is the Baconian method, which involves the application of inductive reasoning, the identification of instances with special powers, and the use of aids to the intellect to accelerate the process of induction.",
    [
      "Baconian method The Baconian method is the investigative method developed by Sir Francis Bacon. The method was put forward in Bacon's book \"Novum Organum\" (1620), or 'New Method', and was supposed to replace the methods put forward in Aristotle's \"Organon\". This method was influential upon the development of the scientific method in modern science; but also more generally in the early modern rejection of medieval Aristotelianism. Bacon's method is an example of the application of inductive reasoning. However, Bacon's method of induction is much more complex than the essential inductive process of making generalizations from observations. Bacon's method begins with",
      "as stated in his \"Novum Organum\". The \"Baconian method\" does not end at the First Vintage. Bacon described numerous classes of \"Instances with Special Powers,\" cases in which the phenomenon one is attempting to explain is particularly relevant. These instances, of which Bacon describes 27 in the \"Novum Organum\", aid and accelerate the process of induction. Aside from the First Vintage and the Instances with Special Powers, Bacon enumerates additional \"aids to the intellect\" which presumably are the next steps in his method. These additional aids, however, were never explained beyond their initial limited appearance in \"Novum Organum\". The \"Natural",
      "credulous, submissive stance toward worldly power. Horkheimer and Adorno offer a plea to recover the virtues of the \"metaphysical apologia,\" which is able to reveal the injustice of effective procedures rather than merely employing them. Baconian method The Baconian method is the investigative method developed by Sir Francis Bacon. The method was put forward in Bacon's book \"Novum Organum\" (1620), or 'New Method', and was supposed to replace the methods put forward in Aristotle's \"Organon\". This method was influential upon the development of the scientific method in modern science; but also more generally in the early modern rejection of medieval"
    ]
  ],
  [
    "The events described in the passage most likely occurred in France.",
    [
      "in Europe and in the world. All statements refer to France as understood in its modern borders; this pertains also to other countries. Historians suggest that France was the most populous state in Europe from at least the period of Charlemagne and the Frankish Empire, if not earlier, to the 19th century. Population statistics prior to the modern era are historical estimates as official counts were not made. In the above list, Turkey is not considered a European country. Turkey was less populous than metropolitan France until 1992, and has been more populous since then. Please note: Source: Source: Louis",
      "II) and the emergence of socialist countries that led to the Cold War. The \"contemporary era\" follows shortly afterward with the explosion of research and increase of knowledge known as the Information Age in the latter 20th and the early 21st century. Today's Postmodern era is seen in widespread digitality. Historians consider the early modern period to be approximately between 1500 and 1800. It follows the Late Middle Ages and is marked by the first European colonies, the rise of strong centralized governments, and the beginnings of recognizable nation-states that are the direct antecedents of today's states. The expansion of",
      "and Late Modern periods) was developed for, and applies best to, the history of the Old World, particularly Europe and the Mediterranean. Outside this region, including ancient China and ancient India, historical timelines unfolded differently. However, by the 18th century, due to extensive world trade and colonization, the histories of most civilizations had become substantially intertwined. In the last quarter-millennium, the rates of growth of population, knowledge, technology, communications, commerce, weapons destructiveness, and environmental degradation have greatly accelerated, creating opportunities and perils that now confront the planet's human communities. Genetic measurements indicate that the ape lineage which would lead to"
    ]
  ],
  [
    "Thomas Paine's beliefs in \"The Age of Reason\" support the demand for a secular image of utopia and human perfectibility without God's aid, in line with Enlightenment philosophes' emphasis on progress and reason.",
    [
      "secular image of utopia,\" emphasizing the possibilities of \"progress\" and \"human perfectibility\" that could be achieved by humankind, without God's aid. Although Paine liked to say that he read very little, his writings belie this statement; \"The Age of Reason\" has intellectual roots in the traditions of David Hume, Spinoza, and Voltaire. Since Hume had already made many of the same \"moral attacks upon Christianity\" that Paine popularized in \"The Age of Reason\", scholars have concluded that Paine probably read Hume's works on religion or had at least heard about them through the Joseph Johnson circle. Paine would have been",
      "beginning of Part I of the \"Age of Reason\", Paine lays out his personal belief: Paine's creed encapsulates many of the major themes of the rest of his text: a firm belief in a creator-God; a skepticism regarding most supernatural claims (miracles are specifically mentioned later in the text); a conviction that virtues should be derived from a consideration for others rather than oneself; an animus against corrupt religious institutions; and an emphasis on the individual's right of conscience. Paine begins \"The Age of Reason\" by attacking revelation. Revelation, he maintains, can only be verified by the individual receivers of",
      "dark time, we shall require an age of reason\". His ends with the claim that \"in a time ... when both rights and reason are under several kinds of open and covert attack, the life and writing of Thomas Paine will always be part of the arsenal on which we shall need to depend.\" The Age of Reason The Age of Reason; Being an Investigation of True and Fabulous Theology is a work by English and American political activist Thomas Paine, arguing for the philosophical position of Deism. It follows in the tradition of eighteenth-century British deism, and challenges institutionalized"
    ]
  ],
  [
    "The fateful essence of the year 1848, according to A. J. P. Taylor's analysis, was the upheavals which remodeled the map of Europe and obscured the balance of power, underlying all efforts of diplomacy to control nationalist forces unleashed by the French Revolution.",
    [
      "spending per capita, coal and steel production and manufacturing production. He maintains that determining the strength of a state requires an assessment of its economic resources. Taylor rejects the idea of fate or inevitability: \"No war is inevitable until it breaks out\". Introduction: The great powers of Europe -- The diplomacy of revolution, 1848 -- The diplomacy of revolution, 1849-50 -- The end of the holy alliance, 1852-3 -- The Crimean War, 1854-6 -- The Congress of Paris and its consequences, 1856-8 -- The Italian war and the disruption of the settlement of Vienna, 1858-61 -- The Polish crisis, and",
      "been written. He argued that: \"The history of the Germans is a history of extremes. It contains everything except moderation, and in the course of a thousand years the Germans have experienced everything except normality.\" The \"German problem\" in Taylor's eyes has two sides: how can Europe be protected against repeated German aggression and how can the Germans secure a peaceful form of political existence? Taylor also notes the repeated failure of the German left to choose a democratic Germany when the choice was between democracy and unity. In his discussion of the revolutions of 1848 Taylor said: 1848 was",
      "upheavals which remodeled the map of Europe obscured the balance of power. Yet, it underlaid all the efforts of diplomacy to stay, or to direct, the elemental forces of nationalism let loose by the French Revolution. In the revolution's aftermath, with the restoration of comparative calm, the principle once more emerged as the operative motive for the various political alliances, of which the ostensible object was the preservation of peace. Regarding the era 1848\u20131914, English diplomatic historian A.J.P. Taylor argued: Regarding the last quarter-century of the period outlined by Taylor, his American colleague, diplomatic historian Edward Mead Earle, argued: \u201cDuring"
    ]
  ],
  [
    "The result of Article III of the Treaty of Brest-Litovsk was that eleven nations in Eastern Europe and western Russia became independent.",
    [
      "Treaty of Brest-Litovsk The Treaty of Brest-Litovsk was a peace treaty signed on 3 March 1918 between the new Bolshevik government of Russia and the Central Powers (German Empire, Austria-Hungary, Bulgaria, and the Ottoman Empire), that ended Russia's participation in World War I. The treaty was signed at German-controlled Brest-Litovsk (; since 1945, Brest, nowadays in Belarus), after two months of negotiations. The treaty was agreed upon by the Russians to stop further invasion. According to the treaty, Soviet Russia defaulted on all of Imperial Russia's commitments to the Allies and eleven nations became independent in Eastern Europe and western",
      "citizenship, the default being that individuals were citizens of the state in which they resided at the time the treaty was signed. Articles 11 through 16 dealt with reparations Russia was to make to the Latvian state and its citizens. Articles 17 and 18 dealt with commercial, transit, postal and navigation arrangements and Article 19 with diplomatic relations. Article 20 address nationality issues and Article 21 established a commission to handle issues of mutual interest. Articles 22 and 23 deal with treaty technicalities such as language and ratification. While the treaty included provisions for reparations, Latvia had no practical recourse",
      "Treaty of Brest-Litovsk (Ukraine\u2013Central Powers) The Treaty of Brest-Litovsk was a peace treaty signed on 3 March 1918 between the Russian SFSR and the Central Powers, but prior to that on 9 February 1918, the Central Powers signed an exclusive protectorate treaty (, \"peace for bread\") with the Ukrainian People's Republic as part of the negotiations that took place in Brest-Litovsk, Grodno Governorate (now Brest, Belarus) recognizing the sovereignty of the Ukrainian republic. Although not formally annexing the territory of the former Russian Empire, Germany and Austria-Hungary secured food-supply support in return for providing military protection. The Quadruple Alliance recognized"
    ]
  ],
  [
    "The author expresses bittersweet feelings about the results of the nationalist movement in Italy in the nineteenth century.",
    [
      "Empire). Initially, the movement can be described as part of the more general nation-building process in Europe in the 19th and 20th centuries when the multi-national Austro-Hungarian, Russian and Ottoman Empires were being replaced by nation-states. The Italian nation-building process can be compared to similar movements in Germany (\"Gro\u00dfdeutschland\"), Hungary, Serbia and in pre-1914 Poland. Simultaneously, in many parts of 19th century Europe liberalism and nationalism were ideologies which were coming to the forefront of political culture. In Eastern Europe, where the Habsburg Empire had long asserted control over a variety of ethnic and cultural groups, nationalism appeared in a",
      "political upheavals throughout the European continent. Described as a revolutionary wave, the period of unrest began in France and then, further propelled by the French Revolution of 1848, soon spread to the rest of Europe. Although most of the revolutions were quickly put down, there was a significant amount of violence in many areas, with tens of thousands of people tortured and killed. While the immediate political effects of the revolutions were reversed, the long-term reverberations of the events were far-reaching. Industrial age [[reform movement]]s began the gradual change of society rather than with episodes of rapid fundamental changes. The",
      "threatened the old order. Numerous movements developed around various cultural groups, who began to develop a sense of national identity. While initially, all of these revolutions failed, and reactionary forces would re-establish political control, the revolutions marked the start of the steady progress towards the end of the Concert of Europe under the dominance of a few multi-national empires and led to the establishment of the modern nation state in Europe; a process that would not be complete for over a century and a half. Central and Eastern Europe's political situation was partly shaped by the two World Wars, while"
    ]
  ],
  [
    "Cort\u00e9s's actions upon returning from the expedition to Mexico led to accusations of brutality and violence towards the Indians, causing him to suffer from guilt and insomnia.",
    [
      "of his ships but one, which he sent back to Spain for King Charles. The fear of his men returning to Cuba, rather than embarking on the journey to the Aztec Empire, made him decide to demolish his ships. They no longer had an option but to accompany him on this journey. This decision brought about severe consequences because Cort\u00e9s had trapped himself in Mexico. Cort\u00e9s created strife between his men and the Aztecs by taking over the city. However, worried his men would revolt, Montezuma decided to convince them to delay the attack. Cort\u00e9s now had time to build",
      "that Cort\u00e9s's men should also be remembered as important participants in the undertakings in Mexico. In the years following the conquest more critical accounts of the Spanish arrival in Mexico were written. The Dominican friar Bartolom\u00e9 de Las Casas wrote his \"A Short Account of the Destruction of the Indies\" which raises strong accusations of brutality and heinous violence towards the Indians; accusations against both the conquistadors in general and Cort\u00e9s in particular. The accounts of the conquest given in the Florentine Codex by the Franciscan Bernardino de Sahag\u00fan and his native informants are also less than flattering towards Cort\u00e9s.",
      "He also records Cuauht\u00e9moc giving the following speech to Cort\u00e9s through his interpreter Malinche: D\u00edaz wrote that afterwards, Cort\u00e9s suffered from insomnia because of guilt and badly injured himself while he was wandering at night. Fernando de Alva Cort\u00e9s Ixtlilx\u00f3chitl, a mestizo historian and descendant of Coanacoch, wrote an account of the executions in the 17th century partly based on Texcocan oral tradition. According to Ixtlilx\u00f3chitl, the three lords were joking cheerfully with one another because of a rumor that Cort\u00e9s had decided to return the expedition to Mexico, when Cort\u00e9s asked a spy to tell him what they were"
    ]
  ],
  [
    "Napoleon's genius and ambition shaped his impact on history and his legacy by establishing himself as one of the greatest military leaders of all time and as a controversial figure in politics, leaving a lasting legacy of both admiration and condemnation.",
    [
      "says there is no question that as a military genius Napoleon ranks with Alexander the Great and Julius Caesar in greatness. However, in the political realm, historians debate whether Napoleon was \"an enlightened despot who laid the foundations of modern Europe or, instead, a megalomaniac who wrought greater misery than any man before the coming of Hitler\". The Manchu or Qing regime in Beijing used military force, diplomacy, and reliance on local leaders to extend its domain to western regions where the Han Chinese had not settled, but where Russian expansion was a threat. It launched Ten Great Campaigns in",
      "died in exile on a remote island, remembered as a great hero by many Frenchmen and as a great villain by British and other enemies. Napoleon, despite his youth, was France's most successful general in the Revolutionary wars, having conquered large parts of Italy and forced the Austrians to sue for peace. In 1799 on 18 Brumaire (9 November) he overthrew the feeble government, replacing it with the Consulate, which he dominated. He gained popularity in France by restoring the Church, keeping taxes low, centralizing power in Paris, and winning glory on the battlefield. In 1804 he crowned himself Emperor.",
      "force in each of the examples, and how it was used to achieve particular aims. The discussion of history begins with Napoleon, who invented the paradigm of industrial warfare. Prior to Napoleon, wars were mostly fought for territory rather than ideology, and did not substantially alter the balance of power between nations. Napoleon's concept of war involved using the entire resources of the nation with the aim of comprehensively defeating his opponent and replacing the political order; in order to meet the new challenge, the Prussian Army undertook sweeping reforms. Napoleon's strategy and the Prussian response influenced Prussian general Carl"
    ]
  ],
  [
    "Napoleon attributed his conquest of most of Europe to the establishment of the Order of the Union, which gave him control over all access points to the sea under the same authority.",
    [
      "subjects, maintained an extensive military presence in Germany, Italy, Spain, and the Duchy of Warsaw, and counted Prussia and Austria as nominal allies. Early French victories exported many ideological features of the French Revolution throughout Europe: the introduction of the Napoleonic Code throughout the continent increased legal equality, established jury systems and legalised divorce, and seigneurial dues and seigneurial justice were abolished, as were aristocratic privileges in all places except Poland. France's defeat in 1814 (and then again in 1815), marked the end of the Empire. In 1799, Napoleon Bonaparte was confronted by Emmanuel Joseph Siey\u00e8s \u2013 one of five",
      "on the continent. Napoleon quickly defeated Prussia at the battles of Jena and Auerstedt, then marched his \"Grande Arm\u00e9e\" deep into Eastern Europe and annihilated the Russians in June 1807 at the Battle of Friedland. France then forced the defeated nations of the Fourth Coalition to sign the Treaties of Tilsit in July 1807, bringing an uneasy peace to the continent. Tilsit signified the high-water mark of the French Empire. In 1809, the Austrians and the British challenged the French again during the War of the Fifth Coalition, but Napoleon solidified his grip over Europe after triumphing at the Battle",
      "great empire, saying \"We must look for a motto which gives a sense of the advantages of the union of the Baltic, Mediterranean, Adriatic and the [Atlantic] Ocean. This great event that truly characterises the Empire, could be called the Order of the Union.\". Napoleon eventually occupied large territories in north-west Germany and the Illyrian provinces on the Dalmatian coast - the name of the order he founded referred to the fact that (for the first time since the Roman Empire) all access points to the sea were under the same authority. Napoleon reserved himself the exclusive right to exclude"
    ]
  ],
  [
    "The change brought on by industrialization that motivated the Luddite movement was the introduction of machinery that was perceived to not benefit the common good.",
    [
      "caused by industrialism resulted in some reforms by the mid-19th century. The economic model of the West also began to change, with mercantilism being replaced by capitalism, in which companies, and later, large corporations, were run by individual investor(s). New ideological movements began as a result of the Industrial Revolution, including the Luddite movement, which opposed machinery, feeling it did not benefit the common good, and the socialists, whose beliefs usually included the elimination of private property and the sharing of industrial wealth. Unions were founded among industrial workers to help secure better wages and rights. Another result of the",
      "to a more primitive society. His essay, \"Industrial Society and Its Future\", describes different political factions and laments the direction of technology and the modern world. Industrial society In sociology, an industrial society is a society driven by the use of technology to enable mass production, supporting a large population with a high capacity for division of labour. Such a structure developed in the Western world in the period of time following the Industrial Revolution, and replaced the agrarian societies of the pre-modern, pre-industrial age. Industrial societies are generally mass societies, and may be succeeded by an information society. They",
      "experiencing profound change could be attributed to the end of the \"Machine Age\" and the onset of the \"Systems Age\". The Machine Age, bequeathed by the Industrial Revolution, was underpinned by two concepts \u2013 reductionism (everything can in the end be decomposed into indivisible parts) and mechanism (cause-effect relationships)\". Hereby \"all phenomena were believed to be explained by using only one ultimately simple relationship, \"cause-effect\"\", which in the Systems Age are replaced by \"expansionism\" and \"teleology\" with \"producer-product\" replacing \"cause-effect\". \"\"Expansionism\" is a doctrine maintaining that all objects and events, and all experiences of them, are parts of larger wholes.\""
    ]
  ],
  [
    "Galileo Galilei concluded that the \"three fixed stars\" near Jupiter were actually moons orbiting the planet.",
    [
      "had been available before. He described \"three fixed stars, totally invisible by their smallness\", all close to Jupiter, and lying on a straight line through it. Observations on subsequent nights showed that the positions of these \"stars\" relative to Jupiter were changing in a way that would have been inexplicable if they had really been fixed stars. On 10 January Galileo noted that one of them had disappeared, an observation which he attributed to its being hidden behind Jupiter. Within a few days he concluded that they were orbiting Jupiter: Galileo stated that he had reached this conclusion on 11",
      "at the time as \"three fixed stars, totally invisible by their smallness\", all close to Jupiter, and lying on a straight line through it. Observations on subsequent nights showed that the positions of these \"stars\" relative to Jupiter were changing in a way that would have been inexplicable if they had really been fixed stars. On 10 January, Galileo noted that one of them had disappeared, an observation which he attributed to its being hidden behind Jupiter. Within a few days, he concluded that they were orbiting Jupiter: he had discovered three of Jupiter's four largest moons. He discovered the",
      "what he thought were three stars near Jupiter, including what turned out to be Ganymede, Callisto, and one body that turned out to be the combined light from Io and Europa; the next night he noticed that they had moved. On January 13, he saw all four at once for the first time, but had seen each of the moons before this date at least once. By January 15, Galileo came to the conclusion that the stars were actually bodies orbiting Jupiter. He claimed the right to name the moons; he considered \"Cosmian Stars\" and settled on \"Medicean Stars\". The"
    ]
  ],
  [
    "Mikhail Gorbachev argued for the urgent necessity of restructuring the Soviet political and economic system in his promotion of perestroika.",
    [
      "of reform was to prop up the centrally planned economy\u2014not to transition to market socialism. Speaking in late summer 1985 to the secretaries for economic affairs of the central committees of the East European communist parties, Gorbachev said: \"Many of you see the solution to your problems in resorting to market mechanisms in place of direct planning. Some of you look at the market as a lifesaver for your economies. But, comrades, you should not think about lifesavers but about the ship, and the ship is socialism.\" Gorbachev initiated his new policy of \"perestroika\" (literally \"restructuring\" in Russian) and its",
      "Perestroika Perestroika (; ) was a political movement for reformation within the Communist Party of the Soviet Union during the 1980s and 1990s and is widely associated with Soviet leader Mikhail Gorbachev and his glasnost (meaning \"openness\") policy reform. The literal meaning of perestroika is \"restructuring,\" referring to the restructuring of the Soviet political and economic system. Perestroika is sometimes argued to be a significant cause of the dissolution of the Soviet Union, the revolutions of 1989 in Eastern Europe, and the end of the Cold War. Perestroika allowed more independent actions from various ministries and introduced some market-like reforms.",
      "other countries [that had] achieved their independence,\" and who was \"under extraordinary pressure at home, particularly on the economy.\" Perestroika Perestroika (; ) was a political movement for reformation within the Communist Party of the Soviet Union during the 1980s and 1990s and is widely associated with Soviet leader Mikhail Gorbachev and his glasnost (meaning \"openness\") policy reform. The literal meaning of perestroika is \"restructuring,\" referring to the restructuring of the Soviet political and economic system. Perestroika is sometimes argued to be a significant cause of the dissolution of the Soviet Union, the revolutions of 1989 in Eastern Europe, and"
    ]
  ],
  [
    "The publication of The Communist Manifesto coincides with the 1848 revolutions in Europe, according to modern historians.",
    [
      "was a landmark in the ideological arming of the International Left Opposition. In a truly prophetic statement, Trotsky warned that if this position were adopted by the Communist International, it would inevitably mark the beginning of a process that would lead to the nationalist and reformist degeneration of every Communist Party in the world. Three generations later, his prediction - which was ridiculed by the Stalinists at the time - has been shown to be correct. Stalin had no intention of circulating Trotsky's document. But by a strange accident of history, that is what happened. At that time, when the",
      "Engels wrote to Marx recommending a further re-draft, in historical prose: Following the second congress of the Communist League, it commissioned Marx to write a final program. Drawing directly upon the ideas in \"Principles of Communism\", Marx delivered a final revision, the \"Manifesto\", in early 1848. Although Marx was the exclusive author of the \"Manifesto's\" manuscript, the ideas were adapted from Engels' earlier drafts, with the result that the \"Manifesto\" was credited to both authors. Beginning with a definition of communism as a political theory for the liberation of the proletariat, Engels provides a brief history of the proletariat as",
      "introduction to \"The Communist Manifesto\": \"The history of all hitherto existing society is the history of class struggles\". The influence of his ideas, already popular during his life, was given added impetus by the victory of the Russian Bolsheviks in the 1917 October Revolution and there are few parts of the world which were not significantly touched by Marxian ideas in the course of the twentieth century. As the American Marx scholar Hal Draper remarked: \"[T]here are few thinkers in modern history whose thought has been so badly misrepresented, by Marxists and anti-Marxists alike\". The early influences on Marx are"
    ]
  ],
  [
    "The result of the Treaty of Brest-Litovsk was Soviet Russia's exit from World War I and the nominal independence of several Eastern European countries, although they were actually under German control.",
    [
      "Treaty of Brest-Litovsk The Treaty of Brest-Litovsk was a peace treaty signed on 3 March 1918 between the new Bolshevik government of Russia and the Central Powers (German Empire, Austria-Hungary, Bulgaria, and the Ottoman Empire), that ended Russia's participation in World War I. The treaty was signed at German-controlled Brest-Litovsk (; since 1945, Brest, nowadays in Belarus), after two months of negotiations. The treaty was agreed upon by the Russians to stop further invasion. According to the treaty, Soviet Russia defaulted on all of Imperial Russia's commitments to the Allies and eleven nations became independent in Eastern Europe and western",
      "in exchange for their recognition of German gains in the east. Emil Orlik, the Viennese Secessionist artist, attended the conference, at the invitation of Richard von K\u00fchlmann. He drew portraits of all the participants, along with a series of smaller caricatures. These were gathered together into a book, \"Brest-Litovsk,\" a copy of which was given to each of the participants. Treaty of Brest-Litovsk The Treaty of Brest-Litovsk was a peace treaty signed on 3 March 1918 between the new Bolshevik government of Russia and the Central Powers (German Empire, Austria-Hungary, Bulgaria, and the Ottoman Empire), that ended Russia's participation in",
      "Treaty of Brest-Litovsk in March 1918 led to Soviet Russia's exit from the war and the nominal independence of Armenia, Finland, Estonia, Latvia, Ukraine, Lithuania, Georgia and Poland, though in fact those territories were under German control. The end of the war led to the dissolution of the defeated Austro-Hungarian Empire and Czechoslovakia and the union of the State of Slovenes, Croats and Serbs and the Kingdom of Serbia as new states out of the wreckage of the Habsburg empire. However, this imposition of states where some nationalities (especially Poles, Czechs, and Serbs and Romanians) were given power over nationalities"
    ]
  ],
  [
    "Anthropology evolved from its roots in natural history to become a distinct discipline by the end of the 19th century through a process of differentiation from biological approaches and a focus on studying human behavior systematically during the Age of Enlightenment.",
    [
      "racism, which first formulation may be found in Arthur de Gobineau's \"An Essay on the Inequality of Human Races\" (1853\u201355). In 1931, the Colonial Exhibition in Paris still displayed Kanaks from New Caledonia in the \"indigenous village\"; it received 24 million visitors in six months, thus demonstrating the popularity of such \"human zoos\". Anthropology grew increasingly distinct from natural history and by the end of the nineteenth century the discipline began to crystallize into its modern form\u2014by 1935, for example, it was possible for T.K. Penniman to write a history of the discipline entitled \"A Hundred Years of Anthropology\". At",
      "which they did. This part of History is named Anthropology.\" Many scholars consider modern anthropology as an outgrowth of the Age of Enlightenment (1715\u201389), a period when Europeans attempted to study human behavior systematically, the known varieties of which had been increasing since the fifteenth century as a result of the first European colonization wave. The traditions of jurisprudence, history, philology, and sociology then evolved into something more closely resembling the modern views of these disciplines and informed the development of the social sciences, of which anthropology was a part. It took Immanuel Kant (1724-1804) 25 years to write one",
      "humanities to explore the biological, linguistic, material, and symbolic dimensions of humankind in all forms. As academic disciplines began to differentiate over the course of the nineteenth century, anthropology grew increasingly distinct from the biological approach of natural history, on the one hand, and from purely historical or literary fields such as Classics, on the other. A common criticism was that many social sciences (such as economists, sociologists, and psychologists) in Western countries focused disproportionately on Western subjects, while anthropology focuseed disproportionately on the \"other\". Museums such as the British Museum weren't the only site of anthropological studies: with the"
    ]
  ],
  [
    "Germany did not follow a unique path in history, but rather followed a similar course to other European nations according to prevailing opinions.",
    [
      "different course in comparison with the other nations of the West, which had a normal development of their histories. The German historian Heinrich August Winkler wrote about the question of there being a : \"For a long time, educated Germans answered it in the positive, initially by laying claim to a special German mission, then, after the collapse of 1945, by criticizing Germany's deviation from the West. Today, the negative view is predominant. Germany did not, according to the now prevailing opinion, differ from the great European nations to an extent that would justify speaking of a 'unique German path'.",
      "that would justify speaking of a 'unique German path.' And, in any case, no country on earth ever took what can be described as the 'normal path.' Fritz Fischer (1908 \u2013 1999) was best known for his analysis of the causes of World War I. In the early 1960s Fischer published Germany's Aims in the First World War. He put forward the controversial thesis that responsibility for the outbreak of the war rested solely with Imperial Germany. That set off a long debate that reverberates into the 21st century. He has been described by \"The Encyclopedia of Historians and Historical",
      "was much contingency and chance in German history. In addition, there was much debate within the supporters of the \"Sonderweg\" concept as for the reasons for the \"Sonderweg\", and whether or not the \"Sonderweg\" ended in 1945. Was there a Sonderweg? Winkler says: For a long time, educated Germans answered it in the positive, initially by laying claim to a special German mission, then, after the collapse of 1945, by criticizing Germany's deviation from the West. Today, the negative view is predominant. Germany did not, according to the now prevailing opinion, differ from the great European nations to an extent"
    ]
  ],
  [
    "Frederick Douglass would have been the LEAST likely to oppose the governmental regulation described in the Carlsbad Resolutions of 1819.",
    [
      "was John Quincy Adams of Massachusetts. Adams had first voiced concerns about expanding into Mexican territory in 1836 when he opposed Texas annexation. He continued this argument in 1846 for the same reason. War with Mexico would add new slavery territory to the nation. When the vote to go to war with Mexico came to a vote on May 13, Adams spoke a resounding \"No!\" in the chamber. Only 13 others followed his lead. Ex-slave Frederick Douglass opposed the war and was dismayed by the weakness of the anti-war movement. \"The determination of our slave holding president, and the probability",
      "Carlsbad Decrees The Carlsbad Decrees were a set of reactionary restrictions introduced in the states of the German Confederation by resolution of the Bundesversammlung on 20 September 1819 after a conference held in the spa town of Carlsbad, Bohemia. They banned nationalist fraternities (\"Burschenschaften\"), removed liberal university professors, and expanded the censorship of the press. They were aimed at quelling a growing sentiment for German unification and were passed during ongoing Hep-Hep riots which ended within a month after the resolution was passed. The meeting of the state's representatives was called by the Austrian Minister of State Prince Klemens Wenzel",
      "included the lines: Charles Hammond (lawyer and journalist) Charles Hammond (September 19, 1779 \u2013 April 3, 1840) was a lawyer, newspaper editor, and state legislator in Ohio in the early nineteenth century. He attained renown in his time as both a lawyer and a journalist, but was largely neglected later. Hammond is best known today for his role as the intellectual leader of Ohio's ultimately failed opposition to the Second Bank of the United States. Charles Hammond was born on September 19, 1779 to George and Elizabeth (n\u00e9e Wells) Hammond. The family lived in Baltimore County, Maryland at the time"
    ]
  ],
  [
    "The quantum efficiency of a photon detector is the fraction of photon flux that contributes to the photocurrent in the detector, and it affects the detection of individual photons by determining the number of signal electrons created per incident photon.",
    [
      "Photon counting Photon counting is a technique in which individual photons are counted using some single-photon detector (SPD). The counting efficiency is determined by the quantum efficiency and any electronic losses that are present in the system. Many photodetectors can be configured to detect individual photons, each with relative advantages and disadvantages, including a photomultiplier, geiger counter, single-photon avalanche diode, superconducting nanowire single-photon detector, transition edge sensor, or scintillation counter. Charge-coupled devices can also sometimes be used. Single-photon detection is useful in many fields including fiber-optic communication, quantum information science, quantum encryption, medical imaging, light detection and ranging, DNA sequencing,",
      "(QE) is the fraction of photon flux that contributes to the photocurrent in a photodetector or a pixel. Quantum efficiency is one of the most important parameters used to evaluate the quality of a detector and is often called the spectral response to reflect its wavelength dependence. It is defined as the number of signal electrons created per incident photon. In some cases it can exceed 100% (i.e. when more than one electron is created per incident photon). EQE mapping : Conventional measurement of the EQE will give the efficiency of the overall device. However it is often useful to",
      "classifying the signal and noise performance of various optical detectors such as television cameras and photoconductive devices. It was shown, for example, that image quality is limited by the number of quanta used to produce an image. The quantum efficiency of a detector is a primary metric of performance because it describes the fraction of incident quanta that interact and therefore image quality. However, other physical processes may also degrade image quality, and in 1946, Albert Rose proposed the concept of a \"useful quantum efficiency\" or \"equivalent quantum efficiency\" to describe the performance of those systems, which we now call"
    ]
  ],
  [
    "The most strongly reflected wavelength in a thin layer of oil (index of refraction 1.5) floating on a puddle of water (index of refraction 1.33) when white light is incident on it is determined by the thin-film interference phenomenon and can be calculated using the formula for the difference in optical path length between the two reflections at the top and bottom surfaces of the oil layer.",
    [
      "formula_18 is the film thickness, formula_19 is the refractive index of the film, formula_20 is the angle of incidence of the wave on the lower boundary, formula_21 is an integer, and formula_10 is the wavelength of light. In the case of a thin oil film, a layer of oil sits on top of a layer of water. The oil may have an index of refraction near 1.5 and the water has an index of 1.33. As in the case of the soap bubble, the materials on either side of the oil film (air and water) both have refractive indices that",
      "Thin-film optics Thin-film optics is the branch of optics that deals with very thin structured layers of different materials. In order to exhibit thin-film optics, the thickness of the layers of material must be on the order of the wavelengths of visible light (about 500 nm). Layers at this scale can have remarkable reflective properties due to light wave interference and the difference in refractive index between the layers, the air, and the substrate. These effects alter the way the optic reflects and transmits light. This effect, known as thin-film interference, is observable in soap bubbles and oil slicks. More",
      "is used in numerous applications, including light scattering and absorption by ice crystals and cloud water droplets, theories of the rainbow, determination of the single scattering albedo, ocean color, and many others. Over the wavelengths from 0.2 \u03bcm to 1.2 \u03bcm, and over temperatures from \u221212 \u00b0C to 500 \u00b0C, the real part of the index of refraction of water can be calculated by the following empirical expression: Where: and the appropriate constants are formula_2 = 0.244257733, formula_3 = 0.00974634476, formula_4 = \u22120.00373234996, formula_5 = 0.000268678472, formula_6 = 0.0015892057, formula_7 = 0.00245934259, formula_8 = 0.90070492, formula_9 = \u22120.0166626219, formula_10 ="
    ]
  ],
  [
    "Any system undergoing a reversible thermodynamic process will be returned to its original state if one half cycle is followed by the other half cycle.",
    [
      "\"reversible cycle\", a cyclical reversible process, the system and its surroundings will be returned to their original states if one half cycle is followed by the other half cycle. Thermodynamic processes can be carried out in one of two ways: reversibly or irreversibly. Reversibility means the reaction operates continuously at quasiequilibrium. In an ideal thermodynamically reversible process, the energy from work performed by or on the system would be maximized, and that from heat would be zero. However, heat cannot fully be converted to work and will always be lost to some degree (to the surroundings). (This is true only",
      "valid for an adiabatic system which is also undergoing a reversible process provided it is a closed system having an ideal gas. If it fails to satisfy any of these conditions then only formula_18 is true and it cannot be represented in an equation like formula_17=constant Thermodynamic system A thermodynamic system is a group of material and/or radiative contents. Its properties may be described by thermodynamic state variables such as temperature, entropy, internal energy, and pressure. The simplest state of a thermodynamic system is a state of thermodynamic equilibrium, as opposed to a non-equilibrium state. The system can be separated",
      "properties are, by definition, unchanging in time. Systems in equilibrium are much simpler and easier to understand than are systems which are not in equilibrium. Often, when analysing a dynamic thermodynamic process, the simplifying assumption is made that each intermediate state in the process is at equilibrium, producing thermodynamic processes which develop so slowly as to allow each intermediate step to be an equilibrium state and are said to be reversible processes. When a system is at equilibrium under a given set of conditions, it is said to be in a definite thermodynamic state. The state of the system can"
    ]
  ],
  [
    "Dye lasers are best for spectroscopy over a range of visible wavelengths due to their tunability from the near-infrared to near-ultraviolet, narrow bandwidth, and high intensity.",
    [
      "other applications. In spectroscopy, dye lasers can be used to study the absorption and emission spectra of various materials. Their tunability, (from the near-infrared to the near-ultraviolet), narrow bandwidth, and high intensity allows a much greater diversity than other light sources. The variety of pulse widths, from ultra-short, femtosecond pulses to continuous-wave operation, makes them suitable for a wide range of applications, from the study of fluorescent lifetimes and semiconductor properties to lunar laser ranging experiments. Tunable lasers are used in swept-frequency metrology to enable measurement of absolute distances with very high accuracy. A two axis interferometer is set up",
      "\"scanning\" systems and \"broadband\" systems. For the former type of spectrometer, one of the beams is a visible wavelength laser held at a constant frequency and the other is a tunable infrared laser - by tuning the IR laser, the system can scan over resonances and obtain the vibrational spectrum of the interfacial region in a piecewise fashion.. In the case of broadband spectrometers, one laser (typically the visible laser) is kept at a fixed narrow wavelength, and the other laser produces a spectrally broad beam. These laser beams again overlap at the interface being studied, but can cover a",
      "find applications in spectroscopy, photochemistry, atomic vapor laser isotope separation, and optical communications. Since no real laser is truly monochromatic, all lasers can emit light over some range of frequencies, known as the linewidth of the laser transition. In most lasers, this linewidth is quite narrow (for example, the nm wavelength transition of a has a linewidth of approximately 120 GHz, or 0.45 nm). Tuning of the laser output across this range can be achieved by placing wavelength-selective optical elements (such as an etalon) into the laser's optical cavity, to provide selection of a particular longitudinal mode of the cavity."
    ]
  ],
  [
    "The observation that an ortho-state has lower energy than the corresponding para-state can be understood in terms of the total nuclear spin by recognizing that ortho-hydrogen has a total nuclear spin of 1, while para-hydrogen has a total nuclear spin of 0, resulting in different energy levels due to spin alignment with the magnetic field.",
    [
      "parallel while the other is anti-parallel, yielding a total nuclear spin of . Similarly, H also has ortho and para states, with ortho-H having a total nuclear spin 1 and para-H having a total nuclear spin of 0. When an ortho- and a para-H collide, the transferred proton changes the total spins of the molecules, yielding instead a para- and an ortho-H. The spectroscopy of is challenging. The pure rotational spectrum is exceedingly weak. Ultraviolet light is too energetic and would dissociate the molecule. Rovibronic (Infrared) spectroscopy provides the ability to observe . Rovibronic spectroscopy is possible with because one",
      "or alternatively: As a result, the different nuclear spin states have different energies in a non-zero magnetic field. In less formal language, we can talk about the two spin states of a spin as being \"aligned\" either with or against the magnetic field. If \"\u03b3\" is positive (true for most isotopes used in NMR) then is the lower energy state. The energy difference between the two states is: and this results in a small population bias favoring the lower energy state in thermal equilibrium. With more spins pointing up than down, a net spin magnetization along the magnetic field B",
      "the three values M = 1, 0, or \u22121. The corresponding nuclear spin wavefunctions are formula_1 and formula_2. This uses standard bra\u2013ket notation; the symbol \u2191 represents the spin-up wavefunction and the symbol \u2193 the spin-down wavefunction, so \u2191\u2193 means that the first nucleus is up and the second down. Each orthohydrogen energy level then has a (nuclear) spin degeneracy of three, meaning that it corresponds to three states of the same energy (in the absence of a magnetic field). The singlet parahydrogen state has nuclear spin quantum numbers I = 0 and M = 0, with wavefunction formula_3. Since"
    ]
  ],
  [
    ") of a given particle.",
    [
      "Spin quantum number In atomic physics, the spin quantum number is a quantum number that parameterizes the intrinsic angular momentum (or spin angular momentum, or simply spin) of a given particle. The spin quantum number is the fourth of a set of quantum numbers (the principal quantum number, the azimuthal quantum number, the magnetic quantum number, and the spin quantum number), which completely describe the quantum state of an electron. It is designated by the letter . It describes the energy, shape and orientation of orbitals. The name comes from a physical spinning (denoted by the letter s) about an",
      "\"S\" = with two spin projections (\"S\" = +, and \"S\" = \u2212). There is another quantity of angular momentum, called the orbital angular momentum (azimuthal quantum number \"L\"), that comes in increments of 1 \u0127, which represent the angular moment due to quarks orbiting around each other. The total angular momentum (total angular momentum quantum number \"J\") of a particle is therefore the combination of intrinsic angular momentum (spin) and orbital angular momentum. It can take any value from to , in increments of 1. Particle physicists are most interested in baryons with no orbital angular momentum (\"L\" =",
      "This is equal to the number of different possible values of the total (orbital plus spin) angular momentum J for a given (L, S) combination, provided that S \u2264 L (the typical case). For example, if S = 1, there are three states which form a triplet. The eigenvalues of S for these three states are +1\u0127, 0 and -1\u0127. The term symbol of an atomic state indicates its values of L, S, and J. Spin quantum number In atomic physics, the spin quantum number is a quantum number that parameterizes the intrinsic angular momentum (or spin angular momentum, or"
    ]
  ],
  [
    "1/3 of the initial voltage on the first capacitor.",
    [
      "the voltage on the two capacitors must be equal since they are connected together. Since they both have the same capacitance formula_3 the charge will be divided equally between the capacitors so each capacitor will have a charge of formula_4 and a voltage of formula_5. At the beginning of the experiment the total initial energy formula_6 in the circuit is the energy stored in the charged capacitor: At the end of the experiment the final energy formula_8 is equal to the sum of the energy in the two capacitors Thus the final energy formula_8 is equal to half of the",
      "the capacitors are charged in parallel, they are connected to the load in series. Since \"C2\" and \"C4\" are in series between the output and ground, the total output voltage (under no-load conditions) is \"V\" = 4\"V\". This circuit can be extended to any number of stages. The no-load output voltage is twice the peak input voltage multiplied by the number of stages N or equivalently the peak-to-peak input voltage swing (\"V\") times the number of stages The number of stages is equal to the number of capacitors in series between the output and ground. One way to look at",
      "inductance \"L\". A series circuit containing only a resistor, a capacitor, a switch and a constant DC source of voltage \"V\" is known as a \"charging circuit\". If the capacitor is initially uncharged while the switch is open, and the switch is closed at \"t\", it follows from Kirchhoff's voltage law that Taking the derivative and multiplying by \"C\", gives a first-order differential equation: At \"t\" = 0, the voltage across the capacitor is zero and the voltage across the resistor is \"V\". The initial current is then \"I\"(0) =\"V\"/\"R\". With this assumption, solving the differential equation yields where \u03c4"
    ]
  ],
  [
    "The energy of protons used in cancer therapy damages tumor cells by targeting their DNA, ultimately killing them due to their reduced ability to repair DNA damage and high rate of division, while minimizing side effects to surrounding healthy tissue due to the beam's focused delivery and Bragg peak effect.",
    [
      "tumor with a beam of protons. These charged particles damage the DNA of cells, ultimately killing them by stopping their reproduction. Cancerous cells are particularly vulnerable to attacks on DNA because of their high rate of division and their reduced abilities to repair DNA damage. Some cancers with specific defects in DNA repair may be more sensitive to proton radiation. Because of their relatively large mass, protons have little lateral side scatter in the tissue; the beam does not broaden much, stays focused on the tumor shape, and delivers only low-dose side effects to surrounding tissue. All protons of a",
      "delivers small dose side-effects to surrounding tissue. They also more precisely target the tumor using the Bragg peak effect. See proton therapy for a good example of the different effects of intensity-modulated radiation therapy (IMRT) vs. charged particle therapy. This procedure reduces damage to healthy tissue between the charged particle radiation source and the tumor and sets a finite range for tissue damage after the tumor has been reached. In contrast, IMRT's use of uncharged particles causes its energy to damage healthy cells when it exits the body. This exiting damage is not therapeutic, can increase treatment side effects, and",
      "death. Because of their reduced ability to repair damaged DNA, cancerous cells are particularly vulnerable to attack. The figure shows how beams of electrons, X-rays or protons of different energies (expressed in MeV) penetrate human tissue. Electrons have a short range and are therefore only of interest close to the skin (see electron therapy). Bremsstrahlung X-rays penetrate more deeply, but the dose absorbed by the tissue then shows the typical exponential decay with increasing thickness. For protons and heavier ions, on the other hand, the dose increases while the particle penetrates the tissue and loses energy continuously. Hence the dose"
    ]
  ],
  [
    "The constant related to the linear momentum and wavelength of a free massive particle according to De Broglie's hypothesis is the Planck constant, denoted as h.",
    [
      "hypothesis, and the relation has been shown explicitly for particles as large as molecules. De Broglie deduced that if the duality equations already known for light were the same for any particle, then his hypothesis would hold. This means that where is the total energy of the particle, is its momentum, is the reduced Planck constant. For a free non-relativistic particle it follows that where is the mass of the particle and its velocity. Also in special relativity we find that where is the rest mass of the particle and is the speed of light in a vacuum. But (see",
      "10,123 amu. Still one step further than Louis De Broglie go theories which in quantum mechanics eliminate the concept of a pointlike classical particle and explain the observed facts by means of wavepackets of matter waves alone. The de Broglie equations relate the wavelength to the momentum , and frequency to the total energy of a particle: formula_5 where \"h\" is the Planck constant. The equations can also be written as formula_6 or formula_7 where is the reduced Planck constant, is the wave vector, is the phase constant, and is the angular frequency. In each pair, the second equation is",
      "the matter. The de Broglie wavelength associated with a massive particle is where h is the Planck constant. The momentum can be found from the kinetic theory of gases, where Here, the temperature can be found as Of course, we can replace the momentum here with the momentum derived from the de Broglie wavelength like so: Hence, we can say that quantum fluids will manifest at approximate temperature regions where formula_5, where d is the lattice spacing (or inter-particle spacing). Mathematically, this is stated like so: It is easy to see how the above definition relates to the particle density,"
    ]
  ],
  [
    "The magnitude of the particle's relativistic momentum is equal to the square root of the difference between the total energy and twice the rest energy squared, all divided by the speed of light.",
    [
      "does momentum. However, for single particles the rest mass remains constant, and for systems of particles the invariant mass remain constant, because in both cases, the energy and momentum increases subtract from each other, and cancel. Thus, the invariant mass of systems of particles is a calculated constant for all observers, as is the rest mass of single particles. For systems of particles, the energy\u2013momentum equation requires summing the momentum vectors of the particles: The inertial frame in which the momenta of all particles sums to zero is called the center of momentum frame. In this special frame, the relativistic",
      "relativistic mass corresponds to the total energy. The relativistic mass is the mass of the system as it would be measured on a scale, but in some cases (such as the box above) this fact remains true only because the system on average must be at rest to be weighed (it must have zero net momentum, which is to say, the measurement is in its center of momentum frame). For example, if an electron in a cyclotron is moving in circles with a relativistic velocity, the mass of the cyclotron+electron system is increased by the relativistic mass of the electron,",
      "for clarity. The energy and momentum of an object with invariant mass \"m\" are related by the formulas The first is referred to as the \"relativistic energy\u2013momentum relation\". It can be derived by considering that formula_18 can be written as formula_19 where the denominator can be written as formula_20. Now, gamma can be replaced in the expression of energy. While the energy \"E\" and the momentum p depend on the frame of reference in which they are measured, the quantity \"E\" \u2212 (\"pc\") is invariant, and arises as \u2212\"c\" times the squared magnitude of the 4-momentum vector which is \u2212(\"m\"\"c\")."
    ]
  ],
  [
    "The chemical bonds in diamonds form an inflexible three-dimensional lattice, while in graphite, the atoms are tightly bonded into sheets that can easily slide over each other, resulting in different physical properties such as hardness and conductivity.",
    [
      "structure -(C:::C)-. The system of carbon allotropes spans an astounding range of extremes, considering that they are all merely structural formations of the same element. Between diamond and graphite: Despite the hardness of diamonds, the chemical bonds that hold the carbon atoms in diamonds together are actually weaker than those that hold together graphite. The difference is that in diamond, the bonds form an inflexible three-dimensional lattice. In graphite, the atoms are tightly bonded into sheets, but the sheets can slide easily over each other, making graphite soft. Allotropes of carbon Carbon is capable of forming many allotropes due to",
      "and the least compressible. It also has a high density, ranging from 3150 to 3530 kilograms per cubic metre (over three times the density of water) in natural diamonds and 3520 kg/m\u00b3 in pure diamond. In graphite, the bonds between nearest neighbors are even stronger but the bonds between planes are weak, so the planes can easily slip past each other. Thus, graphite is much softer than diamond. However, the stronger bonds make graphite less flammable. Diamonds have been adapted for many uses because of the material's exceptional physical characteristics. Most notable are its extreme hardness and thermal conductivity (900\u2013",
      "as conductivity. In diamond all four outer electrons of each carbon atom are 'localized' between the atoms in covalent bonding. The movement of electrons is restricted and diamond does not conduct an electric current. In graphite, each carbon atom uses only 3 of its 4 outer energy level electrons in covalently bonding to three other carbon atoms in a plane. Each carbon atom contributes one electron to a delocalized system of electrons that is also a part of the chemical bonding. The delocalized electrons are free to move throughout the plane. For this reason, graphite conducts electricity along the planes"
    ]
  ],
  [
    "The maximum distance from the center of the turntable at which the coin will not slide is determined by the formula for the center of oscillation and the moment of inertia of the coin.",
    [
      "a point called the center of oscillation of the compound pendulum. This point also corresponds to the center of percussion. The length formula_5 is determined from the formula, or The seconds pendulum, which provides the \"tick\" and \"tock\" of a grandfather clock, takes one second to swing from side-to-side. This is a period of two seconds, or a natural frequency of formula_69 for the pendulum. In this case, the distance to the center of oscillation, formula_5, can be computed to be Notice that the distance to the center of oscillation of the seconds pendulum must be adjusted to accommodate different",
      "in PAL. f) One half of a complete interlaced video picture (frame), containing all the odd or even scanning lines of the picture. The maximum angle of view that can be seen through a lens. A rapid variation in the pitch or of a recorded audio signal usually on a turntable or tape recording, caused by variations in the speed of the turntable or tape drive. The distance from the focal point to the principal point of the lens. The focal length is usually measured in millimeters of the lens. Focal length is an indication of the lens capability to",
      "coin may be caught; caught and inverted; or allowed to land on the ground. When the coin comes to rest, the toss is complete and the party who called correctly or was assigned the upper side is declared the winner. It is possible for a coin to land on its edge, usually by landing up against an object (such as a shoe) or by getting stuck in the ground. However, even on a flat surface it is possible for a coin to land on its edge, with a chance of about 1 in 6000 for an American nickel. Angular momentum"
    ]
  ],
  [
    "The sign of the charge carriers in a doped semiconductor can be deduced based on whether it is doped with \"donor\" impurities (n-type) or \"acceptor\" impurities (p-type).",
    [
      "a factor of 10,000. The materials chosen as suitable dopants depend on the atomic properties of both the dopant and the material to be doped. In general, dopants that produce the desired controlled changes are classified as either electron acceptors or donors. Semiconductors doped with \"donor\" impurities are called \"n-type\", while those doped with \"acceptor\" impurities are known as \"p-type\". The n and p type designations indicate which charge carrier acts as the material's majority carrier. The opposite carrier is called the minority carrier, which exists due to thermal excitation at a much lower concentration compared to the majority carrier.",
      "of charge carriers in a crystal lattice. Doping greatly increases the number of charge carriers within the crystal. When a doped semiconductor contains mostly free holes it is called \"p-type\", and when it contains mostly free electrons it is known as \"n-type\". The semiconductor materials used in electronic devices are doped under precise conditions to control the concentration and regions of p- and n-type dopants. A single semiconductor crystal can have many p- and n-type regions; the p\u2013n junctions between these regions are responsible for the useful electronic behavior. Although some pure elements and many compounds display semiconductor properties, silicon,",
      "Semiconductor materials are useful because their behavior can be easily manipulated by the addition of impurities, known as doping. Semiconductor conductivity can be controlled by the introduction of an electric or magnetic field, by exposure to light or heat, or by the mechanical deformation of a doped monocrystalline grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs via mobile or \"free\" \"electrons\" and \"holes\", collectively known as \"charge carriers\". Doping a semiconductor such as silicon with a small proportion of an atomic impurity, such as phosphorus or boron, greatly increases the number of free electrons or"
    ]
  ],
  [
    "The primary source of energy in the Sun and similar size stars is nuclear fusion of hydrogen to form helium.",
    [
      "the time, reasoned that: All of these speculations were proven correct in the following decades. The primary source of solar energy, and similar size stars, is the fusion of hydrogen to form helium (the proton-proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of",
      "Sun The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process.<ref name=\"doi10.1146/annurev-astro-081913-040012\"></ref> It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers, or 109 times that of Earth, and its mass is about 330,000 times that of Earth. It accounts for about 99.86% of the total mass of the Solar System. Roughly three quarters of the Sun's mass consists of hydrogen (~73%); the rest is",
      "equilibrium, becoming what is known as a main-sequence star. Nuclear fusion powers a star for most of its life. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red giant phase. Stars with at least half the mass of the Sun can also begin to generate"
    ]
  ],
  [
    "The characteristic distribution of electromagnetic radiation emitted by or absorbed by a particular object is called the electromagnetic spectrum.",
    [
      "browning reactions in common food items induced by infrared radiation, during broiling-type cooking. The electromagnetic spectrum is the range of all possible electromagnetic radiation frequencies. The electromagnetic spectrum (usually just spectrum) of an object is the characteristic distribution of electromagnetic radiation emitted by, or absorbed by, that particular object. The non-ionizing portion of electromagnetic radiation consists of electromagnetic waves that (as individual quanta or particles, see photon) are not energetic enough to detach electrons from atoms or molecules and hence cause their ionization. These include radio waves, microwaves, infrared, and (sometimes) visible light. The lower frequencies of ultraviolet light may",
      "fundamental quantity that describes a field of radiation is called spectral radiance in radiometric terms (in other fields it is often called specific intensity). For a very small area element in the radiation field, there can be electromagnetic radiation passing in both senses in every spatial direction through it. In radiometric terms, the passage can be completely characterized by the amount of energy radiated in each of the two senses in each spatial direction, per unit time, per unit area of surface of sourcing passage, per unit solid angle of reception at a distance, per unit wavelength interval being considered",
      "Absorption (electromagnetic radiation) In physics, absorption of electromagnetic radiation is the way in which the energy of a photon is taken up by matter, typically the electrons of an atom. Thus, the electromagnetic energy is transformed into internal energy of the absorber, for example thermal energy. The reduction in intensity of a light wave propagating through a medium by absorption of a part of its photons is often called attenuation. Usually, the absorption of waves does not depend on their intensity (linear absorption), although in certain conditions (usually, in optics), the medium changes its transparency dependently on the intensity of"
    ]
  ],
  [
    "The distinction between bosons and fermions lies in their spin values, with bosons having integer spins and fermions having half-integer spins in quantum mechanics.",
    [
      "divided into two types: bosons and fermions. The distinction between bosons and fermions is basic. Fermions are particles which have half integer spin (1/2, 3/2, 5/2 and so on), measured in units of Planck's constant and bosons are particles which have integer spin (0, 1, 2 and so on), measured in units of Planck's constant. Examples of fermions are quarks, leptons and baryons. Quantum of fundamental forces such as gravitons, photons, etc. are all bosons. In quantum field theory, fermions interact by exchanging bosons. Early string theory proposed by Yoichiro Nambu and others in 1970 was only a bosonic string.",
      "In the theory of quantum mechanics fermions are described by antisymmetric states. In contrast, particles with integer spin (called bosons) have symmetric wave functions; unlike fermions they may share the same quantum states. Bosons include the photon, the Cooper pairs which are responsible for superconductivity, and the W and Z bosons. (Fermions take their name from the Fermi\u2013Dirac statistical distribution that they obey, and bosons from their Bose\u2013Einstein distribution.) In the early 20th century it became evident that atoms and molecules with even numbers of electrons are more chemically stable than those with odd numbers of electrons. In the 1916",
      "ones. Fermions are particles \"like electrons and nucleons\" and generally comprise the matter. Note that any subatomic or atomic particle composed of even \"total\" number of fermions (such as protons, neutrons, and electrons) is a boson, so a boson is not necessarily a force transmitter and perfectly can be an ordinary material particle. The spin is the quantity that distinguishes bosons and fermions. Practically it appears as an intrinsic angular momentum of a particle, that is unrelated to its motion but is linked with some other features like a magnetic dipole. Theoretically it is explained from different types representations of"
    ]
  ],
  [
    "The ortho form of hydrogen has a higher energy state than the para form due to the residual rotational energy and nuclear-spin entropy associated with the triplet state, making it unstable and unable to be purified.",
    [
      "para form and 75% of the ortho form, also known as the \"normal form\". The equilibrium ratio of orthohydrogen to parahydrogen depends on temperature, but because the ortho form is an excited state and has a higher energy than the para form, it is unstable and cannot be purified. At very low temperatures, the equilibrium state is composed almost exclusively of the para form. The liquid and gas phase thermal properties of pure parahydrogen differ significantly from those of the normal form because of differences in rotational heat capacities, as discussed more fully in \"spin isomers of hydrogen\". The ortho/para",
      "are derived for any of these cases from: formula_8 Plots shown here are molar rotational energies and heat capacities for ortho- and parahydrogen, and the \"normal\" ortho/para (3:1) and equilibrium mixtures: Because of the antisymmetry-imposed restriction on possible rotational states, orthohydrogen has residual rotational energy at low temperature wherein nearly all the molecules are in the J = 1 state (molecules in the symmetric spin-triplet state cannot fall into the lowest, symmetric rotational state) and possesses nuclear-spin entropy due to the triplet state's threefold degeneracy. The residual energy is significant because the rotational energy levels are relatively widely spaced in",
      "H; the gap between the first two levels when expressed in temperature units is twice the characteristic rotational temperature for H: formula_9. This is the T = 0 intercept seen in the molar energy of orthohydrogen. Since \"normal\" room-temperature hydrogen is a 3:1 ortho:para mixture, its molar residual rotational energy at low temperature is (3/4) x 2R\u03b8 = 1091 J/mol, which is somewhat larger than the enthalpy of vaporization of normal hydrogen, 904 J/mol at the boiling point, T = 20.369 K. Notably, the boiling points of parahydrogen and normal (3:1) hydrogen are nearly equal; for parahydrogen \u2206H = 898"
    ]
  ],
  [
    "The total mechanical energy of the system consisting of a mass attached to a horizontal spring with a wall at one end is the sum of the potential energy stored in the spring and the kinetic energy of the mass, which oscillates back and forth between the two forms of energy.",
    [
      "an ideal massless spring, formula_76 is the mass on the end of the spring. If the spring itself has mass, its effective mass must be included in formula_76. In terms of energy, all systems have two types of energy: potential energy and kinetic energy. When a spring is stretched or compressed, it stores elastic potential energy, which then is transferred into kinetic energy. The potential energy within a spring is determined by the equation formula_78 When the spring is stretched or compressed, kinetic energy of the mass gets converted into potential energy of the spring. By conservation of energy, assuming",
      "energy. The energy of the system is oscillating back and forth between kinetic energy and potential energy. In the animation with the two circling masses there is a back and forth oscillation of kinetic energy and potential energy. When the spring is at its maximal extension then the potential energy is largest, when the angular velocity is at its maximum the kinetic energy is at largest. With a real spring there is friction involved. With a real spring the vibration will be damped and the final situation will be that the masses circle each other at a constant distance, with",
      "Effective mass (spring\u2013mass system) In a real spring\u2013mass system, the spring has a non-negligible mass formula_1. Since not all of the spring's length moves at the same velocity formula_2 as the suspended mass formula_3, its kinetic energy is not equal to formula_4. As such, formula_1 cannot be simply added to formula_3 to determine the frequency of oscillation, and the effective mass of the spring is defined as the mass that needs to be added to formula_3 to correctly predict the behavior of the system. The effective mass of the spring in a spring-mass system when using an ideal spring of"
    ]
  ],
  [
    "An observer's state of motion can affect their measurements of the passing meter stick's length and time, resulting in different values observed by different observers due to effects such as time dilation and length contraction (Special relativity; geometry).",
    [
      "passed for the primed observer than has passed for the unprimed observer. Each observer measures the clocks of the other observer as running more slowly. Please note the importance of the word \"measure\". An observer's state of motion cannot affect an observed object, but it \"can\" affect the observer's \"observations\" of the object. In Fig. 2-10, each line drawn parallel to the \"x\" axis represents a line of simultaneity for the unprimed observer. All events on that line have the same time value of \"ct\". Likewise, each line drawn parallel to the axis represents a line of simultaneity for the",
      "one observer may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage). Similarly, suppose a measuring rod is at rest and aligned along the \"x\"-axis in the unprimed system \"S\". In this system, the length of this rod is written as \u0394\"x\". To measure the length of this rod in the system \"S\"\u2032, in which the rod is moving, the distances \"x\"\u2032 to the end points of the rod must be measured simultaneously",
      "geometry). Because of this rotation, the projection of a primed meter-stick onto the unprimed x-axis is foreshortened, while the projection of an unprimed meter-stick onto the primed x\u2032-axis is likewise foreshortened. Fig. 2-10 reinforces previous discussions about mutual time dilation. In this figure, Events A and C are separated from event O by equal timelike intervals. From the unprimed frame, events A and B are measured as simultaneous, but more time has passed for the unprimed observer than has passed for the primed observer. From the primed frame, events C and D are measured as simultaneous, but more time has"
    ]
  ],
  [
    "The work done on a particle of mass m in order for it to reach a speed of 0.6c from rest can be calculated using the work-energy principle, which states that the work done is equal to the change in kinetic energy of the particle, and can be expressed as W = (\u03b3 - 1)mc^2, where \u03b3 is the Lorentz factor given by 1/sqrt(1 - (v^2/c^2)), v is the final speed of the particle (0.6c in this case), c is the speed of light, and m is the mass of the particle.",
    [
      "(1 m/s) from a state of rest, when it achieves the speed of 5 m/s after 5 seconds and 10 m/s after 10 seconds. The average acceleration can be calculated by dividing the speed (m/s) by the time (s), so the average acceleration in the first example would be calculated formula_1. Newton's Second Law states that force equals mass multiplied by acceleration. The unit of force is the newton (N), and mass has the SI unit kilogram (kg). One newton equals one kilogram metre per second squared. Therefore, the unit metre per second squared is equivalent to newton per kilogram,",
      "equals the change in the kinetic energy of the particle.\" That is, the work \"W\" done by the resultant force on a particle equals the change in the particle's kinetic energy formula_42, where formula_44 and formula_45 are the speeds of the particle before and after the work is done, and \"m\" is its mass. The derivation of the \"work\u2013energy principle\" begins with Newton\u2019s second law of motion and the resultant force on a particle. Computation of the scalar product of the forces with the velocity of the particle evaluates the instantaneous power added to the system. Constraints define the direction",
      "a given velocity is not just 4 times as high as in the case of one particle resting (as it would be in non-relativistic physics); it can be orders of magnitude higher if the collision velocity is near the speed of light. In the case of a collider where the collision point is at rest in the laboratory frame (i.e. formula_1), the center of mass energy formula_2 (the energy available for producing new particles in the collision) is simply formula_3, where formula_4 and formula_5 is the total energy of a particle from each beam. For a fixed target experiment where"
    ]
  ],
  [
    "The process that results in the production of characteristic X-rays when high-energy electrons bombard a metal target is when outer-shell electrons fill a vacancy in the inner shell of an atom, releasing X-rays in a pattern that is \"characteristic\" to each element.",
    [
      "tube, electrons are accelerated in a vacuum by an electric field and shot into a piece of metal called the \"target\". X-rays are emitted as the electrons slow down (decelerate) in the metal. The output spectrum consists of a continuous spectrum of X-rays, with additional sharp peaks at certain energies (see graph on right). The continuous spectrum is due to \"bremsstrahlung\", while the sharp peaks are characteristic X-rays associated with the atoms in the target. The spectrum has a sharp cutoff at low wavelength (high frequency), which is due to the limited energy of the incoming electrons. For example, if",
      "produced. X-rays can be generated by an X-ray tube, a vacuum tube that uses a high voltage to accelerate the electrons released by a hot cathode to a high velocity. The high velocity electrons collide with a metal target, the anode, creating the X-rays. In medical X-ray tubes the target is usually tungsten or a more crack-resistant alloy of rhenium (5%) and tungsten (95%), but sometimes molybdenum for more specialized applications, such as when softer X-rays are needed as in mammography. In crystallography, a copper target is most common, with cobalt often being used when fluorescence from iron content in",
      "Characteristic X-ray Characteristic X-rays are emitted when outer-shell electrons fill a vacancy in the inner shell of an atom, releasing X-rays in a pattern that is \"characteristic\" to each element. Characteristic X-rays were discovered by Charles Glover Barkla in 1909, who later won the Nobel Prize in Physics for his discovery in 1917. Characteristic X-rays are produced when an element is bombarded with high-energy particles, which can be photons, electrons or ions (such as protons). When the incident particle strikes a bound electron (the target electron) in an atom, the target electron is ejected from the inner shell of the"
    ]
  ],
  [
    "The n=3 energy level of an atom can hold a maximum of 18 electrons.",
    [
      "all atoms so far discovered. For a given value of the principal quantum number \"n\", the possible values of \"\u2113\" range from 0 to \"n\" \u2212 1; therefore, the \"n\" = 1 shell only possesses an s subshell and can only take 2 electrons, the \"n\" = 2 shell possesses an s and a p subshell and can take 8 electrons overall, the \"n\" = 3 shell possesses s, p, and d subshells and has a maximum of 18 electrons, and so on. Generally speaking, the maximum number of electrons in the \"n\"th energy level is 2\"n\". The angular momentum",
      "terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the \"n\" = 1 state can hold one or two electrons, while the \"n\" = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all \"n\" = 1 states are fully occupied; the same for \"n\" = 1 and \"n\" = 2 in neon. In argon the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the",
      "energy increases so much that the electron can easily escape from the atom. In single electron atoms, all energy levels with the same principle quantum number are degenerate, and have the same energy. In atoms with more than one electron, the energy of an electron depends not only on the properties of the orbital it resides in, but also on its interactions with the other electrons in other orbitals. This requires consideration of the l quantum number. Higher values of l are associated with higher values of energy; for instance, the 2p state is higher than the 2s state. When"
    ]
  ],
  [
    "If the frequency of the laser light is doubled, the separation of the bright fringes will remain the same.",
    [
      "simplest case, the laser's optical cavity is formed by two opposed plane (flat) mirrors surrounding the gain medium (a plane-parallel or Fabry\u2013P\u00e9rot cavity). The allowed modes of the cavity are those where the mirror separation distance \"L\" is equal to an exact multiple of half the wavelength, \"\u03bb\": where \"q\" is an integer known as the mode order. In practice, the separation distance of the mirrors \"L\" is usually much greater than the wavelength of light \"\u03bb\", so the relevant values of \"q\" are large (around 10 to 10). The frequency separation between any two adjacent modes, \"q\" and \"q\"+1,",
      "is divided into two beams. Those cross inside the electrophresis cell at a fixed angle to produce a fringe pattern. The scattered light from the particles, which migrates inside the fringe, is intensity-modulated. The frequency shifts from both types of optics obey the same equations. The observed spectra resemble each other. Oka et al. developed an ELS instrument of heterodyne-type optics that is now available commercially. Its optics is shown in Fig. 3. If the frequencies of the intersecting laser beams are the same then it is not possible to resolve the direction of the motion of the migrating particles.",
      "so the relevant values of \"q\" are large (around 10 to 10). Of more interest is the frequency separation between any two adjacent modes \"q\" and \"q\"+1; this is given (for an empty linear resonator of length \"L\") by \u0394\"\u03bd\": where \"c\" is the speed of light (\u22483\u00d710 m\u00b7s). Using the above equation, a small laser with a mirror separation of 30 cm has a frequency separation between longitudinal modes of 0.5 GHz. Thus for the two lasers referenced above, with a 30-cm cavity, the 1.5 GHz bandwidth of the HeNe laser would support up to 3 longitudinal modes, whereas"
    ]
  ],
  [
    "The radiative forcing for solar irradiance on the Earth's surface is typically quantified at the top of the atmosphere in units of watts per square meter.",
    [
      "other forcings. Radiative forcing (measured in watts per square meter) can be estimated in different ways for different components. For solar irradiance (\"i.e.,\" \"solar forcing\"), the radiative forcing is simply the change in the average amount of solar energy absorbed per square meter of the Earth's area. Since the Earth's cross-sectional area exposed to the Sun (\u03c0r) is equal to 1/4 of the surface area of the Earth (4\u03c0r), the solar input per unit area is one quarter the change in solar intensity. This must be multiplied by the fraction of incident sunlight that is absorbed, F=(1-R), where R is",
      "Radiative forcing Radiative forcing or climate forcing is the difference between insolation (sunlight) absorbed by the Earth and energy radiated back to space. The influences that cause changes to the Earth\u2019s climate system altering Earth\u2019s radiative equilibrium, forcing temperatures to rise or fall, are called climate forcings. Positive radiative forcing means Earth receives more incoming energy from sunlight than it radiates to space. This net gain of energy will cause warming. Conversely, negative radiative forcing means that Earth loses more energy to space than it receives from the sun, which produces cooling. Typically, radiative forcing is quantified at the tropopause",
      "or at the top of the atmosphere (often accounting for rapid adjustments in temperature) in units of watts per square meter of the Earth's surface. Positive forcing (incoming energy exceeding outgoing energy) warms the system, while negative forcing (outgoing energy exceeding incoming energy) cools it. Causes of radiative forcing include changes in insolation and the concentrations of radiatively active gases, commonly known as greenhouse gases, and aerosols. Almost all of the energy that affects Earth's climate is received as radiant energy from the Sun. The planet and its atmosphere absorb and reflect some of the energy, while long-wave energy is"
    ]
  ],
  [
    "The doping concentration can be measured to deduce the sign of charge carriers in a doped semiconductor. (Semiconductor characterization techniques)",
    [
      "resulting photoconductivity is measured using a terahertz probe, which detects changes in the terahertz electric field. The charge carriers in semiconductors are electrons and holes. Their numbers are controlled by the concentrations of impurity elements, i.e. doping concentration. Thus doping concentration has great influence on carrier mobility. While there is considerable scatter in the experimental data, for noncompensated material (no counter doping) for heavily doped substrates (i.e. formula_43 and up), the mobility in silicon is often characterized by the empirical relationship: where \"N\" is the doping concentration (either \"N\" or \"N\"), and \"N\" and \u03b1 are fitting parameters. At room",
      "Semiconductor characterization techniques The purpose of this article is to summarize the methods used to experimentally characterize a semiconductor material or device (PN junction, Schottky diode, etc.). Some examples of semiconductor quantities that could be characterized include depletion width, carrier concentration, optical generation and recombination rate, carrier lifetimes, defect concentration, trap states, etc. These quantities fall into three categories when it comes to characterization methods: 1) Electrical Characterization 2) Optical Characterization 3) Physical/Chemical Characterization Electrical Characterization can be used to determine resistivity, carrier concentration, mobility, contact resistance, barrier height, depletion width, oxide charge, interface states, carrier lifetimes, and deep level",
      "Semiconductor materials are useful because their behavior can be easily manipulated by the addition of impurities, known as doping. Semiconductor conductivity can be controlled by the introduction of an electric or magnetic field, by exposure to light or heat, or by the mechanical deformation of a doped monocrystalline grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs via mobile or \"free\" \"electrons\" and \"holes\", collectively known as \"charge carriers\". Doping a semiconductor such as silicon with a small proportion of an atomic impurity, such as phosphorus or boron, greatly increases the number of free electrons or"
    ]
  ],
  [
    "Boron ion cannot be used to make an n-type semiconductor when doping germanium.",
    [
      "group IV atom acts as an acceptor. Group IV atoms can act as both donors and acceptors; therefore, they are known as amphoteric impurities. \"N-type\" semiconductors are created by doping an intrinsic semiconductor with an electron donor element during manufacture. The term \"n-type\" comes from the negative charge of the electron. In \"n-type\" semiconductors, electrons are the majority carriers and holes are the minority carriers. A common dopant for \"n-type\" silicon is phosphorus or arsenic. In an \"n-type\" semiconductor, the Fermi level is greater than that of the intrinsic semiconductor and lies closer to the conduction band than the valence",
      "semiconductor to conduct electricity. When on the order of one dopant atom is added per 100 million atoms, the doping is said to be \"low\" or \"light\". When many more dopant atoms are added, on the order of one per ten thousand atoms, the doping is referred to as \"high\" or \"heavy\". This is often shown as \"n+\" for n-type doping or \"p+\" for p-type doping. (\"See the article on semiconductors for a more detailed description of the doping mechanism.\") For the Group IV semiconductors such as diamond, silicon, germanium, silicon carbide, and silicon germanium, the most common dopants are",
      "a factor of 10,000. The materials chosen as suitable dopants depend on the atomic properties of both the dopant and the material to be doped. In general, dopants that produce the desired controlled changes are classified as either electron acceptors or donors. Semiconductors doped with \"donor\" impurities are called \"n-type\", while those doped with \"acceptor\" impurities are known as \"p-type\". The n and p type designations indicate which charge carrier acts as the material's majority carrier. The opposite carrier is called the minority carrier, which exists due to thermal excitation at a much lower concentration compared to the majority carrier."
    ]
  ],
  [
    "The mass of the planet in this system is equal to one Jupiter mass.",
    [
      "the mass of Jupiter. In practice, the masses of celestial bodies appear in the dynamics of the Solar System only through the products \"GM\", where \"G\" is the constant of gravitation. In the past, \"GM\" of the Sun could be determined experimentally with only limited accuracy. Its present accepted value is Jupiter mass ( or \"M\"), is the unit of mass equal to the total mass of the planet Jupiter, . Jupiter mass is used to describe masses of the gas giants, such as the outer planets and extrasolar planets. It is also used in describing brown dwarfs and Neptune-mass",
      "century. Earth lost about 3473 tons in the initial 53 years of the space age, but the trend is currently decreasing. Earth mass Earth mass (M or , where \u2295 is the standard astronomical symbol for planet Earth) is the unit of mass equal to that of Earth. The current best estimate for Earth mass is , with a standard uncertainty of It is equivalent to an average density of . The Earth mass is a standard unit of mass in astronomy that is used to indicate the masses of other planets, including rocky terrestrial planets and exoplanets. One Solar",
      "Earth mass Earth mass (M or , where \u2295 is the standard astronomical symbol for planet Earth) is the unit of mass equal to that of Earth. The current best estimate for Earth mass is , with a standard uncertainty of It is equivalent to an average density of . The Earth mass is a standard unit of mass in astronomy that is used to indicate the masses of other planets, including rocky terrestrial planets and exoplanets. One Solar mass is close to 333,000 Earth masses. The Earth mass excludes the mass of the Moon. The mass of the Moon"
    ]
  ],
  [
    "The observer must move at a velocity greater than zero relative to the rod to measure its length to be shorter than its rest length.",
    [
      "one observer may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage). Similarly, suppose a measuring rod is at rest and aligned along the \"x\"-axis in the unprimed system \"S\". In this system, the length of this rod is written as \u0394\"x\". To measure the length of this rod in the system \"S\"\u2032, in which the rod is moving, the distances \"x\"\u2032 to the end points of the rod must be measured simultaneously",
      "velocity between an observer (or his measuring instruments) and the observed object is zero, then the proper length formula_1 of the object can simply be determined by directly superposing a measuring rod. However, if the relative velocity > 0, then one can proceed as follows: The observer installs a row of clocks that either are synchronized a) by exchanging light signals according to the Poincar\u00e9-Einstein synchronization, or b) by \"slow clock transport\", that is, one clock is transported along the row of clocks in the limit of vanishing transport velocity. Now, when the synchronization process is finished, the object is",
      "in that system \"S\"\u2032. In other words, the measurement is characterized by , which can be combined with the fourth equation to find the relation between the lengths \u0394\"x\" and \u0394\"x\"\u2032: This shows that the length (\u0394\"x\"\u2032) of the rod as measured in the frame in which it is moving (\"S\"\u2032), is \"shorter\" than its length (\u0394\"x\") in its own rest frame (\"S\"). In special relativity quantities, representing the velocity of an object, as measured in different frames, must obey to the differential form of the Lorentz transformation. In standard configuration the measurement of a speed \"u\" (in frame \"S\")"
    ]
  ],
  [
    "The Lorentz force causes a proton moving through uniform electric and magnetic fields to follow a circular trajectory due to the perpendicular force acting on the particle.",
    [
      "by the action of a static magnetic field, whose direction is perpendicular to the velocity vector of the particle. This will cause a force perpendicular both to the velocity vector, and to the field, defining a plane through which the particle moves. The definition of the Lorentz force implies that the particle's motion will be circular in a uniform field, thus giving a constant radius of curvature. If the particle momentum, \"p\", is given in GeV/\"c\", then the rigidity, in tesla-metres, is \"B\u03c1\" = 3.3356\"p\"/\"q\". Rigidity (electromagnetism) In accelerator physics, rigidity is the effect of particular magnetic fields on the",
      "the product of the particle's charge and the magnitude of the electric field, (oriented in the direction of the electric field). In a uniform magnetic field however, charged particles follow a circular trajectory due to the cross product in the magnetic field term of the Lorentz force. (That is, the force from the magnetic field acts on the particle in a direction perpendicular to the particle's direction of motion. See: Lorentz force for more details.) The 'teltron' apparatus consists of a Teltron type electron deflection tube, a Teltron stand, EHT power supply (, variable). In an evacuated glass bulb some",
      "any other.) It is expressed in terms of the four-potential as follows: In the Lorenz gauge, the microscopic Maxwell's equations can be written as: Electromagnetic (EM) fields affect the motion of electrically charged matter: due to the Lorentz force. In this way, EM fields can be detected (with applications in particle physics, and natural occurrences such as in aurorae). In relativistic form, the Lorentz force uses the field strength tensor as follows. Expressed in terms of coordinate time \"t\", it is: where \"p\" is the four-momentum, \"q\" is the charge, and \"x\" is the position. In the co-moving reference frame,"
    ]
  ],
  [
    "The average induced EMF in the coil is 0.2 V.",
    [
      "words, the emf generated is proportional to the rate of change of the magnetic flux. More simply stated, an electric field is induced in any system in which a magnetic field is changing with time. The change could be changes in direction of force or strength. The effects described by this law are those by which generators, motors, alternators, and transformers function. There are two main concepts to be taken from Faraday's Law that apply to the design of inductive discharge ignitions. One is that moving a wire through a magnetic field will induce an electric voltage and current in",
      "variation of the Magnetic Flux formula_1 through a \"N\" turns circuit will induce a voltage \"formula_2\" which follows: which can be expressed in a simpler way: by assuming that the induced magnetic field B is homogeneous over a section \"S\" (the Magnetic flux will be expressed formula_5). The induced voltage (\"formula_2\") may be increased several ways: When a coil is wound around a ferromagnetic core, that increases the sensitivity of the sensor thanks to the apparent permeability of the ferromagnetic core. The magnetic amplification, known as apparent permeability \"formula_7\", is the result of the magnetization of the ferromagnetic core response",
      "flux linked with the coil changes. This induces a current in the coil which can be registered on the galvanometer. Since induced current is directly proportional to rate of change of flux linkage and assuming the coil is removed from the magnetic field very quickly, the maximum current measured by the ammeter is proportional to the magnetic field formula_12. The search coil can be calibrated by repeating this in a known magnetic field. A reference table shows open coil voltage versus magnetic field A/m (or teslas) or short current versus magnetic field. Spacecraft with on board search coil magnetometers have"
    ]
  ],
  [
    "The experimental anomaly of the photoelectric effect, where the energy of ejected electrons was proportional to the frequency of light rather than its intensity, led Einstein to propose that light was composed of particles.",
    [
      "momentum \"p\" of a photon is also proportional to its frequency and inversely proportional to its wavelength: The source of Einstein's proposal that light was composed of particles (or could act as particles in some circumstances) was an experimental anomaly not explained by the wave theory: the photoelectric effect, in which light striking a metal surface ejected electrons from the surface, causing an electric current to flow across an applied voltage. Experimental measurements demonstrated that the energy of individual ejected electrons was proportional to the \"frequency\", rather than the \"intensity\", of the light. Furthermore, below a certain minimum frequency, which",
      "that the energy of photoelectrons increases with increasing \"frequency\" of incident light and is independent of the \"intensity\" of the light. However, the manner of the increase was not experimentally determined until 1914 when Robert Andrews Millikan showed that Einstein's prediction was correct. The photoelectric effect helped to propel the then-emerging concept of wave\u2013particle duality in the nature of light. Light simultaneously possesses the characteristics of both waves and particles, each being manifested according to the circumstances. The effect was impossible to understand in terms of the classical wave description of light, as the energy of the emitted electrons did",
      "atomic system can theoretically be divided into a number of discrete \"energy elements\" \"\u03b5\" (epsilon) such that each of these energy elements is proportional to the frequency \"\u03bd\" with which each of them individually radiate energy, as defined by the following formula: where \"h\" is a numerical value called Planck's constant. Then, Albert Einstein in 1905, in order to explain the photoelectric effect previously reported by Heinrich Hertz in 1887, postulated consistently with Max Planck's quantum hypothesis that light itself is made of individual quantum particles, which in 1926 came to be called photons by Gilbert N. Lewis. The photoelectric"
    ]
  ],
  [
    "An object orbiting at the speed of light with an orbital radius of 1.5 times the Schwarzschild radius is in a special orbit known as the photon sphere.",
    [
      "formula_5 is physical, and cannot be removed. The Schwarzschild radius is nonetheless a physically relevant quantity, as noted above and below. This expression had previously been calculated, using Newtonian mechanics, as the radius of a spherically symmetric body at which the escape velocity was equal to the speed of light. It had been identified in the 18th century by John Michell and by 19th century astronomers such as Pierre-Simon Laplace. The Schwarzschild radius of an object is proportional to the mass. Accordingly, the Sun has a Schwarzschild radius of approximately , whereas Earth's is only about and the Moon's is",
      "accounting for time dilation in the velocity term: This final equation indicates that an object orbiting at the speed of light would have an orbital radius of 1.5 times the Schwarzschild radius. This is a special orbit known as the photon sphere. For the Planck mass formula_24, the Schwarzschild radius formula_25 and the Compton wavelength formula_26 are of the same order as the Planck length formula_27. Classification of black holes by type: A classification of black holes by mass: Schwarzschild radius The Schwarzschild radius (sometimes historically referred to as the gravitational radius) is a physical parameter that shows up in",
      "that: formula_94 is the definition of the Schwarzschild radius for an object of mass formula_87, so the Schwarzschild metric may be rewritten in the alternative form: formula_96 which shows that the metric becomes singular approaching the event horizon (that is, formula_97). The metric singularity is not a physical one (although there is a real physical singularity at formula_98), as can be shown by using a suitable coordinate transformation (e.g. the Kruskal\u2013Szekeres coordinate system). The Schwarzschild metric can also be derived using the known physics for a circular orbit and a temporarily stationary point mass. Start with the metric with coefficients"
    ]
  ],
  [
    "The same pipe resonates at a frequency of 68.5 Hz when the speed of sound is 3 percent lower than it would be at 20\u00b0C.",
    [
      "be calculated as follows. If a pipe is open at both ends, as is true of most organ pipes, its fundamental frequency \"f\" can be calculated (approximately) as follows: where If \"v\" is assumed to be 343 m/s (the speed of sound at sea level, with temperature of 20 \u00b0C, and the pipe length \"l\" is assumed to be eight feet (2.44 m), then the formula yields the value of 70.4 hertz (Hz; cycles per second). This is not far from the pitch of the C two octaves below 440 Hz, which (when concert pitch is set at A =",
      "find the fundamental frequency in terms of the speed of the wave and the length of the tube: If the ends of the same tube are now both closed or both opened as in the last two animations, the wavelength of the fundamental harmonic becomes formula_14. By the same method as above, the fundamental frequency is found to be At 20 \u00b0C (68 \u00b0F) the speed of sound in air is 343 m/s (1129 ft/s). This speed is temperature dependent and increases at a rate of 0.6 m/s for each degree Celsius increase in temperature (1.1 ft/s for every increase",
      "End correction An acoustic pipe, such as an organ pipe, marimba, or flute resonates at a specific pitch or frequency. Longer pipes resonate at lower frequencies, producing lower-pitched sounds. The details of acoustic resonance are taught in many elementary physics classes. In an ideal tube, the wavelength of the sound produced is directly proportional to the length of the tube. A tube which is open at one end and closed at the other produces sound with a wavelength equal to four times the length of the tube. A tube which is open at both ends produces sound whose wavelength is"
    ]
  ],
  [
    "The quantum efficiency of a photon detector is the fraction of photon flux that contributes to the photocurrent, and it affects the detection of individual photons by determining the number of signal electrons created per incident photon and ultimately influences the overall performance and image quality of the detector.",
    [
      "Photon counting Photon counting is a technique in which individual photons are counted using some single-photon detector (SPD). The counting efficiency is determined by the quantum efficiency and any electronic losses that are present in the system. Many photodetectors can be configured to detect individual photons, each with relative advantages and disadvantages, including a photomultiplier, geiger counter, single-photon avalanche diode, superconducting nanowire single-photon detector, transition edge sensor, or scintillation counter. Charge-coupled devices can also sometimes be used. Single-photon detection is useful in many fields including fiber-optic communication, quantum information science, quantum encryption, medical imaging, light detection and ranging, DNA sequencing,",
      "(QE) is the fraction of photon flux that contributes to the photocurrent in a photodetector or a pixel. Quantum efficiency is one of the most important parameters used to evaluate the quality of a detector and is often called the spectral response to reflect its wavelength dependence. It is defined as the number of signal electrons created per incident photon. In some cases it can exceed 100% (i.e. when more than one electron is created per incident photon). EQE mapping : Conventional measurement of the EQE will give the efficiency of the overall device. However it is often useful to",
      "classifying the signal and noise performance of various optical detectors such as television cameras and photoconductive devices. It was shown, for example, that image quality is limited by the number of quanta used to produce an image. The quantum efficiency of a detector is a primary metric of performance because it describes the fraction of incident quanta that interact and therefore image quality. However, other physical processes may also degrade image quality, and in 1946, Albert Rose proposed the concept of a \"useful quantum efficiency\" or \"equivalent quantum efficiency\" to describe the performance of those systems, which we now call"
    ]
  ],
  [
    "The average total energy of a three-dimensional harmonic oscillator in thermal equilibrium with a temperature reservoir at temperature T is 3kT.",
    [
      "a one-dimensional system has mass \"m\", then its kinetic energy \"H\" is where \"v\" and \"p\" = \"mv\" denote the velocity and momentum of the oscillator. Combining these terms yields the total energy Equipartition therefore implies that in thermal equilibrium, the oscillator has average energy where the angular brackets formula_8 denote the average of the enclosed quantity, This result is valid for any type of harmonic oscillator, such as a pendulum, a vibrating molecule or a passive electronic oscillator. Systems of such oscillators arise in many situations; by equipartition, each such oscillator receives an average total energy \"k\"\"T\" and hence",
      "assuming that they are occupied with a Boltzmann weight: \"kT\" is Boltzmann constant times the absolute temperature, which is the temperature as measured in more natural units of energy. The quantity formula_10 is more fundamental in thermodynamics than the temperature, because it is the thermodynamic potential associated to the energy. From this expression, it is easy to see that for large values of formula_10, for very low temperatures, the average energy U in the Harmonic oscillator approaches zero very quickly, exponentially fast. The reason is that kT is the typical energy of random motion at temperature T, and when this",
      "that where formula_8 is the average energy averaged over an oscillation. Since the frequency is changing with time due to external action, conservation of energy no longer holds and the energy over a single oscillation is not constant. During an oscillation, the frequency changes (however slowly), so does its energy. Therefore, to describe the system, one defines the average energy per unit mass for a given potential formula_9 as follows where the closed integral denotes that it is taken over a complete oscillation. Defined this way, it can be seen that the averaging is done, weighting each element of the"
    ]
  ],
  [
    "The student should count for a sufficient duration to establish the rate to an uncertainty of 1 percent.",
    [
      "is arrived at by taking the number of new cases in the population at risk and dividing by the number of persons at risk in the population. formula_1 Rates are determined from the beginning of the outbreak to its end. The term should probably not be described as a rate because its time dimension is uncertain. While the duration of an epidemic can be predicted given other variables such as early intervention, it cannot be known in absolute terms. In epidemiology, a rate requires a defined unit change (in this instance, time) over which the rate applies. For this reason,",
      "a census is conducted on the population of study. However, there are statistical methods that can be used to get a reasonable estimation for a parameter. These methods include confidence intervals and hypothesis testing. Estimating the value of a population proportion can be of great implication in the areas of agriculture, business, economics, education, engineering, environmental studies, medicine, law, political science, psychology, and sociology. A population proportion can be estimated through the usage of a confidence interval known as a one-sample proportion in the Z-interval whose formula is given below: formula_13 where formula_3 is the sample proportion, formula_15 is the",
      "is defined as: Here N is also the number of samples, M is the number of classes, formula_8 is the indicator function which equals 1 when formula_6 observation is in class j, equals 0 when in other classes. No probability is considered here. There is another method which is similar to error rate to measure accuracy: Here N is the number of samples, M is the number of classes, formula_8 is the indicator function which equals 1 when formula_6 observation is in class j, equals 0 when in other classes. formula_13 is the predicted probability of formula_6 observation in class"
    ]
  ],
  [
    "The equation for calculating the current \"I\" in amperes for a steady flow of charge through a surface is I = Q/t, where \"Q\" is the electric charge transferred through the surface over a time \"t\" measured in coulombs and seconds respectively.",
    [
      "an electric current.\" When a metal wire is connected across the two terminals of a DC voltage source such as a battery, the source places an electric field across the conductor. The moment contact is made, the free electrons of the conductor are forced to drift toward the positive terminal under the influence of this field. The free electrons are therefore the charge carrier in a typical solid conductor. For a steady flow of charge through a surface, the current \"I\" (in amperes) can be calculated with the following equation: where \"Q\" is the electric charge transferred through the surface",
      "over a time \"t\". If \"Q\" and \"t\" are measured in coulombs and seconds respectively, \"I\" is in amperes. More generally, electric current can be represented as the rate at which charge flows through a given surface as: Electric currents in electrolytes are flows of electrically charged particles (ions). For example, if an electric field is placed across a solution of Na and Cl (and conditions are right) the sodium ions move towards the negative electrode (cathode), while the chloride ions move towards the positive electrode (anode). Reactions take place at both electrode surfaces, neutralizing each ion. Water-ice and certain",
      "only the charge contained in the volume formed by \"dA\" and will flow through \"dA\". This charge is equal to , where \"\u03c1\" is the charge density at \"M\", and the electric current at \"M\" is . It follows that the current density vector can be expressed as: The surface integral of J over a surface \"S\", followed by an integral over the time duration \"t\" to \"t\", gives the total amount of charge flowing through the surface in that time (): More concisely, this is the integral of the flux of J across \"S\" between \"t\" and \"t\". The"
    ]
  ],
  [
    "The small difference between the muon \"g\"-factor and the electron \"g\"-factor in quantum electrodynamics theory is mainly due to the lack of heavy-particle diagrams contributing to the probability for emission of a photon representing the magnetic dipole field, which are present for muons but not electrons, as well as potential effects of physics beyond the Standard Model.",
    [
      "moment resulting from the muon\u2019s spin, S is the spin angular momentum, and \"m\" is the muon mass. That the muon \"g\"-factor is not quite the same as the electron \"g\"-factor is mostly explained by quantum electrodynamics and its calculation of the anomalous magnetic dipole moment. Almost all of the small difference between the two values (99.96% of it) is due to a well-understood lack of a heavy-particle diagrams contributing to the probability for emission of a photon representing the magnetic dipole field, which are present for muons, but not electrons, in QED theory. These are entirely a result of",
      "the mass difference between the particles. However, not all of the difference between the \"g\"-factors for electrons and muons is exactly explained by the Standard Model. The muon \"g\"-factor can, in theory, be affected by physics beyond the Standard Model, so it has been measured very precisely, in particular at the Brookhaven National Laboratory. In the E821 collaboration final report in November 2006, the experimental measured value is , compared to the theoretical prediction of . This is a difference of 3.4 standard deviations, suggesting that beyond-the-Standard-Model physics may be having an effect. The Brookhaven muon storage ring has been",
      "of a particle stems from the small contributions of quantum mechanical fluctuations to the magnetic moment of that particle. The g-factor for a \"Dirac\" magnetic moment is predicted to be for a negatively charged, spin 1/2 particle. For particles such as the electron, this \"classical\" result differs from the observed value by a small fraction of a percent; the difference compared to the classical value is the anomalous magnetic moment. The actual g-factor for the electron is measured to be . QED results from the mediation of the electromagnetic force by photons. The physical picture is that the \"effective\" magnetic"
    ]
  ],
  [
    "The speed of light inside of a nonmagnetic dielectric material with a dielectric constant of 4.0 is approximately 150,000 km/s.",
    [
      "is the speed of light in vacuum and \"\u03ba\" = \"\u00b5\"\"c\"/2\u03c0 = 59.95849 \u03a9 \u2248 60.0 \u03a9 is a newly introduced constant (units ohms, or reciprocal siemens, such that \"\u03c3\u03bb\u03ba\" = \"\u03b5\" remains unitless). Permittivity is typically associated with dielectric materials, however metals are described as having an effective permittivity, with real relative permittivity equal to one. In the low-frequency region, which extends from radio frequencies to the far infrared and terahertz region, the plasma frequency of the electron gas is much greater than the electromagnetic propagation frequency, so the refraction index \"n\" of a metal is very nearly a",
      "physical constant is commonly referred to as the speed of light. The postulate of the constancy of the speed of light in all inertial reference frames lies at the heart of special relativity and has given rise to a popular notion that the \"speed of light is always the same\". However, in many situations light is more than a disturbance in the electromagnetic field. Light traveling within a medium is no longer a disturbance solely of the electromagnetic field, but rather a disturbance of the field and the positions and velocities of the charged particles (electrons) within the material. The",
      "at which light travels in a material is called the refractive index of the material (). For example, for visible light the refractive index of glass is typically around 1.5, meaning that light in glass travels at ; the refractive index of air for visible light is about 1.0003, so the speed of light in air is about , which is about slower than . For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. In communicating with distant space probes, it"
    ]
  ],
  [
    "The increase in Kelvin temperature of a cavity will increase the mass of ice that can be melted in a fixed amount of time due to the higher heat energy available for melting the ice.",
    [
      "more heat must be removed before the water changes to solid state (ice), so water can be liquid or solid at . With the above definition there are simple ways in which the effect might be observed: For example, if the hotter temperature melts the frost on a cooling surface and thus increases the thermal conductivity between the cooling surface and the water container. On the other hand, there may be many circumstances in which the effect is not observed. Various effects of heat on the freezing of water were described by ancient scientists such as Aristotle: \"The fact that",
      "applicable to newly formed ice: any warming will tend to generate air pockets as the brine volume will increase more slowly than the ice volume decreases, again due to the density difference. Cox and Weeks provide the following formula determining the ratio of total ice salinity between temperatures, \"T\" and \"T\" where \"T\" < \"T\": where \"c\"=0.8 kg m is a constant. As the ice goes through constant warming and cooling cycles, it becomes progressively more porous through ejection of the brine and drainage through the resulting channels. The figure above shows a scatter plot of salinity versus ice thickness",
      "super cooling and only eventual non-equilibrium freezing \u2013 to obtain a measurement of the equilibrium freezing event, it is necessary to first cool enough to freeze a sample with excess liquid outside the pores, then warm the sample until the liquid in the pores is all melted, but the bulk material is still frozen. Then on re-cooling the equilibrium freezing event can be measured, as the external ice will then grow into the pores. This is in effect an \"ice intrusion\" measurement (c.f. Mercury Intrusion Porosimetry), and as such in part may provide information on pore throat properties. The melting"
    ]
  ],
  [
    "If the voltage across a resistor is doubled, the new rate of energy dissipation will be four times higher.",
    [
      "any instant, the power \"P\" (watts) consumed by a resistor of resistance \"R\" (ohms) is calculated as: formula_4 where \"V\" (volts) is the voltage across the resistor and \"I\" (amps) is the current flowing through it. Using Ohm's law, the two other forms can be derived. This power is converted into heat which must be dissipated by the resistor's package before its temperature rises excessively. Resistors are rated according to their maximum power dissipation. Discrete resistors in solid-state electronic systems are typically rated as 1/10, 1/8, or 1/4 watt. They usually absorb much less than a watt of electrical power",
      "to the first resistor; as this occurs, some of the supplied energy is \"lost\" (unavailable to the load), due to the resistance of the conductor. Voltage drop exists in both the supply and return wires of a circuit. If the voltage across each resistor is measured, the measurement will be a significant number. That represents the energy used by the resistor. The larger the resistor, the more energy used by that resistor, and the bigger the voltage drop across that resistor. Ohm's Law can be used to verify voltage drop. In a DC circuit, voltage equals current multiplied by resistance.",
      "current is halved (i.e. the voltage is doubled), the power loss due to the wire's resistance will reduced to one quarter. The power transmitted is equal to the product of the current and the voltage (assuming no phase difference); that is, Consequently, power transmitted at a higher voltage requires less loss-producing current than for the same power at a lower voltage. Power is often transmitted at hundreds of kilovolts, and transformed to 100 V \u2013 240 V for domestic use. High voltages have disadvantages, such as the increased insulation required, and generally increased difficulty in their safe handling. In a"
    ]
  ],
  [
    "The refractive index of the original gas in the gas cell placed in the Michelson interferometer is not specified in the provided text.",
    [
      "quoted instead of the refractive index in tables. Because of dispersion, it is usually important to specify the vacuum wavelength of light for which a refractive index is measured. Typically, measurements are done at various well-defined spectral emission lines; for example, \"n\" usually denotes the refractive index at the Fraunhofer \"D\" line, the centre of the yellow sodium double emission at 589.29 nm wavelength. When light passes through a medium, some part of it will always be attenuated. This can be conveniently taken into account by defining a complex refractive index, Here, the real part \"n\" is the refractive index",
      "unique chatoyance each individual stone displays. A refractometer is the instrument used to measure refractive index. For a solution of sugar, the refractive index can be used to determine the sugar content (see Brix). Refractive index In optics, the refractive index or index of refraction of a material is a dimensionless number that describes how fast light propagates through the material. It is defined as where \"c\" is the speed of light in vacuum and \"v\" is the phase velocity of light in the medium. For example, the refractive index of water is 1.333, meaning that light travels 1.333 times",
      "the chemical composition does not change. This means that it is also proportional to the pressure and inversely proportional to the temperature for ideal gases. Sometimes, a \"group velocity refractive index\", usually called the \"group index\" is defined: where \"v\" is the group velocity. This value should not be confused with \"n\", which is always defined with respect to the phase velocity. When the dispersion is small, the group velocity can be linked to the phase velocity by the relation where \"\u03bb\" is the wavelength in the medium. In this case the group index can thus be written in terms"
    ]
  ],
  [
    "The electron density within a crystal is related to the diffraction patterns obtained from a beam of electrons through X-ray crystallography, where the angles and intensities of the diffracted beams indicate a three-dimensional density of electrons within the crystal.",
    [
      "transmit through it. The diffracting planes in the crystal are determined by knowing that the normal to the diffracting plane bisects the angle between the incident beam and the diffracted beam. A Greninger chart can be used to interpret the back reflection Laue photograph. Other particles, such as electrons and neutrons, may be used to produce a diffraction pattern. Although electron, neutron, and X-ray scattering are based on different physical processes, the resulting diffraction patterns are analyzed using the same coherent diffraction imaging techniques. As derived below, the electron density within the crystal and the diffraction patterns are related by",
      "the scattering be recorded at least three (and usually four, for redundancy) wavelengths of the incoming X-ray radiation. A single crystal may degrade too much during the collection of one data set, owing to radiation damage; in such cases, data sets on multiple crystals must be taken. The recorded series of two-dimensional diffraction patterns, each corresponding to a different crystal orientation, is converted into a three-dimensional model of the electron density; the conversion uses the mathematical technique of Fourier transforms, which is explained below. Each spot corresponds to a different type of variation in the electron density; the crystallographer must",
      "the Fabry-P\u00e9rot effect. These oscillations can be used to infer layer thicknesses and other properties. In X-ray diffraction a beam strikes a crystal and diffracts into many specific directions. The angles and intensities of the diffracted beams indicate a three-dimensional density of electrons within the crystal. X-rays produce a diffraction pattern because their wavelength is typically the same order of magnitude (0.1-10.0 nm) as the spacing between the atomic planes in the crystal. Each atom re-radiates a small portion of an incoming beam's intensity as a spherical wave. If the atoms are arranged symmetrically (as is found in a crystal)"
    ]
  ],
  [
    "The rotational kinetic energy of a rigid body is a component of its total kinetic energy, which includes both translational and rotational kinetic energy.",
    [
      "can also be applied to rigid bodies, stating that the kinetic energy T of a rigid body, as viewed by an observer fixed in some inertial reference frame N, can be written as: formula_1 where formula_2 is the mass of the rigid body; formula_3 is the velocity of the center of mass of the rigid body, as viewed by an observer fixed in an inertial frame N; formula_4 is the angular momentum of the rigid body about the center of mass, also taken in the inertial frame N; and formula_5 is the angular velocity of the rigid body R relative",
      "work done by the force to bring the object from rest to velocity \"v\" is equal to the work necessary to do the reverse: This equation states that the kinetic energy (\"E\") is equal to the integral of the dot product of the velocity (v) of a body and the infinitesimal change of the body's momentum (p). It is assumed that the body starts with no kinetic energy when it is at rest (motionless). If a rigid body Q is rotating about any line through the center of mass then it has \"rotational kinetic energy\" (formula_18) which is simply the",
      "the article on invariant mass). The relativistic energy of a single massive particle contains a term related to its rest mass in addition to its kinetic energy of motion. In the limit of zero kinetic energy (or equivalently in the rest frame) of a massive particle, or else in the center of momentum frame for objects or systems which retain kinetic energy, the total energy of particle or object (including internal kinetic energy in systems) is related to its rest mass or its invariant mass via the famous equation formula_26. Thus, the rule of \"conservation of energy\" over time in"
    ]
  ],
  [
    "The minimum thickness of the oil film on a glass slide for blue light of wavelength 480 nanometers to be most strongly reflected is 240 nanometers, as calculated using the formula for thin-film interference.",
    [
      "formula_18 is the film thickness, formula_19 is the refractive index of the film, formula_20 is the angle of incidence of the wave on the lower boundary, formula_21 is an integer, and formula_10 is the wavelength of light. In the case of a thin oil film, a layer of oil sits on top of a layer of water. The oil may have an index of refraction near 1.5 and the water has an index of 1.33. As in the case of the soap bubble, the materials on either side of the oil film (air and water) both have refractive indices that",
      "Thin-film optics Thin-film optics is the branch of optics that deals with very thin structured layers of different materials. In order to exhibit thin-film optics, the thickness of the layers of material must be on the order of the wavelengths of visible light (about 500 nm). Layers at this scale can have remarkable reflective properties due to light wave interference and the difference in refractive index between the layers, the air, and the substrate. These effects alter the way the optic reflects and transmits light. This effect, known as thin-film interference, is observable in soap bubbles and oil slicks. More",
      "in the second figure. The type of interference that occurs when light is reflected from a thin film is dependent upon the wavelength and angle of the incident light, the thickness of the film, the refractive indices of the material on either side of the film, and the index of the film medium. Various possible film configurations and the related equations are explained in more detail in the examples below. In the case of a soap bubble, light travels through air and strikes a soap film. The air has a refractive index of 1 (formula_12) and the film has an"
    ]
  ],
  [
    "The smallest amount of work that must be supplied to an air source heat pump designed for very cold climates in order to deliver 15,000 J of heat indoors from an outdoor temperature of -30\u00b0C is 6,000 J.",
    [
      "easy and inexpensive to install and have therefore historically been the most widely used heat pump type. However, they suffer limitations due to their use of the outside air as a heat source. The higher temperature differential during periods of extreme cold leads to declining efficiency. In mild weather, COP may be around 4.0, while at temperatures below around 0 \u00b0C (32 \u00b0F) an air-source heat pump may still achieve a COP of 2.5. The average COP over seasonal variation is typically 2.5-2.8, with exceptional models able to exceed this in mild climates. The heating output of low temperature optimized",
      "\u00b0C, the COP for many machines is fairly stable at 3\u20133.5. In very mild weather, the COP of an air source heat pump can be up to 4. However, on a cold winter day, it takes more work to move the same amount of heat indoors than on a mild day. The heat pump's performance is limited by the Carnot cycle and will approach 1.0 as the outdoor-to-indoor temperature difference increases, which for most air source heat pumps happens as outdoor temperatures approach \u221218 \u00b0C / 0 \u00b0F. Heat pump construction that enables carbon dioxide as a refrigerant may have",
      "an outdoor temperature around \u221218 \u00b0C (0 \u00b0F) for air source heat pumps. Also, as the heat pump takes heat out of the air, some moisture in the outdoor air may condense and possibly freeze on the outdoor heat exchanger. The system must periodically melt this ice; this defrosting translates into an additional energy (electricity) expenditure. When it is extremely cold outside, it is simpler to heat using an alternative heat source (such as an electric resistance heater, oil furnace, or gas furnace) rather than to run an air-source heat pump. Also, avoiding the use of the heat pump during"
    ]
  ],
  [
    "The driver of a police car will hear the echo of the car's siren at a frequency higher than 600 Hz when moving towards a wall at 3.5 m/s with a speed of sound of 350 m/s.",
    [
      "differences also account for increases, based upon the frequency and the speaker spacing. However, sirens are designed to sweep the frequency of their sound output, typically, no less than one octave. This sweeping minimizes the effects of phase cancellation. The end result is that the average sound output from a dual speaker siren system is 3 dB greater than a single speaker system. Siren (alarm) A siren is a loud noise-making device. Civil defense sirens are mounted in fixed locations and used to warn of natural disasters or attacks. Sirens are used on emergency service vehicles such as ambulances, police",
      "being replaced with more specialized warnings, such as the Emergency Alert System. A mechanical siren generates sound by spinning a slotted chopper wheel to interrupt a stream of air at a regular rate. Modern sirens can develop a sound level of up to 135 decibels at . The Chrysler air raid siren, driven by a 331-cubic-inch Chrysler Hemi gasoline engine, generates 138 dB at 100 feet away. By use of varying tones or on/off patterns of sound, different alert conditions can be signaled. Electronic sirens can transmit voice announcements in addition to alert tone signals. Siren systems may be electronically",
      "In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second. The siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus: In other words, if the siren approached the observer directly, the pitch would remain constant, at a higher than stationary"
    ]
  ],
  [
    "The mass of an unstable particle affects its decay rate and travel distance differently in different frames of reference, with the particle usually decaying before completing its travel if the decay rate is large compared to the mass (Particle decay).",
    [
      "in the rest frame of the parent particle is The angle of an emitted particle in the lab frame is related to the angle it has emitted in the center of momentum frame by the equation Note that this section uses natural units, where formula_4 The mass of an unstable particle is formally a complex number, with the real part being its mass in the usual sense, and the imaginary part being its decay rate in natural units. When the imaginary part is large compared to the real part, the particle is usually thought of as a resonance more than",
      "a particle. This is because in quantum field theory a particle of mass M (a real number) is often exchanged between two other particles when there is not enough energy to create it, if the time to travel between these other particles is short enough, of order 1/M, according to the uncertainty principle. For a particle of mass formula_27, the particle can travel for time 1/M, but decays after time of order of formula_28. If formula_29 then the particle usually decays before it completes its travel. Particle decay Particle decay is the spontaneous process of one unstable subatomic particle transforming",
      "unstable and the field will roll down the potential. Because a tachyon's squared mass is negative, it formally has an imaginary mass. This is a special case of the general rule, where unstable massive particles are formally described as having a complex mass, with the real part being their mass in usual sense, and the imaginary part being the decay rate in natural units. However, in quantum field theory, a particle (a \"one-particle state\") is roughly defined as a state which is constant over time; i.e., an eigenvalue of the Hamiltonian. An unstable particle is a state which is only"
    ]
  ],
  [
    "The formula to calculate the speed of light in water relative to the lab frame is \\( w = \\frac{c}{n} \\) where \\( n \\) is the index of refraction of water.",
    [
      "to determine the speed of light traveling along each leg of the tube. Assume that water flows in the pipes with speed \"v\". According to the non-relativistic theory of the luminiferous aether, the speed of light should be increased when \"dragged\" along by the water, and decreased when \"overcoming\" the resistance of the water. The overall speed of a beam of light should be a simple additive sum of its speed \"through\" the water plus the speed \"of\" the water. That is, if \"n\" is the index of refraction of water, so that \"c/n\" is the velocity of light in",
      "stationary water, then the predicted speed of light \"w\" in one arm would be and the predicted speed in the other arm would be Hence light traveling against the flow of water should be slower than light traveling with the flow of water. The interference pattern between the two beams when the light is recombined at the observer depends upon the transit times over the two paths, and can be used to calculate the speed of light as a function of the speed of the water. Fizeau found that In other words, light appeared to be dragged by the water,",
      "the theory of light as an electromagnetic wave (Iksander 1992), light takes a straight-line path through water at reduced speed (v) and wavelength (\u03bb). The ratio v/\u03bb is a constant equal to the frequency (\u03bd) of the light, as is the quantized (photon) energy using Planck's constant and E = h\u03bd. Compared to the constant speed of light in a vacuum (c), the index of refraction of water is n = c/v. The Gladstone\u2013Dale term (n\u22121) is the non-linear optical path length or time delay. Using Isaac Newton's theory of light as a stream of particles refracted locally by (electric)"
    ]
  ],
  [
    "The rest mass of a particle with total energy 5.0 GeV and momentum 4.9 GeV/c is 0.6 GeV/c\u00b2.",
    [
      "does momentum. However, for single particles the rest mass remains constant, and for systems of particles the invariant mass remain constant, because in both cases, the energy and momentum increases subtract from each other, and cancel. Thus, the invariant mass of systems of particles is a calculated constant for all observers, as is the rest mass of single particles. For systems of particles, the energy\u2013momentum equation requires summing the momentum vectors of the particles: The inertial frame in which the momenta of all particles sums to zero is called the center of momentum frame. In this special frame, the relativistic",
      "is equal to the total system energy (in the zero-momentum frame) divided by . In particle physics, the invariant mass is equal to the mass in the rest frame of the particle, and can be calculated by the particle's energy and its momentum as measured in \"any\" frame, by the energy\u2013momentum relation: or in natural units where , This invariant mass is the same in all frames of reference (see also special relativity). This equation says that the invariant mass is the pseudo-Euclidean length of the four-vector , calculated using the relativistic version of the Pythagorean theorem which has a",
      "momentum, formula_11, is usually measured. In this case if the particles are massless, or highly relativistic ( formula_12,) then the invariant mass becomes: The rest energy formula_13 of a particle is defined as: where formula_15 is the speed of light in vacuum. In general, only differences in energy have physical significance. The concept of rest energy follows from the special theory of relativity that leads to Einstein's famous conclusion about equivalence of energy and mass. See background for mass\u2013energy equivalence. On the other hand, the concept of the equivalent Dirac invariant rest mass may be defined in terms of the"
    ]
  ],
  [
    "The resolving power of a grating spectrometer is calculated using the formula R = \u03bb/\u0394\u03bb, where R is the resolving power, \u03bb is the wavelength, and \u0394\u03bb is the smallest difference in wavelengths that can be distinguished at a specific wavelength.",
    [
      "Spectral resolution The spectral resolution of a spectrograph, or, more generally, of a frequency spectrum, is a measure of its ability to resolve features in the electromagnetic spectrum. It is usually denoted by formula_1, and is closely related to the resolving power of the spectrograph, defined as formula_2, where formula_1 is the smallest difference in wavelengths that can be distinguished at a wavelength of formula_4. For example, the Space Telescope Imaging Spectrograph (STIS) can distinguish features 0.17 nm apart at a wavelength of 1000 nm, giving it a resolution of 0.17 nm and a resolving power of about 5,900. An",
      "wavenumber, wavelength or frequency difference between two lines in a spectrum that can be distinguished. Resolving power, \"R\", is given by the transition wavenumber, wavelength or frequency, divided by the resolution. Spectral resolution The spectral resolution of a spectrograph, or, more generally, of a frequency spectrum, is a measure of its ability to resolve features in the electromagnetic spectrum. It is usually denoted by formula_1, and is closely related to the resolving power of the spectrograph, defined as formula_2, where formula_1 is the smallest difference in wavelengths that can be distinguished at a wavelength of formula_4. For example, the Space",
      "spacing of two peaks of equal intensity with the valley (lowest value of signal) between them less than a specified fraction of the peak height. Typical values are 10% or 50%. The value obtained from a 5% peak width is roughly equivalent to a 10% valley. Resolution (mass spectrometry) In mass spectrometry, resolution is a measure of the ability to distinguish two peaks of slightly different mass-to-charge ratios \"\u0394M\", in a mass spectrum. There are two different definitions of resolution and resolving power in mass spectrometry. The IUPAC definition for resolution in mass spectrometry is Where a larger resolution indicates"
    ]
  ],
  [
    "Dye lasers are best suited for spectroscopy over a range of visible wavelengths due to their tunability from the near-infrared to the near-ultraviolet, narrow bandwidth, and high intensity (Dye laser).",
    [
      "other applications. In spectroscopy, dye lasers can be used to study the absorption and emission spectra of various materials. Their tunability, (from the near-infrared to the near-ultraviolet), narrow bandwidth, and high intensity allows a much greater diversity than other light sources. The variety of pulse widths, from ultra-short, femtosecond pulses to continuous-wave operation, makes them suitable for a wide range of applications, from the study of fluorescent lifetimes and semiconductor properties to lunar laser ranging experiments. Tunable lasers are used in swept-frequency metrology to enable measurement of absolute distances with very high accuracy. A two axis interferometer is set up",
      "find applications in spectroscopy, photochemistry, atomic vapor laser isotope separation, and optical communications. Since no real laser is truly monochromatic, all lasers can emit light over some range of frequencies, known as the linewidth of the laser transition. In most lasers, this linewidth is quite narrow (for example, the nm wavelength transition of a has a linewidth of approximately 120 GHz, or 0.45 nm). Tuning of the laser output across this range can be achieved by placing wavelength-selective optical elements (such as an etalon) into the laser's optical cavity, to provide selection of a particular longitudinal mode of the cavity.",
      "Tunable laser A tunable laser is a laser whose wavelength of operation can be altered in a controlled manner. While all laser gain media allow small shifts in output wavelength, only a few types of lasers allow continuous tuning over a significant wavelength range. There are many types and categories of tunable lasers. They exist in the gas, liquid, and solid state. Among the types of tunable lasers are excimer lasers, gas lasers (such as CO and He-Ne lasers), dye lasers (liquid and solid state), transition metal solid-state lasers, semiconductor crystal and diode lasers, and free electron lasers. Tunable lasers"
    ]
  ],
  [
    "Yes, the eigenvalues of a Hermitian operator are always real numbers.",
    [
      "Hilbert space. The spectrum of allowed energy levels of the system is given by the set of eigenvalues, denoted {\"E\"}, solving the equation: Since \"H\" is a Hermitian operator, the energy is always a real number. From a mathematically rigorous point of view, care must be taken with the above assumptions. Operators on infinite-dimensional Hilbert spaces need not have eigenvalues (the set of eigenvalues does not necessarily coincide with the spectrum of an operator). However, all routine quantum mechanical calculations can be done using the physical formulation. Following are expressions for the Hamiltonian in a number of situations. Typical ways",
      "values which may come up as the result of the experiment. Although traditionally physicists associated real eigenvalues with Hermiticity, in 1998 physicists realized that there also exist non-Hermitian operators with all-real spectra; namely, parity-time (PT) symmetric operators. For Hermitian operators, the probability of each eigenvalue is related to the projection of the physical state on the subspace related to that eigenvalue. See below for mathematical details about Hermitian operators. In the wave mechanics formulation of QM, the wavefunction varies with space and time, or equivalently momentum and time (see position and momentum space for details), so observables are differential operators.",
      "formula_7 invariant under formula_2 satisfies: and thus: for any state \"\u03c8\". Quantum operators representing observables are also required to be Hermitian so that their eigenvalues are real numbers, i.e. the operator equals its Hermitian conjugate, formula_11. Following are the key points of group theory relevant to quantum theory, examples are given throughout the article. For an alternative approach using matrix groups, see the books of Hall Let \"G\" be a \"Lie group\", which is a group that locally is parameterized by a finite number \"N\" of real continuously varying parameters \"\u03be\", \"\u03be\", ... \"\u03be\". In more mathematical language, this means"
    ]
  ],
  [
    "Forces F_A and F_B are not necessarily equal when the respective masses of object A and object B are different, as the forces will depend on the specific circumstances and interactions involved.",
    [
      "for forces F and F. Forces F and F are equal if and only if the object is in equilibrium, and no other forces are applied. (This has nothing to do with Newton's Third Law.) If a mass is hanging from a spring, the same considerations apply as before. However, if this system is then perturbed (e.g., the mass is given a slight kick upwards or downwards, say), the mass starts to oscillate up and down. Because of these accelerations (and subsequent decelerations), we conclude from Newton's second law that a net force is responsible for the observed change in",
      "changing mass, such as rocket exhaust) and is included in the quantity F. Then, by substituting the definition of acceleration, the equation becomes F = \"m\"a. The third law states that all forces between two objects exist in equal magnitude and opposite direction: if one object \"A\" exerts a force F on a second object \"B\", then \"B\" simultaneously exerts a force F on \"A\", and the two forces are equal in magnitude and opposite in direction: F = \u2212F. The third law means that all forces are \"interactions\" between different bodies, or different regions within one body, and thus",
      "force. However, physical forces are generally between two bodies; and by Newton's third law, if the first body applies a force on the second, the second body applies an equal and opposite force on the first. Therefore, both bodies are accelerated if a force is present between them; there is no perfectly immovable center of force. However, if one body is overwhelmingly more massive than the other, its acceleration relative to the other may be neglected; the center of the more massive body may be treated as approximately fixed. For example, the Sun is overwhelmingly more massive than the planet"
    ]
  ],
  [
    "Just outside the surface of a superconductor, the magnetic field decays exponentially from the value it possesses at the surface according to the London equation.",
    [
      "language, when a charged field has a nonzero vacuum expectation value. Interaction with the quantum fluid filling the space prevents certain forces from propagating over long distances (as it does in a superconducting medium; e.g., in the Ginzburg\u2013Landau theory). A superconductor expels all magnetic fields from its interior, a phenomenon known as the Meissner effect. This was mysterious for a long time, because it implies that electromagnetic forces somehow become short-range inside the superconductor. Contrast this with the behavior of an ordinary metal. In a metal, the conductivity shields electric fields by rearranging charges on the surface until the total",
      "is known as the London equation, predicts that the magnetic field in a superconductor decays exponentially from whatever value it possesses at the surface. In a weak applied field, a superconductor \"expels\" nearly all magnetic flux. It does this by setting up electric currents near its surface. The magnetic field of these surface currents cancels the applied magnetic field within the bulk of the superconductor. As the field expulsion, or cancellation, does not change with time, the currents producing this effect (called persistent currents) do not decay with time. Near the surface, within the London penetration depth, the magnetic field",
      "n\u03a9, giving \"Q\"=2.7\u00d710. The critical parameter for SRF cavities in the above equations is the surface resistance \"R\", and is where the complex physics comes into play. For normal-conducting copper cavities operating near room temperature, \"R\" is simply determined by the empirically measured bulk electrical conductivity \"\u03c3\" by For copper at 300 K, \"\u03c3\"=5.8\u00d710 (\u03a9\u00b7m) and at 1.3 GHz, \"R\"= 9.4 m\u03a9. For Type II superconductors in RF fields, \"R\" can be viewed as the sum of the superconducting BCS resistance and temperature-independent \"residual resistances\", The \"BCS resistance\" derives from BCS theory. One way to view the nature of the"
    ]
  ],
  [
    "The uncertainty in the kinetic energy of the ball dropped from a known height above the ground, given an estimated uncertainty of 10 percent in the measured speed of the ball just before it strikes the ground, is 10 percent.",
    [
      "in state 1, which is given by The particle, therefore, always has a positive energy. This contrasts with classical systems, where the particle can have zero energy by resting motionlessly. This can be explained in terms of the uncertainty principle, which states that the product of the uncertainties in the position and momentum of a particle is limited by It can be shown that the uncertainty in the position of the particle is proportional to the width of the box. Thus, the uncertainty in momentum is roughly inversely proportional to the width of the box. The kinetic energy of a",
      "ceases and particles come completely to rest. In fact, however, kinetic energy is retained by particles even at the lowest possible temperature. The random motion corresponding to this zero-point energy never vanishes as a consequence of the uncertainty principle of quantum mechanics. The uncertainty principle states that no object can ever have precise values of position and velocity simultaneously. The total energy of a quantum mechanical object (potential and kinetic) is described by its Hamiltonian which also describes the system as a harmonic oscillator, or wave function, that fluctuates between various energy states (see wave-particle duality). All quantum mechanical systems",
      "like a baseball, the uncertainty principle predicts that it cannot really have zero kinetic energy, but the uncertainty in kinetic energy is so small that the baseball can effectively appear to be at rest, and hence it appears to obey classical mechanics. In general, if large energies and large objects (relative to the size and energy levels of an electron) are considered in quantum mechanics, the result will appear to obey classical mechanics. The typical occupation numbers involved are huge: a macroscopic harmonic oscillator with = 2 Hz, = 10 g, and maximum amplitude = 10 cm, has = ,"
    ]
  ],
  [
    "The angle between the particle's velocity and acceleration vectors in circular motion is 90 degrees.",
    [
      "in a time \"t\" is: The angular acceleration, \"\u03b1\", of the particle is: In the case of uniform circular motion, \u03b1 will be zero. The acceleration due to change in the direction is: The centripetal and centrifugal force can also be found out using acceleration: The vector relationships are shown in Figure 1. The axis of rotation is shown as a vector \u03c9 perpendicular to the plane of the orbit and with a magnitude \u03c9 = \"d\"\u03b8 / \"dt\". The direction of \u03c9 is chosen using the right-hand rule. With this convention for depicting rotation, the velocity is given by",
      "V is given by: where is the angular velocity of the unit vector around the \"z\" axis of the cylinder. The acceleration A of the particle \"P\" is now given by: The components are called, respectively, the \"radial\" and \"tangential components\" of acceleration. The notation for angular velocity and angular acceleration is often defined as so the radial and tangential acceleration components for circular trajectories are also written as The movement of components of a mechanical system are analyzed by attaching a reference frame to each part and determining how the various reference frames move relative to each other. If",
      "is formula_5, and the linear velocity is formula_6, so that formula_7. In the general case of a particle moving in the plane, the angular velocity is measured relative to a chosen center point, called the origin. The diagram shows the radius vector r from the origin \"O\" to the particle \"P\", with its polar coordinates formula_8. (All variables are functions of time \"t\".) The particle has linear velocity splitting as formula_9, with the radial component formula_10 parallel to the radius, and the cross-radial (or tangential or circular) component formula_11 perpendicular to the radius. When there is no radial component, the"
    ]
  ],
  [
    "If the absolute temperature of a blackbody is increased by a factor of 3, the energy radiated per second per unit area will increase by a factor of 81.",
    [
      "a body that is at a uniform temperature. The shape of the spectrum and the total amount of energy emitted by the body is a function of the absolute temperature of that body. The radiation emitted covers the entire electromagnetic spectrum and the intensity of the radiation (power/unit-area) at a given frequency is described by Planck's law of radiation. For a given temperature of a black-body there is a particular frequency at which the radiation emitted is at its maximum intensity. That maximum radiation frequency moves toward higher frequencies as the temperature of the body increases. The frequency at which",
      "can also be derived from Planck's law, by integrating over all wavelengths at a given temperature, which will represent a small flat black body box. \"The amount of thermal radiation emitted increases rapidly and the principal frequency of the radiation becomes higher with increasing temperatures\". The Stefan\u2013Boltzmann constant can be used to measure the amount of heat that is emitted by a blackbody, which absorbs all of the radiant energy that hits it, and will emit all the radiant energy. Furthermore, the Stefan\u2013Boltzmann constant allows for temperature (K) to be converted to units for intensity (W m), which is power",
      "extreme cryogenic temperatures emit at long radio wavelengths whereas extremely hot temperatures produce short gamma rays (see \"Table of common temperatures\"). Black-body radiation diffuses thermal energy throughout a substance as the photons are absorbed by neighboring atoms, transferring momentum in the process. Black-body photons also easily escape from a substance and can be absorbed by the ambient environment; kinetic energy is lost in the process. As established by the Stefan\u2013Boltzmann law, the intensity of black-body radiation increases as the fourth power of absolute temperature. Thus, a black-body at 824 K (just short of glowing dull red) emits \"60 times\" the"
    ]
  ],
  [
    "The average induced EMF in the coil is 0.2 V.",
    [
      "words, the emf generated is proportional to the rate of change of the magnetic flux. More simply stated, an electric field is induced in any system in which a magnetic field is changing with time. The change could be changes in direction of force or strength. The effects described by this law are those by which generators, motors, alternators, and transformers function. There are two main concepts to be taken from Faraday's Law that apply to the design of inductive discharge ignitions. One is that moving a wire through a magnetic field will induce an electric voltage and current in",
      "variation of the Magnetic Flux formula_1 through a \"N\" turns circuit will induce a voltage \"formula_2\" which follows: which can be expressed in a simpler way: by assuming that the induced magnetic field B is homogeneous over a section \"S\" (the Magnetic flux will be expressed formula_5). The induced voltage (\"formula_2\") may be increased several ways: When a coil is wound around a ferromagnetic core, that increases the sensitivity of the sensor thanks to the apparent permeability of the ferromagnetic core. The magnetic amplification, known as apparent permeability \"formula_7\", is the result of the magnetization of the ferromagnetic core response",
      "flux linked with the coil changes. This induces a current in the coil which can be registered on the galvanometer. Since induced current is directly proportional to rate of change of flux linkage and assuming the coil is removed from the magnetic field very quickly, the maximum current measured by the ammeter is proportional to the magnetic field formula_12. The search coil can be calibrated by repeating this in a known magnetic field. A reference table shows open coil voltage versus magnetic field A/m (or teslas) or short current versus magnetic field. Spacecraft with on board search coil magnetometers have"
    ]
  ],
  [
    "The second law of thermodynamics can be used to determine whether a process is reversible or not by examining whether there is dissipation present in the system, as reversible processes have no dissipation.",
    [
      "function, the change in entropy of the system is the same, whether the process is reversible or irreversible. The second law of thermodynamics can be used to determine whether a process is reversible or not. Intuitively, a process is reversible if there is no dissipation. For example, Joule expansion is irreversible because initially the system is not uniform. Initially, there is part of the system with gas in it, and part of the system with no gas. For dissipation to occur, there needs to be such a non uniformity. This is just the same as if in a system on",
      "work done on the system. The second law of thermodynamics for a reversible process yields formula_6. In case of a reversible change, the work done can be expressed as formula_7 (ignoring electrical and other non-\"PV\" work): Applying the product rule for differentiation to d(\"TS\") = \"T\" d\"S\" + \"S\" d\"T\", it follows and The definition of \"A\" = \"U\" \u2212 \"TS\" enables to rewrite this as Because \"A\" is a thermodynamic function of state, this relation is also valid for a process (without electrical work or composition change) that is not reversible, as long as the system pressure and temperature",
      "\"reversible cycle\", a cyclical reversible process, the system and its surroundings will be returned to their original states if one half cycle is followed by the other half cycle. Thermodynamic processes can be carried out in one of two ways: reversibly or irreversibly. Reversibility means the reaction operates continuously at quasiequilibrium. In an ideal thermodynamically reversible process, the energy from work performed by or on the system would be maximized, and that from heat would be zero. However, heat cannot fully be converted to work and will always be lost to some degree (to the surroundings). (This is true only"
    ]
  ],
  [
    "The second harmonic of an organ pipe open at both ends will survive once the pipe is closed at one end.",
    [
      "is \"stopped\" (closed at one end and open at the other), or \"open\" (at both ends). For an open pipe, the wavelength produced by the first normal mode (the fundamental note) is approximately twice the length of the pipe. The wavelength produced by the second normal mode is half that, that is, the length of the pipe, so its pitch is an octave higher; thus an open cylindrical bore instrument overblows at the octave. This corresponds to the second harmonic, and generally the harmonic spectrum of an open cylindrical bore instrument is strong in both even and odd harmonics. For",
      "freely. In the first harmonic, the closed tube contains exactly half of a standing wave (node-antinode-node). In cylinders with both ends open, air molecules near the end move freely in and out of the tube. This movement produces displacement antinodes in the standing wave. Nodes tend to form inside the cylinder, away from the ends. In the first harmonic, the open tube contains exactly half of a standing wave (antinode-node-antinode). Thus the harmonics of the open cylinder are calculated in the same way as the harmonics of a closed/closed cylinder. The physics of a pipe open at both ends are",
      "open ends. Musically useful tube shapes are \"conical\" and \"cylindrical\" (see bore). A pipe that is closed at one end is said to be \"stopped\" while an \"open\" pipe is open at both ends. Modern orchestral flutes behave as open cylindrical pipes; clarinets and lip-reed instruments (brass instruments) behave as closed cylindrical pipes; and saxophones, oboes, and bassoons as closed conical pipes. Vibrating air columns also have resonances at harmonics, like strings. Any cylinder resonates at multiple frequencies, producing multiple musical pitches. The lowest frequency is called the fundamental frequency or the first harmonic. Cylinders used as musical instruments are"
    ]
  ],
  [
    "Gamma rays are the most penetrating and high-energy type of electromagnetic radiation emitted from a nucleus.",
    [
      "emission of electromagnetic energy (such as gamma rays) from the nucleus of an atom. This usually occurs during alpha or beta radioactive decay. These three types of radiation can be distinguished by their difference in penetrating power. Alpha can be stopped quite easily by a few centimetres in air or a piece of paper and is equivalent to a helium nucleus. Beta can be cut off by an aluminium sheet just a few millimetres thick and are electrons. Gamma is the most penetrating of the three and is a massless chargeless high energy photon. Gamma radiation requires an appreciable amount",
      "just as in optical spectroscopy, the optical spectrum is characteristic of the material contained in a sample. Gamma rays are the highest-energy form of electromagnetic radiation, being physically the same as all other forms (e.g., X rays, visible light, infrared, radio) but having (in general) higher photon energy due to their shorter wavelength. Because of this, the energy of gamma-ray photons can be resolved individually, and a gamma-ray spectrometer can measure and display the energies of the gamma-ray photons detected. Radioactive nuclei (radionuclides) commonly emit gamma rays in the energy range from a few keV to ~10 MeV, corresponding to",
      "are highly ionizing forms of radiation, which produce distinct biological damage compared to X-rays and gamma-rays. Microscopic energy deposition from highly ionizing particles consists of a core radiation track due to direct ionizations by the particle and low energy electrons produced in ionization, and a penumbra of higher energy electrons that may extend hundreds of microns from the particles path in tissue. The core track produces extremely large clusters of ionizations within a few nanometres, which is qualitatively distinct from energy deposition by X-rays and gamma rays; hence human epidemiology data which only exists for these latter forms of radiation"
    ]
  ],
  [
    "The frequency of the next higher harmonic for a closed pipe with a fundamental frequency of C (131 Hz) is G2 (196 Hz).",
    [
      "closed tube, a note can be obtained that is approximately a twelfth above the fundamental note of the tube. This is sometimes described as one-fifth above the octave of the fundamental note. For example, if the fundamental note of a closed pipe is C1, then overblowing the pipe gives G2, which is one-twelfth above C1. Alternatively we can say that G2 is one-fifth above C2 \u2014 the octave above C1. Adjusting the taper of this cylinder for a decreasing cone can tune the second harmonic or overblown note close to the octave position or 8th. Opening a small \"speaker hole\"",
      "is also periodic at that frequency. For example, if the fundamental frequency is 50 Hz, a common AC power supply frequency, the frequencies of the first three higher harmonics are 100 Hz (2nd harmonic), 150 Hz (3rd harmonic), 200 Hz (4th harmonic) and any addition of waves with these frequencies is periodic at 50 Hz. In music, harmonics are used on string instruments and wind instruments as a way of producing sound on the instrument, particularly to play higher notes and, with strings, obtain notes that have a unique sound quality or \"tone colour\". On strings, harmonics that are bowed",
      "is \"stopped\" (closed at one end and open at the other), or \"open\" (at both ends). For an open pipe, the wavelength produced by the first normal mode (the fundamental note) is approximately twice the length of the pipe. The wavelength produced by the second normal mode is half that, that is, the length of the pipe, so its pitch is an octave higher; thus an open cylindrical bore instrument overblows at the octave. This corresponds to the second harmonic, and generally the harmonic spectrum of an open cylindrical bore instrument is strong in both even and odd harmonics. For"
    ]
  ],
  [
    "There are 5 allowed values of the quantum number m_l for an electron in the l = 2 state of a single-electron atom.",
    [
      "energy increases so much that the electron can easily escape from the atom. In single electron atoms, all energy levels with the same principle quantum number are degenerate, and have the same energy. In atoms with more than one electron, the energy of an electron depends not only on the properties of the orbital it resides in, but also on its interactions with the other electrons in other orbitals. This requires consideration of the l quantum number. Higher values of l are associated with higher values of energy; for instance, the 2p state is higher than the 2s state. When",
      "shells and subshells despite the advances in understanding of the quantum-mechanical nature of electrons. An electron shell is the set of allowed states that share the same principal quantum number, \"n\" (the number before the letter in the orbital label), that electrons may occupy. An atom's \"n\"th electron shell can accommodate 2\"n\" electrons, \"e.g.\" the first shell can accommodate 2 electrons, the second shell 8 electrons, the third shell 18 electrons and so on. The factor of two arises because the allowed states are doubled due to electron spin\u2014each atomic orbital admits up to two otherwise identical electrons with opposite",
      "the electron configuration of the atoms and the periodic table. The stationary states (quantum states) of the hydrogen-like atoms are its atomic orbitals. However, in general, an electron's behavior is not fully described by a single orbital. Electron states are best represented by time-depending \"mixtures\" (linear combinations) of multiple orbitals. See Linear combination of atomic orbitals molecular orbital method. The quantum number first appeared in the Bohr model where it determines the radius of each circular electron orbit. In modern quantum mechanics however, determines the mean distance of the electron from the nucleus; all electrons with the same value of"
    ]
  ],
  [
    "The length of pendulum B is four times the length of pendulum A.",
    [
      "real period and an imaginary period. The real period is of course the time it takes the pendulum to go through one full cycle. Paul Appell pointed out a physical interpretation of the imaginary period: if is the maximum angle of one pendulum and is the maximum angle of another, then the real period of each is the magnitude of the imaginary period of the other. This interpretation, involving dual forces in opposite directions, might be further clarified and generalized to other classical problems in mechanics with dual solutions. Coupled pendulums can affect each other's motion, either through a direction",
      "from equation (1) that for a seconds pendulum, the length is simply proportional to \"g\": In Kater's time, the period \"T\" of pendulums could be measured very precisely by timing them with precision clocks set by the passage of stars overhead. Prior to Kater's discovery, the accuracy of \"g\" measurements was limited by the difficulty of measuring the other factor \"L\", the length of the pendulum, accurately. \"L\" in equation (1) above was the length of an ideal mathematical 'simple pendulum' consisting of a point mass swinging on the end of a massless cord. However the 'length' of a real",
      "of a simple gravity pendulum depends on its length, the local strength of gravity, and to a small extent on the maximum angle that the pendulum swings away from vertical, \"\u03b8\", called the amplitude. It is independent of the mass of the bob. If the amplitude is limited to small swings, the period \"T\" of a simple pendulum, the time taken for a complete cycle, is: where formula_2 is the length of the pendulum and formula_3 is the local acceleration of gravity. For small swings the period of swing is approximately the same for different size swings: that is, \"the"
    ]
  ],
  [
    "According to the BCS theory, the origin of the attraction between Cooper pairs in a superconductor is generally attributed to an electron-lattice interaction (BCS theory; an attraction is generally attributed to an electron-lattice interaction).",
    [
      "necessary for superconductivity. BCS theory starts from the assumption that there is some attraction between electrons, which can overcome the Coulomb repulsion. In most materials (in low temperature superconductors), this attraction is brought about indirectly by the coupling of electrons to the crystal lattice (as explained above). However, the results of BCS theory do \"not\" depend on the origin of the attractive interaction. For instance, Cooper pairs have been observed in ultracold gases of fermions where a homogeneous magnetic field has been tuned to their Feshbach resonance. The original results of BCS (discussed below) described an s-wave superconducting state, which",
      "an attraction is generally attributed to an electron-lattice interaction. The BCS theory, however, requires only that the potential be attractive, regardless of its origin. In the BCS framework, superconductivity is a macroscopic effect which results from the condensation of Cooper pairs. These have some bosonic properties, and bosons, at sufficiently low temperature, can form a large Bose\u2013Einstein condensate. Superconductivity was simultaneously explained by Nikolay Bogolyubov, by means of the Bogoliubov transformations. In many superconductors, the attractive interaction between electrons (necessary for pairing) is brought about indirectly by the interaction between the electrons and the vibrating crystal lattice (the phonons). Roughly",
      "Cooper pair In condensed matter physics, a Cooper pair or BCS pair is a pair of electrons (or other fermions) bound together at low temperatures in a certain manner first described in 1956 by American physicist Leon Cooper. Cooper showed that an arbitrarily small attraction between electrons in a metal can cause a paired state of electrons to have a lower energy than the Fermi energy, which implies that the pair is bound. In conventional superconductors, this attraction is due to the electron\u2013phonon interaction. The Cooper pair state is responsible for superconductivity, as described in the BCS theory developed by"
    ]
  ],
  [
    "The observer must move at a speed such that the relative velocity between the observer and the rod is greater than zero in order to measure the rod's length to be shorter than its rest length.",
    [
      "one observer may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage). Similarly, suppose a measuring rod is at rest and aligned along the \"x\"-axis in the unprimed system \"S\". In this system, the length of this rod is written as \u0394\"x\". To measure the length of this rod in the system \"S\"\u2032, in which the rod is moving, the distances \"x\"\u2032 to the end points of the rod must be measured simultaneously",
      "in that system \"S\"\u2032. In other words, the measurement is characterized by , which can be combined with the fourth equation to find the relation between the lengths \u0394\"x\" and \u0394\"x\"\u2032: This shows that the length (\u0394\"x\"\u2032) of the rod as measured in the frame in which it is moving (\"S\"\u2032), is \"shorter\" than its length (\u0394\"x\") in its own rest frame (\"S\"). In special relativity quantities, representing the velocity of an object, as measured in different frames, must obey to the differential form of the Lorentz transformation. In standard configuration the measurement of a speed \"u\" (in frame \"S\")",
      "velocity between an observer (or his measuring instruments) and the observed object is zero, then the proper length formula_1 of the object can simply be determined by directly superposing a measuring rod. However, if the relative velocity > 0, then one can proceed as follows: The observer installs a row of clocks that either are synchronized a) by exchanging light signals according to the Poincar\u00e9-Einstein synchronization, or b) by \"slow clock transport\", that is, one clock is transported along the row of clocks in the limit of vanishing transport velocity. Now, when the synchronization process is finished, the object is"
    ]
  ],
  [
    "The energy required to remove one electron from a helium atom in its ground state is 24.58738889 eV.",
    [
      "\u221254.41776311(2) eV. The total ground state energy of the helium atom is \u221279.005151042(40) eV, or \u22122.90338583(13) a.u. Helium atom A helium atom is an atom of the chemical element helium. Helium is composed of two electrons bound by the electromagnetic force to a nucleus containing two protons along with either one or two neutrons, depending on the isotope, held together by the strong force. Unlike for hydrogen, a closed-form solution to the Schr\u00f6dinger equation for the helium atom has not been found. However, various approximations, such as the Hartree\u2013Fock method, can be used to estimate the ground state energy and",
      "Ionization energy In physics and chemistry, ionization energy (American English spelling) or ionisation energy (British English spelling), denoted \"E\", is the minimum amount of energy required to remove the most loosely bound electron, the valence electron, of an isolated neutral gaseous atom or molecule. It is quantitatively expressed as where is any atom or molecule capable of ionization, is that atom or molecule with an electron removed, and is the removed electron. This is generally an endothermic process. Generally, the closer the outermost electrons are to the nucleus of the atom , the higher the atom's or element's ionization energy.",
      "Helium atom A helium atom is an atom of the chemical element helium. Helium is composed of two electrons bound by the electromagnetic force to a nucleus containing two protons along with either one or two neutrons, depending on the isotope, held together by the strong force. Unlike for hydrogen, a closed-form solution to the Schr\u00f6dinger equation for the helium atom has not been found. However, various approximations, such as the Hartree\u2013Fock method, can be used to estimate the ground state energy and wavefunction of the atom. The quantum mechanical description of the helium atom is of special interest, because"
    ]
  ],
  [
    "The current temperature of the universe's microwave radiation background is 2.7 kelvins.",
    [
      "universe contains a cosmological constant, denoted by Lambda (Greek \u039b), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology. The cosmic microwave background is radiation left over from decoupling after the epoch of recombination when neutral atoms first formed. At this point, radiation produced in the Big Bang stopped Thomson scattering from charged ions. The radiation, first observed in 1965 by Arno Penzias and Robert Woodrow Wilson, has a perfect thermal black-body spectrum. It has a temperature of 2.7 kelvins today and is isotropic to one",
      "for albedo, and an insolation of 1400 W m, one obtains an effective temperature of about 245 K. Similarly using albedo 0.3 and solar constant of 1372 W m, one obtains an effective temperature of 255 K. The cosmic microwave background radiation observed today is the most perfect black-body radiation ever observed in nature, with a temperature of about 2.7 K. It is a \"snapshot\" of the radiation at the time of decoupling between matter and radiation in the early universe. Prior to this time, most matter in the universe was in the form of an ionized plasma in thermal,",
      "have a temperature greater than that of the present-day blackbody radiation of the universe of 2.7 K = . This implies that must be less than 0.8% of the mass of the Earth \u2013 approximately the mass of the Moon. Cosmic microwave background radiation universe temperature: Hawking total black hole mass: where is the total Earth mass. In common units, So, for instance, a 1-second-life black hole has a mass of , equivalent to an energy of that could be released by megatons of TNT. The initial power is . Black hole evaporation has several significant consequences: The formulae from"
    ]
  ],
  [
    "The equation for calculating the current through a surface for a steady flow of charge is \\(I = \\iint_S \\mathbf{J} \\cdot d\\mathbf{A}\\), where \\(I\\) is the electric current, \\(\\mathbf{J}\\) is the current density vector, and \\(d\\mathbf{A}\\) is a surface element.",
    [
      "an electric current.\" When a metal wire is connected across the two terminals of a DC voltage source such as a battery, the source places an electric field across the conductor. The moment contact is made, the free electrons of the conductor are forced to drift toward the positive terminal under the influence of this field. The free electrons are therefore the charge carrier in a typical solid conductor. For a steady flow of charge through a surface, the current \"I\" (in amperes) can be calculated with the following equation: where \"Q\" is the electric charge transferred through the surface",
      "only the charge contained in the volume formed by \"dA\" and will flow through \"dA\". This charge is equal to , where \"\u03c1\" is the charge density at \"M\", and the electric current at \"M\" is . It follows that the current density vector can be expressed as: The surface integral of J over a surface \"S\", followed by an integral over the time duration \"t\" to \"t\", gives the total amount of charge flowing through the surface in that time (): More concisely, this is the integral of the flux of J across \"S\" between \"t\" and \"t\". The",
      "one of Maxwell's equations, since absence of this term would not predict electromagnetic waves to propagate, or the time evolution of electric fields in general. Since charge is conserved, current density must satisfy a continuity equation. Here is a derivation from first principles. The net flow out of some volume \"V\" (which can have an arbitrary shape but fixed for the calculation) must equal the net change in charge held inside the volume: where \"\u03c1\" is the charge density, and \"dA\" is a surface element of the surface \"S\" enclosing the volume \"V\". The surface integral on the left expresses"
    ]
  ],
  [
    "The emergency brakes of the elevator must dissipate enough heat to safely bring the elevator to rest after falling a distance of 100 m, taking into account the elevator's mass distribution and potential risks of structural failure due to overheating, ice buildup, and vibrational harmonics within the cable.",
    [
      "ropes had a tendency to overheat and cause slipping (or, in this case, a free-fall). While it is possible (though extraordinarily unlikely) for an elevator's cable to snap, all elevators in the modern era have been fitted with several safety devices which prevent the elevator from simply free-falling and crashing. An elevator cab is typically borne by 2 to 6 (up to 12 or more in high rise installations) hoist cables or belts, each of which is capable on its own of supporting the full load of the elevator plus twenty-five percent more weight. In addition, there is a device",
      "the cable would depend on the details of the elevator's mass distribution. If the break occurred at higher altitude, up to about 25,000 km, the lower portion of the elevator would descend to Earth and drape itself along the equator east of the anchor point, while the now unbalanced upper portion would rise to a higher orbit. Some authors (such as science fiction writers David Gerrold in \"Jumping off the Planet\" and Kim Stanley Robinson in \"Red Mars\") have suggested that such a failure would be catastrophic, with the thousands of kilometers of falling cable creating a swath of meteoric",
      "and affect the passage of elevator cars. Also, ice falling from the cable could damage elevator cars or the cable itself. To get rid of ice, special elevator cars could scrape the ice off. A final risk of structural failure comes from the possibility of vibrational harmonics within the cable. Like the shorter and more familiar strings of stringed musical instruments, the cable of a space elevator has a natural resonant frequency. If the cable is excited at this frequency, for example by the travel of elevators up and down it, the vibrational energy could build up to dangerous levels"
    ]
  ],
  [
    "The relative speed of the quasar 3C9 and Earth can be determined based on the redshift of the hydrogen lines in its spectrum.",
    [
      "watch a series of different galaxies pass that distance, later galaxies would pass that distance at a smaller velocity than earlier ones. Redshift can be measured by determining the wavelength of a known transition, such as hydrogen \u03b1-lines for distant quasars, and finding the fractional shift compared to a stationary reference. Thus redshift is a quantity unambiguous for experimental observation. The relation of redshift to recessional velocity is another matter. For an extensive discussion, see Harrison. The redshift \"z\" is often described as a \"redshift velocity\", which is the recessional velocity that would produce the same redshift \"if\" it were",
      "determine the redshift, one searches for features in the spectrum such as absorption lines, emission lines, or other variations in light intensity. If found, these features can be compared with known features in the spectrum of various chemical compounds found in experiments where that compound is located on Earth. A very common atomic element in space is hydrogen. The spectrum of originally featureless light shone through hydrogen will show a signature spectrum specific to hydrogen that has features at regular intervals. If restricted to absorption lines it would look similar to the illustration (top right). If the same pattern of",
      "IGM and is redshifted, wavelengths which had been below the Lyman Alpha limit are stretched, and will in effect begin to fill in the Lyman absorption band. This means that instead of showing sharp spectral absorption lines, a quasar's light which has traveled through a large, spread out region of neutral hydrogen will show a Gunn-Peterson trough. The redshifting for a particular quasar provides temporal (time) information about reionization. Since an object's redshift corresponds to the time at which it emitted the light, it is possible to determine when reionization ended. Quasars below a certain redshift (closer in space and"
    ]
  ],
  [
    "The magnitude of the applied force needed to move the 10 kg box 5 m with a constant force, leaving it with a speed of 2 m/s afterwards can be calculated using Newton's Second Law, which states that force equals mass multiplied by acceleration, where acceleration is the change in speed over time, giving a force of approximately 60 N.",
    [
      "(1 m/s) from a state of rest, when it achieves the speed of 5 m/s after 5 seconds and 10 m/s after 10 seconds. The average acceleration can be calculated by dividing the speed (m/s) by the time (s), so the average acceleration in the first example would be calculated formula_1. Newton's Second Law states that force equals mass multiplied by acceleration. The unit of force is the newton (N), and mass has the SI unit kilogram (kg). One newton equals one kilogram metre per second squared. Therefore, the unit metre per second squared is equivalent to newton per kilogram,",
      "measuring unit. The work formula_1 done by a constant force of magnitude formula_2 on a point that moves a displacement formula_3 in a straight line in the direction of the force is the product For example, if a force of 10 newtons (formula_2 = 10 N) acts along a point that travels 2 metres (formula_3 = 2 m), then formula_7 This is approximately the work done lifting a 1 kg object from ground level to over a person's head against the force of gravity. The work is doubled either by lifting twice the weight the same distance or by lifting",
      "relativity. The second law states that the rate of change of momentum of a body is directly proportional to the force applied, and this change in momentum takes place in the direction of the applied force. The second law can also be stated in terms of an object's acceleration. Since Newton's second law is valid only for constant-mass systems, can be taken outside the differentiation operator by the constant factor rule in differentiation. Thus, where F is the net force applied, \"m\" is the mass of the body, and a is the body's acceleration. Thus, the net force applied to"
    ]
  ],
  [
    "The difference in time elapsed between the astronaut's ship and Earth after traveling to a star 32 light-years away and back is 36 years.",
    [
      "a difference between the time elapsed on the astronaut's ship and the time elapsed on Earth. For example, a spaceship could travel to a star 32 light-years away, initially accelerating at a constant 1.03g (i.e. 10.1 m/s) for 1.32 years (ship time), then stopping its engines and coasting for the next 17.3 years (ship time) at a constant speed, then decelerating again for 1.32 ship-years, and coming to a stop at the destination. After a short visit, the astronaut could return to Earth the same way. After the full round-trip, the clocks on board the ship show that 40 years",
      "have passed, but according to those on Earth, the ship comes back 76 years after launch. From the viewpoint of the astronaut, onboard clocks seem to be running normally. The star ahead seems to be approaching at a speed of 0.87 light years per ship-year. The universe would appear contracted along the direction of travel to half the size it had when the ship was at rest; the distance between that star and the Sun would seem to be 16 light years as measured by the astronaut. At higher speeds, the time on board will run even slower, so the",
      "astronaut could travel to the center of the Milky Way (30,000 light years from Earth) and back in 40 years ship-time. But the speed according to Earth clocks will always be less than 1 light year per Earth year, so, when back home, the astronaut will find that more than 60 thousand years will have passed on Earth. Regardless of how it is achieved, a propulsion system that could produce acceleration continuously from departure to arrival would be the fastest method of travel. A constant acceleration journey is one where the propulsion system accelerates the ship at a constant rate"
    ]
  ],
  [
    "The emergency brakes must dissipate a significant amount of heat to safely stop the elevator after falling 100m.",
    [
      "up disabling all of the dynamic brakes on the train, allowing the train to pick up speed. After the activation of the emergency brake, the only brakes which were operational were the air brakes, which were now melting from the friction and heat. When the NTSB investigators arrived at the crash site (about twelve hours after the accident), they observed that the wheels had gotten so hot that they had started to expand off the wheel axles by the time they left the rails. The train was traveling at a calculated speed of 110 mph (177 km/h) when it entered",
      "brake assembly of Flight 420 revealed that at the time, the brake assembly's temperature were exposed to a temperature of more than 600 degree Celsius, which may indicated that there was a significant amount of drag force at the time of the crash. Another test was conducted by investigators. This time, they conducted the test to know if hydraulic fluids of Flight 420 would ignite when it contacted the hot surface of the brake assembly. Investigators used two kinds of hydraulic fluids; the contaminated and uncontaminated one. The result was that an intense fire broke out after it contacted with",
      "the cable would depend on the details of the elevator's mass distribution. If the break occurred at higher altitude, up to about 25,000 km, the lower portion of the elevator would descend to Earth and drape itself along the equator east of the anchor point, while the now unbalanced upper portion would rise to a higher orbit. Some authors (such as science fiction writers David Gerrold in \"Jumping off the Planet\" and Kim Stanley Robinson in \"Red Mars\") have suggested that such a failure would be catastrophic, with the thousands of kilometers of falling cable creating a swath of meteoric"
    ]
  ],
  [
    "The mean kinetic energy of conduction electrons in metals is much higher than kT because of the high concentration of conduction electrons in the metal and the small mass of the electron, leading to a deviation from the classical regime and requiring Fermi-Dirac statistics.",
    [
      "in that region. In contrast, the low energy states are rigidly filled with a fixed number of electrons at all times, and the high energy states are empty of electrons at all times. Electric current consists of a flow of electrons. In metals there are many electron energy levels near the Fermi level, so there are many electrons available to move. This is what causes the high electronic conductivity of metals. An important part of band theory is that there may be forbidden bands of energy: energy intervals that contain no energy levels. In insulators and semiconductors, the number of",
      "situation prevails when the concentration of particles corresponds to an average interparticle separation formula_17 that is much greater than the average de Broglie wavelength formula_18 of the particles: where is Planck's constant, and is the mass of a particle. For the case of conduction electrons in a typical metal at = 300 K (i.e. approximately room temperature), the system is far from the classical regime because formula_20 . This is due to the small mass of the electron and the high concentration (i.e. small formula_21) of conduction electrons in the metal. Thus Fermi\u2013Dirac statistics is needed for conduction electrons in",
      "favorable for welding. At higher power density, the material affected by the beam can totally evaporate in a very short time; this is no longer electron-beam welding; it is electron-beam machining. Conduction electrons (those not bound to the nucleus of atoms) move in a crystal lattice of metals with velocities distributed according to Gauss's law and depending on temperature. They cannot leave the metal unless their kinetic energy (in eV) is higher than the potential barrier at the metal surface. The number of electrons fulfilling this condition increases exponentially with increasing temperature of the metal, following Richardson's rule. As a"
    ]
  ],
  [
    "The magnitude of the applied force required to move the box 5m with a constant speed of 2 m/s after the force is removed is 1000 lbf.",
    [
      "equivalent to 1000 lbf. See also Ton-force. See force gauge, spring scale, load cell Force In physics, a force is any interaction that, when unopposed, will change the motion of an object. A force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described intuitively as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F. The original form of",
      "(metric system) as metres per second (m/s) or as the SI base unit of (m\u22c5s). For example, \"5 metres per second\" is a scalar, whereas \"5 metres per second east\" is a vector. If there is a change in speed, direction or both, then the object has a changing velocity and is said to be undergoing an \"acceleration\". To have a constant velocity, an object must have a constant speed in a constant direction. Constant direction constrains the object to motion in a straight path thus, a constant velocity means motion in a straight line at a constant speed. For",
      "measuring unit. The work formula_1 done by a constant force of magnitude formula_2 on a point that moves a displacement formula_3 in a straight line in the direction of the force is the product For example, if a force of 10 newtons (formula_2 = 10 N) acts along a point that travels 2 metres (formula_3 = 2 m), then formula_7 This is approximately the work done lifting a 1 kg object from ground level to over a person's head against the force of gravity. The work is doubled either by lifting twice the weight the same distance or by lifting"
    ]
  ],
  [
    "The experimental anomaly of the photoelectric effect, where the energy of ejected electrons was proportional to the frequency of light rather than its intensity, led Einstein to propose that light could act as particles in some circumstances.",
    [
      "momentum \"p\" of a photon is also proportional to its frequency and inversely proportional to its wavelength: The source of Einstein's proposal that light was composed of particles (or could act as particles in some circumstances) was an experimental anomaly not explained by the wave theory: the photoelectric effect, in which light striking a metal surface ejected electrons from the surface, causing an electric current to flow across an applied voltage. Experimental measurements demonstrated that the energy of individual ejected electrons was proportional to the \"frequency\", rather than the \"intensity\", of the light. Furthermore, below a certain minimum frequency, which",
      "depended on the particular metal, no current would flow regardless of the intensity. These observations appeared to contradict the wave theory, and for years physicists tried in vain to find an explanation. In 1905, Einstein explained this puzzle by resurrecting the particle theory of light to explain the observed effect. Because of the preponderance of evidence in favor of the wave theory, however, Einstein's ideas were met initially with great skepticism among established physicists. Eventually Einstein's explanation was accepted as new particle-like behavior of light was observed, such as the Compton effect. As a photon is absorbed by an atom,",
      "action. Einstein's hypothesis of light \"quanta\" (photons), based on Heinrich Hertz's 1887 discovery (and further investigation by Philipp Lenard) of the photoelectric effect, was initially rejected by Planck. He was unwilling to discard completely Maxwell's theory of electrodynamics. \"The theory of light would be thrown back not by decades, but by centuries, into the age when Christiaan Huygens dared to fight against the mighty emission theory of Isaac Newton ...\" In 1910, Einstein pointed out the anomalous behavior of specific heat at low temperatures as another example of a phenomenon which defies explanation by classical physics. Planck and Nernst, seeking"
    ]
  ],
  [
    "The force constant of the second spring that requires twice as much work to stretch by half the distance as the first spring with force constant k is 4k.",
    [
      "model for our observations, There are many methods we might use to estimate the unknown parameter \"k\". Noting that the \"n\" equations in the \"m\" variables in our data comprise an overdetermined system with one unknown and \"n\" equations, we may choose to estimate \"k\" using least squares. The sum of squares to be minimized is The least squares estimate of the force constant, \"k\", is given by Here it is assumed that application of the force causes the spring to expand and, having derived the force constant by least squares fitting, the extension can be predicted from Hooke's law.",
      "the extension of the spring at a time \"t\" as \"x\"(\"t\"). Now, using Newton's second law we can write (using convenient units): where \"m\" is the mass and \"k\" is the spring constant that represents a measure of spring stiffness. For simplicity's sake, let us take \"m=k\" as an example. If we look for solutions that have the form formula_33, where \"C\" is a constant, we discover the relationship formula_34, and thus formula_35 must be one of the complex numbers formula_36 or formula_37. Thus, using Euler's formula we can say that the solution must be of the form: See a",
      "Constant-force spring An ideal constant-force spring is a spring for which the force it exerts over its range of motion is a constant, that is, it does not obey Hooke's law. In reality, \"constant-force springs\" do not provide a truly constant force and are constructed from materials which do obey Hooke's law. Generally constant-force springs are constructed as a rolled ribbon of spring steel such that the spring is in a rolled up form when relaxed. The approximation of \"constant force\" comes from a long travel and a pre-loaded rest position, so that the initial force does not start from"
    ]
  ],
  [
    "The speed of observer O' relative to observer O is given by the formula for relative velocity, which is the velocity of an object or observer B in the rest frame of another object or observer A.",
    [
      "observer A is given by the formula: where The relative speed is given by the formula Relative velocity The \"relative velocity\" formula_1 (also formula_2 or formula_3) is the velocity of an object or observer B in the rest frame of another object or observer A. We begin with relative motion in the classical, (or non-relativistic, or the Newtonian approximation) that all speeds are much less than the speed of light. This limit is associated with the Galilean transformation. The figure shows a man on top of a train, at the back edge. At 1:00 pm he begins to walk forward",
      "observer watching the object moving from O to A, another observer can be found (moving at less than the speed of light with respect to the first) for whom the object moves from A to O. The question of which observer is right has no unique answer, and therefore makes no physical sense. Any such moving object or signal would violate the principle of causality. Also, any general technical means of sending signals faster than light would permit information to be sent into the originator's own past. In the diagram, an observer at O in the system sends a message",
      "speed of light. To synchronize the clocks, in the data reduction following an experiment, the time when a signal is received will be corrected to reflect its actual time were it to have been recorded by an idealized lattice of clocks. In many books on special relativity, especially older ones, the word \"observer\" is used in the more ordinary sense of the word. It is usually clear from context which meaning has been adopted. Physicists distinguish between what one \"measures\" or \"observes\" (after one has factored out signal propagation delays), versus what one visually sees without such corrections. Failure to"
    ]
  ],
  [
    "A black body would emit thermal radiation with a peak wavelength of about 0.5 micrometers at a temperature of approximately 5800 Kelvin, according to Wien's displacement law.",
    [
      "a body that is at a uniform temperature. The shape of the spectrum and the total amount of energy emitted by the body is a function of the absolute temperature of that body. The radiation emitted covers the entire electromagnetic spectrum and the intensity of the radiation (power/unit-area) at a given frequency is described by Planck's law of radiation. For a given temperature of a black-body there is a particular frequency at which the radiation emitted is at its maximum intensity. That maximum radiation frequency moves toward higher frequencies as the temperature of the body increases. The frequency at which",
      "the black-body radiation is at maximum is given by Wien's displacement law and is a function of the body's absolute temperature. A black-body is one that emits at any temperature the maximum possible amount of radiation at any given wavelength. A black-body will also absorb the maximum possible incident radiation at any given wavelength. A black-body with a temperature at or below room temperature would thus appear absolutely black, as it would not reflect any incident light nor would it emit enough radiation at visible wavelengths for our eyes to detect. Theoretically, a black-body emits electromagnetic radiation over the entire",
      "all radiation falling on it, at all wavelengths, is called a black body. When a black body is at a uniform temperature, its emission has a characteristic frequency distribution that depends on the temperature. Its emission is called black-body radiation. The concept of the black body is an idealization, as perfect black bodies do not exist in nature. Graphite and lamp black, with emissivities greater than 0.95, however, are good approximations to a black material. Experimentally, black-body radiation may be established best as the ultimately stable steady state equilibrium radiation in a cavity in a rigid body, at a uniform"
    ]
  ],
  [
    "Transitions from the n = 4, l = 1 state in hydrogen to the n = 1 state are not allowed due to electric dipole selection rules.",
    [
      "consider an atom interacting with the electric dipole field of the photon. Then some transitions are not allowed at all, others are only allowed for photons of a certain polarization. Let's consider for example the hydrogen atom. The transition from the state formula_7 with \"m=-1/2\" to the state formula_8 with \"m=-1/2\" is only allowed for light with polarization along the z axis (quantization axis) of the atom. The state formula_8 with \"m=-1/2\" therefore appears dark for light of other polarizations. Transitions from the \"2S\" level to the \"1S\" level are not allowed at all. The \"2S\" state can not decay",
      "the atom jumps to a final state , This then allowed the magnitude of the matrix elements to be interpreted statistically: \"they give the intensity of the spectral lines, the probability for quantum jumps from the emission of dipole radiation\". Since the transition rates are given by the matrix elements of , wherever is zero, the corresponding transition should be absent. These were called the selection rules, which were a puzzle until the advent of matrix mechanics. An arbitrary state of the Hydrogen atom, ignoring spin, is labelled by |\"n\";\"\u2113,m\" \u27e9, where the value of \u2113 is a measure of",
      "are strictly forbidden under quantum mechanics. The allowed transitions are described by so-called selection rules, which describe the conditions under which a radiative transition is allowed. For instance, transitions are only allowed if \u0394\"S\" = 0, \"S\" being the total spin angular momentum of the system. In real materials other effects, such as interactions with the crystal lattice, intervene to circumvent the formal rules by providing alternate mechanisms. In these systems the forbidden transitions can occur, but usually at slower rates than allowed transitions. A classic example is phosphorescence where a material has a ground state with \"S\" = 0,"
    ]
  ],
  [
    "There are 5 allowed values of the quantum number m_l for an electron in the l = 2 state of a single-electron atom, corresponding to -2, -1, 0, 1, and 2.",
    [
      "energy increases so much that the electron can easily escape from the atom. In single electron atoms, all energy levels with the same principle quantum number are degenerate, and have the same energy. In atoms with more than one electron, the energy of an electron depends not only on the properties of the orbital it resides in, but also on its interactions with the other electrons in other orbitals. This requires consideration of the l quantum number. Higher values of l are associated with higher values of energy; for instance, the 2p state is higher than the 2s state. When",
      "shells and subshells despite the advances in understanding of the quantum-mechanical nature of electrons. An electron shell is the set of allowed states that share the same principal quantum number, \"n\" (the number before the letter in the orbital label), that electrons may occupy. An atom's \"n\"th electron shell can accommodate 2\"n\" electrons, \"e.g.\" the first shell can accommodate 2 electrons, the second shell 8 electrons, the third shell 18 electrons and so on. The factor of two arises because the allowed states are doubled due to electron spin\u2014each atomic orbital admits up to two otherwise identical electrons with opposite",
      "the electron configuration of the atoms and the periodic table. The stationary states (quantum states) of the hydrogen-like atoms are its atomic orbitals. However, in general, an electron's behavior is not fully described by a single orbital. Electron states are best represented by time-depending \"mixtures\" (linear combinations) of multiple orbitals. See Linear combination of atomic orbitals molecular orbital method. The quantum number first appeared in the Bohr model where it determines the radius of each circular electron orbit. In modern quantum mechanics however, determines the mean distance of the electron from the nucleus; all electrons with the same value of"
    ]
  ],
  [
    "The driver of the police car hears the echo of the siren at a frequency of 660 Hz when moving towards a wall at 3.5 m/s with a speed of sound of 350 m/s and a siren frequency of 600 Hz.",
    [
      "differences also account for increases, based upon the frequency and the speaker spacing. However, sirens are designed to sweep the frequency of their sound output, typically, no less than one octave. This sweeping minimizes the effects of phase cancellation. The end result is that the average sound output from a dual speaker siren system is 3 dB greater than a single speaker system. Siren (alarm) A siren is a loud noise-making device. Civil defense sirens are mounted in fixed locations and used to warn of natural disasters or attacks. Sirens are used on emergency service vehicles such as ambulances, police",
      "In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second. The siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus: In other words, if the siren approached the observer directly, the pitch would remain constant, at a higher than stationary",
      "being replaced with more specialized warnings, such as the Emergency Alert System. A mechanical siren generates sound by spinning a slotted chopper wheel to interrupt a stream of air at a regular rate. Modern sirens can develop a sound level of up to 135 decibels at . The Chrysler air raid siren, driven by a 331-cubic-inch Chrysler Hemi gasoline engine, generates 138 dB at 100 feet away. By use of varying tones or on/off patterns of sound, different alert conditions can be signaled. Electronic sirens can transmit voice announcements in addition to alert tone signals. Siren systems may be electronically"
    ]
  ],
  [
    "The minimum ionization energy of a hydrogen atom is 13.6 eV, and it is significant in the emission spectrum of a doubly ionized lithium atom because it corresponds to the energy needed to completely remove the outermost electron, resulting in the emission of a photon.",
    [
      "the case of neutral atomic hydrogen, the minimum ionization energy is equal to the Lyman limit, where the photon has enough energy to completely ionize the atom, resulting in a free proton and a free electron. Above this energy (below this wavelength), \"all\" wavelengths of light may be absorbed. This forms a continuum in the energy spectrum; the spectrum is continuous rather than composed of many discrete lines, which are seen at lower energies. The Lyman limit is at the wavelength of 91.2 nm (912 \u00c5), corresponding to a frequency of 3.29 million GHz and a photon energy of 13.6",
      "Ionization energy In physics and chemistry, ionization energy (American English spelling) or ionisation energy (British English spelling), denoted \"E\", is the minimum amount of energy required to remove the most loosely bound electron, the valence electron, of an isolated neutral gaseous atom or molecule. It is quantitatively expressed as where is any atom or molecule capable of ionization, is that atom or molecule with an electron removed, and is the removed electron. This is generally an endothermic process. Generally, the closer the outermost electrons are to the nucleus of the atom , the higher the atom's or element's ionization energy.",
      "electron binding energy refers the minimum amount of energy required to remove an electron from the dicarboxylate dianion OC(CH)CO. Work function is the minimum amount of energy required to remove an electron from a solid surface. Ionization energy In physics and chemistry, ionization energy (American English spelling) or ionisation energy (British English spelling), denoted \"E\", is the minimum amount of energy required to remove the most loosely bound electron, the valence electron, of an isolated neutral gaseous atom or molecule. It is quantitatively expressed as where is any atom or molecule capable of ionization, is that atom or molecule with"
    ]
  ],
  [
    "The rest energy of an electron-positron pair is converted into the energy of the photons produced during their annihilation.",
    [
      "being at rest. The amount of energy is directly proportional to the mass of the body: where For example, consider electron\u2013positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did",
      "attraction (or repulsion) at a separation of one Compton wavelength, and the rest energy of the charge. It is given by \"\u03b1\" \u2248 , which is approximately equal to . When electrons and positrons collide, they annihilate each other, giving rise to two or more gamma ray photons. If the electron and positron have negligible momentum, a positronium atom can form before annihilation results in two or three gamma ray photons totalling 1.022 MeV. On the other hand, a high-energy photon can transform into an electron and a positron by a process called pair production, but only in the presence",
      "photons were sufficiently energetic that they could react with each other to form pairs of electrons and positrons. Likewise, positron-electron pairs annihilated each other and emitted energetic photons: An equilibrium between electrons, positrons and photons was maintained during this phase of the evolution of the Universe. After 15 seconds had passed, however, the temperature of the universe dropped below the threshold where electron-positron formation could occur. Most of the surviving electrons and positrons annihilated each other, releasing gamma radiation that briefly reheated the universe. For reasons that remain uncertain, during the annihilation process there was an excess in the number"
    ]
  ],
  [
    "A population accumulates in level 3 due to a long lifetime compared to the non-radiative transition Ra.",
    [
      "\"E\", and populations \"N\", \"N\", \"N\", \"N\", respectively. The energies of each level are such that \"E\" < \"E\" < \"E\" < \"E\". In this system, the pumping transition P excites the atoms in the ground state (level 1) into the pump band (level 4). From level 4, the atoms again decay by a fast, non-radiative transition Ra into the level 3. Since the lifetime of the laser transition L is long compared to that of Ra (\u03c4 \u226b \u03c4), a population accumulates in level 3 (the \"upper laser level\"), which may relax by spontaneous or stimulated emission into level",
      "2 (the \"lower laser level\"). This level likewise has a fast, non-radiative decay Rb into the ground state. As before, the presence of a fast, radiationless decay transition results in the population of the pump band being quickly depleted (\"N\" \u2248 0). In a four-level system, any atom in the lower laser level \"E\" is also quickly de-excited, leading to a negligible population in that state (\"N\" \u2248 0). This is important, since any appreciable population accumulating in level 3, the upper laser level, will form a population inversion with respect to level 2. That is, as long as \"N\"",
      "the population of atoms must be excited from the ground state to obtain a population inversion, the laser medium must be very strongly pumped. This makes three-level lasers rather inefficient, despite being the first type of laser to be discovered (based on a ruby laser medium, by Theodore Maiman in 1960). A three-level system could also have a radiative transition between level 3 and 2, and a non-radiative transition between 2 and 1. In this case, the pumping requirements are weaker. In practice, most lasers are \"four-level lasers\", described below. Here, there are four energy levels, energies \"E\", \"E\", \"E\","
    ]
  ],
  [
    "The percentage of transmitted light intensity through both polarizers when unpolarized light is incident on a pair of ideal linear polarizers whose transmission axes make an angle of 45 degrees with each other is approximately 25%.",
    [
      "where A beam of unpolarised light can be thought of as containing a uniform mixture of linear polarizations at all possible angles. Since the average value of formula_26 is 1/2, the transmission coefficient becomes In practice, some light is lost in the polariser and the actual transmission of unpolarised light will be somewhat lower than this, around 38% for Polaroid-type polarisers but considerably higher (>49.9%) for some birefringent prism types. In addition to birefringence and dichroism in extended media, polarization effects can also occur at the (reflective) interface between two materials of different refractive index. These effects are treated by",
      "somewhat lower than this, around 38% for Polaroid-type polarizers but considerably higher (>49.9%) for some birefringent prism types. If two polarizers are placed one after another (the second polarizer is generally called an \"analyzer\"), the mutual angle between their polarizing axes gives the value of \u03b8 in Malus's law. If the two axes are orthogonal, the polarizers are \"crossed\" and in theory no light is transmitted, though again practically speaking no polarizer is perfect and the transmission is not exactly zero (for example, crossed Polaroid sheets appear slightly blue in colour). If a transparent object is placed between the crossed",
      "of the two light waves. An absorbing polarizer rotated to any angle will always transmit half the incident intensity when averaged over time. If the electric field wanders by a smaller amount the light will be partially polarized so that at some angle, the polarizer will transmit more than half the intensity. If a wave is combined with an orthogonally polarized copy of itself delayed by less than the coherence time, partially polarized light is created. The polarization of a light beam is represented by a vector in the Poincar\u00e9 sphere. For polarized light the end of the vector lies"
    ]
  ],
  [
    "The sound will first disappear at an angle of 45 degrees from the normal to the speaker face at a frequency lower than 20 Hz.",
    [
      "hearing loss due to age or prolonged exposure to very loud noises. Audio frequency An audio frequency (abbreviation: AF) or audible frequency is characterized as a periodic vibration whose frequency is audible to the average human. The SI unit of audio frequency is the hertz (Hz). It is the property of sound that most determines pitch. The generally accepted standard range of audible frequencies for humans is 20 to 20,000 Hz, although the range of frequencies individuals hear is greatly influenced by environmental factors. Frequencies below 20 Hz are generally felt rather than heard, assuming the amplitude of the vibration",
      "to frequency. Sound propagates as mechanical vibration waves of pressure and displacement, in air or other substances.. In general, frequency components of a sound determine its \"color\", its timbre. When speaking about the frequency (in singular) of a sound, it means the property that most determines pitch. The frequencies an ear can hear are limited to a specific range of frequencies. The audible frequency range for humans is typically given as being between about 20 Hz and 20,000 Hz (20 kHz), though the high frequency limit usually reduces with age. Other species have different hearing ranges. For example, some dog",
      "is given by: The frequency is decreased if either is moving away from the other. The above formula assumes that the source is either directly approaching or receding from the observer. If the source approaches the observer at an angle (but still with a constant velocity), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach;"
    ]
  ],
  [
    "The minimum ionization energy of neutral atomic hydrogen is 13.6 electron volts.",
    [
      "the case of neutral atomic hydrogen, the minimum ionization energy is equal to the Lyman limit, where the photon has enough energy to completely ionize the atom, resulting in a free proton and a free electron. Above this energy (below this wavelength), \"all\" wavelengths of light may be absorbed. This forms a continuum in the energy spectrum; the spectrum is continuous rather than composed of many discrete lines, which are seen at lower energies. The Lyman limit is at the wavelength of 91.2 nm (912 \u00c5), corresponding to a frequency of 3.29 million GHz and a photon energy of 13.6",
      "Ionization energy In physics and chemistry, ionization energy (American English spelling) or ionisation energy (British English spelling), denoted \"E\", is the minimum amount of energy required to remove the most loosely bound electron, the valence electron, of an isolated neutral gaseous atom or molecule. It is quantitatively expressed as where is any atom or molecule capable of ionization, is that atom or molecule with an electron removed, and is the removed electron. This is generally an endothermic process. Generally, the closer the outermost electrons are to the nucleus of the atom , the higher the atom's or element's ionization energy.",
      "electron binding energy refers the minimum amount of energy required to remove an electron from the dicarboxylate dianion OC(CH)CO. Work function is the minimum amount of energy required to remove an electron from a solid surface. Ionization energy In physics and chemistry, ionization energy (American English spelling) or ionisation energy (British English spelling), denoted \"E\", is the minimum amount of energy required to remove the most loosely bound electron, the valence electron, of an isolated neutral gaseous atom or molecule. It is quantitatively expressed as where is any atom or molecule capable of ionization, is that atom or molecule with"
    ]
  ],
  [
    "The rotational kinetic energy of an object is directly proportional to the moment of inertia of the object.",
    [
      "Rotational energy Rotational energy or angular kinetic energy is kinetic energy due to the rotation of an object and is part of its total kinetic energy. Looking at rotational energy separately around an object's axis of rotation, the following dependence on the object's moment of inertia is observed: where The mechanical work required for / applied during rotation is the torque times the rotation angle. The instantaneous power of an angularly accelerating body is the torque times the angular velocity. For free-floating (unattached) objects, the axis of rotation is commonly around its center of mass. Note the close relationship between",
      "tidal locking for a more detailed explanation of this process). Rotational energy Rotational energy or angular kinetic energy is kinetic energy due to the rotation of an object and is part of its total kinetic energy. Looking at rotational energy separately around an object's axis of rotation, the following dependence on the object's moment of inertia is observed: where The mechanical work required for / applied during rotation is the torque times the rotation angle. The instantaneous power of an angularly accelerating body is the torque times the angular velocity. For free-floating (unattached) objects, the axis of rotation is commonly",
      "moment,rotational inertia, polar moment of inertia of mass, or the angular mass, (SI units kg\u00b7m\u00b2) is a measure of an object's resistance to changes to its rotation. It is the inertia of a rotating body with respect to its rotation. The moment of inertia plays much the same role in rotational dynamics as mass does in linear dynamics, describing the relationship between angular momentum and angular velocity, torque and angular acceleration, and several other quantities. The symbols I and J are usually used to refer to the moment of inertia or polar moment of inertia. While a simple scalar treatment"
    ]
  ],
  [
    "Inside a nonmagnetic dielectric material with a dielectric constant of 4.0, the speed of light is approximately 150,000 km/s.",
    [
      "is the speed of light in vacuum and \"\u03ba\" = \"\u00b5\"\"c\"/2\u03c0 = 59.95849 \u03a9 \u2248 60.0 \u03a9 is a newly introduced constant (units ohms, or reciprocal siemens, such that \"\u03c3\u03bb\u03ba\" = \"\u03b5\" remains unitless). Permittivity is typically associated with dielectric materials, however metals are described as having an effective permittivity, with real relative permittivity equal to one. In the low-frequency region, which extends from radio frequencies to the far infrared and terahertz region, the plasma frequency of the electron gas is much greater than the electromagnetic propagation frequency, so the refraction index \"n\" of a metal is very nearly a",
      "physical constant is commonly referred to as the speed of light. The postulate of the constancy of the speed of light in all inertial reference frames lies at the heart of special relativity and has given rise to a popular notion that the \"speed of light is always the same\". However, in many situations light is more than a disturbance in the electromagnetic field. Light traveling within a medium is no longer a disturbance solely of the electromagnetic field, but rather a disturbance of the field and the positions and velocities of the charged particles (electrons) within the material. The",
      "at which light travels in a material is called the refractive index of the material (). For example, for visible light the refractive index of glass is typically around 1.5, meaning that light in glass travels at ; the refractive index of air for visible light is about 1.0003, so the speed of light in air is about , which is about slower than . For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. In communicating with distant space probes, it"
    ]
  ],
  [
    "The total energy stored in the two capacitors when a 300-volt potential difference is applied across the combination is equal to half of the energy that can be stored in a capacitor, which is given by the formula described in the document.",
    [
      "the voltage on the two capacitors must be equal since they are connected together. Since they both have the same capacitance formula_3 the charge will be divided equally between the capacitors so each capacitor will have a charge of formula_4 and a voltage of formula_5. At the beginning of the experiment the total initial energy formula_6 in the circuit is the energy stored in the charged capacitor: At the end of the experiment the final energy formula_8 is equal to the sum of the energy in the two capacitors Thus the final energy formula_8 is equal to half of the",
      "in Joule) that can be stored in a capacitor is given by the formula This formula describes the amount of energy stored and is often used to describe new research successes. However, only part of the stored energy is available to applications, because the voltage drop and the time constant over the internal resistance mean that some of the stored charge is inaccessible. The effective realized amount of energy W is reduced by the used voltage difference between V and V and can be represented as: This formula also represents the energy asymmetric voltage components such as lithium ion capacitors.",
      "is mirrored (with opposite polarity) in the second Helmholtz layer at the second electrode. Therefore, the total capacitance value of a double-layer capacitor is the result of two capacitors connected in series. If both electrodes have approximately the same capacitance value, as in symmetrical supercapacitors, the total value is roughly half that of one electrode. Double-layer capacitance Double-layer capacitance is the storing of electrical energy by means of the electrical double layer effect. This electrical phenomenon appears at the interface between a conductive electrode and an adjacent liquid electrolyte, as observed, for example, in a supercapacitor. At this boundary two"
    ]
  ],
  [
    "The diffraction patterns obtained from a beam of electrons impinging on a crystal surface are related to the electron density within the crystal, as the angles and intensities of the diffracted beams indicate a three-dimensional density of electrons within the crystal (X-ray crystallography).",
    [
      "transmit through it. The diffracting planes in the crystal are determined by knowing that the normal to the diffracting plane bisects the angle between the incident beam and the diffracted beam. A Greninger chart can be used to interpret the back reflection Laue photograph. Other particles, such as electrons and neutrons, may be used to produce a diffraction pattern. Although electron, neutron, and X-ray scattering are based on different physical processes, the resulting diffraction patterns are analyzed using the same coherent diffraction imaging techniques. As derived below, the electron density within the crystal and the diffraction patterns are related by",
      "the Fabry-P\u00e9rot effect. These oscillations can be used to infer layer thicknesses and other properties. In X-ray diffraction a beam strikes a crystal and diffracts into many specific directions. The angles and intensities of the diffracted beams indicate a three-dimensional density of electrons within the crystal. X-rays produce a diffraction pattern because their wavelength is typically the same order of magnitude (0.1-10.0 nm) as the spacing between the atomic planes in the crystal. Each atom re-radiates a small portion of an incoming beam's intensity as a spherical wave. If the atoms are arranged symmetrically (as is found in a crystal)",
      "the scattering be recorded at least three (and usually four, for redundancy) wavelengths of the incoming X-ray radiation. A single crystal may degrade too much during the collection of one data set, owing to radiation damage; in such cases, data sets on multiple crystals must be taken. The recorded series of two-dimensional diffraction patterns, each corresponding to a different crystal orientation, is converted into a three-dimensional model of the electron density; the conversion uses the mathematical technique of Fourier transforms, which is explained below. Each spot corresponds to a different type of variation in the electron density; the crystallographer must"
    ]
  ],
  [
    "The ratio of the angular momentum of satellite A to the angular momentum of satellite B is 2:1.",
    [
      "is important not to confound the gravitational parameter formula_25 with the reduced mass, which is sometimes also denoted by the same letter formula_25. One obtains the specific relative angular momentum by multiplying (cross product) the equation of the two-body problem with the distance vector formula_23 The cross product of a vector with itself (right hand side) is 0. The left hand side simplifies to according to the product rule of differentiation. This means that formula_30 is constant (i.e., a conserved quantity). And this is exactly the angular momentum per mass of the satellite: This vector is perpendicular to the orbit",
      "Relative angular momentum In celestial mechanics, the relative angular momentum (formula_1) of an orbiting body (formula_2) relative to a central body (formula_3) is the moment of (formula_2)'s relative linear momentum: where: For a body in an unperturbed orbit about a central body, the orbital plane is stationary, and the relative angular momentum (formula_1) is perpendicular to the orbital plane. <br> For perturbed orbits where the orbital plane is in motion, the relative angular momentum vector is perpendicular to the (osculating) orbital plane at only two points in the orbit. In astrodynamics relative angular momentum is usually used to derive specific",
      "where formula_3 is the radius of the primary, formula_4 is the density of the primary, and formula_5 is the density of the satellite. This can be equivalently written as where formula_7 is the radius of the secondary, formula_8 is the mass of the primary, and formula_9 is the mass of the secondary. This does not depend on the size of the objects, but on the ratio of densities. This is the orbital distance inside of which loose material (e.g. regolith) on the surface of the satellite closest to the primary would be pulled away, and likewise material on the side"
    ]
  ],
  [
    "The object was thrown from a height of approximately 19.62 meters.",
    [
      "latitude of 45\u00b032'33\". Assuming the standardized value for g and ignoring air resistance, this means that an object falling freely near the Earth's surface increases its velocity by 9.80665 m/s (32.1740 ft/s or 22 mph) for each second of its descent. Thus, an object starting from rest will attain a velocity of 9.80665 m/s (32.1740 ft/s) after one second, approximately 19.62 m/s (64.4 ft/s) after two seconds, and so on, adding 9.80665 m/s (32.1740 ft/s) to each resulting velocity. Also, again ignoring air resistance, any and all objects, when dropped from the same height, will hit the ground at the",
      "typical .30-06 bullet dropping downwards\u2014when it is returning to the ground having been fired upwards, or dropped from a tower\u2014according to a 1920 U.S. Army Ordnance study. Competition speed skydivers fly in a head-down position and can reach speeds of ; the current record is held by Felix Baumgartner who jumped from a height of and reached , though he achieved this speed at high altitude, where extremely thin air presents less drag force. The biologist J. B. S. Haldane wrote, Using mathematical terms, terminal speed\u2014without considering buoyancy effects\u2014is given by where In reality, an object approaches its terminal speed",
      "as a function of time: Using the figure of 56 m/s for the terminal velocity of a human, one finds that after 10 seconds he will have fallen 348 metres and attained 94% of terminal velocity, and after 12 seconds he will have fallen 455 metres and will have attained 97% of terminal velocity. However, when the air density cannot be assumed to be constant, such as for objects or skydivers falling from high altitude, the equation of motion becomes much more difficult to solve analytically and a numerical simulation of the motion is usually necessary. The figure shows the"
    ]
  ],
  [
    "In a completely inelastic collision between two particles that stick together, 100% of the initial kinetic energy is lost.",
    [
      "two bodies stick together and continue moving as a single particle. This second case is the case of completely inelastic collision. For both cases (1) and (2), momentum, mass, and total energy are conserved. However, kinetic energy is not conserved in cases of inelastic collision. A certain fraction of the initial kinetic energy is converted to heat. In case (2), two masses with momentums and collide to produce a single particle of conserved mass traveling at the center of mass velocity of the original system, . The total momentum is conserved. Fig. 3\u201110 illustrates the inelastic collision of two particles",
      "its velocity will be little affected by a collision while the other body will experience a large change. In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy (such as heat or sound). Examples include traffic collisions, in which the effect of lost kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck\u2013Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles. In a perfectly inelastic collision",
      "kinetic energy before the collision is that of the particle with the smaller mass. In another frame, in addition to the reduction of kinetic energy there may be a transfer of kinetic energy from one particle to the other; the fact that this depends on the frame shows how relative this is. With time reversed we have the situation of two objects pushed away from each other, e.g. shooting a projectile, or a rocket applying thrust (compare the derivation of the Tsiolkovsky rocket equation). Partially inelastic collisions are the most common form of collisions in the real world. In this"
    ]
  ],
  [
    "The most strongly reflected wavelength in the thin oil film floating on the surface of the puddle of water when white light is incident on it is determined by the thickness of the film, the refractive index of the film and the angle of incidence of the light.",
    [
      "Thus when white light, which consists of a range of wavelengths, is incident on the film, certain wavelengths (colors) are intensified while others are attenuated. Thin-film interference explains the multiple colors seen in light reflected from soap bubbles and oil films on water. It is also the mechanism behind the action of antireflection coatings used on glasses and camera lenses. The true thickness of the film depends on both its refractive index and on the angle of incidence of the light. The speed of light is slower in a higher-index medium, thus a film is manufactured in proportion to the",
      "formula_18 is the film thickness, formula_19 is the refractive index of the film, formula_20 is the angle of incidence of the wave on the lower boundary, formula_21 is an integer, and formula_10 is the wavelength of light. In the case of a thin oil film, a layer of oil sits on top of a layer of water. The oil may have an index of refraction near 1.5 and the water has an index of 1.33. As in the case of the soap bubble, the materials on either side of the oil film (air and water) both have refractive indices that",
      "Thin-film optics Thin-film optics is the branch of optics that deals with very thin structured layers of different materials. In order to exhibit thin-film optics, the thickness of the layers of material must be on the order of the wavelengths of visible light (about 500 nm). Layers at this scale can have remarkable reflective properties due to light wave interference and the difference in refractive index between the layers, the air, and the substrate. These effects alter the way the optic reflects and transmits light. This effect, known as thin-film interference, is observable in soap bubbles and oil slicks. More"
    ]
  ],
  [
    "The mass of the nonrelativistic particle with a charge twice that of an electron moving through a uniform magnetic field with a strength of \u03c0/4 tesla and a cyclotron frequency of 1,600 hertz is 3.183 x 10^-26 kg.",
    [
      "frequency of the electric field is given by Where formula_2 is the angular frequency of the electric field, formula_3 is the charge on the particle, formula_4 is the magnetic field, and formula_5 is the mass of the particle. This makes the assumption that the particle is classical, and doesn't experience relativistic phenomena such as length contraction. These effects start to become significant when formula_6, the velocity of the particle greater than formula_7. To correct for this, the relativistic mass is used instead of the rest mass; thus, a factor of formula_8 multiplies the mass, such that where This is then",
      "Cyclotrons can only accelerate particles to speeds much slower than the speed of light, nonrelativistic speeds. For nonrelativistic particles, the centripetal force formula_2 required to keep them in their curved path is where formula_4 is the particle's mass, formula_5 its velocity, and formula_6 is the radius of the path. This force is provided by the Lorentz force formula_7 of the magnetic field formula_8 where formula_10 is the particle's charge. The particles reach their maximum energy at the periphery of the dees, where the radius of their path is formula_11 the radius of the dees. Equating these two forces So the",
      "particle mass \"m\", its charge \"q\", velocity \"v\", and the circular path radius \"r\", also called gyroradius. The angular speed of the rotation is then: Giving the rotational frequency (being the cyclotron frequency) as: It is notable that the cyclotron frequency is independent of the radius and velocity and therefore independent of the particle's kinetic energy; all particles with the same charge-to-mass ratio rotate around magnetic field lines with the same frequency. The cyclotron frequency is also useful in non-uniform magnetic fields, in which (assuming slow variation of magnitude of the magnetic field) the movement is approximately helical - in"
    ]
  ],
  [
    "Ground speed does not directly affect an aircraft's performance in relation to airspeed and wind speed/direction, as it is simply the horizontal speed of the aircraft relative to the ground.",
    [
      "doesn't influence the aircraft performance such as rate of climb. Ground speed Ground speed is the horizontal speed of an aircraft relative to the ground. An aircraft heading vertically would have a ground speed of zero. Information displayed to passengers through the entertainment system often gives the aircraft ground speed rather than airspeed. Ground speed can be determined by the vector sum of the aircraft's true airspeed and the current wind speed and direction; a headwind subtracts from the ground speed, while a tailwind adds to it. Winds at other angles to the heading will have components of either headwind",
      "Ground speed Ground speed is the horizontal speed of an aircraft relative to the ground. An aircraft heading vertically would have a ground speed of zero. Information displayed to passengers through the entertainment system often gives the aircraft ground speed rather than airspeed. Ground speed can be determined by the vector sum of the aircraft's true airspeed and the current wind speed and direction; a headwind subtracts from the ground speed, while a tailwind adds to it. Winds at other angles to the heading will have components of either headwind or tailwind as well as a crosswind component. An airspeed",
      "in both aircraft and Engine performance. High elevation airports are characterized by low pressure and high ambient temperatures. The True Airspeed (TAS) will be higher than the Indicated airspeed indicated by the Airspeed indicator to the pilot in air of low density. This increase in TAS leads to greater touchdown speed hence increases the landing roll. More energy has to be absorbed by the brakes thus demanding the need of a longer runway. An increased density altitude means a longer landing distance. The headwind reduces the landing distance for an aircraft. Landing into a headwind reduces the Ground Speed (GS)"
    ]
  ],
  [
    "The energy of the photon emitted when a positronium atom transitions from the state with n=3 to a state with n=1 is approximately 5.87433 \u00b5eV.",
    [
      "atomic transition of an electron between the two hyperfine levels of the hydrogen 1s ground state that have an energy difference of ~ . It is called the \"spin-flip transition\". The frequency, , of the quanta that are emitted by this transition between two different energy levels is given by the Planck\u2013Einstein relation . According to that relation, the photon energy of a 1,420,405,751.7667 Hz photon is ~ 5.87433 \u00b5eV. The constant of proportionality, , is known as the Planck constant. The ground state of neutral hydrogen consists of an electron bound to a proton. Both the electron and the",
      "the final state. The most probable is the creation of two or more gamma ray photons. Conservation of energy and linear momentum forbid the creation of only one photon. (An exception to this rule can occur for tightly bound atomic electrons.) In the most common case, two photons are created, each with energy equal to the rest energy of the electron or positron (). A convenient frame of reference is that in which the system has no net linear momentum before the annihilation; thus, after collision, the gamma rays are emitted in opposite directions. It is also common for three",
      "difference in energies between the two states as a photon. The photon will have frequency \"\u03bd\" and energy \"h\u03bd\", given by: where \"h\" is Planck's constant. Alternatively, if the excited-state atom is perturbed by an electric field of frequency \"\u03bd\", it may emit an additional photon of the same frequency and in phase, thus augmenting the external field, leaving the atom in the lower energy state. This process is known as stimulated emission. In a group of such atoms, if the number of atoms in the excited state is given by \"N\", the rate at which stimulated emission occurs is"
    ]
  ],
  [
    "The period of a satellite in a synchronous orbit is equal to the rotational period of the planet it is orbiting.",
    [
      "Synchronous orbit A synchronous orbit is an orbit in which an orbiting body (usually a satellite) has a period equal to the average rotational period of the body being orbited (usually a planet), and in the same direction of rotation as that body. A synchronous orbit is an orbit in which the orbiting object (for example, an artificial satellite or a moon) takes the same amount of time to complete an orbit as it takes the object it is orbiting to rotate once. A satellite in a synchronous orbit that is both equatorial and circular will appear to be suspended",
      "body, it always goes in hand with synchronous rotation of the satellite. This is because the smaller body becomes tidally locked faster, and by the time a synchronous orbit is achieved, it has had a locked synchronous rotation for a long time already. Synchronous orbit A synchronous orbit is an orbit in which an orbiting body (usually a satellite) has a period equal to the average rotational period of the body being orbited (usually a planet), and in the same direction of rotation as that body. A synchronous orbit is an orbit in which the orbiting object (for example, an",
      "Rotation period In astronomy, the rotation period of a celestial object is the time that it takes to complete one revolution around its axis of rotation relative to the background stars. It differs from the planet's solar day, which includes an extra fractional rotation needed to accommodate the portion of the planet's orbital period during one day. For solid objects, such as rocky planets and asteroids, the rotation period is a single value. For gaseous/fluid bodies, such as stars and gas giants, the period of rotation varies from the equator to the poles due to a phenomenon called differential rotation."
    ]
  ],
  [
    "A cannula needle in an artery (usually radial, femoral, dorsalis pedis or brachial) would be most appropriate for a rapid blood transfusion in a patient as of 2020 medical knowledge.",
    [
      "reduced to as short as 75 seconds based on a recent article by the CBC. This is now causing an ethical debate as to whether physicians will declare death sooner than is currently required. This is similar to a normal multi-organ retrieval, but prioritises rapid cannulation, perfusion and cooling with ice, with dissection following later. If only the kidneys are suitable for retrieval, either rapid retrieval or cannulation with DBTL catheter can be used. Use of a DBTL catheter allows relatives of the deceased to see them after death, but the donor must be taken to the operating room as",
      "by placing a cannula needle in an artery (usually radial, femoral, dorsalis pedis or brachial). The cannula is inserted either via palpation or with the use of ultrasound guidance. The cannula must be connected to a sterile, fluid-filled system, which is connected to an electronic pressure transducer. The advantage of this system is that pressure is constantly monitored beat-by-beat, and a waveform (a graph of pressure against time) can be displayed. This invasive technique is regularly employed in human and veterinary intensive care medicine, anesthesiology, and for research purposes. Cannulation for invasive vascular pressure monitoring is infrequently associated with complications",
      "oxygenator which is believed to produce less systemic inflammation and decrease the propensity for blood to clot in the CPB circuit. Multiple cannulae are sewn into the patient's body in a variety of locations, depending on the type of surgery. A venous cannula removes oxygen depleted venous blood from a patient's body. An arterial cannula infuses oxygen-rich blood into the arterial system. A cardioplegia cannula delivers a cardioplegia solution to cause the heart to stop beating. Some commonly used cannulation sites: A CPB circuit consists of a systemic circuit for oxygenating blood and reinfusing blood into a patient's body (bypassing"
    ]
  ],
  [
    "A marathon runner's finishing time is determined by physiological characteristics such as aerobic capacity, running economy, stroke volume, blood volume, hemoglobin levels, pulmonary diffusion, mitochondria enzyme activity, and capillary density.",
    [
      "certain physiological characteristics of marathon runners exist. The differing efficiency of certain physiological features in marathon runners evidence the variety of finishing times among elite marathon runners that share similarities in many physiological characteristics. Aside from large aerobic capacities and other biochemical mechanisms, external factors such as the environment and proper nourishment of a marathon runner can further the insight as to why marathon performance is variable despite ideal physiological characteristics obtained by a runner. The first marathon ever run was an unintentional 25 mile trek performed by Pheidippides. Pheidippides was a Greek soldier who ran to Athens from the",
      "oxygen saturation often seen in well trained athletes such as marathoner's. Not all inspired air and its components make it into the pulmonary system due to the human body's anatomical dead space, which, in terms of exercise, is a source of oxygen wasted. Despite being one of the most salient predictors of marathon performance, a large VO is only one of the factors that may effect performance in a marathon. A marathoner's running economy is their sub maximal requirement for oxygen at specific speeds. This concept of running economy helps to explain different marathon time's for runners with very similar",
      "a greater stroke volume. A concomitant decrease in stroke volume occurs with the initial increase in heart rate at the onset of exercise. Despite an increase in cardiac dimensions, a marathoner's aerobic capacity is confined to this capped and ever decreasing heart rate. The amount of oxygen that blood can carry depends on blood volume, which increases during a race, and the amount of hemoglobin in blood. Other physiological factors affecting a marathon runner's aerobic capacity include pulmonary diffusion, mitochondria enzyme activity, and capillary density. A long-distance runner's running economy is their steady state requirement for oxygen at specific speeds"
    ]
  ],
  [
    "The overall rate of Alzheimer's disease in people with dementia in the UK is estimated to be between 5-8 per thousand person-years, with half of new dementia cases each year being attributed to Alzheimer's disease.",
    [
      "than in the general population. Two main measures are used in epidemiological studies: incidence and prevalence. Incidence is the number of new cases per unit of person\u2013time at risk (usually number of new cases per thousand person\u2013years); while prevalence is the total number of cases of the disease in the population at any given time. Regarding incidence, cohort longitudinal studies (studies where a disease-free population is followed over the years) provide rates between 10 and 15 per thousand person\u2013years for all dementias and 5\u20138 for AD, which means that half of new dementia cases each year are AD. Advancing age",
      "0.379% of people worldwide had dementia, and that the prevalence would increase to 0.441% in 2015 and to 0.556% in 2030. Other studies have reached similar conclusions. Another study estimated that in 2006, 0.40% of the world population (range 0.17\u20130.89%; absolute number , range ) were afflicted by AD, and that the prevalence rate would triple and the absolute number would quadruple by 2050. The ancient Greek and Roman philosophers and physicians associated old age with increasing dementia. It was not until 1901 that German psychiatrist Alois Alzheimer identified the first case of what became known as Alzheimer's disease, named",
      "Alzheimer's disease Alzheimer's disease (AD), also referred to simply as Alzheimer's, is a chronic neurodegenerative disease that usually starts slowly and worsens over time. It is the cause of 60\u201370% of cases of dementia. The most common early symptom is difficulty in remembering recent events (short-term memory loss). As the disease advances, symptoms can include problems with language, disorientation (including easily getting lost), mood swings, loss of motivation, not managing self care, and behavioural issues. As a person's condition declines, they often withdraw from family and society. Gradually, bodily functions are lost, ultimately leading to death. Although the speed of"
    ]
  ],
  [
    "Non-immune forms of anaphylaxis can be differentiated from asthma, syncope, and panic attacks by the presence of wheezing, a blocked airway, and cyanosis, as well as symptoms affecting blood circulation such as a weak pulse, pale skin, and fainting.",
    [
      "former can be indicated by wheezing, a blocked airway and cyanosis, the latter by weak pulse, pale skin, and fainting. When these symptoms occur the allergic reaction is called anaphylaxis. Anaphylaxis occurs when IgE antibodies are involved, and areas of the body that are not in direct contact with the food become affected and show severe symptoms. Untreated, this can proceed to vasodilation, a low blood pressure situation called anaphylactic shock, and death (very rare). Non-IgE mediated reactions are slower to appear, and tend to manifest as gastrointestinal symptoms, without cutaneous or respiratory symptoms. Within non-IgE reactions, clinicians distinguish among",
      "can begin when the respiratory tract or blood circulation is affected. The former can be indicated through wheezing and cyanosis. Poor blood circulation leads to a weak pulse, pale skin and fainting. A severe case of an allergic reaction, caused by symptoms affecting the respiratory tract and blood circulation, is called anaphylaxis. When symptoms are related to a drop in blood pressure, the person is said to be in anaphylactic shock. Anaphylaxis occurs when IgE antibodies are involved, and areas of the body that are not in direct contact with the food become affected and show symptoms. Those with asthma",
      "the whole face, difficulty swallowing, runny or congested nose, hoarse voice, wheezing, shortness of breath, diarrhea, abdominal pain, lightheadedness, fainting, nausea and vomiting. Symptoms of allergies vary from person to person and may also vary from incident to incident. Serious danger regarding allergies can begin when the respiratory tract or blood circulation is affected. The former can be indicated by wheezing, a blocked airway and cyanosis, the latter by weak pulse, pale skin, and fainting. When these symptoms occur, the allergic reaction is called anaphylaxis. Anaphylaxis occurs when IgE antibodies are involved, and areas of the body that are not"
    ]
  ],
  [
    "Gas buildup in ostomy pouches can be prevented by using pouches with special charcoal filtered vents that allow gas to escape and prevent ballooning.",
    [
      "basic types: open-end (drainable) and closed-end (disposable). The use of open-end vs. closed-end pouches is dependent on the frequency in which an individual needs to empty the contents, as well as economics. Gas is created during digestion, and an airtight pouch will collect this and inflate. To prevent this some pouches are available with special charcoal filtered vents that will allow the gas to escape, and prevent ballooning at night. Some odor can be expelled through the charcoal filter especially if sufficient deodorant is not used in the pouch. Pouch covers are helpful to disguise the plastic pouch when it",
      "while protecting the surrounding skin from contamination. Ostomy pouching systems are air- and water-tight and allow the wearer to lead an active lifestyle that can include all forms of sports and recreation. Ostomy pouching systems are also sometimes referred to as an appliance, where the term appliance refers to a prosthesis, as a mechanical replacement for a biological function. Most wafers/baseplates, also called ostomy barriers, are manufactured using pectin or similar organic material and are available in a wide variety of sizes to accommodate a person's particular anatomy. The internal opening must be the correct size to accommodate the individual's",
      "Ostomy pouching system An ostomy pouching system Pouching systems usually consist of a collection pouch plastic bag, known as a one-piece system or, in some instances involves a mounting plate, commonly called a flange, wafer or a baseplate, and a collection pouch that is attached mechanically or with an adhesive in an airtight seal, known as a two-piece system. The selection of systems varies greatly between individuals and is often based on personal preference and lifestyle. Ostomy pouching systems collect waste that is output from a stoma. The pouching system allows the stoma to drain into a sealed collection pouch,"
    ]
  ],
  [
    "involves a quick acceleration phase followed by a velocity maintenance phase, with runners tilting their upper body forward initially to direct ground reaction forces more horizontally, then straightening out their torso into an upright position to reach and maintain high top speeds throughout the race.",
    [
      "Biomechanics of sprint running Sprinting involves a quick acceleration phase followed by a velocity maintenance phase. During the initial stage of sprinting, the runners have their upper body tilted forward in order to direct ground reaction forces more horizontally. As they reach their maximum velocity, the torso straightens out into an upright position. The goal of sprinting is to reach and maintain high top speeds to cover a set distance in the shortest possible time. A lot of research has been invested in quantifying the biological factors and mathematics that govern sprinting. In order to achieve these high velocities, it",
      "for sprinting. The tactic relies upon keeping greater energy reserves than your opponent until the last part of the race in order to be able to reach the finish point first. It is the opposing tactic to keeping a steady optimal pace throughout a race to maximise your energy efficiency (see running economy). In track and field, distances from 1500 metres upwards often feature sprint finishes. They can also be found in cross country and road running events, even up to the marathon distance. A runner's ability to sprint at the end of a race is also known as their",
      "position as the contest progresses and momentum is gained. Athletes remain in the same lane on the running track throughout all sprinting events, with the sole exception of the 400 m indoors. Races up to 100 m are largely focused upon acceleration to an athlete's maximum speed. All sprints beyond this distance increasingly incorporate an element of endurance. Human physiology dictates that a runner's near-top speed cannot be maintained for more than thirty seconds or so as lactic acid builds up, and leg muscles begin to be deprived of oxygen. The 60 metres is a common indoor event and it"
    ]
  ],
  [
    "Yes, the theorem for a single point charge also holds for two opposite point charges connected together in the form of an electric dipole.",
    [
      "that if the theorem holds for a single point charge then it would also hold for two opposite point charges connected together. In particular, it would hold in the limit where the distance between the charges is decreased to zero while maintaining the dipole moment \u2013 that is, it would hold for an electric dipole. But if the theorem holds for an electric dipole, then it will also hold for a magnetic dipole, since the (static) force/energy equations take the same form for both electric and magnetic dipoles. As a practical consequence, this theorem also states that there is no",
      "Dipole In electromagnetism, there are two kinds of dipoles: Dipoles can be characterized by their dipole moment, a vector quantity. For the simple electric dipole given above, the electric dipole moment points from the negative charge towards the positive charge, and has a magnitude equal to the strength of each charge times the separation between the charges. (To be precise: for the definition of the dipole moment, one should always consider the \"dipole limit\", where, for example, the distance of the generating charges should \"converge\" to 0 while simultaneously, the charge strength should \"diverge\" to infinity in such a way",
      "equation above, one for each charge. The polarization of a dipole is formula_2 where formula_3 is the distance between the two charges. For a point dipole, the distance is infinitesimal, formula_4 Taking into account that the two charges have opposite signs, the force takes the form Notice that the formula_6 cancel out. Multiplying through by the charge, formula_7, converts position, formula_8, into polarization, formula_9, where in the second equality, it has been assumed that the dielectric particle is linear (i.e. formula_11). In the final steps, two equalities will be used: (1) A Vector Analysis Equality, (2) One of Maxwell's Equations."
    ]
  ],
  [
    "Ingesting small amounts of sodium bicarbonate can improve middle distance running performance by acting as a supplement for athletes in speed-based events lasting about 1-7 minutes (source).",
    [
      "with a cotton swab dipped in the solution. Sodium bicarbonate is used as a cattle feed supplement, in particular as a buffering agent for the rumen. Small amounts of sodium bicarbonate have been shown to be useful as a supplement for athletes in speed-based events, such as middle-distance running, lasting about 1\u20137 minutes. However, overdose is a serious risk because sodium bicarbonate is slightly toxic; and gastrointestinal irritation is of particular concern. Additionally, this practice causes a significant increase in dietary sodium. Sodium bicarbonate is used in a process for removing paint and corrosion called sodablasting; the process is particularly",
      "Once consumed, it causes internal organs of cockroaches to burst due to gas collection. Sodium bicarbonate can be an effective way of controlling fungal growth, and in the United States is registered by the Environmental Protection Agency as a biopesticide. Sodium bicarbonate can be administered to pools, spas, and garden ponds to raise the total alkalinity. This will also raise the pH level and make maintaining proper pH easier. In the event that the pH is high, sodium bicarbonate should not be used to adjust the pH. Sodium bicarbonate is one of the main components of the common \"black snake\"",
      "and carbon dioxide: Sodium bicarbonate reacts with bases such as sodium hydroxide to form carbonates: Sodium bicarbonate reacts with carboxyl groups in proteins to give a brisk effervescence from the formation of . This reaction is used to test for the presence of carboxylic groups in protein. Above , sodium bicarbonate gradually decomposes into sodium carbonate, water, and carbon dioxide. The conversion is fast at : Most bicarbonates undergo this dehydration reaction. Further heating converts the carbonate into the oxide (above ): These conversions are relevant to the use of NaHCO as a fire-suppression agent (\"BC powder\") in some dry-powder"
    ]
  ],
  [
    "Codons are composed of three-nucleotide sequences and function as the \"words\" in the genetic code, specifying the correspondence between codons and amino acids during protein translation.",
    [
      "RNA also contains the base uracil in place of thymine. RNA molecules are less stable than DNA and are typically single-stranded. Genes that encode proteins are composed of a series of three-nucleotide sequences called codons, which serve as the \"words\" in the genetic \"language\". The genetic code specifies the correspondence during protein translation between codons and amino acids. The genetic code is nearly the same for all known organisms. The total complement of genes in an organism or cell is known as its genome, which may be stored on one or more chromosomes. A chromosome consists of a single, very",
      "where the coding sequences are not equally distributed. The genetic code has 64 codons of which 3 function as termination codons: there are only 20 amino acids normally present in proteins. (There are two uncommon amino acids\u2014selenocysteine and pyrrolysine\u2014found in a limited number of proteins and encoded by the stop codons\u2014TGA and TAG respectively.) The mismatch between the number of codons and amino acids allows several codons to code for a single amino acid - such codons normally differ only at the third codon base position. Multivariate statistical analysis of codon use within genomes with unequal quantities of coding sequences",
      "added next during protein synthesis. With some exceptions, a three-nucleotide codon in a nucleic acid sequence specifies a single amino acid. The vast majority of genes are encoded with a single scheme (see the RNA codon table). That scheme is often referred to as the canonical or standard genetic code, or simply \"the\" genetic code, though variant codes (such as in human mitochondria) exist. While the \"genetic code\" determines a protein's amino acid sequence, other genomic regions determine when and where these proteins are produced according to various \"gene regulatory codes\". Efforts to understand how proteins are encoded began after"
    ]
  ],
  [
    "Fatty acids are transported into the mitochondria via specific transport proteins in the cytosol, such as the SLC27 family fatty acid transport protein, before undergoing beta-oxidation for energy production.",
    [
      "activated, the acyl CoA is transported into the mitochondrial matrix. This occurs via a series of similar steps: It is important to note that \"carnitine acyltransferase I\" undergoes allosteric inhibition as a result of malonyl-CoA, an intermediate in fatty acid biosynthesis, in order to prevent futile cycling between beta-oxidation and fatty acid synthesis. The mitochondrial oxidation of fatty acids takes place in three major steps: 1.\u03b2-oxidation: conversion of fatty acids into 2-carbon acetyl Co-A units. 2.entry of acetyl Co-A into TCA cycle to yield energy. 3.finally, the electron transport chain in the mitochondria.though in this step, no direct participation of",
      "overall reaction for one cycle of beta oxidation is: Fatty acid catabolism consists of: Free fatty acids cannot penetrate any biological membrane due to their negative charge. Free fatty acids must cross the cell membrane through specific transport proteins, such as the SLC27 family fatty acid transport protein. Once in the cytosol, the following processes bring fatty acids into the mitochondrial matrix so that beta-oxidation can take place. Once the fatty acid is inside the mitochondrial matrix, beta-oxidation occurs by cleaving two carbons every cycle to form acetyl-CoA. The process consists of 4 steps. Fatty acids are oxidized by most",
      "fatty acids, the reaction continues. Once inside the mitochondria, the \u03b2-oxidation of fatty acids occurs via five recurring steps: Fatty acid degradation Fatty acid degradation is the process in which fatty acids are broken down into their metabolites, in the end generating acetyl-CoA, the entry molecule for the citric acid cycle, the main energy supply of animals. It includes three major steps: Initially in the process of degradation, fatty acids are stored in fat cells (adipocytes). The breakdown of this fat is known as lipolysis. The products of lipolysis, free fatty acids, are released into the bloodstream and circulate throughout"
    ]
  ],
  [
    "B (rhesus negative) can safely receive type O negative blood.",
    [
      "do not have an antibody, since the greatest risk of Rh incompatible blood is to current or future pregnancies. For RBCs, type O negative blood is considered a \"universal donor\" as recipients with types A, B, or AB can almost always receive O negative blood safely. Type AB positive is considered a \"universal recipient\" because they can receive the other ABO/Rh types safely. These are not truly universal, as other red cell antigens can further complicate transfusions. There are many other human blood group systems and most of them are only rarely associated with transfusion problems. A screening test is",
      "less risk of a serious transfusion reaction because it is both ABO compatible and Rhesus (Rh)-compatible. Universal donor blood, which is both type O and Rh negative, can be given if the recipient's blood group is not known, as may happen in an emergency. Some institutions will only release O+ for male and O- blood for female patients. This serves two purposes. First, it preserves the lower stock of O- blood and secondly, this eliminates the risk of O- negative mothers forming anti-D (Rh) antibodies from exposure to O+ blood. Anti-D (Rh) can cross the placenta during pregnancy and attack",
      "clinical use for hemophilia, as the risks of infection transmission that occur with pooled blood products are avoided. An Rh D-negative patient who does not have any anti-D antibodies (never being previously sensitized to D-positive RBCs) can receive a transfusion of D-positive blood once, but this would cause sensitization to the D antigen, and a female patient would become at risk for hemolytic disease of the newborn. If a D-negative patient has developed anti-D antibodies, a subsequent exposure to D-positive blood would lead to a potentially dangerous transfusion reaction. Rh D-positive blood should never be given to D-negative women of"
    ]
  ],
  [
    "One major characteristic used to identify a lower motor neuron lesion in the arm is flaccid paralysis accompanied by loss of muscle tone.",
    [
      "motor neuron degeneration is amyotrophic lateral sclerosis. Lower motor neuron lesion A lower motor neuron lesion is a lesion which affects nerve fibers traveling from the ventral horn or anterior grey column of the spinal cord to the relevant muscle(s) \u2013 the lower motor neuron. One major characteristic used to identify a lower motor neuron lesion is flaccid paralysis \u2013 paralysis accompanied by loss of muscle tone. This is in contrast to an upper motor neuron lesion, which often presents with spastic paralysis \u2013 paralysis accompanied by severe hypertonia. The extensor Babinski reflex is usually absent. Muscle paresis/paralysis, hypotonia/atonia, and",
      "Lower motor neuron lesion A lower motor neuron lesion is a lesion which affects nerve fibers traveling from the ventral horn or anterior grey column of the spinal cord to the relevant muscle(s) \u2013 the lower motor neuron. One major characteristic used to identify a lower motor neuron lesion is flaccid paralysis \u2013 paralysis accompanied by loss of muscle tone. This is in contrast to an upper motor neuron lesion, which often presents with spastic paralysis \u2013 paralysis accompanied by severe hypertonia. The extensor Babinski reflex is usually absent. Muscle paresis/paralysis, hypotonia/atonia, and hyporeflexia/areflexia are usually seen immediately following an",
      "must be observed. Flail arm syndrome, also called brachial amyotrophic diplegia, is characterized by lower motor neuron damage in the cervical spinal cord only, leading to gradual onset of weakness in the proximal arm muscles and decreased or absent reflexes. Flail leg syndrome, also called leg amyotrophic diplegia, is characterized by lower motor neuron damage in the lumbosacral spinal cord only, leading to gradual onset of weakness in the legs and decreased or absent reflexes. Isolated bulbar ALS is characterized by upper and/or lower motor neuron damage in the bulbar region only, leading to gradual onset of difficulty with speech"
    ]
  ],
  [
    "The statement \"Logical truth is not true\" is not a logical truth.",
    [
      "false. One statement logically implies another when it is logically incompatible with the negation of the other. A statement is logically true if, and only if its opposite is logically false. The opposite statements must contradict one another. In this way all logical connectives can be expressed in terms of preserving logical truth. The logical form of a sentence is determined by its semantic or syntactic structure and by the placement of logical constants. Logical constants determine whether a statement is a logical truth when they are combined with a language that limits its meaning. Therefore, until it is determined",
      "or a false truth. Non-classical logic is the name given to formal systems which differ in a significant way from standard logical systems such as propositional and predicate logic. There are several ways in which this is done, including by way of extensions, deviations, and variations. The aim of these departures is to make it possible to construct different models of logical consequence and logical truth. Logical truth Logical truth is one of the most fundamental concepts in logic, and there are different theories on its nature. A logical truth is a statement which is true, and remains true under",
      "is not true (assuming that we are dealing with concrete statements that are either true or not true): We can apply the same process the other way round: We also know that B is either true or not true. If B is not true, then A is also not true. However, it is given that A is true; so, the assumption that B is not true leads to contradiction and must be false. Therefore, B must be true: Combining the two proved statements makes them logically equivalent: Logical equivalence between two propositions means that they are true together or false"
    ]
  ],
  [
    "The breakdown of glycogen into glucose-1-phosphate and glycogen (n-1) is initiated by the enzyme glycogen phosphorylase during muscle degradation (Glycogenolysis).",
    [
      "muscle during exercise. In the muscles, glycogen ensures a rapidly accessible energy source for movement. Glycogenesis refers to the process of synthesizing glycogen. In humans, excess glucose is converted to glycogen via this process. Glycogen is a highly branched structure, consisting of glucose, in the form of glucose-6-phosphate, linked together. The branching of glycogen increases its solubility, and allows for a higher number of glucose molecules to be accessible for breakdown. Glycogenesis occurs primarily in the liver, skeletal muscles, and kidney. The pentose phosphate pathway is an alternative method of oxidizing glucose. It occurs in the liver, adipose tissue, adrenal",
      "to blood sugar levels by glucagon and insulin, and stimulated by epinephrine during the fight-or-flight response. In myocytes, glycogen degradation may also be stimulated by neural signals. Parenteral (intravenous) administration of glucagon is a common human medical intervention in diabetic emergencies when sugar cannot be given orally. It can also be administered intramuscularly. Glycogenolysis Glycogenolysis is the breakdown of glycogen (n) to glucose-1-phosphate and glycogen (n-1). Glycogen branches are catabolized by the sequential removal of glucose monomers via phosphorolysis, by the enzyme glycogen phosphorylase. The overall reaction for the breakdown of glycogen to glucose-1-phosphate is: Here, glycogen phosphorylase cleaves the",
      "the fight-or-flight response and the regulation of glucose levels in the blood. In myocytes (muscle cells), glycogen degradation serves to provide an immediate source of glucose-6-phosphate for glycolysis, to provide energy for muscle contraction. In hepatocytes (liver cells), the main purpose of the breakdown of glycogen is for the release of glucose into the bloodstream for uptake by other cells. The phosphate group of glucose-6-phosphate is removed by the enzyme glucose-6-phosphatase, which is not present in myocytes, and the free glucose exits the cell via GLUT2 facilitated diffusion channels in the hepatocyte cell membrane. Glycogenolysis is regulated hormonally in response"
    ]
  ],
  [
    "The two principal contractile proteins found in skeletal muscle are actin and myosin, which interact to cause muscle contraction.",
    [
      "to actin again to repeat the cycle. The combined effect of the myriad power strokes causes the muscle to contract. The wide variety of myosin genes found throughout the eukaryotic phyla were named according to different schemes as they were discovered. The nomenclature can therefore be somewhat confusing when attempting to compare the functions of myosin proteins within and between organisms. Skeletal muscle myosin, the most conspicuous of the myosin superfamily due to its abundance in muscle fibers, was the first to be discovered. This protein makes up part of the sarcomere and forms macromolecular filaments composed of multiple myosin",
      "its axis. In addition to the actin and myosin components that constitute the sarcomere, skeletal muscle fibers also contain two other important regulatory proteins, troponin and tropomyosin, that are necessary for muscle contraction to occur. These proteins are associated with actin and cooperate to prevent its interaction with myosin. Skeletal muscle cells are excitable and are subject to depolarization by the neurotransmitter acetylcholine, released at the neuromuscular junction by motor neurons. Once a cell is sufficiently stimulated, the cell's sarcoplasmic reticulum releases ionic calcium (Ca2+), which then interacts with the regulatory protein troponin. Calcium-bound troponin undergoes a conformational change that",
      "Differentiation into this state is primarily completed before birth with the cells continuing to grow in size thereafter. Skeletal muscle exhibits a distinctive banding pattern when viewed under the microscope due to the arrangement of cytoskeletal elements in the cytoplasm of the muscle fibers. The principal cytoplasmic proteins are myosin and actin (also known as \"thick\" and \"thin\" filaments, respectively) which are arranged in a repeating unit called a sarcomere. The interaction of myosin and actin is responsible for muscle contraction. Every single organelle and macromolecule of a muscle fiber is arranged to ensure form meets function. The cell membrane"
    ]
  ],
  [
    "Facial nerve injury characterized by paralysis without degeneration of the peripheral nerve is known as Bell's palsy.",
    [
      "The facial muscles are derived from the second branchial/pharyngeal arch. An inability to form facial expressions on one side of the face may be the first sign of damage to the nerve of these muscles. Damage to the facial nerve results in facial paralysis of the muscles of facial expression on the involved side. Paralysis is the loss of voluntary muscle action; the facial nerve has become damaged permanently or temporarily. This damage can occur with a stroke, Bell palsy, or parotid salivary gland cancer (malignant neoplasm) because the facial nerve travels through the gland. The parotid gland can also",
      "facial animation are absent. In Moebius-like syndrome, only one side of the face is affected, but with additional nerve palsies of the affected facial and abducens nerve. Selection of the type of nerve transfer is based on the individualised needs and condition of the patient. Individual factors can be patient age, type of paralysis (partial or complete, uni- or bilateral), denervation time of the mimetic muscles, availability of nerve grafts and medical condition of the patient. If facial paralysis is caused by trauma or tumour surgery, direct reinnervation of the facial muscles (ideally within 72 hours after facial nerve damage)",
      "Facial nerve paralysis Facial nerve paralysis is a common problem that involves the paralysis of any structures innervated by the facial nerve. The pathway of the facial nerve is long and relatively convoluted, so there are a number of causes that may result in facial nerve paralysis. The most common is Bell's palsy, a disease of unknown cause that may only be diagnosed by exclusion. Facial nerve paralysis is characterised by unilateral facial weakness, with other symptoms including loss of taste, hyperacusis and decreased salivation and tear secretion. Other signs may be linked to the cause of the paralysis, such"
    ]
  ],
  [
    "Most often the first symptom of thyroid cancer is a nodule in the thyroid region of the neck, sometimes accompanied by an enlarged lymph node or pain in the anterior region of the neck.",
    [
      "than men. Those of Asian descent are more commonly affected. Rates have increased in the last few decades which is believed to be due to better detection. In 2015 it resulted in 31,900 deaths. Most often the first symptom of thyroid cancer is a nodule in the thyroid region of the neck. However, up to 65% of adults have small nodules in their thyroids, but typically under 10% of these nodules are found to be cancerous. Sometimes the first sign is an enlarged lymph node. Later symptoms that can be present are pain in the anterior region of the neck",
      "often carcinomas, although cancer can occur in any tissue that the thyroid consists of, including cancer of C-cells and lymphomas. Cancers from other sites also rarely lodge in the thyroid. Radiation of the head and neck presents a risk factor for thyroid cancer, and cancer is more common in women than men, occurring at a rate of about 2:1. In most cases, thyroid cancer presents as a painless mass in the neck. It is very unusual for thyroid cancers to present with other symptoms, although in some cases cancer may cause hyperthyroidism. Most malignant thyroid cancers are papillary, followed by",
      "and changes in voice due to an involvement of the recurrent laryngeal nerve. Thyroid cancer is usually found in a euthyroid patient, but symptoms of hyperthyroidism or hypothyroidism may be associated with a large or metastatic well-differentiated tumor. Thyroid nodules are of particular concern when they are found in those under the age of 20. The presentation of benign nodules at this age is less likely, and thus the potential for malignancy is far greater. Thyroid cancers are thought to be related to a number of environmental and genetic predisposing factors, but significant uncertainty remains regarding their causes. Environmental exposure"
    ]
  ],
  [
    "is commonly associated with infection, hepatic tumors, or metabolic disorders.",
    [
      "Hepatomegaly Hepatomegaly is the condition of having an enlarged liver. It is a non-specific medical sign having many causes, which can broadly be broken down into infection, hepatic tumours, or metabolic disorder. Often, hepatomegaly will present as an abdominal mass. Depending on the cause, it may sometimes present along with jaundice. Symptoms having to do with hepatomegaly can include several, among them the individual may experience some weight loss, poor appetite and lethargy (jaundice and bruising may also be present) Among the causes of hepatomegaly are the following: The mechanism of hepatomegaly consists of vascular swelling, inflammation (due to the",
      "chemotherapy and regional radiotherapy, also surgery may be an option in specific situations. Meningococcal group C conjugate vaccine are also used in some cases. In primary biliary cirrhosis ursodeoxycholic acid helps the bloodstream remove bile which may increase survival in some affected individuals. Hepatomegaly Hepatomegaly is the condition of having an enlarged liver. It is a non-specific medical sign having many causes, which can broadly be broken down into infection, hepatic tumours, or metabolic disorder. Often, hepatomegaly will present as an abdominal mass. Depending on the cause, it may sometimes present along with jaundice. Symptoms having to do with hepatomegaly",
      "various causes that are infectious in origin) and deposition of (1) non-hepatic cells or (2) increased cell contents (such due to iron in hemochromatosis or hemosiderosis and fat in fatty liver disease) Suspicion of hepatomegaly indicates a thorough medical history and physical examination, wherein the latter typically includes an increased liver span. On abdominal ultrasonography, the liver can be measured by the \"maximum dimension\" on a sagittal plane view through the midclavicular line, which is normally up to 18 cm in adults. It is also possible to measure the \"cranio-caudal dimension\", which is normally up to 15 cm in adults."
    ]
  ],
  [
    "Decreased reflexes generally indicate a peripheral problem, while lively or exaggerated reflexes indicate a central nervous system issue (Myotatic reflexes).",
    [
      "Reflex A reflex, or reflex action, is an involuntary and nearly instantaneous movement in response to a stimulus. A reflex is made possible by neural pathways called reflex arcs which can act on an impulse before that impulse reaches the brain. The reflex is then an automatic response to a stimulus that does not receive or need conscious thought. Myotatic reflexes The myotatic reflexes (also known as \"deep tendon reflexes\"), provide information on the integrity of the central nervous system and peripheral nervous system. Generally, decreased reflexes indicate a peripheral problem, and lively or exaggerated reflexes a central one. A",
      "activity of a reflex on a scale from 0 to 4. While 2+ is considered normal, some healthy individuals are hypo-reflexive and register all reflexes at 1+, while others are hyper-reflexive and register all reflexes at 3+. Reflex A reflex, or reflex action, is an involuntary and nearly instantaneous movement in response to a stimulus. A reflex is made possible by neural pathways called reflex arcs which can act on an impulse before that impulse reaches the brain. The reflex is then an automatic response to a stimulus that does not receive or need conscious thought. Myotatic reflexes The myotatic",
      "as follows: Phase 1 is characterized by a complete loss\u2014or weakening\u2014of all reflexes below the SCI. This phase lasts for a day. The neurons involved in various reflex arcs normally receive a basal level of excitatory stimulation from the brain. After an SCI, these cells lose this input, and the neurons involved become hyperpolarized and therefore less responsive to stimuli. Phase 2 occurs over the next two days, and is characterized by the return of some, but not all, reflexes below the SCI. The first reflexes to reappear are polysynaptic in nature, such as the bulbocavernosus reflex. Monosynaptic reflexes, such"
    ]
  ],
  [
    "The most common cause of facial nerve palsy is Bell's palsy (idiopathic facial palsy).",
    [
      "the functioning of individual cranial nerves, and detect specific impairments. The facial nerve is the seventh of 12 cranial nerves. This cranial nerve controls the muscles in the face. Facial nerve palsy is more abundant in older adults than in children and is said to affect 15-40 out of 100,000 people per year. This disease comes in many forms which include congenital, infectious, traumatic, neoplastic, or idiopathic. The most common cause of this cranial nerve damage is Bell's palsy (idiopathic facial palsy) which is a paralysis of the facial nerve. Although Bell's palsy is more prominent in adults it seems",
      "employed in the most severe instances (5/6 or 6/6) because in the other cases there is clear evidence that the nerve is mostly intact. Even so, it may be helpful to chart a patient\u2019s progress beginning at the lowest levels of damage. Perhaps the most common cause of damage to the facial nerve is Bell\u2019s palsy (BP). It has a reported incidence of about 0.00015% within the world population each year, and in up to approximately 10% of those cases, the disorder will recur. The etiology of this disease is currently unknown, but hypotheses include infections, genetic predisposition, environmental factors,",
      "drops or an eyepatch. Surgery is generally not recommended. Often signs of improvement begin within 14 days, with complete recovery within six months. A few may not recover completely or have a recurrence of symptoms. Bell's palsy is the most common cause of one-sided facial nerve paralysis (70%). It occurs in 1 to 4 per 10,000 people per year. About 1.5% of people are affected at some point in their life. It most commonly occurs in people between ages 15 and 60. Males and females are affected equally. It is named after Scottish surgeon Charles Bell (1774\u20131842), who first described"
    ]
  ],
  [
    "A Colles fracture is typically treated by putting a splint on the fractured area to decrease pain, bleeding, and movement until the bones are healed.",
    [
      "have pain and will still be able to move, medical help must be sought out immediately. To decrease the pain, bleeding, and movement a physician will put a splint on the fractured area. Treatment for a fracture follows a simple rule: the bones have to be aligned correctly and prevented from moving out of place until the bones are healed. The specific treatment applied depends on how severe the fracture is, if it\u2019s an open or closed fracture, and the specific bone involved in the fracture (a hip fracture is treated differently from a forearm fracture for example) Different treatments",
      "the patient recovers, the fractured bone begins to grow together. While the bone is growing, the frame is adjusted by means of turning the nuts, thus increasing the space between two rings. As the rings are connected to opposite sides of the fracture, this adjustment, done four times a day, moves the now-healing fracture apart by approximately one millimeter per day. The incremental daily increases result in a considerable lengthening of the limb over time. Once the lengthening phase is complete, the apparatus stays on the limb for a consolidation period. The patient is able to fully weight bear on",
      "Bone heals by formation of a callus over the defective area. Speed and quality of healing is directly related to the blood supply and fracture stability. Rest is required immediately following injury to reduce movement of the fracture site. Stability may be improved through use of surgical implants or casting, depending on the location of extent of the fracture. Shock wave therapy is sometimes employed in the case of splint bone fracture or stress fractures to the cannon bones, to improve blood flow to the area. Fractures within a joint, such as chip fractures in the knee, hock, or fetlock,"
    ]
  ],
  [
    "If a transfusion reaction is suspected, immediate discontinuation of the transfusion, notification of healthcare provider, and initiation of appropriate treatment is essential to prevent further complications.",
    [
      "severe, even fatal HDN can occur. An indirect coombs needs to be run in cases of anti-C, anti-c, and anti-M. Anti-M also recommends antigen testing to rule out the presence of HDN. Once a woman has antibodies, she is at high risk for a transfusion reaction. For this reason, she must carry a medical alert card at all times and inform all doctors of her antibody status. \"Acute hemolytic transfusion reactions may be either immune-mediated or nonimmune-mediated. Immune-mediated hemolytic transfusion reactions caused by immunoglobulin M (IgM) anti-A, anti-B, or anti-A,B typically result in severe, potentially fatal complement-mediated intravascular hemolysis. Immune-mediated",
      "cell transfusion and forms an alloantibody (anti-Jka); upon subsequent transfusion with Jka-antigen positive red blood cells, the patient may have a delayed hemolytic transfusion reaction as their anti-Jka antibody hemolyzes the transfused Jka-antigen positive red blood cells. Other common blood groups with this reaction are Duffy, Rhesus and Kell. Delayed hemolytic transfusion reaction A delayed hemolytic transfusion reaction (DHTR) is a type of transfusion reaction. According to the Centers for Disease Control's (CDC) National Healthcare Safety Network's (NHSN) Hemovigilance Module, it is defined as: DEFINITIVE DIAGNOSIS: AND EITHER AND EITHER PROBABLE DIAGNOSIS: Delayed blood transfusion reaction occurs more frequently (1",
      "Delayed hemolytic transfusion reaction A delayed hemolytic transfusion reaction (DHTR) is a type of transfusion reaction. According to the Centers for Disease Control's (CDC) National Healthcare Safety Network's (NHSN) Hemovigilance Module, it is defined as: DEFINITIVE DIAGNOSIS: AND EITHER AND EITHER PROBABLE DIAGNOSIS: Delayed blood transfusion reaction occurs more frequently (1 in 20,569 blood components transfused in the USA in 2011) when compared to acute haemolytic transfusion reaction. An example of this type of reaction is if a person without a Kidd blood antigen (for example a Jka-Jkb+ patient) receives a Kidd antigen (Jka-antigen for example) in a red blood"
    ]
  ],
  [
    "Platelets can be stored for 5 to 7 days before transfusion, while red blood cells can be stored for up to 42 days.",
    [
      "the buffy coat and is sometimes removed to make platelets for transfusion. Platelets are typically pooled before transfusion and have a shelf life of 5 to 7 days, or 3 days once the facility that collected them has completed their tests. Platelets are stored at room temperature () and must be rocked/agitated. Since they are stored at room temperature in nutritive solutions, they are at relatively high risk for growing bacteria. Some blood banks also collect products by apheresis. The most common component collected is plasma via plasmapheresis, but red blood cells and platelets can be collected by similar methods.",
      "RBC viability and quality. Notably, U.S. hospitals spend more on dealing with the consequences of transfusion-related complications than on the combined costs of buying, testing/treating, and transfusing their blood. Routine blood storage is 42 days or 6 weeks for stored packed \"red blood cells\" (also called \"StRBC\" or \"pRBC\"), by far the most commonly transfused blood product, and involves refrigeration but usually not freezing. There has been increasing controversy about whether a given product unit's age is a factor in transfusion efficacy, specifically on whether \"older\" blood directly or indirectly increases risks of complications. Studies have not been consistent on",
      "only be stored for 7 days, due largely to their greater potential for contamination, which is in turn due largely to a higher storage temperature. Insufficient transfusion efficacy can result from red blood cell (RBC) blood product units damaged by so-called \"storage lesion\"\u2014a set of biochemical and biomechanical changes which occur during storage. With red cells, this can decrease viability and ability for tissue oxygenation. Although some of the biochemical changes are reversible after the blood is transfused, the biomechanical changes are less so, and rejuvenation products are not yet able to adequately reverse this phenomenon. Current regulatory measures are"
    ]
  ],
  [
    "Studies have shown that certain behaviors, such as screening, tend to decrease among informal carers in 2020.",
    [
      "is the type of caregiving that supports our biological systems. Current caregiving such as hospital births, solo sleeping, and physical isolation are not the types of early life caregiving to which humans are adapted. The article also mentions \"dearth of touch\" or faulty serotonin receptors affects society, and how they affect our society. There are higher rates of depression and anxiety, which both affect general and moral functioning. There are two categories of the effects of childrearing on moral functioning, dispositional and situational effects. There are two hypotheses relating to the dispositional effects of childrearing on moral functioning. First, \"a",
      "survey: in 1965, 1969, 1974, 1981, 1991, 1999\u20132000, 2004, 2008 and 2014. The following sweep was planned for 2018. Examples of topics which have been included are medical care, health, home environment, educational progress, parental involvement, family relationships, economic activity, income, training and housing. During the period 2002\u20132004, genetic information on participants was also obtained to examine the genetic effects on common traits and diseases. Following the initial birth survey, the four subsequent sweeps were carried out by the National Children's Bureau. In 1985, the NCDS was moved to the Social Statistics Research Unit (SSRU), which in 2016 was known",
      "found in a Norwegian study that the most common stressors reported were \"disorganization of household routines, difficulties with going away for holidays, restrictions on social life, and the disturbances of sleep...\" and that this was common to carers for dementia, stroke and Parkinson's disease patients. In a Japanese study, Hirono et al. assessed that \"the patients' functional and neuropsychiatric impairments were the main patient factors which increased the caregiver's burden.\" An Italian study by Marvardi et al. found \"that patients' behavioral disturbances and disability were the major predictors of the time-dependent burden; the psychophysical burden was explained mainly by caregiver"
    ]
  ],
  [
    "Proteolysis is not used to modify protein structure after translation has occurred.",
    [
      "can recognize nonsense codons and causes the release of the polypeptide chain. The capacity of disabling or inhibiting translation in protein biosynthesis is used by some antibiotics such as anisomycin, cycloheximide, chloramphenicol, tetracycline, streptomycin, erythromycin, puromycin, etc. Events that occur during or following biosynthesis include proteolysis, post-translational modification and protein folding. Proteolysis may remove N-terminal, C-terminal or internal amino-acid residues or peptides from the polypeptide. The termini and side-chains of the polypeptide may be subjected to post-translational modification. These modifications may be required for correct cellular localisation or the natural function of the protein. During and after synthesis, polypeptide chains",
      "shifting the first reading up one letter between the T and H on the first word: Now the sentence makes absolutely no sense. In the case of a translating ribosome, a frameshift can result in nonsense being created after the frameshift or a completely different protein being created after the frameshift. When referring to translational frameshifting, the latter is always implied, the former being usually a result of a point mutation such as a deletion. The main distinction between frameshifts resulting from mutation and those resulting from ribosomal frameshifting is that the latter are controlled by various mechanisms found in",
      "substrates and products bind during catalysis; what changes occur during the reaction; and even the role of particular amino acid residues in the mechanism. Some enzymes change shape significantly during the mechanism; in such cases, it is helpful to determine the enzyme structure with and without bound substrate analogues that do not undergo the enzymatic reaction. Not all biological catalysts are protein enzymes; RNA-based catalysts such as ribozymes and ribosomes are essential to many cellular functions, such as RNA splicing and translation. The main difference between ribozymes and enzymes is that RNA catalysts are composed of nucleotides, whereas enzymes are"
    ]
  ],
  [
    "A patient may need to carry out a bladder washout at home to help manage urinary retention or bladder dysfunction.",
    [
      "Toileting In health care, toileting is the act of assisting a dependent patient with his/her elimination needs. Depending on a patient's condition, his/her toileting needs may need to be met differently. This could be by assisting the patient to walk to a toilet, to a bedside commode chair, onto a bedpan, or to provide a male patient with a urinal. A more dependent or incontinent patient may have his/her toileting needs met solely through the use of adult diapers. Other options are incontinence pads and urinary catheters. Some patients can walk with assistance from another person, usually a health care",
      "disinfected by various chemicals. Urinal (health care) A urinal is a bottle for urination. It is most frequently used in health care for patients who find it impossible or difficult to get out of bed during sleep. Urinals allow the patient who has cognition and movement of their arms to toilet independently. Urinals can also be used for measuring the amount of urine produced by a patient on input & output (I & O), even if not used by the patient for toileting. Generally, patients who are able to are encouraged to walk to the toilet or use a bedside",
      "Urinal (health care) A urinal is a bottle for urination. It is most frequently used in health care for patients who find it impossible or difficult to get out of bed during sleep. Urinals allow the patient who has cognition and movement of their arms to toilet independently. Urinals can also be used for measuring the amount of urine produced by a patient on input & output (I & O), even if not used by the patient for toileting. Generally, patients who are able to are encouraged to walk to the toilet or use a bedside commode as opposed to"
    ]
  ],
  [
    "A 14-16 gauge needle is most likely to be used for resuscitation settings due to its large size (Hypodermic needle).",
    [
      "The needles are designed with the same general features, including a barrel, plunger, needle and cap. The end of the needle is bevelled to create a sharp pointed tip, letting the needle easily penetrate the skin. The main system for measuring the diameter of a hypodermic needle is the Birmingham gauge (whereas French gauge is used mainly for catheters). Various needle lengths are available for any given gauge. Needles in common medical use range from 7 gauge (the largest) to 33 (the smallest). 21-gauge needles are most commonly used for drawing blood for testing purposes, and 16- or 17-gauge needles",
      "used although arm and hand veins are used most commonly, with leg and foot veins used to a much lesser extent. In infants, the scalp veins are sometimes used. The caliber of needles and catheters can be given in Birmingham gauge or French gauge. A Birmingham gauge of 14 is a very large cannula (used in resuscitation settings) and 24-26 is the smallest. The most common sizes are 16-gauge (midsize line used for blood donation and transfusion), 18- and 20-gauge (all-purpose line for infusions and blood draws), and 22-gauge (all-purpose pediatric line). 12- and 14-gauge peripheral lines are capable of",
      "Sewing machine needle A sewing machine needle is a specialized needle for use in a sewing machine. A sewing machine needle consists of: The majority of sewing machine needles are made of various grades of hardened chrome-plated steel, though certain specialty needles are coated with titanium rather than chrome. More than a dozen modern conventions exist for numbering the sizes of sewing machine needles, though only two remain in common use: the American (established and propagated by Singer) and the European (also called the \"number metric\" or \"NM\"). The European designation, established in 1942, is considered the uniform fixed size"
    ]
  ],
  [
    "A pleural friction rub can be distinguished from a pericardial rub based on the location of pain (lateral for pleural, central for pericardial) and changes in intensity upon pressure with a stethoscope (increased for pleural, no change for pericardial).",
    [
      "two-component rubs are ambiguous. A three-component rub distinguishes a pericardial rub and indicates the presence of pericarditis. Also, a pleural rub can only be heard during inspiration and expiration, whereas, the pericardial rub can be heard even after cessation of breathing. Pleural rub creates pain mostly on the lateral part of the chest wall, whereas pain due to pericardial rub is always central in location. The intensity of pleural rub is increased on pressing the diaphragm of the stethoscope over the affected area, whereas there is no such change in case of a pericardial rub. Pericardial friction rub A pericardial",
      "around the heart. The inner and outer (visceral and parietal, respectively) layers are normally lubricated by a small amount of pericardial fluid, but the inflammation of pericardium causes the walls to rub against each other with audible friction. In children, rheumatic fever is often the cause of pericardial friction rub. Pericardial friction rub is one of several, similar sounds. A differential diagnosis may be possible, or not, depending upon the number of components that are audible. Pericardial friction rub may have one, two, or three audible components, whereas the similar pleural friction rub ordinarily has two audible components. One- and",
      "Pericardial friction rub A pericardial friction rub, also pericardial rub, is an audible medical sign used in the diagnosis of pericarditis. Upon auscultation, this sign is an extra heart sound of to-and-fro character, typically with three components, ONE systolic and TWO diastolic. It resembles the sound of squeaky leather and often is described as grating, scratching, or rasping. The sound seems very close to the ear and may seem louder than or may even mask the other heart sounds. The sound usually is best heard between the apex and sternum but may be widespread. The pericardium is a double-walled sac"
    ]
  ],
  [
    "Lactate threshold, or Lactate inflection point (LIP), is the exercise intensity at which the blood concentration of lactate and/or lactic acid begins to exponentially increase.",
    [
      "blood lactate reaches a concentration of 2 mmol/litre (at rest it is around 1). The anaerobic energy system increases the ability to produce blood lactate during maximal exercise, resulting from an increased amount of glycogen stores and glycolytic enzymes. Lactate threshold Lactate inflection point (LIP), is the exercise intensity at which the blood concentration of lactate and/or lactic acid begins to exponentially increase. It is often expressed as 85% of maximum heart rate or 75% of maximum oxygen intake. When exercising at or below the LT, any lactate produced by the muscles is removed by the body without it building",
      "Lactate threshold Lactate inflection point (LIP), is the exercise intensity at which the blood concentration of lactate and/or lactic acid begins to exponentially increase. It is often expressed as 85% of maximum heart rate or 75% of maximum oxygen intake. When exercising at or below the LT, any lactate produced by the muscles is removed by the body without it building up. The onset of blood lactate accumulation (OBLA) is often confused with the lactate threshold. With a higher exercise intensity the lactate production exceeds at a rate which it cannot be broken down, the blood lactate concentration will show",
      "or AerT) is sometimes defined as the exercise intensity at which blood lactate concentrations rise above resting levels. Anaerobic threshold (AnT) is sometimes defined equivalently to the lactate threshold(LT); as the exercise intensity beyond which blood lactate concentration is no longer linearly related to exercise intensity, but increases with both exercise intensity \"and\" duration. The blood lactate concentration at the anaerobic threshold is called the \"maximum steady-state lactate concentration\" (MLSS). AeT is the exercise intensity at which aerobic energy pathways start to operate, considered to be around 65-85% of an individual's maximum heart rate. Some have suggested this is where"
    ]
  ],
  [
    "The production and removal of lactate play a role in promoting ATP generation and acting as a buffer system for muscle acidosis during heavy workloads by providing additional energy to contracting muscles and removing excess protons to prevent muscular acidosis.",
    [
      "second phase of glycolysis to promote ATP generation. This, in effect, provides more energy to contracting muscles under heavy workloads. The production and removal of lactate from the cell also ejects a proton consumed in the LDH reaction- the removal of excess protons produced in the wake of this fermentation reaction serves to act as a buffer system for muscle acidosis. Once proton accumulation exceeds the rate of uptake in lactate production and removal through the LDH symport, muscular acidosis occurs. LDH is a protein that normally appears throughout the body in small amounts. Many cancers can raise LDH levels,",
      "increased to provide additional ATP, and the excess pyruvate produced is converted into lactate and released from the cell into the bloodstream, where it accumulates over time. While increased glycolysis helps compensate for less ATP from oxidative phosphorylation, it cannot bind the hydrogen cations that result from ATP hydrolysis. Therefore, hydrogen cation concentration rises and causes acidosis. The excess hydrogen cations produced during lactic acidosis are widely believed to actually derive from production of lactic acid. This is incorrect, as cells do not produce lactic acid; pyruvate is converted directly into lactate, the anionic form of lactic acid. When excess",
      "fatigue during exercise has been widely adopted. A closer, mechanistic analysis of lactate production under anaerobic conditions shows that there is no biochemical evidence for the production of lactate through LDH contributing to acidosis. While LDH activity is correlated to muscle fatigue, the production of lactate by means of the LDH complex works as a system to delay the onset of muscle fatigue. LDH works to prevent muscular failure and fatigue in multiple ways. The lactate-forming reaction generates cytosolic NAD+, which feeds into the glyceraldehyde 3-phosphate dehydrogenase reaction to help maintain cytosolic redox potential and promote substrate flux through the"
    ]
  ],
  [
    "The three natural curves present in a healthy spine are the cervical curve (neck region bending inward), thoracic curve (upper back region bending outward), and lumbar curve (lower back region bending inward).",
    [
      "Neutral spine A good posture refers to the \"three natural curves [that] are present in a healthy spine.\". It is also called neutral spine. Looking directly at the front or back of the body, the 33 vertebrae in the spinal column should appear completely vertical. From a side view, the cervical (neck) region of the spine (C1-C7) is bent inward, the thoracic (upper back) region (T1-T12) bends outward, and the lumbar (lower back) region (L1-L5) bends inward. The sacrum (tailbone area) (S1-S5 fused) and coccyx (on average 4 fused) rest between the pelvic bones. A neutral pelvis indicates the anterior",
      "for long hours on the job are susceptible to a number of misalignments. \"Neutral spine\" is ideally maintained while sitting, standing, and sleeping. Neutral spine A good posture refers to the \"three natural curves [that] are present in a healthy spine.\". It is also called neutral spine. Looking directly at the front or back of the body, the 33 vertebrae in the spinal column should appear completely vertical. From a side view, the cervical (neck) region of the spine (C1-C7) is bent inward, the thoracic (upper back) region (T1-T12) bends outward, and the lumbar (lower back) region (L1-L5) bends inward.",
      "To increase surface for ligament attachment to help support the abdominal viscera during erect posture, the Ischia spines became more prominent and shifted towards the middle of the body. The vertebral column of humans takes a forward bend in the lumbar (lower) region and a backward bend in the thoracic (upper) region. Without the lumbar curve, the vertebral column would always lean forward, a position that requires much more muscular effort for bipedal animals. With a forward bend, humans use less muscular effort to stand and walk upright. Together the lumbar and thoracic curves bring the body's center of gravity"
    ]
  ],
  [
    "are typically treated with nonsteroidal anti-inflammatory drugs (NSAIDs) and corticosteroids, while more severe cases may require disease-modifying antirheumatic drugs (DMARDs) or biologic agents to suppress the immune system and reduce inflammation (Arthritis Foundation).",
    [
      "joint pain. The condition is caused by bacteria elsewhere in the body. Infectious arthritis must be rapidly diagnosed and treated promptly to prevent irreversible joint damage. Psoriasis can develop into psoriatic arthritis. With psoriatic arthritis, most individuals develop the skin problem first and then the arthritis. The typical features are of continuous joint pains, stiffness and swelling. The disease does recur with periods of remission but there is no cure for the disorder. A small percentage develop a severe painful and destructive form of arthritis which destroys the small joints in the hands and can lead to permanent disability and",
      "as a type of seronegative spondyloarthropathy. Genetics are thought to be strongly involved in the development of psoriatic arthritis. Obesity and certain forms of psoriasis are thought to increase the risk. Psoriatic arthritis affects up to 30% of people with psoriasis and occurs in both children and adults. Approximately 40\u201350% of individuals with psoriatic arthritis have the HLA-B27 genotype. The condition is less common in people of Asian or African descent and affects men and women equally. Pain, swelling, or stiffness in one or more joints is commonly present in psoriatic arthritis. Psoriatic arthritis is inflammatory, and affected joints are",
      "psoriatic arthritis, and affect them symmetrically. Involvement of the spinal joints is more suggestive of psoriatic arthritis than rheumatoid arthritis. Osteoarthritis shares certain clinical features with psoriatic arthritis such as its tendency to affect multiple distal joints in an asymmetric pattern. Unlike psoriatic arthritis, osteoarthritis does not typically involve inflammation of the sacroiliac joint. Psoriatic arthritis sometimes affects only one joint and is sometimes confused for gout or pseudogout when this happens. There are five main types of psoriatic arthritis: The underlying process in psoriatic arthritis is inflammation; therefore, treatments are directed at reducing and controlling inflammation. Milder cases of"
    ]
  ],
  [
    "Graves' disease of the thyroid can be confirmed through laboratory measurements and serological testing by detecting thyroid-stimulating antibodies, radioactive iodine uptake, or thyroid ultrasound with Doppler.",
    [
      "detected thyroid-stimulating antibodies, radioactive iodine (RAI) uptake, or thyroid ultrasound with Doppler all can independently confirm a diagnosis of Grave's disease. Biopsy to obtain histiological testing is not normally required, but may be obtained if thyroidectomy is performed. The goiter in Graves' disease is often not nodular, but thyroid nodules are also common. Differentiating common forms of hyperthyroidism such as Graves' disease, single thyroid adenoma, and toxic multinodular goiter is important to determine proper treatment. The differentiation among these entities has advanced, as imaging and biochemical tests have improved. Measuring TSH-receptor antibodies with the h-TBII assay has been proven efficient",
      "of the thyroid. Another sign of Graves' disease is hyperthyroidism, \"i.e.\", overproduction of the thyroid hormones T3 and T4. Normal thyroid levels are also seen, and occasionally also hypothyroidism, which may assist in causing goiter (though it is not the cause of the Graves' disease). Hyperthyroidism in Graves' disease is confirmed, as with any other cause of hyperthyroidism, by measuring elevated blood levels of free (unbound) T3 and T4. Other useful laboratory measurements in Graves' disease include thyroid-stimulating hormone (TSH, usually undetectable in Graves' disease due to negative feedback from the elevated T3 and T4), and protein-bound iodine (elevated). Serologically",
      "(Mather, 2007). Blood tests also help to determine the kind of thyroiditis and to see how much thyroid stimulating hormone the pituitary gland is producing and what antibodies are present in the body. In some cases a biopsy may be needed to find out what is attacking the thyroid. Treatments for this disease depend on the type of thyroiditis that is diagnosed. For the most common type, which is known as Hashimoto's thyroiditis, the treatment is to immediately start hormone replacement. This prevents or corrects the hypothyroidism, and it also generally keeps the gland from getting bigger. However, Hashimoto's thyroiditis"
    ]
  ],
  [
    "is regulated by calcitonin and parathyroid hormone (PTH), with calcitonin being released by the thyroid gland to lower calcium levels and PTH being released by the parathyroid glands to raise calcium levels when needed (Calcium in biology).",
    [
      "Calcium metabolism Calcium metabolism refers to the movements and regulation of calcium ions (Ca) \"in\" and \"out\" of various body compartments, such as the gastrointestinal tract, the blood plasma, the extracellular and intracellular fluids, and bone tissue. An important aspect of calcium metabolism is plasma calcium homeostasis, the regulation of calcium ions in the blood plasma within narrow limits. In this process, bone tissue acts as a calcium storage center for deposits and withdrawals as needed by the blood, via continual bone remodeling. Derangement of this mechanism leads to hypercalcemia or hypocalcemia, both of which can have consequences for health.",
      "one of the most closely regulated physiological variables in the human body. Normal plasma levels vary between 1 and 2% over any given time. Approximately half of all ionized calcium circulates in its unbound form, with the other half being complexed with plasma proteins such as albumin, as well as anions including bicarbonate, citrate, phosphate, and sulfate. Different tissues contain calcium in different concentrations. For instance, Ca (mostly calcium phosphate and some calcium sulfate) is the most important (and specific) element of bone and calcified cartilage. In humans, the total body content of calcium is present mostly in the form",
      "The level of the calcium in humans' plasma is regulated by calcitonin and parathyroid hormone (PTH); calcitonin is released by the thyroid gland once its plasma level is above its set normal point (in order to lower calcium level); PTH is released by the parathyroid glands when calcium level falls below set point (in order to raise it). Calcium is the most abundant mineral in the human body. The average adult body contains in total approximately 1 kg, 99% in the skeleton in the form of calcium phosphate salts . The extracellular fluid (ECF) contains approximately 22 mmol, of which"
    ]
  ],
  [
    "The systematic name of the enzyme class that catalyzes the transfer of phosphate groups from high-energy molecules to specific substrates is ATP:(deoxy)nucleoside-phosphate phosphotransferase.",
    [
      "As of late 2007, 3 structures have been solved for this class of enzymes, with PDB accession codes , , and . Glucose-1-phosphate adenylyltransferase In enzymology, a glucose-1-phosphate adenylyltransferase () is an enzyme that catalyzes the chemical reaction Thus, the two substrates of this enzyme are ATP and alpha-D-glucose 1-phosphate, whereas its two products are diphosphate and ADP-glucose. This enzyme belongs to the family of transferases, specifically those transferring phosphorus-containing nucleotide groups (nucleotidyltransferases). The systematic name of this enzyme class is ATP:alpha-D-glucose-1-phosphate adenylyltransferase. Other names in common use include ADP glucose pyrophosphorylase, glucose 1-phosphate adenylyltransferase, adenosine diphosphate glucose pyrophosphorylase, adenosine",
      "Kinase In biochemistry, a kinase is an enzyme that catalyzes the transfer of phosphate groups from high-energy, phosphate-donating molecules to specific substrates. This process is known as phosphorylation, where the substrate gains a phosphate group and the high-energy ATP molecule donates a phosphate group. This transesterification produces a phosphorylated substrate and ADP. Conversely, it is referred to as dephosphorylation when the phosphorylated substrate donates a phosphate group and ADP gains a phosphate group (producing a dephosphorylated substrate and the high energy molecule of ATP). These two processes, phosphorylation and dephosphorylation, occur four times during glycolysis. Kinases are part of the",
      ". (deoxy)nucleoside-phosphate kinase In enzymology, a (deoxy)nucleoside-phosphate kinase () is an enzyme that catalyzes the chemical reaction Thus, the two substrates of this enzyme are ATP and deoxynucleoside phosphate, whereas its two products are ADP and deoxynucleoside diphosphate. This enzyme belongs to the family of transferases, specifically those transferring phosphorus-containing groups (phosphotransferases) with a phosphate group as acceptor. The systematic name of this enzyme class is ATP:deoxynucleoside-phosphate phosphotransferase. Other names in common use include deoxynucleoside monophosphate kinase, deoxyribonucleoside monophosphokinase, and deoxynucleoside-5'-monophosphate kinase. As of late 2007, two structures have been solved for this class of enzymes, with PDB accession codes"
    ]
  ],
  [
    "The normal reference range for an adult male's respiratory rate at rest is typically 12-20 breaths per minute.",
    [
      "novel techniques for automatically monitoring respiratory rate using wearable sensors are in development, such as estimation of respiratory rate from the electrocardiogram, photoplethysmogram and accelerometry signals. For humans, the typical respiratory rate for a healthy adult at rest is 12\u201318 breaths per minute. The respiratory center sets the quiet respiratory rhythm at around two seconds for an inhalation and three seconds exhalation. This gives the lower of the average rate at 12 breaths per minute. Average resting respiratory rates by age are: Respiratory minute volume is the volume of air which is inhaled (inhaled minute volume) or exhaled (exhaled minute",
      "and an adult of 50\u201380 bpm. Varies with age, but the normal reference range for an adult is 16\u201320 breaths per minute. The value of respiratory rate as an indicator of potential respiratory dysfunction has been investigated but findings suggest it is of limited value. Respiratory rate is a clear indicator of acidotic states, as the main function of respiration is removal of CO leaving bicarbonate base in circulation. The blood pressure is recorded as two readings: a high systolic pressure, which occurs during the maximal contraction of the heart, and the lower diastolic or resting pressure. A normal blood",
      "Lung volumes Lung volumes and lung capacities refer to the volume of air in the lungs at different phases of the respiratory cycle. The average total lung capacity of an adult human male is about 6 litres of air. Tidal breathing is normal, resting breathing; the tidal volume is the volume of air that is inhaled or exhaled in only a single such breath. The average human respiratory rate is 30-60 breaths per minute at birth, decreasing to 12-20 breaths per minute in adults. Several factors affect lung volumes; some can be controlled and some cannot be controlled. Lung volumes"
    ]
  ],
  [
    "The type of fiber in connective tissue composed of type III collagen and forms a fine meshwork in soft tissues is reticular fibers.",
    [
      "an extracellular matrix. The cells of this type of tissue are generally separated by quite some distance by a gelatinous substance primarily made up of collagenous and elastic fibers. Loose connective tissue is named based on the weave and type of its constituent fibers. There are three main types of connective tissue fiber: Usually \"loose connective tissue\" is considered a parent category that includes the mucous connective tissue of the fetus, areolar connective tissue, reticular connective tissue, and adipose tissue. It is a pliable, mesh-like tissue with a fluid matrix and functions to cushion and protect body organs. Fibroblasts are",
      "arranged in sheets. It is classified as either dense regular connective tissue or dense irregular connective tissue. Dense connective tissue Dense connective tissue, also called dense fibrous tissue, is a type of connective tissue with fibers as its main matrix element. The fibers are mainly composed of type I collagen. Crowded between the collagen fibers are rows of fibroblasts, fiber-forming cells, that generate the fibers. Dense connective tissue forms strong, rope-like structures such as tendons and ligaments. Tendons attach skeletal muscles to bones; ligaments connect bones to bones at joints. Ligaments are more stretchy and contain more elastic fibers than",
      "Dense connective tissue Dense connective tissue, also called dense fibrous tissue, is a type of connective tissue with fibers as its main matrix element. The fibers are mainly composed of type I collagen. Crowded between the collagen fibers are rows of fibroblasts, fiber-forming cells, that generate the fibers. Dense connective tissue forms strong, rope-like structures such as tendons and ligaments. Tendons attach skeletal muscles to bones; ligaments connect bones to bones at joints. Ligaments are more stretchy and contain more elastic fibers than tendons. Dense connective tissue also make up the lower layers of the skin (dermis), where it is"
    ]
  ],
  [
    "The aerobic system releases the most energy when completely oxidized in the body.",
    [
      "and intensity and by the individual's cardiorespiratory fitness level. Three exercise energy systems can be selectively recruited, depending on the amount of oxygen available, as part of the cellular respiration process to generate the ATP for the muscles. They are ATP, the anaerobic system and the aerobic system. ATP is the usable form of chemical energy for muscular activity. It is stored in most cells, particularly in muscle cells. Other forms of chemical energy, such as those available from food, must be transformed into ATP before they can be utilized by the muscle cells. Since energy is released when ATP",
      "total energy requirement during a two-hour aerobic training session. This process could severely degrade the protein structures needed to maintain survival such as contractile properties of proteins in the heart, cellular mitochondria, myoglobin storage, and metabolic enzymes within muscles. The oxidative system (aerobic) is the primary source of ATP supplied to the body at rest and during low intensity activities and uses primarily carbohydrates and fats as substrates. Protein is not normally metabolized significantly, except during long term starvation and long bouts of exercise (greater than 90 minutes.) At rest approximately 70% of the ATP produced is derived from fats",
      "of the body to the muscles. The body undergoes aerobic respiration in order to provide sufficient delivery of O to the exercising skeletal muscles and the main determining factors are shown in figure 1. The rate of maximum O uptake (Omax) depends on cardiac output, O extraction and hemoglobin mass. The cardiac output of an athlete is difficult to manipulate during competitions and the distribution of cardiac output is at the maximum rate (i.e. 80%) during competitions. In addition, the O extraction is approximately 90% at maximal exercise. Therefore, the only method to enhance the physical performance left is to"
    ]
  ],
  [
    "The role of the inner mitochondrial membrane in metabolism is to compartmentalize the mitochondrial matrix from the cytosolic environment, allowing for specialized ion transporters to regulate the exchange of molecules and maintain a membrane potential necessary for ATP production.",
    [
      "by separating the matrix from the cytosolic environment. This compartmentalization is a necessary feature for metabolism. The inner mitochondrial membrane is both an electrical insulator and chemical barrier. Sophisticated ion transporters exist to allow specific molecules to cross this barrier. There are several antiport systems embedded in the inner membrane, allowing exchange of anions between the cytosol and the mitochondrial matrix. Inner mitochondrial membrane The inner mitochondrial membrane (IMM) is the mitochondrial membrane which separates the mitochondrial matrix from the intermembrane space. The structure of the inner mitochondrial membrane is extensively folded and compartmentalized. The numerous invaginations of the membrane",
      "and molecules require special membrane transporters to enter or exit the matrix. Proteins are ferried into the matrix via the translocase of the inner membrane (TIM) complex or via Oxa1. In addition, there is a membrane potential across the inner membrane, formed by the action of the enzymes of the electron transport chain. The inner mitochondrial membrane is compartmentalized into numerous cristae, which expand the surface area of the inner mitochondrial membrane, enhancing its ability to produce ATP. For typical liver mitochondria, the area of the inner membrane is about five times as large as the outer membrane. This ratio",
      "Inner mitochondrial membrane The inner mitochondrial membrane (IMM) is the mitochondrial membrane which separates the mitochondrial matrix from the intermembrane space. The structure of the inner mitochondrial membrane is extensively folded and compartmentalized. The numerous invaginations of the membrane are called cristae, separated by crista junctions from the inner boundary membrane juxtaposed to the outer membrane. Cristae significantly increases the total membrane surface area compared to a smooth inner membrane and thereby the available working space. The inner membrane creates two compartments. The region between the inner and outer membrane, called the intermembrane space which is largely continuous with the"
    ]
  ],
  [
    "The rate of energy expenditure for male athletes during a training session can range from 200 W to 500 W, depending on the individual's fitness level and intensity of the workout.",
    [
      "seconds). 5 W/kg is about the level reachable by the highest tier of male amateurs for longer periods. Maximum power levels during one hour range from about 200 W (\"healthy men\") to 500 W (exceptionally athletic men). The energy input to the human body is in the form of food energy, usually quantified in kilocalories [kcal] or kiloJoules [kJ=kWs]. This can be related to a certain distance travelled and to body weight, giving units such as kJ/(km\u2219kg). The rate of food consumption, i.e. the amount consumed during a certain period of time, is the input power. This can be measured",
      "coaches to fine tune his training program so that he could recover between swim events that were sometimes several minutes apart. Much similar to blood glucose for diabetes, lower priced lactate measurement devices are now available but in general the lactate measurement approach is still the domain of the professional coach and elite athlete. Endurance training Endurance training is the act of exercising to increase endurance. The term endurance training generally refers to training the aerobic system as opposed to the anaerobic system. The need for endurance in sports is often predicated as the need of cardiovascular and simple muscular",
      "Endurance training Endurance training is the act of exercising to increase endurance. The term endurance training generally refers to training the aerobic system as opposed to the anaerobic system. The need for endurance in sports is often predicated as the need of cardiovascular and simple muscular endurance, but the issue of endurance is far more complex. Endurance can be divided into two categories including: general endurance and specific endurance. It can be shown that endurance in sport is closely tied to the execution of skill and technique. A well conditioned athlete can be defined as, the athlete who executes his"
    ]
  ],
  [
    "The normal heart rate range for an adult at rest is 60-100 beats per minute (bpm) (source: Heart rate; Heart rate; Heart rate).",
    [
      "number of different metrics are used to describe heart rate. Normal pulse rates at rest, in beats per minute (BPM): The basal or resting heart rate (HR) is defined as the heart rate when a person is awake, in a neutrally temperate environment, and has not been subject to any recent exertion or stimulation, such as stress or surprise. A large body of evidence indicates that the normal range is 60-100 beats per minute. This resting heart rate is often correlated with mortality. For example, all-cause mortality is increased by 1.22 (hazard ratio) when heart rate exceeds 90 beats per",
      "Heart rate Heart rate is the speed of the heartbeat measured by the number of contractions (beats) of the heart per minute (bpm). The heart rate can vary according to the body's physical needs, including the need to absorb oxygen and excrete carbon dioxide. It is usually equal or close to the pulse measured at any peripheral point. Activities that can provoke change include physical exercise, sleep, anxiety, stress, illness, and ingestion of drugs. The American Heart Association states the normal resting adult human heart rate is 60\u2013100 bpm. Tachycardia is a fast heart rate, defined as above 100 bpm",
      "rate, like other vital signs like blood pressure and respiratory rate, change with age. In adults, a normal heart rate is between 60 and 100 bpm (normocardic) where in children it is higher. A heart rate less than normal is called bradycardia (<60 in adults) and higher than normal is tachycardia (>100 in adults). A complication of this is when the atria and ventricles are not in synchrony and the \"heart rate\" must be specified as atrial or ventricular (e.g., the ventricular rate in ventricular fibrillation is 300\u2013600 bpm, whereas the atrial rate can be normal [60\u2013100] or faster [100\u2013150])."
    ]
  ],
  [
    "The statement \"If B is not true, then A is also not true\" is logically false in the given information.",
    [
      "false. One statement logically implies another when it is logically incompatible with the negation of the other. A statement is logically true if, and only if its opposite is logically false. The opposite statements must contradict one another. In this way all logical connectives can be expressed in terms of preserving logical truth. The logical form of a sentence is determined by its semantic or syntactic structure and by the placement of logical constants. Logical constants determine whether a statement is a logical truth when they are combined with a language that limits its meaning. Therefore, until it is determined",
      "is not true (assuming that we are dealing with concrete statements that are either true or not true): We can apply the same process the other way round: We also know that B is either true or not true. If B is not true, then A is also not true. However, it is given that A is true; so, the assumption that B is not true leads to contradiction and must be false. Therefore, B must be true: Combining the two proved statements makes them logically equivalent: Logical equivalence between two propositions means that they are true together or false",
      "negations of the other literals. That is, if a truth assignment causes a clause to be true, and none of the literals of the body satisfy the clause, then the head must also be true. This equivalence is commonly used in logic programming, where clauses are usually written as an implication in this form. More generally, the head may be a disjunction of literals. If formula_10 are the literals in the body of a clause and formula_11 are those of its head, the clause is usually written as follows: Clause (logic) In logic, a clause is an expression formed from"
    ]
  ],
  [
    "Surgery is the most radical treatment option for ulnar nerve palsy, especially for those who do not improve with conservative therapy or have serious or progressive symptoms.",
    [
      "such as Anaprox, or other medications such as Ultracet, Neurontin and Lyrica. Lidocaine patches are also a treatment that helps some patients. There are multiple ways that tarsal tunnel can be treated and the pain can be reduced. The initial treatment, whether it be conservative or surgical, depends on the severity of the tarsal tunnel and how much pain the patient is in. There was a study done that treated patients diagnosed with tarsal tunnel syndrome with a conservative approach. Meaning that the program these patients were participated in consisted of physiotherapy exercises and orthopedic shoe inserts in addition to",
      "headset will provide immediate symptomatic relief and reduce the likelihood of further damage and inflammation to the nerve. For cubital tunnel syndrome, it is recommended to avoid repetitive elbow flexion and also avoiding prolonged elbow flexion during sleep, as this position puts stress of the ulnar nerve. Surgery is recommended for those who are not improved with conservative therapy or those with serious or progressive symptoms. The surgical approaches vary, and may depend on the location or cause of impingement. Cubital and ulnar tunnel release can be performed wide awake with no general anaesthesia, no regional anaesthesia, no sedation and",
      "perform activities of daily living. Goals of therapy are to improve tactile sensation, proprioception, and range of motion. Acute treatment of a severe injury will involve repositioning and splinting or casting of the extremity. Klumpke Palsy is listed as a 'rare disease' by the Office of Rare Diseases (ORD) of the National Institutes of Health (NIH). This means that Klumpke palsy, or a subtype of Klumpke palsy, affects fewer than 200,000 people in the US population. Klumpke paralysis Klumpke's paralysis (or Klumpke's palsy or Dejerine\u2013Klumpke palsy) is a variety of partial palsy of the lower roots of the brachial plexus."
    ]
  ],
  [
    " powder, denture cleaner, tooth brushing with toothpaste alternatives containing strontium chloride, potassium nitrate, arginine, calcium sodium phosphosilicate, and sodium polyphosphate.",
    [
      "about 13%. Tooth powder (or 'toothpaste powder') is an alternative to toothpaste. It may be recommended for people with sensitive teeth. Tooth powder typically does not contain the chemical sodium lauryl sulphate which can be a skin irritant. The function of sodium lauryl sulphate is to form suds when teeth are brushed. It is a common chemical in toothpaste. Those with dentures may also use denture cleaner which can also come in powder format. Tooth brushing Tooth brushing is the act of scrubbing teeth with a toothbrush equipped with toothpaste. Interdental cleaning (with floss or an interdental brush) can be",
      "include: Less commonly used tests might include trans-illumination (to detect congestion of the maxillary sinus or to highlight a crack in a tooth), dyes (to help visualize a crack), a test cavity, selective anaesthesia and laser doppler flowmetry. When it becomes extremely painful and decayed the tooth may be known as a hot tooth. Since most toothache is the result of plaque-related diseases, such as tooth decay and periodontal disease, the majority of cases could be prevented by avoidance of a cariogenic diet and maintenance of good oral hygiene. That is, reduction in the number times that refined sugars are",
      "tendency of toothpaste to dry into a powder. Included are various sugar alcohols, such as glycerol, sorbitol, or xylitol, or related derivatives, such as 1,2-propylene glycol and polyethyleneglycol. Strontium chloride or potassium nitrate is included in some toothpastes to reduce sensitivity. Two systemic meta-analysis reviews reported that arginine, and calcium sodium phosphosilicate - CSPS containing toothpastes are also effective in alleviating dentinal hypersensitivity respectively. Another randomized clinical trial found superior effects when both formulas were combined together. Sodium polyphosphate is added to minimize the formation of tartar. Other example to components in toothpastes is the Biotene, which has proved its"
    ]
  ],
  [
    "In some East European and Asian countries, Cushing's disease is also known as Itsenko-Kushing disease.",
    [
      "when associated with Cushing's syndrome (Carney complex) can infrequently cause spontaneous symptom regression of the latter. In 1924, the Soviet neurologist Nikolai Itsenko reported two patients with pituitary adenoma. The resulting excessive adrenocorticotropic hormone secretion led to the production of large amounts of cortisol by the adrenal glands. Considering this impact, the name of Itsenko was added to the title in some East European and Asian countries, and the disease is called Itsenko-Kushing disease. Cushing's disease Cushing's disease is one cause of Cushing's syndrome characterised by increased secretion of adrenocorticotropic hormone (ACTH) from the anterior pituitary (secondary hypercortisolism). This is",
      "0.3 times less likely to have adverse outcomes in comparison to men. Cases of Cushing's disease are rare, and little epidemiological data is available on the disease. An 18-year study conducted on the population of Vizcaya, Spain reported a 0.004% prevalence of Cushing's disease. The average incidence of newly diagnosed cases was 2.4 cases per million inhabitants per year. The disease is often diagnosed 3\u20136 years after the onset of illness. Several studies have shown that Cushing's disease is more prevalent in women than men at a ratio of 3\u20136:1, respectively. Moreover, most women affected were between the ages of",
      "14th-century epidemic as a proper name. However, ' is used to refer to a pestilential fever (') already in the 12th-century \"On the Signs and Symptoms of Diseases\" () by French physician Gilles de Corbeil. In English, the term was first used in 1755. Writers contemporary with the plague described the event as \"great plague\" or \"great pestilence\". Black Death The Black Death, also known as the Great Plague, the Black Plague, or simply the Plague, was one of the most devastating pandemics in human history, resulting in the deaths of an estimated people in Eurasia and peaking in Europe"
    ]
  ],
  [
    "Racial characteristics impact success in sports by influencing societal beliefs and stereotypes about certain racial groups' natural physical abilities and athletic prowess.",
    [
      "and/or environmental factors that allow them to excel over other races in athletic competition. Whites are more likely to hold these views; however, some blacks and other racial affiliations do as well. A 1991 poll in the United States indicated that half of the respondents agreed with the belief that \"blacks have more natural physical ability\". Various theories regarding racial differences of black and white people and their possible effect on sports performance have been put forth since the later part of the nineteenth century by professionals in many different fields. In the United States, attention to the subject faded",
      "studies to give the reader an understanding of the magnitude that biology plays in athletics. Topics such as the effects of gender, race, genetics, culture, and physical environment are discussed as contributors to success in specific sports. The chapters are: Epstein explores racial differences in sports performance and argues that on a genetic-level, persons of some African groups (such Jamaicans and Kalenjins) have an advantage in some sports such as the 100 meters sprint and marathons respectively. He explores the topic by use of interviews with experts and summarizing scientific studies. The book has generally positive reviews on Amazon and",
      "quarterbacks, were described with terms such as \u201cphysical specimen\u201d and \u201cimpressive specimen,\u201d respectively. Meanwhile, white players have been described as \u201cgood signal callers,\u201d and \u201creal student[s] of the game.\u201d These racial stereotypes are likely the result of over-generalizations of group differences in average mental and athletic abilities. They likely have roots in Social Darwinism, which looks at human races as species with different natural abilities. Social Darwinists associate darker skin with natural strength and instincts, albeit with intellectual deficiencies. These ideas have been largely disproven, but they still influence how many Americans see race. Sports have typically been an arena"
    ]
  ],
  [
    "The ratio of purines to pyrimidines in a double-stranded molecule of DNA is 1:1.",
    [
      "pyrimidines are thymine and cytosine; the purines are adenine and guanine. Both strands of double-stranded DNA store the same biological information. This information is replicated as and when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (informally, \"bases\"). It is the sequence of these four nucleobases along the backbone that encodes genetic information.",
      "Chargaff's rules Chargaff's rules state that DNA from any cell of any organisms should have a 1:1 ratio (base Pair Rule) of pyrimidine and purine bases and, more specifically, that the amount of guanine should be equal to cytosine and the amount of adenine should be equal to thymine. This pattern is found in both strands of the DNA. They were discovered by Austrian born chemist Erwin Chargaff, in the late 1940s. The first rule holds that a double-stranded DNA molecule \"globally\" has percentage base pair equality: %A = %T and %G = %C. The rigorous validation of the rule",
      "of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, pyrimidines and purines. In DNA, the"
    ]
  ],
  [
    "Synthetic steroids are related to the hormone testosterone as they are often derived from testosterone or have a similar structure, such as 1-Testosterone (1-T) which is a synthetic derivative of dihydrotestosterone (DHT).",
    [
      "are also many synthetic sex steroids. Synthetic androgens are often referred to as anabolic steroids. Synthetic estrogens and progestins are used in methods of hormonal contraception. Ethinylestradiol is a semi-synthetic estrogen. Specific compounds that have partial agonist activity for steroid receptors, and therefore act in part like natural steroid hormones, are in use in medical conditions that require treatment with steroid in one cell type, but where systemic effects of the particular steroid in the entire organism are only desirable within certain limits. Sex steroid Sex steroids, also known as gonadocorticoids and gonadal steroids, are steroid hormones that interact with",
      "1-Testosterone 1-Testosterone (abbreviated and nicknamed as 1-Testo, 1-T), also known as \u03b4-dihydrotestosterone (\u03b4-DHT), as well as dihydroboldenone, is a synthetic anabolic\u2013androgenic steroid (AAS) and derivative of dihydrotestosterone (DHT) which was never marketed. It differs from testosterone by having a 1(2)-double bond instead of a 4(5)-double bond in its . It was legally sold online in the United States until 2005, when it was reclassified as a Schedule III drug. A 2006 study determined that 1-testosterone has a high androgenic and anabolic potency even without being metabolized, so it can be characterized as a typical anabolic steroid. 1-Testosterone binds in a",
      "manner that is highly selective to the androgen receptor (AR) and has a high potency to stimulate AR-dependent transactivation. \"In vivo\", an equimolar dose of 1-testosterone has the same potency to stimulate the growth of the prostate, the seminal vesicles and the androgen-sensitive levator ani muscle as the reference anabolic steroid testosterone propionate, but, unlike testosterone propionate, 1-testosterone also increases liver weight. 1-Testosterone, IUPAC name 17\u03b2-hydroxy-5\u03b1-androst-1-en-3-one, also known as 4,5\u03b1-dihydro-\u03b4-testosterone (\u0394-DHT) or as 5\u03b1-androst-1-en-17\u03b2-ol-3-one, is a synthetic androstane steroid and a derivative of dihydrotestosterone (DHT). Two prohormones of 1-testosterone are 1-androstenediol and 1-androstenedione, the latter of which may be synthesized"
    ]
  ],
  [
    "Glycogen phosphorylase is responsible for the breakdown of glycogen in exercising muscle. (source)",
    [
      "the other hand, in the skeletal muscle, glycogen is used as an energy source to perform muscular contraction during exercise. The different functions of glycogen in muscles or liver make the regulation mechanisms of its metabolism differ in each tissue. These mechanisms are based mainly in the differences on structure and on the regulations of the enzymes that catalyze the way for its synthesis, glycogen synthase (GS), and for its degradation, glycogen phosphorylase (GF). Glycogenin is the initiator of the glycogen biosynthesis. This protein is a glycosyl transferase that have the ability of autoglycosilation using UDP-glucose, which helps in the",
      "known as glycogen synthase. This bond may be broken by amylase when the body wishes to break down glycogen into glucose for energy. Glycogen branching enzyme is responsible for the required \u03b1-1,6-glycosidic bonds needed to start a branch off of these linear chains. These branches are important, as they provide additional \"free ends\" for linear chains of \u03b1-1,4-glycosidic bonds, which can then be broken down by amylase. This allows for glucose to be removed at a faster rate than if all glucose molecules were in a single chain with only two free ends on which amylase could attach. GBED is",
      "Glycogen debranching enzyme A debranching enzyme is a molecule that helps facilitate the breakdown of glycogen, which serves as a store of glucose in the body, through glucosyltransferase and glucosidase activity. Together with phosphorylases, debranching enzymes mobilize glucose reserves from glycogen deposits in the muscles and liver. This constitutes a major source of energy reserves in most organisms. Glycogen breakdown is highly regulated in the body, especially in the liver, by various hormones including insulin and glucagon, to maintain a homeostatic balance of blood-glucose levels. When glycogen breakdown is compromised by mutations in the glycogen debranching enzyme, metabolic diseases such"
    ]
  ],
  [
    "The suggested total blood cholesterol level according to the 1987 report of the National Cholesterol Education Program, Adult Treatment Panels is < 200 mg/dL for normal blood cholesterol, 200\u2013239 mg/dL for borderline-high, and > 240 mg/dL for high cholesterol.",
    [
      "statistically significant for both genders. Though this result was met with some skepticism, later studies and meta-analyses likewise demonstrated statistically significant (but smaller) reductions in all-cause and cardiovascular mortality, without significant heterogeneity by gender. The 1987 report of National Cholesterol Education Program, Adult Treatment Panels suggests the total blood cholesterol level should be: < 200 mg/dL normal blood cholesterol, 200\u2013239 mg/dL borderline-high, > 240 mg/dL high cholesterol. The American Heart Association provides a similar set of guidelines for total (fasting) blood cholesterol levels and risk for heart disease: However, as today's testing methods determine LDL (\"bad\") and HDL (\"good\") cholesterol",
      "types: Current recommendations for cholesterol testing come from the Adult Treatment Panel (ATP) III guidelines, and are based on many large clinical studies, such as the Framingham Heart Study. For healthy adults with no cardiovascular risk factors, the ATP III guidelines recommend screening once every five years. A lipid profile may also be ordered at regular intervals to evaluate the success of lipid-lowering drugs such as statins. In the pediatric and adolescent population, lipid testing is not routinely performed. However, the American Academy of Pediatrics and NHLBI now recommend that children aged 9\u201311 be screened once for severe cholesterol abnormalities.",
      "milligrams per deciliter (mg/dL) of blood in the United States and some other countries. In the United Kingdom, most European countries and Canada, millimoles per liter of blood (mmol/Ll) is the measure. For healthy adults, the UK National Health Service recommends upper limits of total cholesterol of 5 mmol/L, and low-density lipoprotein cholesterol (LDL) of 3 mmol/L. For people at high risk of cardiovascular disease, the recommended limit for total cholesterol is 4 mmol/L, and 2 mmol/L for LDL. In the United States, the National Heart, Lung, and Blood Institute within the National Institutes of Health classifies total cholesterol of"
    ]
  ],
  [
    "A 5ml syringe of saline should be flushed into the medication port of the cannula's connecting hub after insertion to keep the cannula clean and sterile.",
    [
      "Saline flush A saline flush is the method of clearing intravenous lines (IVs), Central Lines or Arterial Lines of any medicine or other perishable liquids to keep the lines (tubes) and entry area clean and sterile. Typically in flushing an intravenous cannula, a 5ml syringe of saline is emptied into the medication port of the cannula's connecting hub after insertion of the cannula. Blood left in the cannula or hub can lead to clots forming and blocking the cannula. Flushing is required before a drip is connected to ensure that the IV is still patent. Flushing is also used after",
      "medications are delivered by the medication port to ensure all the drug is delivered fully. If multiple medications are given through the same line flushing can be used in between drugs to ensure that the medicines won't react. This is especially important if complex regimes of intravenous medication is used such as in chemotherapy. Flushing with saline should be painless if the cannula is in its proper place, although if the saline is not warmed there may be a cold sensation running up the vein. A painful flush may indicate tissuing or phlebitis and is an indication that the cannula",
      "the cannula in place below the skin. Insertion should be done in a fairly swift movement to avoid the cannula \"bunching up\" around the needle. A few pump users prefer an infusion set with a steel needle instead of a cannula. The cannula surrounds a steel needle similar to that found on a hypodermic syringe. The same used device can't be reused for the other patient, even if the set is washed or re-sterile. As it is not a surgical instrument. It is for single use only. Some pump users prefer to use an insertion device to insert their sets,"
    ]
  ],
  [
    "Creatine kinase activity is significant in the diagnosis of myocardial infarction as CK-MB is released from damaged myocardial cells into the blood, serving as a marker for heart attacks.",
    [
      "regeneration of ATP \"in situ\", as well as for intracellular energy transport by the PCr shuttle or circuit. Thus creatine kinase is an important enzyme in such tissues. Clinically, creatine kinase is assayed in blood tests as a marker of damage of CK-rich tissue such as in myocardial infarction (heart attack), rhabdomyolysis (severe muscle breakdown), muscular dystrophy, autoimmune myositides, and acute kidney injury. In the cells, the \"cytosolic\" CK enzymes consist of two subunits, which can be either \"B\" (brain type) or \"M\" (muscle type). There are, therefore, three different isoenzymes: CK-MM, CK-BB and CK-MB. The genes for these subunits",
      "be elevated in a wide range of clinical conditions including the use of medication such as statins; endocrine disorders such as hypothyroidism; and skeletal muscle diseases and disorders including malignant hyperthermia, and neuroleptic malignant syndrome. Furthermore, the isoenzyme determination has been used extensively as an indication for myocardial damage in heart attacks. Troponin measurement has largely replaced this in many hospitals, although some centers still rely on CK-MB. Creatine kinase Creatine kinase (CK), also known as creatine phosphokinase (CPK) or phosphocreatine kinase, is an enzyme () expressed by various tissues and cell types. CK catalyses the conversion of creatine and",
      "heart, in addition to the MM-CK homodimer, also the heterodimer MB-CK consisting of one muscle (M-CK) and one brain-type (B-CK) subunit is expressed. The latter may be an important serum marker for myocardial infarction, if released from damaged myocardial cells into the blood where it can be detected by clinical chemistry. CKM (gene) Creatine kinase, muscle also known as CKM is a creatine kinase that in humans is encoded by the \"CKM\" gene. In the figure to the right, the crystal structure of the muscle-type M-CK monomer is shown. In vivo, two such monomers arrange symmetrically to form the active"
    ]
  ],
  [
    "The smallest increments measured on a mercury sphygmomanometer are typically 2 mmHg, while the smallest increments on an aneroid sphygmomanometer can be 1 mmHg.",
    [
      "meter\", itself coined from \"manos\" \"thin, sparse\", and \"metron\" \"measure\". Most sphygmomanometers were mechanical gauges with dial faces during the first half of the 20th century. Since the advent of electronic medical devices, names such as \"meter\" and \"monitor\" can also apply, as devices can automatically monitor blood pressure on an ongoing basis. Sphygmomanometer A sphygmomanometer, also known as a blood pressure meter, blood pressure monitor, or blood pressure gauge, is a device used to measure blood pressure, composed of an inflatable cuff to collapse and then release the artery under the cuff in a controlled manner, and a mercury",
      "Sphygmomanometer A sphygmomanometer, also known as a blood pressure meter, blood pressure monitor, or blood pressure gauge, is a device used to measure blood pressure, composed of an inflatable cuff to collapse and then release the artery under the cuff in a controlled manner, and a mercury or mechanical manometer to measure the pressure. It is always used in conjunction with a means to determine at what pressure blood flow is just starting, and at what pressure it is unimpeded. Manual sphygmomanometers are used in conjunction with a stethoscope. A sphygmomanometer consists of an inflatable cuff, a measuring unit (the",
      "Blood pressure measurement Arterial blood pressure is most commonly measured via a sphygmomanometer, which historically used the height of a column of mercury to reflect the circulating pressure. Blood pressure values are generally reported in millimetres of mercury (mmHg), though aneroid and electronic devices do not contain mercury. For each heartbeat, blood pressure varies between systolic and diastolic pressures. Systolic pressure is peak pressure in the arteries, which occurs near the end of the cardiac cycle when the ventricles are contracting. Diastolic pressure is minimum pressure in the arteries, which occurs near the beginning of the cardiac cycle when the"
    ]
  ],
  [
    "Apneustic breathing is not typically seen in head injury patients with altered conscious levels.",
    [
      "status, heart function, vascular integrity, and tissue oxygenation. Respiratory pattern (breathing rhythm) is significant and should be noted in a comatose patient. Certain stereotypical patterns of breathing have been identified including Cheyne\u2013Stokes, a form of breathing in which the patient's breathing pattern is described as alternating episodes of hyperventilation and apnea. This is a dangerous pattern and is often seen in pending herniations, extensive cortical lesions, or brainstem damage. Another pattern of breathing is apneustic breathing, which is characterized by sudden pauses of Inhalation and is due to a lesion of the pons. Ataxic breathing is irregular and is due",
      "than six hours in which a person cannot be awakened, fails to respond normally to painful stimuli, light, sound, lacks a normal sleep-wake cycle and does not initiate voluntary actions. Although, according to the Glasgow Coma Scale, a person with confusion is considered to be in the mildest coma. But cerebral metabolism has been shown to correlate poorly with the level of consciousness in patients with mild to severe injury within the first month after traumatic brain injury (TBI). A person in a state of coma is described as comatose. In general patients surviving a coma recover gradually within 2\u20134",
      "appear to be the most prominent feature. This abnormal pattern of breathing, in which breathing is absent for a period and then rapid for a period, can be seen in patients with heart failure, strokes, hyponatremia, traumatic brain injuries and brain tumors. In some instances, it can occur in otherwise healthy people during sleep at high altitudes. It can occur in all forms of toxic metabolic encephalopathy. It is a symptom of carbon monoxide poisoning, along with syncope or coma. This type of respiration is also often seen after morphine administration. Hospices sometimes document the presence of Cheyne\u2013Stokes breathing as"
    ]
  ],
  [
    "The recommended chest compression to ventilation ratio during CPR for adults is 30:2, for children if at least 2 trained rescuers are present is 15:2, and for newborns is 30:2 with one rescuer and 15:2 with 2 rescuers (AHA 2015 Guidelines).",
    [
      "aspect of CPR are: few interruptions of chest compressions, a sufficient speed and depth of compressions, completely relaxing pressure between compressions, and not ventilating too much. It is unclear if a few minutes of CPR before defibrillation results in different outcomes than immediate defibrillation. A universal compression to ventilation ratio of 30:2 is recommended for adults. With children, if at least 2 trained rescuers are present a ratio of 15:2 is preferred. According to AHA 2015 Guidelines In newborns a ratio is 30:2 if One rescuer and 15:2 if 2 rescuers. If an advanced airway such as an endotracheal tube",
      "or nose (mouth-to-mouth resuscitation) or using a device that pushes air into the subject's lungs (mechanical ventilation). Current recommendations place emphasis on early and high-quality chest compressions over artificial ventilation; a simplified CPR method involving chest compressions only is recommended for untrained rescuers. In children, however, only doing compressions may result in worse outcomes. Chest compression to breathing ratios is set at 30 to 2 in adults. CPR alone is unlikely to restart the heart. Its main purpose is to restore partial flow of oxygenated blood to the brain and heart. The objective is to delay tissue death and to",
      "or laryngeal mask airway is in place, artificial ventilation should occur without pauses in compressions at a rate of 8\u201310 per minute. The recommended order of interventions is chest compressions, airway, breathing or CAB in most situations, with a compression rate of at least 100 per minute in all groups. Recommended compression depth in adults and children is at least 5 cm (2 inches) and in infants it is . As of 2010 the Resuscitation Council (UK) still recommends ABC for children. As it can be difficult to determine the presence or absence of a pulse, the pulse check has"
    ]
  ],
  [
    "The dose for a person weighing 62 kg with a drug dose of 15 mg/kg would be 930 mg.",
    [
      "Dose (biochemistry) A dose is a measured quantity of a medicine, nutrient, or pathogen which is delivered as a unit. The greater the quantity delivered, the larger the dose. Doses are most commonly measured for compounds in medicine. The term is usually applied to the quantity of a drug or other agent administered for therapeutic purposes, but may be used to describe any case where a substance is introduced to the body. In nutrition, the term is usually applied to how much of a specific nutrient is in a person's diet or in a particular food, meal, or dietary supplement.",
      "a set of instructions directing the patient to take a certain small dose, followed by another small dose if their symptoms don\u2019t subside. Under-dosing is a common problem in pharmacy, as predicting an average dose that is effective for all individuals is extremely challenging because body weight and size impacts how the dose acts within the body. Prescription drug dosage is based typically on body weight. Drugs come with a recommended dose in milligrams or micrograms per kilogram of body weight, and that is used in conjunction with the patient's body weight to determine a safe dosage. In single dosage",
      "cell). The measured dose (usually in milligrams, micrograms, or grams per kilogram of body-weight for oral exposures or milligrams per cubic meter of ambient air for inhalation exposures) is generally plotted on the X axis and the response is plotted on the Y axis. Other dose units include moles per body-weight, moles per animal, and for dermal exposure, moles per square centimeter. In some cases, it is the logarithm of the dose that is plotted on the X axis, and in such cases the curve is typically sigmoidal, with the steepest portion in the middle. Biologically based models using dose"
    ]
  ],
  [
    "One expected side effect of creatine supplementation is weight gain due to increased water retention.",
    [
      "by 5 to 15%. Creatine has no significant effect on aerobic endurance, though it will increase power during short sessions of high-intensity aerobic exercise. A survey of 21,000 college athletes showed that 14% of athletes take creatine supplements to improve performance. Non-athletes report taking creatine supplements to improve appearance. Side effects include: Use of creatine by healthy adults in normal dosages does not harm kidneys; its effects on the kidney in elderly people and adolescents were not well understood as of 2012. Both the American Academy of Pediatrics and the American College of Sports Medicine recommend that individuals younger than",
      "18 years old not use creatine. People with kidney disease, high blood pressure, or liver disease should not take creatine as a dietary supplement. One well-documented effect of creatine supplementation is weight gain within the first week of the supplement schedule, likely attributable to greater water retention due to the increased muscle creatine concentrations. A 2009 systematic review discredited concerns that creatine supplementation could affect hydration status and heat tolerance and lead to muscle cramping and diarrhea. Creatine taken with medications that can harm the kidney can increase the risk of kidney damage: A National Institutes of Health study suggests",
      "side-effects. Creatinine is the most widely used biomarker of kidney function. It is inaccurate at detecting mild renal impairment, and levels can vary with muscle mass but not with protein intake. Urea levels might change with protein intake. Formulas such as the Cockcroft and Gault formula and the MDRD formula (see Renal function) try to adjust for these variables. Cystatin C has a low molecular weight (approximately 13.3 kilodaltons), and it is removed from the bloodstream by glomerular filtration in the kidneys. If kidney function and glomerular filtration rate decline, the blood levels of cystatin C rise. Cross-sectional studies (based"
    ]
  ],
  [
    "The left ventricular systole contributes to systemic circulation by pumping newly oxygenated blood throughout the body via the aorta and all other arteries.",
    [
      "split into pulmonary circulation\u2014during which the right ventricle pumps oxygen-depleted blood to the lungs through the pulmonary trunk and arteries; or the systemic circulation\u2014in which the left ventricle pumps/ejects newly oxygenated blood throughout the body via the aorta and all other arteries. In a healthy heart all activities and rests during each individual cardiac cycle, or heartbeat, are initiated and orchestrated by signals of the heart's electrical conduction system, which is the \"wiring\" of the heart that carries electrical impulses throughout the body of cardiomyocytes, the specialized muscle cells of the heart. These impulses ultimately stimulate heart muscle to contract",
      "resting values and stated as relative to surrounding atmospheric which is the typical \"0\" reference pressure used in medicine.) During systole, the ventricles contract, pumping blood through the body. During diastole, the ventricles relax and fill with blood again. The left ventricle receives oxygenated blood from the left atrium via the mitral valve and pumps it through the aorta via the aortic valve, into the systemic circulation. The left ventricular muscle must relax and contract quickly and be able to increase or lower its pumping capacity under the control of the nervous system. In the diastolic phase, it has to",
      "oxygen. The red blood cells and the hemoglobin present in the blood, which is the main carrier of oxygen in the blood are responsible for this exchange of gases before they are carried to the left ventricle of the heart. The systemic circulation is responsible for taking the oxygenated blood to various organs and tissues via the arterial tree before taking the deoxygenated blood to the right ventricle using the venous system (a network of veins). Arteries carry the oxygenated blood while the veins carry the deoxygenated blood. The fluids associated with the human body include air, oxygen, carbon dioxide,"
    ]
  ],
  [
    "Factors that influence success in sports include individual and team achievements, physical ability and skills, popularity among fans, and the transition from high school to NCAA to professional teams. (Box score; Sport; Race and sports)",
    [
      "the success of a team. This information is then correlated to a player, or a team where it is read to obtain a general idea of how the game was played or how the player performed during the game, a season, or their career. Box score A box score is a structured summary of the results from a sport competition. The box score lists the game score as well as individual and team achievements in the game. Among the sports in which box scores are common are baseball, basketball, football and hockey. The box score data is derived from a",
      "of keeping with the religion of Christ. Popularity of major sports by size of fan base: Sport Sport (British English) or sports (American English) includes all forms of competitive physical activity or games which, through casual or organised participation, aim to use, maintain or improve physical ability and skills while providing enjoyment to participants, and in some cases, entertainment for spectators. Hundreds of sports exist, from those between single contestants, through to those with hundreds of simultaneous participants, either in teams or competing as individuals. In certain sports such as racing, many contestants may compete, simultaneously or consecutively, with one",
      "that racial characteristics, per se, is a factor in success in sports. For all races and sports, from 3.3% (basketball) to 11.3% (ice hockey) are successful in making the transition from high school varsity to an NCAA team. From .8% (men's ice hockey) to 9.4% (baseball) successfully transition from NCAA to professional teams. Therefore, the overall success rate of high school athletes progressing to professional athletes was from .03% (men and women's basketball) to .5% (baseball). The annual number of NCAA athletes drafted into professional sports annually varied from seven (men's ice hockey) to 678 (baseball). Unlike black athletes, blacks"
    ]
  ],
  [
    "The correct order of the four links in the chain of survival that can improve survival rates in cardiac arrest situations is early access, early CPR, early defibrillation, and early advanced cardiac life support.",
    [
      "Chain of survival The chain of survival refers to a series of actions that, properly executed, reduce the mortality associated with cardiac arrest. Like any chain, the chain of survival is only as strong as its weakest link. The four interdependent links in the chain of survival are early access, early CPR, early defibrillation, and early advanced cardiac life support The \"chain of survival\" metaphor was first published in the March 1981 newsletter of CPR for Citizens in Orlando, Florida. It was further developed by Mary M. Newman of the Sudden Cardiac Arrest Foundation, and used as a slogan for",
      "defibrillation is the link in the chain most likely to improve survival. Public access defibillation may be the key to improving survival rates in out-of-hospital cardiac arrest, but is of the greatest value when the other links in the chain do not fail. Early advanced cardiac life support by paramedics is another critical link in the chain of survival. In communities with survival rates > 20%, a minimum of two of the rescuers are trained to the advanced level. In some countries, EMS delivery may be performed by ambulance officers, paramedics, nurses, or doctors. Chain of survival The chain of",
      "event of cardiac arrest, the person does not wish to receive cardiopulmonary resuscitation. Other directives may be made to stipulate the desire for intubation in the event of respiratory failure or, if comfort measures are all that are desired, by stipulating that healthcare providers should \"allow natural death\". Several organizations promote the idea of a chain of survival. The chain consists of the following \"links\": If one or more links in the chain are missing or delayed, then the chances of survival drop significantly. These protocols are often initiated by a code blue, which usually denotes impending or acute onset"
    ]
  ],
  [
    "The main cause of fatigue in high intensity exercise is likely due to central nervous system fatigue, which can result from changes in neurotransmitter concentration and increased input requirements for sustained force.",
    [
      "difficulty in accomplishing consistent exercise. The main cause of fatigue in chronic fatigue syndrome most likely lies in the central nervous system. A defect in one of its components could cause a greater requirement of input to result in sustained force. It has been shown that with very high motivation, subjects with chronic fatigue can exert force effectively. Further investigation into central nervous system fatigue may result in medical applications. Central nervous system fatigue Central nervous system fatigue, or central fatigue, is a form of fatigue that is associated with changes in the synaptic concentration of neurotransmitters within the central",
      "Fatigue Fatigue is a subjective feeling of tiredness that has a gradual onset. Unlike weakness, fatigue can be alleviated by periods of rest. Fatigue can have physical or mental causes. Physical fatigue is the transient inability of a muscle to maintain optimal physical performance, and is made more severe by intense physical exercise. Mental fatigue is a transient decrease in maximal cognitive performance resulting from prolonged periods of cognitive activity. It can manifest as somnolence, lethargy, or directed attention fatigue. Medically, fatigue is a non-specific symptom, which means that it has many possible causes and accompanies many different conditions. Fatigue",
      "fatigue is only caused by mechanical failure of the exercising muscles (\"peripheral fatigue\"). Instead, the brain models the metabolic limits of the body to ensure that whole body homeostasis is protected, in particular that the heart is guarded from hypoxia, and an emergency reserve is always maintained. The idea of the central governor has been questioned since \u2018physiological catastrophes\u2019 can and do occur suggesting athletes (such as Dorando Pietri, Jim Peters and Gabriela Andersen-Schiess) can over-ride the \u2018\u2018central governor\u2019. The exercise fatigue has also been suggested to be effected by: Prolonged exercise such as marathons can increase cardiac biomarkers such"
    ]
  ],
  [
    "The most common finding with clinic defecation imaging is the uptake of gallium in a wide range of locations which do not indicate a positive finding, including soft tissues, liver, and bone.",
    [
      "imaging to reduce bowel activity and reduce dose to large bowel; however, the usefulness of bowel preparation is controversial. 10% to 25% of the dose of gallium-67 is excreted within 24 hours after injection (the majority of which is excreted through the kidneys). After 24 hours the principal excretory pathway is colon. The \"target organ\" (organ that receives the largest radiation dose in the average scan) is the colon (large bowel). In a normal scan, uptake of gallium is seen in wide range of locations which do not indicate a positive finding. These typically include soft tissues, liver, and bone.",
      "a computer monitor and examined by the doctor. If the electrical activity of the contractions appear normal, but the patient still results in constipation, it would indicate that there is a problem in the muscle activity or that there might be a tear in the muscle. This can help lead to a diagnosis of dyssynergia or an alternative surgical cure. In defecography studies, doctors take an X-Ray of the patient and examine their rectum as it empties during defecation. Before the examination, patients are instructed to drink barium an hour before the examination. Barium paste is then inserted into the",
      "barium paste does not stay in the rectum. Cinedefecography is a technique that is an evolution of defecagography. The defecation cycle is recorded as a continuous series rather than individual still radiographs. More recent techniques involve the use of advanced, cross-sectional imaging modalities such as magnetic resonance imaging. This is known as dynamic pelvic MRI, or MRI proctography. The MRI proctography also called MRI defecography is not as efficient as conventional x-ray defecography for some problems. Defecography Defecography (also known as proctography, defecating/defecation proctography, evacuating/evacuation proctography or dynamic rectal examination) is a type of medical radiological imaging in which the"
    ]
  ],
  [
    "Common factors contributing to skin excoriation disorder include elevated levels of turmoil, arousal, or stress, impaired stress response, skin conditions such as keratosis pilaris, psoriasis, and eczema, and certain stressful events such as marital conflicts or deaths of friends or family. (Excoriation disorder)",
    [
      "shown that excoriation disorder presented suicidal ideation in 12% of individuals with this condition, suicide attempts in 11.5% of individuals with this condition, and psychiatric hospitalizations in 15% of individuals with this condition. There have been many different theories regarding the causes of excoriation disorder including biological and environmental factors. A common hypothesis is that excoriation disorder is often a coping mechanism to deal with elevated levels of turmoil, arousal or stress within the individual, and that the individual has an impaired stress response. A review of behavioral studies found support in this hypothesis in that skin-picking appears to be",
      "prevalence of excoriation disorder is not well understood. Estimates of prevalence of the condition range from 1.4 to 5.4% in the general population. One U.S. telephone survey found that 16.6% of respondents \"picked their skin to the point of noticeable tissue damage\" and that 1.4% would qualify as meeting the requirements of excoriation disorder. Another community survey found a rate of 5.4% had excoriation disorder. A survey of college students found a rate of 4%. One study found that among non-disabled adults, 63% of individuals engaged in some form of skin picking and 5.4% engaged in serious skin picking. Lastly,",
      "a survey of dermatology patients found that 2% suffered from excoriation disorder. In some patients excoriation disorder begins with the onset of acne in adolescence, but the compulsion continues even after the acne has gone away. Skin conditions such as keratosis pilaris, psoriasis, and eczema can also provoke the behavior. In patients with acne, the grooming of the skin is disproportionate to the severity of the acne. Certain stressful events including marital conflicts, deaths of friends or family, and unwanted pregnancies have been linked to the onset of the condition. If excoriation disorder does not occur during adolescence another common"
    ]
  ],
  [
    "Thermography may help aid in the prediction of systemic sclerosis if suspected to be secondary to the condition.",
    [
      "as the vascular disease progresses. Its pathophysiology includes hyperactivation of the sympathetic nervous system causing extreme vasoconstriction of the peripheral blood vessels, leading to tissue hypoxia. Distinguishing Raynaud's disease (primary Raynaud's) from phenomenon (secondary Raynaud's) is important. Looking for signs of arthritis or vasculitis, as well as a number of laboratory tests, may separate them. If suspected to be secondary to systemic sclerosis, one tool which may help aid in the prediction of systemic sclerosis is thermography. A careful medical history will often reveal whether the condition is primary or secondary. Once this has been established, an examination is largely",
      "multiple sclerosis (MS) or neuromyelitis optica. In order for such a diagnosis, multiple sites in the central nervous system must present lesions, typically over multiple episodes, and for which no other diagnosis is likely. A clinically definitive diagnosis of MS is made once an MRI detects lesions in the brain, consistent with those typical of MS. Other diagnostics include cerebrospinal fluid analysis and evoked response testing. Currently it is considered that the best predictor of future development of clinical multiple sclerosis is the number of T2 lesions visualized by magnetic resonance imaging during the CIS. It is normal to evaluate",
      "of other conditions. Also, patients with SS symptoms approach different specialities for treatment, which can make diagnosis difficult. Since dry eyes and dry mouth are very common symptoms, and frequently occur in people over 40, people often think the symptoms are age-related and ignore them. However, some medications can cause symptoms similar to those of SS. The combination of several tests, which can be done in a series, can eventually diagnose SS. SS is usually classified as either 'primary' or 'secondary'. Primary Sj\u00f6gren syndrome occurs by itself and secondary Sj\u00f6gren syndrome occurs when another connective tissue disease is present. Blood"
    ]
  ],
  [
    "A palpable left parasternal impulse suggests the possibility of severe left-sided heart failure.",
    [
      "the palpitations. Palpitation can be attributed to one of four main causes: Anxiety and stress elevate the body's level of cortisol and adrenaline, which in turn can interfere with the normal functioning of the parasympathetic nervous system resulting in overstimulation of the vagus nerve. Vagus nerve induced palpitation is felt as a thud, a hollow fluttery sensation, or a skipped beat, depending on at what point during the heart's normal rhythm the vagus nerve fires. In many cases, the anxiety and panic of experiencing palpitations causes a sufferer to experience further anxiety and increased vagus nerve stimulation. The link between",
      "Shock (circulatory) Shock is the state of not enough blood flow to the tissues of the body as a result of problems with the circulatory system. Initial symptoms may include weakness, fast heart rate, fast breathing, sweating, anxiety, and increased thirst. This may be followed by confusion, unconsciousness, or cardiac arrest as complications worsen. Shock is divided into four main types based on the underlying cause: low volume, cardiogenic, obstructive, and distributive shock. Low volume shock may be from bleeding, vomiting, or pancreatitis. Cardiogenic shock may be due to a heart attack or cardiac contusion. Obstructive shock may be due",
      "stronger systolic pulse. There may initially be a tachycardia as a compensatory mechanism to try to keep the cardiac output constant. Begin by palpating the radial or femoral arteries, feeling for a regular rhythm but alternating strong and weak pulses. Next use a blood pressure cuff to confirm the finding: Inflate the blood pressure cuff past systolic pressure and then slowly lower cuff pressure towards the systolic level. If alternating loud & soft Korotkoff sounds are heard, pulsus alternans is indicated. A finding of pulsus alternans is indicative of severe left-sided heart failure. D.H. Lawrence elegantly describes pulsus alternans in"
    ]
  ],
  [
    "The signs and symptoms of the major stages of hypovolemic shock include confusion and weakness, low blood pressure, decreased urine output, and fast heart rate, with additional symptoms such as pain, deformity, guarding, discoloration, or swelling in the chest and abdomen.",
    [
      "game of tennis: 15, 15\u201330, 30\u201340 and 40. It is basically the same as used in classifying bleeding by blood loss. The signs and symptoms of the major stages of hypovolemic shock include: In a hospital, physicians respond to a case of hypovolemic shock by conducting these investigations: The most important step in treatment of hypovolemic shock is to identify and control the source of bleeding. Medical personnel should immediately supply emergency oxygen to increase efficiency of the patient's remaining blood supply. This intervention can be life-saving. The use of intravenous fluids (IVs) may help compensate for lost fluid volume,",
      "as confusion and weakness. While the general signs for all types of shock are low blood pressure, decreased urine output, and confusion, these may not always be present. While a fast heart rate is common, those on \u03b2-blockers, those who are athletic and in 30% of cases of those with shock due to intra abdominal bleeding may have a normal or slow heart rate. Specific subtypes of shock may have additional symptoms. Hypovolemia is a direct loss of effective circulating blood volume leading to: The severity of hemorrhagic shock can be graded on a 1\u20134 scale on the physical signs.",
      "the chest and abdomen for pain, deformity, guarding, discoloration or swelling. Bleeding into the abdominal cavity can cause the classical bruising patterns of Grey Turner's sign or Cullen's sign. Usually referred to as a \"class\" of shock. Most sources state that there are 4 stages of hypovolemic shock; however, a number of other systems exist with as many as 6 stages. The 4 stages are sometimes known as the \"Tennis\" staging of hypovolemic shock, as the stages of blood loss (under 15% of volume, 15\u201330% of volume, 30\u201340% of volume and above 40% of volume) mimic the scores in a"
    ]
  ],
  [
    "Morphine exerts its principal pharmacological effects on the central nervous system and gastrointestinal tract, providing analgesia and sedation through activation of the \u03bc-opioid receptor (MOR) and slowing intestinal motility.",
    [
      "be a partial agonist or even antagonist. In clinical settings, morphine exerts its principal pharmacological effect on the central nervous system and gastrointestinal tract. Its primary actions of therapeutic value are analgesia and sedation. Activation of the MOR is associated with analgesia, sedation, euphoria, physical dependence, and respiratory depression. Morphine is also a \u03ba-opioid receptor (KOR) and \u03b4-opioid receptor (DOR) agonist. Activation of the KOR is associated with spinal analgesia, miosis (pinpoint pupils), and psychotomimetic effects. The DOR is thought to play a role in analgesia. Although morphine does not bind to the \u03c3 receptor, it has been shown that",
      "high densities in the posterior amygdala, hypothalamus, thalamus, nucleus caudatus, putamen, and certain cortical areas. They are also found on the terminal axons of primary afferents within laminae I and II (substantia gelatinosa) of the spinal cord and in the spinal nucleus of the trigeminal nerve. Morphine is a phenanthrene opioid receptor agonist \u2013 its main effect is binding to and activating the \u03bc-opioid receptor (MOR) in the central nervous system. Its intrinsic activity at the MOR is heavily dependent on the assay and tissue being tested; in some situations it is a full agonist while in others it can",
      "slows intestinal motility, giving the intestines greater time to absorb fluid in the stool. Despite the historically negative view of opium as a cause of addiction, the use of morphine and other derivatives isolated from opium in the treatment of chronic pain has been reestablished. If given in controlled doses, modern opiates can be an effective treatment for neuropathic pain and other forms of chronic pain. Opium contains two main groups of alkaloids. Phenanthrenes such as morphine, codeine, and thebaine are the main psychoactive constituents. Isoquinolines such as papaverine and noscapine have no significant central nervous system effects. Morphine is"
    ]
  ],
  [
    "Some potential causes of headaches include dehydration, fatigue, sleep deprivation, stress, medication side effects, recreational drug use, mechanical factors like whiplash, emotional factors like post-traumatic stress disorder, and neurotransmitter imbalances like serotonin dysfunction.",
    [
      "Headache Headache is the symptom of pain anywhere in the region of the head or neck. It occurs in migraines (sharp, or throbbing pains), tension-type headaches, and cluster headaches. Frequent headaches can affect relationships and employment. There is also an increased risk of depression in those with severe headaches. Headaches can occur as a result of many conditions whether serious or not. There are a number of different classification systems for headaches. The most well-recognized is that of the International Headache Society. Causes of headaches may include dehydration, fatigue, sleep deprivation, stress, the effects of medications, the effects of recreational",
      "there is wide heterogeneity in the source and causes of headaches. They point out that the International Headache Society lists 14 known causes of headaches, as well. Furthermore, the headaches may be better accounted for by mechanical causes, such as whiplash, which is often mistaken for PCS. An additional possibility is that Post-traumatic Stress Disorder can account for some cases diagnosed as PCS, but for emotional regulation as well. Depression, post-traumatic stress disorder, and chronic pain share symptoms resembling those of PCS. One study found that while people with chronic pain without TBI do report many symptoms similar to those",
      "be sub-classified as Various precipitating factors may cause tension-type headaches in susceptible individuals: Tension-type headaches may be caused by muscle tension around the head and neck. Another theory is that the pain may be caused by a malfunctioning pain filter which is located in the brain stem. The view is that the brain misinterprets information\u2014for example from the temporal muscle or other muscles\u2014and interprets this signal as pain. One of the main neurotransmitters that is probably involved is serotonin. Evidence for this theory comes from the fact that chronic tension-type headaches may be successfully treated with certain antidepressants such as"
    ]
  ],
  [
    "The blood continues to flow through the vessels between contractions of the left ventricle because the left ventricular systole pumps oxygenated blood through the aorta and arteries to provide systemic circulation to all body systems.",
    [
      "Simultaneously, contractions of the left ventricular systole provide systemic circulation of oxygenated blood to all body systems by pumping blood through the aortic valve, the aorta, and all the arteries. (Blood pressure is routinely measured in the larger arteries off the left ventricle during the left ventricular systole). Cardiac cycle The cardiac cycle is the performance of the human heart from the beginning of one heartbeat to the beginning of the next. It consists of two periods: one during which the heart muscle relaxes and refills with blood, called diastole (die-ASS-toe-lee), followed by a period of robust contraction and pumping",
      "all body systems, and simultaneously pumping oxygen-poor blood from the right ventricle through the pulmonic valve and pulmonary artery to the lungs. Thus, the pairs of chambers (upper atria and lower ventricles) contract in alternating sequence to each other. First, the two atria feed blood simultaneously into the ventricles, which two, contracting together, then pump blood out of the heart to the body systems, including the lungs for resupply of oxygen. Cardiac systole is the contraction of the cardiac muscle in response to an electrochemical stimulus to the heart's cells (cardiomyocytes). Cardiac output (CO) is the volume of blood pumped",
      "simultaneously into the two bottom chambers, or ventricles. Blood first enters the heart at the right atrium, which then empties blood into the right ventricle, which pumps the blood into the lungs through the pulmonary artery to get oxygen. From the lungs, the blood enters the left atrium through the pulmonary vein; the left atrium empties into the left ventricle, which pumps the blood into the aorta and from there reaches the rest of the body. Because the left ventricle is responsible for getting blood to the entire body through the aorta, it is usually the biggest and strongest chamber"
    ]
  ],
  [
    "Medical conditions such as asthma, which may be potentially aggravated by stress, can prevent a patient from being able to use an inhaler for conscious sedation.",
    [
      "Inhalation sedation Inhalation sedation is a form of conscious sedation where an inhaled drug should: The following are possible uses for conscious sedation \"dental anxiety and phobia, a need for prolonged or traumatic dental procedures, medical conditions potentially aggravated by stress (such as ischaemic heard disease, hypertension, asthma and epilepsy), medical or behavioural conditions affecting a patient's ability to cooperate, special care requirements\". Complications arising in inhalation sedation are rare and are termed as are those which require intervention in order to correct adverse physiological consequences that can inadvertently accompany the sedation technique. They include \"over-sedation, respiratory depression/apnoea, unconscious patient,",
      "comfortable during a medical procedure, along with other drugs to help relax the body. It can also help control breathing, blood pressure, blood flow, and heart rate and rhythm, when needed. There are four levels of sedation by anesthesia. This level called \"minimal sedation\" causes anxiolysis, a drug induced state in which the patient responds normally to verbal commands. Although the cognition and coordination of the patient are impaired, cardiovascular and ventilatory functions remain unaffected. In this level called \"moderate sedation/analgesia\" or \"conscious sedation\", a drug induced depression of consciousness during which the patient responds purposefully to verbal commands, either",
      "or discomfort. The aim of conscious sedation or monitored anesthetic care is to provide a safe and comfortable anesthetic while maintaining the patient's ability to follow commands. Under certain circumstances, a general anesthetic, whereby the patient is completely unconscious, may be unnecessary and/or undesirable. For instance, with a cesarean delivery, the goal is to provide comfort with neuraxial anesthetic yet maintain consciousness so that the mother can participate in the birth of her child. Other circumstances may include, but are not limited to, procedures that are minimally invasive or purely diagnostic (and thus not uncomfortable). Sometimes, the patient's health may"
    ]
  ],
  [
    "Residue left behind from solvents when removing adhesive strips can be effectively removed by using a cloth or rag soaked in the appropriate solvent to wipe down the surface of the adherend and ensure all contaminants are removed (Adhesive bonding).",
    [
      "a cloth or rag soaked in solvent, which can be used to wipe down the surface of the adherend to remove contaminants. It is important that all residue that had been left behind from the solvents be removed, so that there is no detrimental effects to the adhesive bonding. After degreasing, a good test to determine cleanliness of the surface is to use a drop of water. If the drop spreads on the surface, a low contact angle and good wettability has been achieved, which indicates the surface is clean and ready for application of the adhesive. If the drop",
      "Every adhesive has a particular solvent that work best to break down its chemical composition. Color, hardness, and other physical properties will allow for identification of the adhesive. The adhesive can be soften once exposed, either in a liquid of vapor form, of the solvent for some time. The length of time depends on the solubility of the adhesive and the thickness of the joint. Porous bodies, low-fired clays, are sometimes pre-soaked in water to prevent the adhesive from being drawn back into the body once it joins with the removal solution. If the adhesive that is being removed is",
      "such as dust, paper, and oils will reduce the contact area for the adhesives and lower the adhesives bonding strength. If contaminants are present it may be necessary to clean the surface with a suitable solvent such as benzene, alcohols, esters, or ketones. Surfaces with textures may also lower the bonding strength of an adhesive. Textures create an uneven surface which will make it harder for the adhesives to be in contact with the surface thus lowers its wetting ability. Water or moisture of any form will reduce surface adhesion and reduce tape tackiness. Moisture can be removed off the"
    ]
  ],
  [
    "Both children and adults should receive a rate of 100 chest compressions per minute during CPR.",
    [
      "children. Both children and adults should receive a hundred chest compressions per minute. Other exceptions besides children include cases of drownings and drug overdose. In both these cases, compressions and rescue breaths are recommended if the bystander is trained and is willing to do so. As per the American Heart Association, the beat of the Bee Gees song \"Stayin' Alive\" provides an ideal rhythm in terms of beats per minute to use for hands-only CPR. One can also hum Queen's \"Another One Bites The Dust\", which is 110 beats-per-minute and contains a memorable repeating drum pattern. For those in cardiac",
      "aspect of CPR are: few interruptions of chest compressions, a sufficient speed and depth of compressions, completely relaxing pressure between compressions, and not ventilating too much. It is unclear if a few minutes of CPR before defibrillation results in different outcomes than immediate defibrillation. A universal compression to ventilation ratio of 30:2 is recommended for adults. With children, if at least 2 trained rescuers are present a ratio of 15:2 is preferred. According to AHA 2015 Guidelines In newborns a ratio is 30:2 if One rescuer and 15:2 if 2 rescuers. If an advanced airway such as an endotracheal tube",
      "or nose (mouth-to-mouth resuscitation) or using a device that pushes air into the subject's lungs (mechanical ventilation). Current recommendations place emphasis on early and high-quality chest compressions over artificial ventilation; a simplified CPR method involving chest compressions only is recommended for untrained rescuers. In children, however, only doing compressions may result in worse outcomes. Chest compression to breathing ratios is set at 30 to 2 in adults. CPR alone is unlikely to restart the heart. Its main purpose is to restore partial flow of oxygenated blood to the brain and heart. The objective is to delay tissue death and to"
    ]
  ],
  [
    "Marathon runners consistently present large aerobic capacities and high levels of VO2 max for success in endurance events.",
    [
      "certain physiological characteristics of marathon runners exist. The differing efficiency of certain physiological features in marathon runners evidence the variety of finishing times among elite marathon runners that share similarities in many physiological characteristics. Aside from large aerobic capacities and other biochemical mechanisms, external factors such as the environment and proper nourishment of a marathon runner can further the insight as to why marathon performance is variable despite ideal physiological characteristics obtained by a runner. The first marathon ever run was an unintentional 25 mile trek performed by Pheidippides. Pheidippides was a Greek soldier who ran to Athens from the",
      "to how lactate threshold effects endurance performance. The contribution of one's blood lactate levels accumulating is attributed to potential skeletal muscle hypoxemia but also to the production of more glucose that can be used as energy. The inability to establish a singular set of physiological contributions to blood lactate accumulation's effect on the exercising individual creates a correlative role for lactate threshold in marathon performance as opposed to a causal role. In order to sustain high intensity running, a marathon runner must obtain sufficient glycogen stores. Glycogen can be found in the skeletal muscles or liver. With low levels of",
      "oxygen within muscle cells. It is said that VO is one of the most salient indicators of endurance exercise performance. The VO of an elite runner at maximal exercise is almost 2 times the value of a fit or trained adult at maximal exercise. Marathon runners demonstrate physiological characteristics that enable them to deal with the high demands of a 26.2 mile (42.195 km) run. The primary components of an individual's VO are the properties of aerobic capacity that influence the fractional utilization (%VO) of this ability to take up and consume oxygen during exhaustive exercise. The transportation of large"
    ]
  ],
  [
    "should not be rinsed out with water immediately after use in order to maximize the benefits of fluoride residue left from toothpaste.",
    [
      "there was a limited evidence base for best practice. Common use involves rinsing the mouth with about 20-50 ml (2/3 fl oz) of mouthwash. The wash is typically swished or gargled for about half a minute and then spat out. Most companies suggest not drinking water immediately after using mouthwash. In some brands, the expectorate is stained, so that one can see the bacteria and debris. Mouthwash should not be used immediately after brushing the teeth so as not to wash away the beneficial fluoride residue left from the toothpaste. Similarly, the mouth should not be rinsed out with water",
      "of healing wounds and to prevent infection. Some oral surgeons consider salt water mouthwashes the mainstay of wound cleanliness after surgery. In dental extractions, hot salt water mouthbaths should start about 24 hours after a dental extraction. The term \"mouth bath\" implies that the liquid is passively held in the mouth rather than vigorously swilled around, which could dislodge a blood clot. Once the blood clot has stabilized, the mouth wash can be used more vigorously. These mouthwashes tend to be advised about 6 times per day, especially after meals to remove food from the socket. Sodium lauryl sulfate (SLS)",
      "the mouth moist in xerostomia (dry mouth). Cosmetic mouthrinses temporarily control or reduce bad breath and leave the mouth with a pleasant taste. Rinsing with water or mouthwash after brushing with a fluoride toothpaste can reduce the availability of salivary fluoride. This can lower the anti-cavity re-mineralization and antibacterial effects of fluoride. Fluoridated mouthwash may mitigate this effect or in high concentrations increase available fluoride. A group of experts discussing post brushing rinsing in 2012 found that although there was clear guidance given in many public health advice publications to \"spit, avoid rinsing with water/excessive rinsing with water\" they believed"
    ]
  ],
  [
    " is typically associated with the production of lactate, which is a limiting factor of exercise performance, but regular endurance exercise can lead to adaptations in skeletal muscle that prevent lactate levels from rising during strength training by altering the LDH isoenzyme complex composition and decreasing the activity of the lactate generating enzyme LDHA while increasing the activity of the lactate metabolizing enzyme LDHB.",
    [
      "an increase equal to 4.0mM; it then accumulates at the muscle and then moves to the bloodstream. Regular endurance exercise leads to adaptations in skeletal muscle which prevent lactate levels from rising. This is mediated via activation of PGC-1\u03b1 which alters the isoenzyme composition of the LDH complex and decreases the activity of the lactate generating enzyme LDHA, while increasing the activity of the lactate metabolizing enzyme LDHB. The lactate threshold is a useful measure for deciding exercise intensity for training and racing in endurance sports (e.g., long distance running, cycling, rowing, long distance swimming and cross country skiing), but",
      "to how lactate threshold effects endurance performance. The contribution of one's blood lactate levels accumulating is attributed to potential skeletal muscle hypoxemia but also to the production of more glucose that can be used as energy. The inability to establish a singular set of physiological contributions to blood lactate accumulation's effect on the exercising individual creates a correlative role for lactate threshold in marathon performance as opposed to a causal role. In order to sustain high intensity running, a marathon runner must obtain sufficient glycogen stores. Glycogen can be found in the skeletal muscles or liver. With low levels of",
      "and types of to target specific muscle groups. Strength training is primarily an anaerobic activity, although some proponents have adapted it to provide the benefits of aerobic exercise through circuit training. Strength training is typically associated with the production of lactate, which is a limiting factor of exercise performance. Regular endurance exercise leads to adaptations in skeletal muscle which can prevent lactate levels from rising during strength training. This is mediated via activation of PGC-1alpha which alter the LDH (lactate dehydrogenase) isoenzyme complex composition and decreases the activity of the lactate generating enzyme LDHA, while increasing the activity of the"
    ]
  ],
  [
    "Static stretching should be held for at least ten seconds in order to aid in pain relief. (Dynamic stretching increases flexibility; minutes is considered adequate)",
    [
      "a feeling of pain and needs to be held for at least ten seconds. Increasing the range of motion creates good posture and develops proficient performance in everyday activities increasing the length of life and overall health of the individual. Dynamic flexibility is classified as the ability to complete a full range of motion of a joint. It also controls movement as the speed increases while stretching parts of the body. This form of stretching prepares the body for physical exertion and sports performance. In the past it was the practice to undertake static stretching before exercise. Dynamic stretching increases",
      "minutes is considered adequate. Stretching, especially static stretching allows the muscles to be elongated and lengthened. This is the next step athletes should take to cool down. Rehydration is an essential part of the procedure and should be done either during stretching and light intensity or after these steps. Refuelling the body with water and sports drinks will keep the body hydrated. Stretching is a very important factor in the procedure of cooling down. Stretching allows the bodies muscles to build elasticity and repair from aerobic and anaerobic exercise. Static stretching is the appropriate form of stretching to aid in",
      "considered largely self-limiting and spontaneous recovery is usually on the order of a few days or a week to six weeks or longer if left untreated. Most practitioners agree that spasm, strain, or pain in any muscle can often be treated by regular stretching exercise of that muscle, no matter the cause of the pain. Stretching is recommended every two to three waking hours. Anterior and posterior movement of the hip joint capsule may help optimize the patient's stretching capacity. The muscle can be manually stretched by applying pressure perpendicular to the long axis of the muscle and parallel to"
    ]
  ],
  [
    "is crucial in understanding the factors that regulate stroke volume in the cardiovascular system, including preload, afterload, heart size, contractility, duration of contraction, and venous return.",
    [
      "\"Stroke work\" refers to the work, or pressure of the blood (\"P\") multiplied by the stroke volume. ESV and EDV are fixed variables. Heart rate and Stroke Volume are unfixed. Men, on average, have higher stroke volumes than women due to the larger size of their hearts. However, stroke volume depends on several factors such as heart size, contractility, duration of contraction, preload (end-diastolic volume), and afterload. Prolonged aerobic exercise training may also increase stroke volume, which frequently results in a lower (resting) heart rate. Reduced heart rate prolongs ventricular diastole (filling), increasing end-diastolic volume, and ultimately allowing more blood",
      "to be ejected. Stroke volume is intrinsically controlled by preload (the degree to which the ventricles are stretched prior to contracting). An increase in the volume or speed of venous return will increase preload and, through the Frank\u2013Starling law of the heart, will increase stroke volume. Decreased venous return has the opposite effect, causing a reduction in stroke volume. Elevated afterload (commonly measured as the aortic pressure during systole) reduces stroke volume. Though not usually affecting stroke volume in healthy individuals, increased afterload will hinder the ventricles in ejecting blood, causing reduced stroke volume. Increased afterload may be found in",
      "beats per minute\u2014while stroke volume (SV) can vary between , a factor of only 1.7. Diseases of the cardiovascular system are often associated with changes in \"Q\", particularly the pandemic diseases hypertension and heart failure. Increased \"Q\" can be associated with cardiovascular disease that can occur during infection and sepsis. Decreased \"Q\" can be associated with cardiomyopathy and heart failure. Sometimes, in the presence of ventricular disease associated with dilatation, EDV may vary. An increase in EDV could counterbalance LV dilatation and impaired contraction. From equation (), the resulting cardiac output Q may remain constant. The ability to accurately measure"
    ]
  ],
  [
    "The correct order of bones in the middle finger from the hand is metacarpal bone, proximal phalanx, and distal phalanx.",
    [
      "the hand: In the hand proper a total of 13 bones form part of the wrist: eight carpal bones\u2014scaphoid, lunate, triquetral, pisiform, trapezium, trapezoid, capitate, and hamate\u2014 and five metacarpal bones\u2014the first, second, third, fourth, and fifth metacarpal bones. The midcarpal joint is the S-shaped joint space separating the proximal and distal rows of carpal bones. The intercarpal joints, between the bones of each row, are strengthened by the radiate carpal and pisohamate ligaments and the palmar, interosseous, and dorsal intercarpal ligaments. Some degree of mobility is possible between the bones of the proximal row while the bones of the",
      "of eight small carpal bones. Each of these carpal bones has a different size and shape. They contribute towards the stability of the wrist and are ranked in two rows, each consisting of four bones. From lateral to medial and when viewed from anterior, the proximal row is formed by the: From lateral to medial and when viewed from anterior, the distal row is formed by the: Osteoarthritis of the wrist is predominantly a clinical diagnosis, and thus is primarily based on the patients medical history, physical examination and wrist X-rays. Medical history of the patient should include age, hand",
      "The phalangeal formula of the hand (which describes the number of bones in each finger starting with finger I) is ?-3-4-5-3. Each finger ends in a curved claw. The metacarpal (hand bone) corresponding to finger I is characteristically wide. The illium is short and tapers towards the rear. The top edge of the pubis is large and rounded while the back edge is arched, creating a large thyroid fenestra between the pubis and ischium. The ischium is larger than the pubis and has a bony spur which could have connected to tail muscles. The femur is S-shaped but the finer"
    ]
  ],
  [
    "Muscle contraction promotes glucose and amino acid uptake by muscle through the translocation of GLUT4 into the plasma membrane and the glucose-alanine cycle, allowing for increased energy production and efficient removal of metabolites from the muscle tissue (Smith, 2015).",
    [
      "of either exercise or muscle contraction. During exercise, the body needs to convert glucose to ATP to be used as energy. As G-6-P concentrations decrease, hexokinase becomes less inhibited, and the glycolytic and oxidative pathways that make ATP are able to proceed. This also means that muscle cells are able to take in more glucose as its intracellular concentrations decrease. In order to increase glucose levels in the cell, GLUT4 is the primary transporter used in this facilitated diffusion. Although muscle contractions function in a similar way and also induce the translocation of GLUT4 into the plasma membrane, the two",
      "cycle to form urea which is excreted through the kidneys. The glucose\u2013alanine cycle enables pyruvate and glutamate to be removed from the muscle and safely transported to the liver, where glucose is regenerated from pyruvate and then returned to muscle: this moves the energetic burden of gluconeogenesis to the liver instead of the muscle, and all available ATP in the muscle can be devoted to muscle contraction. It is a catabolic pathway, and relies upon protein breakdown in the muscle tissue. Whether and to what extent it occurs in non-mammals is unclear. Alterations in the alanine cycle that increase the",
      "They include molecules such as adenosine triphosphate (ATP), glycogen and creatine phosphate. ATP binds to the myosin head and causes the \u2018ratchetting\u2019 that results in contraction according to the sliding filament model. Creatine phosphate stores energy so ATP can be rapidly regenerated within the muscle cells from adenosine diphosphate (ADP) and inorganic phosphate ions, allowing for sustained powerful contractions that last between 5\u20137 seconds. Glycogen is the intramuscular storage form of glucose, used to generate energy quickly once intramuscular creatine stores are exhausted, producing lactic acid as a metabolic byproduct. Substrate shortage is one of the causes of metabolic fatigue."
    ]
  ],
  [
    "An ileostomy opening is used for excretion when patients require alternative means of excretion after a bowel resection, and potential complications include rectal perforation, parastomal hernia, bowel prolapse, and the need for a temporary ileostomy.",
    [
      "insufficient, either to prevent polyps from ever becoming cancerous or because they are causing or threatening bowel obstruction, such as in familial adenomatous polyposis, Peutz\u2013Jeghers syndrome, or other polyposis syndromes. Some patients require ileostomy or colostomy after this procedure as alternative means of excretion. Depending on which part and how much of the intestines are removed, there may be digestive and metabolic challenges afterward, such as short bowel syndrome. Types of enterectomy are named according to the relevant bowel segment, as follows: Bowel resection A bowel resection or enterectomy (\"enter-\" + \"-ectomy\") is a surgical procedure in which a part",
      "effects. The most common \u2013 but still rare \u2013 complication is a perforation of the rectum caused by the foreign object itself or attempts to remove it. Diagnosed perforations are operated immediately by opening the abdomen and removal or suturing of the perforated area. In order to suppress infections, antibiotics are usually prescribed. Often, a temporary ileostomy is necessary to protect the stitches. After a contrast medium applied by an enema proves the complete healing of the perforated area, the ileostomy is reversed. This usually takes between three and six months. Medical literature describes some deaths due to rectal foreign",
      "varied with each patient's bowel habit but that most patients developed a routine of every-other-day irrigation, whereas a few needed no irrigation. Parastomal hernia is the most common late complication of stomata through the abdominal wall, occurring in 10 to 25% of the patients. Prolapse of bowel wall through the stoma occasionally happens and can require reoperation to repair. Colostomy A colostomy is an opening (stoma) in the large intestine (colon), or the surgical procedure that creates one. The opening is formed by drawing the healthy end of the colon through an incision in the anterior abdominal wall and suturing"
    ]
  ],
  [
    "Before catheterizing a patient, it is important to assess their understanding of the process, provide education on catheter care, and monitor for complications.",
    [
      "dialysis state that before peritoneal dialysis should be implemented, the person's understanding of the process and support systems should be assessed, with education on how to care for the catheter and to address any gaps in understanding that may exist. The person should receive ongoing monitoring to ensure adequate dialysis, and be regularly assessed for complications. Finally, they should be educated on the importance of infection control and an appropriate medical regimen established with their cooperation. The abdomen is cleaned in preparation for surgery and a catheter is surgically inserted with one end in the abdomen and the other protruding",
      "dressing change, monitoring for signs of infection and for signs of catheter blockage. The patient will be given prescriptions for an antibiotic or anti-infective agent, a urinary anti-spasmodic, and a mild to moderate pain medication (no more than a few days worth of pain is expected). The patient will be instructed to optimize bed rest for the first two days after the operation, be limited to absolutely no lifting, and instructed to consume a high fiber diet and use a stool softener such as docusate sodium to help in avoiding straining during evacuation. After days 1 and 2, the patient",
      "body. To prevent overdoses, there is a limit on the number of times a patient can push the button. If a patient pushes the button too much at once, the PCA will reject the request. For the patient's bladder control, a catheter will be inserted so that a patient can urinate without having to move. A catheter is inserted because the patient will not have much free movement to be able to get up and walk to the bathroom. The most common type of catheter used after major surgeries is an indwelling Foley catheter. The indwelling Foley catheter is most"
    ]
  ],
  [
    "When ammonia is dissolved in water, hydrogen ions are formed through the self-ionization of water, as well as the formation of ammonium ions when the equilibrium shifts to the left at high pH levels.",
    [
      "gives ammonia. When ammonia is dissolved in water, a tiny amount of it converts to ammonium ions: The degree to which ammonia forms the ammonium ion depends on the pH of the solution. If the pH is low, the equilibrium shifts to the right: more ammonia molecules are converted into ammonium ions. If the pH is high (the concentration of hydrogen ions is low), the equilibrium shifts to the left: the hydroxide ion abstracts a proton from the ammonium ion, generating ammonia. Formation of ammonium compounds can also occur in the vapor phase; for example, when ammonia vapor comes in",
      "NH-N removal is also commonly used in scientific publications as a short way to depict Ammonia in water, and not the measure of its quantity. Ammonium is an ionized form of ammonia. The chemical structure for ammonium is NH. The chemical structure for ammonia is NH. Ammonia is highly soluble in water. Ammonia reacts with water (HO) and forms the ionized form: The reaction is reversible. The hydroxide ion (OH) plus NH forms NH + HO. The percentage of ammonia increases with increasing alkalinity of dissolved ammonium in water. Ammonium ions are formed with increasing acidity of dissolved ammonia in",
      "concentration, measured as pH, is also responsible for the acidic or basic nature of a compound. Water molecules split to form H and hydroxide anions. This process is referred to as the self-ionization of water. Hydrogen ion A hydrogen ion is created when a hydrogen atom loses or gains an electron. A positively charged hydrogen ion (or proton) can readily combine with other particles and therefore is only seen isolated when it is in a gaseous state or a nearly particle-free space. Due to its extremely high charge density of approximately 2\u00d710 times that of a sodium ion, the bare"
    ]
  ],
  [
    "The sternal rub is not recommended due to potential bruising on fair-skinned patients.",
    [
      "paresthesia or paralysis in a particular limb. Central stimuli are likely to have to be applied for at least 15 and potentially up to 30 seconds in order for the clinician to accurately assess their efficacy. The various acceptable central stimuli have been criticised or deemed suboptimal for various reasons. For instance, the sternal rub may leave bruising (especially on fair skinned patients) and for this reason is discouraged by some. It has been claimed that supraorbital pressure and trapezius squeeze are more effective than the sternal rub or peripheral stimulation, but sternal rub remains the most common. Supraorbital and",
      "are normally considered acceptable. The pain stimulus can be applied centrally and/or peripherally, and there are benefits and drawbacks to each type of stimulus, depending on the type of patient and the response being assessed. A central stimulus is one which can only be successfully found if the brain is involved in the response to the pain (as opposed to peripheral stimuli, which can induce a result as a result of reflex. The four commonly used central pain stimuli are: Central stimuli should always be used when attempting to assess if the patient is localising to pain (i.e. moving their",
      "arms to the site where the pain is being applied), however it has been suggested that central stimuli are less suitable for the assessment of eye opening, compared to peripheral stimuli, as they can cause grimacing. There is also a statistical reason behind central pain stimuli being inaccurate, especially regarding the GCS, which depending on the patient's eye response, the total score, and thus severity of patients' condition, can be altered with varying prognostic accuracy. If the patient reacts to the central pain stimulus normally, then a peripheral stimulus is unlikely to be required, unless there is suspicion of localised"
    ]
  ],
  [
    "To achieve plasma levels of 12-14 \u00b5g/mL of AMPT after 1 to 3 hours, a dose of approximately 250-300 milligrams of AMPT would be needed based on similar drug quantification methods for MDPV and DMT.",
    [
      "chain then takes place, followed by an oxidation of the pyrrolidine ring to the corresponding lactam, with subsequent detachment and ring opening to the corresponding carboxylic acid. MDPV may be quantified in blood, plasma or urine by gas chromatography-mass spectrometry or liquid chromatography-mass spectrometry to confirm a diagnosis of poisoning in hospitalized patients or to provide evidence in a medicolegal death investigation. Blood or plasma MDPV concentrations are expected to be in a range of 10\u201350 \u03bcg/L in persons using the drug recreationally, >50 \u03bcg/L in intoxicated patients, and >300 \u03bcg/L in victims of acute overdose. In the UK, following",
      "and nanograms per kilogram, respectively): A 2013 study found DMT in microdialysate obtained from a rat's pineal gland, providing evidence of endogenous DMT in the mammalian brain. DMT may be measured in blood, plasma or urine using chromatographic techniques as a diagnostic tool in clinical poisoning situations or to aid in the medicolegal investigation of suspicious deaths. In general, blood or plasma DMT levels in recreational users of the drug are in the 10\u201330 \u03bcg/L range during the first several hours post-ingestion. Less than 0.1% of an oral dose is eliminated unchanged in the 24-hour urine of humans. Before techniques",
      "for occupational exposure to PCP are 5 mg/L in an end-of-shift plasma specimen and 2 mg/g creatinine in an end-of-shift urine specimen. PCP is quickly absorbed through the gastrointestinal tract following ingestion. Accumulation is not common, but if it does occur, the major sites are the liver, kidneys, plasma protein, spleen and fat. Unless kidney and liver functions are impaired, PCP is quickly eliminated from tissues and blood, and is excreted, mainly unchanged or in conjugated form, via the urine. Single doses of PCP have half-lives in blood of 30 to 50 hours in humans. Biomagnification of PCP in the"
    ]
  ],
  [
    "The digital rectal examination (DRE) is not a reliable screening tool for colorectal cancer due to insufficient evidence of its effectiveness in detecting the disease.",
    [
      "to other causes. During a digital rectal examination (DRE), a healthcare provider slides a gloved finger into the rectum and presses on the prostate, to check its size and to check whether any lumps are present. If the examination suggests anomalies, a PSA test is performed. If an elevated PSA level is found, a follow-up test is then performed. A 2018 review recommended against primary care screening for prostate cancer with DRE due to the lack of evidence of the effectiveness of the practice. Prostate biopsies are considered the gold standard in detecting prostate cancer. Infection is a possible risk.",
      "Prostate cancer screening Prostate cancer screening is the screening process used to detect undiagnosed prostate cancer in those without signs or symptoms. When abnormal prostate tissue or cancer is found early, it may be easier to treat and cure, but it is unclear if early detection reduces mortality rates. Screening precedes a diagnosis and subsequent treatment. The digital rectal examination (DRE) is one screening tool during which the prostate is manually assessed through the wall of the rectum. The second screening tool is the measurement of prostate-specific antigen (PSA) in the blood. The evidence remains insufficient to determine whether screening",
      "with prostate-specific antigen (PSA) or digital rectal exam (DRE) reduces mortality from prostate cancer. A 2013 Cochrane review concluded PSA screening results in \"no statistically significant difference in prostate cancer-specific mortality...\" The American studies were determined to have a high bias. European studies included in this review were of low bias and one reported \"a significant reduction in prostate cancer-specific mortality.\" PSA screening with DRE was not assessed in this review. DRE was not assessed separately. Guidelines generally recommend that the decisions whether or not to screen be based on shared decision-making. This involves men being informed of the risks"
    ]
  ],
  [
    "Whispered pectoriloquy is heard more clearly in areas of lung consolidation because sound travels faster and with lower loss of intensity through solid or liquid media compared to gaseous media.",
    [
      "or \"solid mass,\" respectively, in the lung) versus gaseous (air in the lung) media. Whispered pectoriloquy is a clinical test typically performed during a medical physical examination to evaluate for the presence of lung consolidation, which could be caused by cancer (solid mass) or pneumonia (fluid mass). The whispered pectoriloquy test is similar to bronchophony, but not so much egophony.<ref name=\"ISBN/ISSN: 9781451175646\"></ref> In bronchophony, the physician often asks the patient to say \"ninety-nine\" or \"baseball\" while listening over a lung field with a stethoscope. The spoken word sounds will be louder in areas where consolidation is present. The only difference",
      "Whispered pectoriloquy Whispered pectoriloquy refers to an increased loudness of whispering noted during auscultation with a stethoscope on the lung fields on a patient's torso. Usually spoken sounds of a whispered volume by the patient would not be heard by the clinician auscultating a lung field with a stethoscope. However, in areas of the lung where there is lung consolidation, these whispered spoken sounds by the patient (such as saying 'ninety-nine') will be clearly heard through the stethoscope. This increase in sound exists because sound travels faster and thus with lower loss of intensity through liquid or solid (\"fluid mass\"",
      "between whispered pectoriloquy and bronchophony is the volume at which the patient is asked by the clinician to repeat \"ninety-nine\" or \"baseball.\" That is, in whispered pectoriloquy, the repeated words are whispered at low volume, and in bronchophony, they are spoken at normal volume. The clinical observation being determined is whether or not an increase in volume is heard at the clinician's stethoscope over the lung field being auscultated which would indicate lung consolidation. In UK bronchi is often called \"vocal resonance\" and is similar to \"tactile vocal fremitus\" (TVF); the difference being that in TVF the sensor is the"
    ]
  ],
  [
    "Wound cleansing plays a role in wound management by aiding in the removal of surface contaminants, bacteria, and debris from the wound surface and surrounding skin as part of the WBP model (Wound bed preparation).",
    [
      "when: Wound cleansing is often undertaken as a ritual exercise rather than as an evidence-based activity. However, it has a role to play in all four domains of the WBP model. Wounds that are \u2018clean\u2019 and progressing do not require extraneous cleansing. Wound cleansing forms an integral part of wound management and generally suggests the application of a fluid to aid the removal of surface contaminants, bacteria and debris from the wound surface and surrounding skin. Water as a cleansing agent, especially in chronic wounds has been proposed and is widely used especially in the management of infected wounds. Despite",
      "a wide range of wounds. the WBP model can be effectively applied only when a high level of precision is utilized in assessment of the patient and their wound. The corollary of this is that intervention demands an equally high level of precision and this should be preceded by comprehensive wound assessment. Wound assessment is a vital first step in precision management process. The purpose of wound assessment is: To identify; To determine; To gather data, Unfortunately, universal agreement as regards to the precise mechanisms of how this should be accomplished has yet to be agreed. Debridement is an essential",
      "above circumstances or when a wound has become infected, wound cleansing activities beyond the natural biological processes are required so that the wound bioburden is maintained at a level where the host can remain in control. Wound cleansing is a fundamental component of wound care. It consists of the removal of foreign matter, dead (devitalized) tissue and other physical impedimenta to healing, such as ragged edges. Despite the move in the 21st century toward evidence-based practice, the only general consensus that exists here is that cleansing and excision reduce infection rates. The recommendation has been made that cleansing is required"
    ]
  ],
  [
    "Creatine is synthesized from cyanamide with sarcosine in the industrial synthesis process by reacting cyanamide with glycine to obtain guanidinoacetic acid, which is then further reacted with sarcosine to produce creatine, avoiding the formation of toxic by-products such as dihydrotriazine and iminodiacetic acid.",
    [
      "of cyanamide with sarcosine In the industrial synthesis of creatine: This synthesis route mostly avoids problematic impurities like chloroacetic acid, iminodiacetic acid or dihydrotriazine that occur in other routes. The physiological precursor guanidinoacetic is obtained analogously by reacting cyanamide with glycine. Since the mid-1960s there are methods to stabilize cyanamide in order to make it available on an industrial scale. Due to the strong affinity towards self-condensation in alkaline media (see above) solutions of cyanamide are stabilized by the addition of 0.5 wt% of monosodium phosphate as buffer. Solid cyanamide is produced by careful evaporation of the solvent and subsequent",
      "also be converted to glycocyamine with \"S\"-methylisothiourea or with \"O\"-alkylisoureas as a guanylation agent. The recent patent literature describes the synthesis of glycocyamine by catalytic oxidation of ethanolamine to glycine and subsequent reaction with cyanamide in aqueous solution in high yield, analogous to the synthesis of creatine starting from 2-methylaminoethanol via sarcosine. This synthetic route suppresses the formation of toxic dihydrotriazine and other undesired by-products (such as iminodiacetic acid). Industrially produced guanidinoacetic acid is sold as a white (to yellowish) fine powder, which is granulated for improve handling, metering and uptake with starch into aggregates with a mean diameter of",
      "deficiency (GAMT deficiency), caused by variants in \"GAMT\". The single enzyme defects are both inherited in an autosomal recessive manner. Creatine is synthesized in the kidney and liver, by a two step enzymatic process. In the first step, glycine and arginine are combined by to form guanidinoacetate. This step also results in the production of ornithine. Creatine is produced by the enzyme guanidinoacetate methyltransferase. After production in the liver and kidneys, creatine is transported to organs and tissues with high energy demands, most commonly the brain and skeletal muscles. In addition to endogenous production, creatine can be obtained from dietary"
    ]
  ],
  [
    "The purpose of using an indwelling Foley catheter after major surgeries is to provide drainage of urine produced in the kidneys and secure incised areas for proper healing of the urethra.",
    [
      "and an appropriately sized Foley catheter will be inserted through the repair and into the urinary bladder, and locked into place by filling its balloon (positioned inside of the bladder near the urethral junction) with sterile water. The Foley catheter serves two purposes, first, it provides drainage of the urine produced in the kidneys, and secondly, it secures the incised areas, holding them open for three to seven days to permit thorough healing of the urethra. The catheter is then attached to a urinary catheter drainage system (large bag or leg bag) via clear polypropylene tubing. Prior to discharge from",
      "bladder to stop it from slipping out. Manufacturers usually produce Foley catheters using silicone or coated natural latex. Coatings include polytetrafluoroethylene, hydrogel, or a silicon elastomer \u2013 the different properties of these surface coatings determine whether the catheter is suitable for 28-day or 3-month indwelling duration. Foley catheters should be used only when indicated, as use increases the risk of catheter-associated urinary tract infection and other adverse effects. Indwelling urinary catheters are most commonly used to assist people who cannot urinate on their own. Indications for using a catheter include providing relief when there is urinary retention, monitoring urine output",
      "for critically ill persons, managing urination during surgery, and providing end-of-life care. Foley catheters are used during the following situations: A Foley catheter can also be used to ripen the cervix during induction of labor. When used for this purpose, the procedure is called \"extra-amniotic saline infusion\". In this procedure, the balloon is inserted behind the cervical wall and inflated, such for example with 30 mL of saline. The remaining length of the catheter is pulled slightly taut and taped to the inside of the woman's leg. The inflated balloon applies pressure to the cervix as the baby's head would"
    ]
  ],
  [
    "Absorbent products for urinary incontinence may cause side effects such as leaks, odors, skin breakdown, and urinary tract infections (UTI).",
    [
      "a technique that encourages people to modify their voiding habits (lengthening the time between voiding). Weak evidence suggests that bladder training may be helpful for the treatment of urinary incontinence. This type of intervention can take a person months to learn and would not be a therapy option for people who are not physically or mentally able to control their voiding. An incontinence pad is a multi-layered, absorbent sheet that collects urine resulting from urinary incontinence. Similar solutions include absorbent undergarments and adult diapers. Absorbent products may cause side effects of leaks, odors, skin breakdown, and UTI. Incontinence pads may",
      "provide consumers with more options, shows the industry recognition of the need to reduce the stigma around urinary incontinence, and offer wearers product choices that are more comfortable and less bulky. Products available for those with severe urinary incontinence are still predominantly single use disposable pads. Related types of incontinence products include absorbent pads for chairs or beds, and underwear for children who experience nocturnal enuresis. The US incontinence market is forecast to reach a value of US$1.6 billion by 2017, driven by an aging population and a gradual breakdown of the taboo surrounding incontinence. These factors will contribute to",
      "Incontinence underwear Incontinence underwear is a type of reusable undergarment designed to absorb urine. It provides an alternative to traditional disposable incontinence products, which are often bulky and plastic-based. Due to concerns about the environmental impact of disposable products, incontinence underwear is becoming an alternative to pads. Only recently has the textile technology existed to enable the design and manufacture of reusable products with comparable functionality to a disposable pad or diaper. It is estimated that 1 in 4 women over the age of 35 experience some level of urinary incontinence, often following childbirth or during menopause. Incontinence is also"
    ]
  ],
  [
    "Pain management needs for changing cancer pain should be addressed by utilizing a combination of drug therapy, modifying the underlying disease, raising the pain threshold, interrupting, destroying or stimulating pain pathways, suggesting lifestyle modifications, and providing relief for psychological, social, and spiritual distress.",
    [
      "worldwide receive less than optimal care. Cancer changes over time, and pain management needs to reflect this. Several different types of treatment may be required as the disease progresses. Pain managers should clearly explain to the patient the cause of the pain and the various treatment possibilities, and should consider, as well as drug therapy, directly modifying the underlying disease, raising the pain threshold, interrupting, destroying or stimulating pain pathways, and suggesting lifestyle modification. The relief of psychological, social and spiritual distress is a key element in effective pain management. A person whose pain cannot be well controlled should be",
      "for cancer pain management is to get the patient as comfortable as possible using the least amount of medications possible but opioids, surgery, and physical measures are often required. Historically, doctors were reluctant to prescribe narcotics to terminal cancer patients due to addiction and respiratory function suppression. The palliative care movement, a more recent offshoot of the hospice movement, has engendered more widespread support for preemptive pain treatment for cancer patients. The World Health Organization also noted uncontrolled cancer pain as a worldwide problem and established a \"ladder\" as a guideline for how practitioners should treat pain in patients who",
      "diagnosed with malignant cancer are experiencing pain, and two thirds of those with advanced cancer experience pain of such intensity that it adversely affects their sleep, mood, social relations and activities of daily living. With competent management, cancer pain can be eliminated or well controlled in 80% to 90% of cases, but nearly 50% of cancer patients in the developed world receive less than optimal care. Worldwide, nearly 80% of people with cancer receive little or no pain medication. Cancer pain in children is also reported as being under-treated. Guidelines for the use of drugs in the management of cancer"
    ]
  ],
  [
    "Alleles in the TREM2 gene are associated with an increased risk of Alzheimer's disease, and potential risk reduction can be achieved through the degradation of beta-amyloid by angiotensin converting enzyme (ACE).",
    [
      "areas in genes that appear to affect the risk. These genes include: CASS4, CELF1, FERMT2, HLA-DRB5, INPP5D, MEF2C, NME8, PTK2B, SORL1, ZCWPW1, SlC24A4, CLU, PICALM, CR1, BIN1, MS4A, ABCA7, EPHA1, and CD2AP. Alleles in the TREM2 gene have been associated with a 3 to 5 times higher risk of developing Alzheimer's disease. A suggested mechanism of action is that in some variants in TREM2 white blood cells in the brain are no longer able to control the amount of beta amyloid present. Many SNPs are associated with Alzheimer's with a 2018 study adding 30 SNPs by differentiating AD into 6",
      "risk of disease and no single variant is enough to cause the disease. An individual must have many of these common gene variants in order for the risk of disease to be substantial. More recent research indicates that the \"common disease-rare variant\" may be a better explanation for many common diseases. In this model, rare but higher-risk gene variants cause common diseases. This model may be relevant for diseases that reduces fertility. In contrast, for common genes associated with common disease to persist they must either have little effect during the reproductive period of life (like Alzheimer's disease) or provide",
      "of ACE can prevent Alzheimer's. It is assumed that ACE can degrade beta-amyloid in brain blood vessels and therefore help prevent the digression of the disease. Studies have shown that different genotypes of angiotensin converting enzyme can lead to varying influence on athletic performance. ACE I/D polymorphism consists of either an insertion (I) or absence (D) of a 287 base pair alanine sequence in intron 16 of the gene. People carrying the I-allele usually have lower ACE levels while people carrying the D-allele have higher ACE levels. People carrying the D-allele are associated with higher ACE levels that cause higher"
    ]
  ],
  [
    "Drugs containing Pilocarpine, Butyrylcholine, Huperzine A, Donepezil, and Methamphetamine may cause dry mouth.",
    [
      "relaxation resolves. Drugs containing Mivacurium - e.g. Mivacron Mivacron is also a muscle relaxant that is used prior to inserting a tube for breathing. Drugs containing Pilocarpine - e.g. Salagen Salagen is used to treat dry mouth. As the name suggests, dry mouth is a medical condition that occurs when saliva production goes down. There are lots of different causes of dry mouth including side effect of various drugs. Drugs containing Butyrylcholine Use of butyrylcholine is not common. It can be used to treat exposure to nerve agents, pesticides, toxins, etc. Drugs containing Huperzine A and Donepezil These drugs are",
      "term can describe oral and eye dryness that is not caused by autoimmune diseases (e.g., Sj\u00f6gren syndrome). Oral dryness may also be caused by mouth breathing, usually caused by partial obstruction of the upper respiratory tract. Examples include hemorrhage, vomiting, diarrhea, and fever. Alcohol may be involved in the cause of salivary gland disease, liver disease, or dehydration. Smoking is another possible cause. Other recreational drugs such as methamphetamine, cannabis, hallucinogens, or heroin, may be implicated. Hormonal disorders, such as poorly controlled diabetes, chronic graft versus host disease or low fluid intake in people undergoing haemodialysis for renal impairment may",
      "health problems. Pilocarpine and cevimeline are sialogogues approved by the Food and Drug Administration (FDA) to treat low salivation caused by Sjogren's syndrome and may have the potential to effectively treat dry mouth caused by methamphetamine use. There have not been any controlled studies on meth mouth, and several of its aspects are unclear. Although the condition has been popularized by media coverage and case reports, no systematic studies have been conducted to conclusively tie methamphetamine use to symptoms that are commonly described as meth mouth. There are few ties between dental scholars and those who study drug use, and"
    ]
  ],
  [
    "A false statement need not be a lie.",
    [
      "False statement A false statement is a statement that is not true. Although the word \"fallacy\" is sometimes used as a synonym for \"false statement\", that is not how the word is used in philosophy, mathematics, logic and most formal contexts. A false statement need not be a lie. A lie is a statement that is known to be untrue and is used to mislead. A false statement is a statement that is untrue but not necessarily told to mislead, as a statement given by someone who does not know it is untrue. John told his little brother that sea",
      "the form of a declarative statement which the answering party must then either admit, deny, or state in detail why he or she can neither admit nor deny the truthfulness of the statement (e.g. for lack of knowledge, etc.). This effectively puts the admissions in the form of true-false questions. For example, in a case involving an automobile accident, the plaintiff might include in his or her request a statement such as \"Defendant Smith was driving a blue Dodge Caravan on the morning of the accident\". Under Rule 36(a)(5) of the Federal Rules of Civil Procedure, the answering party may",
      "declarative sentence that declares itself, as its own subject, to be false. Al-Baghd\u0101d\u012b's definition of \"truth\" and \"falsity\" says that: \"\"truth is an agreement with the subject, and falsity is the opposite of that\"\". \u1e6c\u016bs\u012b argues that this definition cannot be applied to a declarative sentence that declares its own subject to be false because then there are at least two opposite parts that are in disagreement with each other. The same subject cannot be in disagreement with itself. Therefore a self\u2013referenced declarative sentence that declares itself to be false is neither false nor true, and truth/falsity definitions are not"
    ]
  ],
  [
    "If a patient is cyanosed after a hyperoxia test, their oxygen saturation level might be below 60%.",
    [
      "Hyperoxia test A hyperoxia test cyanosis is due to lung disease or a problem with blood circulation. It is performed by measuring the arterial blood gases of the patient while they breathe room air, then re-measuring the blood gases after the patient has breathed 100% oxygen for 10 minutes. If the cause of the cyanosis is poor oxygen saturation by the lungs, allowing the pto saturate the blood with oxygen, and the partial pressure of oxygen in the arterial blood will rise (usually above 150 mmHg). However, if the lungs are healthy and already fully saturating the blood that is",
      "delivered to them, then supplemental oxygen will have no effect, and the partial pressure of oxygen will usually remain below 100 mmHg. In this case, the cyanosis is most likely due to blood that moves from the systemic veins to the systemic arteries via a right-to-left shunt without ever going through the lungs. Hyperoxia test A hyperoxia test cyanosis is due to lung disease or a problem with blood circulation. It is performed by measuring the arterial blood gases of the patient while they breathe room air, then re-measuring the blood gases after the patient has breathed 100% oxygen for",
      "cell is said to be \"saturated\" when carrying a normal amount of oxygen. Both too high and too low levels can have adverse effects on the body. An SaO (arterial oxygen saturation, as determined by an arterial blood gas test) value below 60% causes hypoxemia (which can also be caused by anemia). Hypoxemia due to low SaO is indicated by cyanosis. Oxygen saturation can be measured in different tissues: Oxygen saturation (medicine) Oxygen saturation is the fraction of oxygen-saturated hemoglobin relative to total hemoglobin (unsaturated + saturated) in the blood. The human body requires and regulates a very precise and"
    ]
  ],
  [
    "A physician can assess a patient's breathing using the mnemonic PIPPA by checking the Integrated Pulmonary Index (IPI) before evaluating the four parameters that make up this number, which can help determine the patient's overall ventilatory status.",
    [
      "identified and simultaneously resuscitation is begun. A simple mnemonic, ABCDE, is used as a mnemonic for the order in which problems should be addressed. The first stage of the primary survey is to assess the airway. If the patient is able to talk, the airway is likely to be clear. If the patient is unconscious, he/she may not be able to maintain his/her own airway. The airway can be opened using a chin lift or jaw thrust. Airway adjuncts may be required. If the airway is blocked (e.g., by blood or vomit), the fluid must be cleaned out of the",
      "and that circulation\u2014i.e. pulses that can be felt\u2014is present. This is sometimes described as the \"A, B, C's\"\u2014Airway, Breathing, and Circulation\u2014and is the first step in any resuscitation or triage. Then, the history of the accident or injury is amplified with any medical, dietary (timing of last oral intake) and past history, from whatever sources such as family, friends, previous treating physicians that might be available. This method is sometimes given the mnemonic \"SAMPLE\". The amount of time spent on diagnosis should be minimized and expedited by a combination of clinical assessment and appropriate use of technology, such as diagnostic",
      "previous minutes or hours, to help the clinician ascertain if the patient\u2019s overall ventilatory status is worsening, remaining steady, or improving. This information can help determine the next steps in patient care. Thus, IPI can simplify the monitoring of patients in clinical environments. The caregiver can quickly and easily assess a patient\u2019s ventilatory status by following one number, the IPI, before checking the four parameters that make up this number. The four parameters continue to be displayed on the monitor screen. A significant change in the IPI is a \u201cred flag\u201d indicator, indicating that the clinician should review other monitored"
    ]
  ],
  [
    "Each dose of Ropinirole would contain 500 micrograms if the total prescribed amount is 1.5 mg divided into three doses.",
    [
      "For Parkinson's disease, the maximum recommended dose is 24 mg per day, taken in three separate doses spread throughout the day. The maximum dose recommendations of ropinirole for subjects with end stage renal disease (ESRD) should be reduced by 25% compared with those recommended for subjects with normal renal function. A 25% dose reduction represents a more straightforward dosage regimen in terms of available tablet strength, compared with a 30% dose reduction. For RLS, the maximum recommended dose is 4 mg per day, taken 1 to 3 hours before bedtime. A 52-week open label study had a mean dosage of",
      "1.90 mg, once daily 1 to 3 hours before bedtime. Ropinirole acts as a D, D, and D dopamine receptor agonist with highest affinity for D. It is weakly active at the 5-HT, and \u03b1 receptors and is said to have virtually no affinity for the 5-HT, GABA, mAChRs, \u03b1, and \u03b2-adrenoreceptors. Ropinirole is metabolized primarily by cytochrome P450 CYP1A2 to form two metabolites; SK&F-104557 and SK&F-89124, both of which are renally excreted, and at doses higher than clinical, is also metabolized by CYP3A4. At doses greater than 24 mg, CYP2D6 may be inhibited, although this has been tested only",
      "generic form. Ropinirole is prescribed for mainly Parkinson's disease, RLS and extrapyramidal symptoms. It can also reduce the side effects caused by selective serotonin reuptake inhibitors, including Parkinsonism syndrome as well as sexual dysfunction and erectile dysfunction caused by either SSRIs or antipsychotics. Ropinirole in the Requip form is available in various preparations, ranging from a 0.25 mg tablet to a 5 mg tablet. The primary reason is dose titration. This implies that the person taking Requip has to closely interact and communicate with the primary care physician with regard to how much should actually be taken by the patient."
    ]
  ],
  [
    "Peripheral muscle fatigue during multiple sprint activities is thought to be implicated by mechanical failure of the exercising muscles, inadequate oxygen supply, lactic acid buildup, or total energy depletion in the exhausted muscles.",
    [
      "upon exercise fatigue was modeled in terms of it being due to a mechanical failure of the exercising muscles (\"peripheral muscle fatigue\"). This failure was caused either by an inadequate oxygen supply to the exercising muscles, lactic acid buildup, or total energy depletion in the exhausted muscles. Tim Noakes, a professor of exercise and sports science at the University of Cape Town, in 1997 has renewed Hill\u2019s argument on the basis of modern research. In his approach, the power output by muscles during exercise is continuously adjusted in regard to calculations made by the brain in regard to a safe",
      "considering the cost of running against wind resistance (formula_13), which is known to be: We combine the two equations to arrive at: Where formula_7 is the acceleration of the runner's body, formula_17 the forward acceleration, formula_18 the acceleration of gravity, formula_19 a proportionality constant and formula_20 the velocity. Fatigue is a prominent factor in sprinting, and it is already widely known that it hinders maximal power output in muscles, but it also affects the acceleration of runners in the ways listed below. A study on muscle coordination in which subjects performed repeated 6-second cycling sprints, or intermittent sprints of short",
      "fatigue is only caused by mechanical failure of the exercising muscles (\"peripheral fatigue\"). Instead, the brain models the metabolic limits of the body to ensure that whole body homeostasis is protected, in particular that the heart is guarded from hypoxia, and an emergency reserve is always maintained. The idea of the central governor has been questioned since \u2018physiological catastrophes\u2019 can and do occur suggesting athletes (such as Dorando Pietri, Jim Peters and Gabriela Andersen-Schiess) can over-ride the \u2018\u2018central governor\u2019. The exercise fatigue has also been suggested to be effected by: Prolonged exercise such as marathons can increase cardiac biomarkers such"
    ]
  ],
  [
    "The three exercise energy systems differ in terms of their recruitment and utilization for ATP production in muscle cells based on the amount of oxygen available and the duration and intensity of the exercise being performed, with ATP being the usable form of chemical energy for muscular activity, stored in muscle cells, and transformed from other forms of chemical energy before being utilized.",
    [
      "and intensity and by the individual's cardiorespiratory fitness level. Three exercise energy systems can be selectively recruited, depending on the amount of oxygen available, as part of the cellular respiration process to generate the ATP for the muscles. They are ATP, the anaerobic system and the aerobic system. ATP is the usable form of chemical energy for muscular activity. It is stored in most cells, particularly in muscle cells. Other forms of chemical energy, such as those available from food, must be transformed into ATP before they can be utilized by the muscle cells. Since energy is released when ATP",
      "in such a way that the energy released by the one is always used by the other. Three methods can synthesize ATP: Aerobic and anaerobic systems usually work concurrently. When describing activity, it is not a question of which energy system is working, but which predominates. The term metabolism refers to the various series of chemical reactions that take place within the body. Aerobic refers to the presence of oxygen, whereas anaerobic means with series of chemical reactions that does not require the presence of oxygen. The ATP-CP series and the lactic acid series are anaerobic, whereas the oxygen series",
      "to supply its self with enough energy to support all the corresponding changes in the body at work. The 3 energy systems involved in exercise are the Phosphogenic, Anaerobic and Aerobic energy pathways. The simultaneous action of these three energy pathways prioritizes one specific pathway over the others depending on the type of exercise an individual is partaking in. This differential prioritization is based on the duration and intensity of the particular exercise being performed. The variable usage of these energy pathways is central to the mechanisms that allow for long, sustained exercise, such as a marathon running, to be"
    ]
  ],
  [
    "If a patient takes 24 tablets of Cotrimoxazole, each containing 500 mg, over a certain span of time, they would consume a total of 12 grams of Cotrimoxazole.",
    [
      "for comparison and control over nationwide total drug consumption. For example, the overall drug consumption of beta blockers can be measured in DDDs and compared between different countries, sexes or other populations. If the DDD for a certain drug is given, the number of DDDs used by an individual patient or (more commonly) by a collective of patients is as follows. formula_1 For example, the analgesic (pain reliever) paracetamol has a DDD of 3 g, which means that an average patient who takes paracetamol for its main indication, which is pain relief, uses 3 grams per day. This is equivalent",
      "to six standard tablets of 500 mg each. If a patient consumes 24 such tablets (12 g of paracetamol in total) over a certain span of time, this equals a consumption of four DDDs. formula_2 Defined daily dose The defined daily dose (DDD) is a statistical measure of drug consumption, defined by the World Health Organization (WHO). It is used to standardize the comparison of drug usage between different drugs or between different health care environments. The DDD is not to be confused with the therapeutic dose or with the dose actually prescribed by a physician for an individual patient.",
      "is exactly one gram. So, the maintenance dose of \"foosporin\" is 100 milligrams (100 mg) per day\u2014just enough to offset the amount cleared. Suppose a patient just started taking 100 mg of \"foosporin\" every day. As one can see, it would take many days for the total amount of drug within the body to come close to 1 gram (1000 mg) and achieve its full therapeutic effect. For a drug such as this, a doctor might prescribe a loading dose of \"one gram\" to be taken on the first day. That immediately gets the drug's concentration in the body up"
    ]
  ],
  [
    "The main categories of thyroid conditions are hypothyroidism, hyperthyroidism, thyroid disease, thyroiditis, and certain medication-induced thyroid conditions, which are diagnosed through blood tests, hormone level monitoring, and sometimes biopsies to determine the type and severity of the condition.",
    [
      "muscle weakness / aches (myalgia), and different forms of myxedema. Hypothyroidism is a state in which the body is not producing enough thyroid hormones, or is not able to respond to / utilize existing thyroid hormones properly. The main categories are: Hyperthyroidism is a state in which the body is producing too much thyroid hormone. The main hyperthyroid conditions are: Certain medications can have the unintended side effect of affecting thyroid function. While some medications can lead to significant hypothyroidism or hyperthyroidism and those at risk will need to be carefully monitored, some medications may affect thyroid hormone lab tests",
      "Thyroid disease Thyroid disease is a medical condition that affects the function of the thyroid gland. The thyroid gland is located at the front of the neck and produces thyroid hormones that travel through the blood to help regulate many other organs, meaning that it is an endocrine organ. These hormones normally act in the body to regulate energy use, infant development, and childhood development. There are five general types of thyroid disease, each with their own symptoms. A person may have one or several different types at the same time. The five groups are: 1) Hypothyroidism (low function) caused",
      "(Mather, 2007). Blood tests also help to determine the kind of thyroiditis and to see how much thyroid stimulating hormone the pituitary gland is producing and what antibodies are present in the body. In some cases a biopsy may be needed to find out what is attacking the thyroid. Treatments for this disease depend on the type of thyroiditis that is diagnosed. For the most common type, which is known as Hashimoto's thyroiditis, the treatment is to immediately start hormone replacement. This prevents or corrects the hypothyroidism, and it also generally keeps the gland from getting bigger. However, Hashimoto's thyroiditis"
    ]
  ],
  [
    "The defining characteristic of hyperchlorhydria is when gastric acid levels in the stomach are higher than the reference range, typically defined as having a pH less than 2.",
    [
      "dehydration; symptoms are most often caused by rapid rehydration which results in cerebral edema. Hyperchloremia can affect oxygen transport. Chlorine is a toxic gas that attacks the respiratory system, eyes, and skin. Because it is denser than air, it tends to accumulate at the bottom of poorly ventilated spaces. Chlorine gas is a strong oxidizer, which may react with flammable materials. Chlorine is detectable with measuring devices in concentrations as low as 0.2 parts per million (ppm), and by smell at 3 ppm. Coughing and vomiting may occur at 30 ppm and lung damage at 60 ppm. About 1000 ppm",
      "Hyperchlorhydria Hyperchlorhydria, sometimes called chlorhydria, sour stomach or acid stomach, refers to the state in the stomach where gastric acid levels are higher than the reference range. The combining forms of the name (\"chlor-\" + \"hydr-\"), referring to chlorine and hydrogen, are the same as those in the name of hydrochloric acid, which is the active constituent of gastric acid. In humans, the normal pH is around 1 to 3, which varies throughout the day. The highest basal secretion levels are in the late evening (around 12 A.M. to 3 A.M.). Hyperchlorhydria is usually defined as having a pH less",
      "than 2. It has no negative consequences unless other conditions are also present such as gastroesophageal reflux disease (GERD). Hyperchlorhydria Hyperchlorhydria, sometimes called chlorhydria, sour stomach or acid stomach, refers to the state in the stomach where gastric acid levels are higher than the reference range. The combining forms of the name (\"chlor-\" + \"hydr-\"), referring to chlorine and hydrogen, are the same as those in the name of hydrochloric acid, which is the active constituent of gastric acid. In humans, the normal pH is around 1 to 3, which varies throughout the day. The highest basal secretion levels are"
    ]
  ],
  [
    "The recommended solution for completing bladder washout in a 67-year-old man with a blocked catheter post-transurethral resection of the prostate is to perform a retrograde urethrogram to coincide with the anticipated removal date of the catheter.",
    [
      "body. To prevent overdoses, there is a limit on the number of times a patient can push the button. If a patient pushes the button too much at once, the PCA will reject the request. For the patient's bladder control, a catheter will be inserted so that a patient can urinate without having to move. A catheter is inserted because the patient will not have much free movement to be able to get up and walk to the bathroom. The most common type of catheter used after major surgeries is an indwelling Foley catheter. The indwelling Foley catheter is most",
      "in people with urinary retention. Self-catheterization is an option in BPH when it is difficult or impossible to completely empty the bladder. Urinary tract infection is the most common complication of intermittent catheterization. Several techniques and types of catheter are available, including sterile (single-use) and clean (multiple use) catheters, but, based on current information, none is superior to others in reducing the incidence of urinary tract infection. If medical treatment is not effective a person may try office-based therapies or transurethral resection of prostate (TURP), surgery may need to be performed. Surgical techniques used include the following: The latest alternative",
      "will be instructed to sensibly increase physical activity, and avoid becoming sedentary. Adequate hydration is essential during the post-recovery phase of the procedure. In accordance with the preference of the surgeon, a retrograde urethrogram will be scheduled to coincide with the anticipated removal date of the suprapubic or Foley catheter (usually 7 to 14 days post-procedure, however some surgeons will attempt removal in 3 to 5 days). At 10 days post procedure, the suture line(s) will be evaluated, and the sutures removed if applicable (in many cases, the surgeon will utilize absorbable sutures, which do not require removal). The length"
    ]
  ],
  [
    "A keloid is an abnormal scar that grows beyond the boundaries of the original injury, and the spectrum of Kelu-1 A revealed a composition of water ice, carbon-rich material, and possibly magnesium-rich silicates, similar to Jupiter trojans.",
    [
      "are similar to those of the irregular moons of Jupiter and, to a certain extent, comet nuclei, though Jupiter trojans are spectrally very different from the redder Kuiper belt objects. A Jupiter trojan's spectrum can be matched to a mixture of water ice, a large amount of carbon-rich material (charcoal), and possibly magnesium-rich silicates. The composition of the Jupiter trojan population appears to be markedly uniform, with little or no differentiation between the two swarms. A team from the Keck Observatory in Hawaii announced in 2006 that it had measured the density of the binary Jupiter trojan 617 Patroclus as",
      "Keck I telescope to obtain the spectrum of the star's host galaxy, and the Deep Imaging and Multi-Object Spectrograph (DEIMOS) on Keck II to obtain high-resolution spectra of the unusual supernova itself. The host galaxy of iPTF14hls is a star-forming dwarf galaxy, implying low metal content, and the weak iron-line absorption seen in the supernova spectra are consistent with a low metallicity progenitor. The study estimates that the star that exploded was at least 50 times more massive than the Sun. The researchers also remark that the debris expansion rate is slower than any other known supernova by a factor",
      "of the exposure shown; the Cyg X-1 data, on the other hand, clearly exhibit the chaotic \"shot noise\" behavior characteristic of this black-hole candidate and also provided preliminary evidence for the additional feature of millisecond \"burst\" sub-structure, noted for the first time in this observation. The sharp cut-off at ~24 keV in the flat spectrum observed for Her X-1 in this exposure provided the first reported evidence for radiative transfer effects to be associated with a highly magnetized plasma near the surface of a neutron star. The black-body spectral component observed for Cyg X-3 during this experiment gave strong evidence"
    ]
  ],
  [
    "Ornithine is a non-proteinogenic amino acid.",
    [
      "Non-proteinogenic amino acids In biochemistry, non-coded or non-proteinogenic amino acids are those not naturally encoded or found in the genetic code of any organism. Despite the use of only 22 amino acids (21 in eukaryotes) by the translational machinery to assemble proteins (the proteinogenic amino acids), over 140 amino acids are known to occur naturally in proteins and thousands more may occur in nature or be synthesized in the laboratory. Many non-proteinogenic amino acids are noteworthy because they are; Technically, any organic compound with an amine (-NH) and a carboxylic acid (-COOH) functional group is an amino acid. The proteinogenic",
      "of vitamins (cofactor auxotrophy). The osmolytes, sarcosine and glycine betaine are derived from amino acids, but have a secondary and quaternary amine respectively. Non-proteinogenic amino acids In biochemistry, non-coded or non-proteinogenic amino acids are those not naturally encoded or found in the genetic code of any organism. Despite the use of only 22 amino acids (21 in eukaryotes) by the translational machinery to assemble proteins (the proteinogenic amino acids), over 140 amino acids are known to occur naturally in proteins and thousands more may occur in nature or be synthesized in the laboratory. Many non-proteinogenic amino acids are noteworthy because",
      "but are added in place of a stop codon when a specific sequence is present, UGA codon and SECIS element for selenocysteine, UAG PYLIS downstream sequence for pyrrolysine. All other amino acids are termed \"non-proteinogenic\". There are various groups of amino acids: These groups overlap, but are not identical. All 22 proteinogenic amino acids are biosynthesised by organisms and some, but not all, of them also are abiotic (found in prebiotic experiments and meteorites). Some natural amino acids, such as norleucine, are misincorporated translationally into proteins due to infidelity of the protein-synthesis process. Many amino acids, such as ornithine, are"
    ]
  ],
  [
    "Techniques for wound drainage and prevention of re-accumulation of blood after the acute stage include the use of surgical drains to remove blood or fluid from the surgical wound during recovery, which are removed once the volume tapers off.",
    [
      "such as debridement, hyperbaric oxygen treatment therapy, dressing selection, special shoes, and patient education. When wounds persist, a specialized approach is required for healing. To heal a wound, the body undertakes a series of actions collectively known as the wound healing process. A wound may be recorded for follow-up and observing progress of healing with different techniques which include: The overall treatment depends on the type, cause, and depth of the wound, and whether other structures beyond the skin (dermis) are involved. Treatment of recent lacerations involves examining, cleaning, and closing the wound. Minor wounds, like bruises, will heal on",
      "disorders such as obesity hypoventilation syndrome, atelectasis and pulmonary embolism, adverse cardiovascular effects, and wound healing complications. If removable skin closures are used, they are removed after 7 to 10 days post-operatively, or after healing of the incision is well under way. It is not uncommon for surgical drains (see Drain (surgery)) to be required to remove blood or fluid from the surgical wound during recovery. Mostly these drains stay in until the volume tapers off, then they are removed. These drains can become clogged, leading to abscess. Postoperative therapy may include adjuvant treatment such as chemotherapy, radiation therapy, or",
      "Key principles of wound management are: Elevation was commonly recommended for the control of haemorrhage. Some protocols continue to include it, but recent studies have failed to find any evidence of its effectiveness and it was removed from the PHTLS guidance in 2006. Placing pressure on the wound constricts the blood vessels manually, helping to stem blood flow. When applying pressure, the type and direction of the wound may have an effect, for instance, a cut lengthways on the hand would be opened up by closing the hand into a fist, whilst a cut across the hand would be sealed"
    ]
  ],
  [
    "In genetic sex-determination systems, the sex of an organism is determined by the presence or absence of specific sex chromosomes or by the combination of sex chromosomes it inherits.",
    [
      "Sex-determination system A sex-determination system is a biological system that determines the development of sexual characteristics in an organism. Most organisms that create their offspring using sexual reproduction have two sexes. Occasionally, there are hermaphrodites in place of one or both sexes. There are also some species that are only one sex due to parthenogenesis, the act of a female reproducing without fertilization. In many species, sex determination is genetic: males and females have different alleles or even different genes that specify their sexual morphology. In animals this is often accompanied by chromosomal differences, generally through combinations of XY, ZW,",
      "intersex individuals are unusual cases and are not typically fertile in both male and female aspects. In genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring. Humans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible",
      "XY sex-determination system The XY sex-determination system is the sex-determination system found in humans, most other mammals, some insects (\"Drosophila\"), some snakes, and some plants (\"Ginkgo\"). In this system, the sex of an individual is determined by a pair of sex chromosomes. Females typically have two of the same kind of sex chromosome (XX), and are called the homogametic sex. Males typically have two different kinds of sex chromosomes (XY), and are called the heterogametic sex. Exceptions to this are cases of XX males or XY females, or other syndromes. The XY system contrasts in several ways with the ZW"
    ]
  ],
  [
    "Spastic quadriplegia in cerebral palsy primarily involves physical impairments related to muscle stiffness and coordination, while cognitive impairments refer to challenges in intellectual functioning and development.",
    [
      "palsy include: Although it has its origins in a brain injury, spastic CP can largely be thought of as a collection of orthopaedic and neuromuscular issues because of how it manifests symptomatically over the course of the person's lifespan. It is therefore not the same as \"brain damage\" and it need not be thought of as such. Spastic quadriplegia in particular, especially if it is combined with verbal speech challenges and strabismus, may be misinterpreted by the general population as alluding to cognitive dimensions to the disability atop the physical ones, but this is false; the intelligence of a person",
      "Spastic cerebral palsy Spastic cerebral palsy is the type of cerebral palsy wherein spasticity is the exclusive impairment present. Itself an umbrella term encompassing spastic hemiplegia, spastic diplegia, spastic quadriplegia and \u2014 where solely one limb or one specific area of the body is affected\u2014 spastic monoplegia. Spastic cerebral palsy affects the cerebral cortex and is overwhelmingly the most common type of overall cerebral palsy. The Society for Cerebral Palsy in Europe (SCPE) estimates that the spasticity-only cerebral palsy classification sweeps in 90% of global cerebral palsy cases. But even if the 90% assertion is an exaggeration, more conservative scientific",
      "hemiplegia are meningitis, multiple sclerosis, and encephalitis. The spasticity occurs when the afferent pathways in the brain are compromised and the communication between the brain to the motor fibers is lost. When the inhibitory signals to deactivate the stretch reflex is lost the muscle remains in a constant contracted state. With spastic hemiplegia, one upper extremity and one lower extremity is affected, so cervical, lumbar and sacral segments of the spinal column can be affected. Infants with spastic hemiplegia may develop a hand preference earlier than is typical. There is no known cure for cerebral palsy, however there is a"
    ]
  ],
  [
    "The process of synthesizing glucose from lactate, glycerol, or certain amino acids is called gluconeogenesis.",
    [
      "regenerates the glucose, using a process called gluconeogenesis. This process is not quite the opposite of glycolysis, and actually requires three times the amount of energy gained from glycolysis (six molecules of ATP are used, compared to the two gained in glycolysis). Analogous to the above reactions, the glucose produced can then undergo glycolysis in tissues that need energy, be stored as glycogen (or starch in plants), or be converted to other monosaccharides or joined into di- or oligosaccharides. The combined pathways of glycolysis during exercise, lactate's crossing via the bloodstream to the liver, subsequent gluconeogenesis and release of glucose",
      "contain lactose. Galactose metabolism, which converts galactose into glucose, is carried out by the three principal enzymes in a mechanism known as the Leloir pathway. The enzymes are listed in the order of the metabolic pathway: galactokinase (GALK), galactose-1-phosphate uridyltransferase (GALT), and UDP-galactose-4\u2019-epimerase (GALE). In human lactation, glucose is changed into galactose via hexoneogenesis to enable the mammary glands to secrete lactose. However, most lactose in breast milk is synthesized from galactose taken up from the blood, and only 35\u00b16% is made from galactose from \"de novo\" synthesis. Glycerol also contributes some to the mammary galactose production. Glucose is the",
      "a process called glycosylation. This is often critical for their functioning. The enzymes that join glucose to other molecules usually use phosphorylated glucose to power the formation of the new bond by coupling it with the breaking of the glucose-phosphate bond. Other than its direct use as a monomer, glucose can be broken down to synthesize a wide variety of other biomolecules. This is important, as glucose serves both as a primary store of energy and as a source of organic carbon. Glucose can be broken down and converted into lipids. It is also a precursor for the synthesis of"
    ]
  ],
  [
    "Red blood cells washed with the IBM 2991 machine expire after 24 hours when kept at 1-6 \u00b0C.",
    [
      "lines branching from a central tube from the circular bag. These lines are inserted into the unit of blood to be processed, saline solutions, or an attached waste bag. The washed blood can be labeled in the circular bag or transferred into a more traditionally shaped blood storage bag. Because the washing process creates an open system, red blood cells washed with this machine expire after 24 hours when kept at 1-6 \u00b0C. The IBM 2991 is still in use today in many blood banks for washing red blood cell units. The machine is now manufactured and supported by Terumo",
      "the red blood cells using special equipment, such as the IBM 2991 cell processor in a similar manner to washing RBCs. The processing (often termed \"manufacture\", since the end result is deemed a biologic biopharmaceutical product) and the storage can occur at a collection center and/or a blood bank. RBCs are mixed with an anticoagulant and storage solution which provides nutrients and aims to preserve viability and functionality of the cells (limiting their so-called \"storage lesion\"), which are stored at refrigerated temperatures for up to 42 days (in the US), except for the rather unusual long-term storage in which case",
      "BCT and sold as the COBE\u00ae 2991 cell Processor. IBM 2991 The IBM 2991 Blood Cell Processor was a blood cell washer developed by IBM Systems Development Division in Endicott, NY. It was first marketed by IBM Systems Supplies Division (SSD) in 1972. The processor washed fresh blood or frozen, thawed blood. In the case of frozen, thawed blood, the blood was washed to remove the cryogenic agent, typically, glycerol. In 1964, IBM received a grant from the National Cancer Institute (NCI) of the National Institutes of Health (NIH). This was a result of an IBM engineer, George T. Judson,"
    ]
  ],
  [
    "The significance of the auscultatory gap in the Timor Gap region lies in the territorial disputes between Australia, East Timor, and Indonesia over the ownership and boundaries of the Timor Sea.",
    [
      "the area of the Timor Sea which lies outside the territorial boundaries of the nations to the north and south of the Timor Sea. These disagreements initially involved Australia and Indonesia, although a resolution was eventually reached in the form of the Timor Gap Treaty. After declaration of East Timor's nationhood in 1999, the terms of the Timor Gap Treaty were abandoned and negotiations commenced between Australia and East Timor, culminating in the Timor Sea Treaty. Australia's territorial claim extends to the bathymetric axis (the line of greatest sea-bed depth) at the Timor Trough. It overlaps East Timor's own territorial",
      "south of the Timor Sea. These disagreements initially involved Australia and Indonesia, although a resolution was eventually reached in the form of the Timor Gap Treaty. After declaration of East Timor's nationhood in 1999, the terms of the Timor Gap Treaty were abandoned and negotiations commenced between Australia and East Timor, culminating in the Timor Sea Treaty. Australia's territorial claim extends to the bathymetric axis (the line of greatest sea-bed depth) at the Timor Trough. It overlaps East Timor's own territorial claim, which follows the former colonial power Portugal and the United Nations Convention on the Law of the Sea",
      "lay in between the extremes of the claims by the two countries. The 1989 treaty was no longer valid once East Timor seceded from Indonesia in 1999, and the new state was not bound by the treaty. Indonesia had also informed Australia later that year that it was no longer bound by the Timor Gap Treaty. The dispute over the Timor Gap now is between Australia and East Timor and does not involve Indonesia. However, Indonesia may have to become a party in negotiations to establish the Australia-East Timor-Indonesia border tripoints if East Timor chooses not to be bound by"
    ]
  ],
  [
    "It is estimated that there are 500 to 1,000 different species of bacteria that may inhabit the human mouth at any given time.",
    [
      "found throughout the mouth are of multiple subtypes, preferring to inhabit distinctly different locations in the mouth. Even the enterotypes in the human gut, previously thought to be well understood, are from a broad spectrum of communities with blurred taxon boundaries. It is estimated that 500 to 1,000 species of bacteria live in the human gut but belong to just a few phyla: Firmicutes and Bacteroidetes dominate but there are also Proteobacteria, Verrumicrobia, Actinobacteria, Fusobacteria and Cyanobacteria. A number of types of bacteria, such as \"Actinomyces viscosus\" and \"A. naeslundii\", live in the mouth, where they are part of a",
      "oral diseases. The following table provides a more detailed (six-step) explanation of biofilm formation: Different types of bacteria are normally present in the mouth. These bacteria, as well as leukocytes, neutrophils, macrophages, and lymphocytes, are part of the normal oral cavity and contribute to the individual's health. Approximately 80\u201390% of the weight of plaque is water. While 70% of the dry weight is bacteria, the remaining 30% consists of polysaccharides and glycoproteins. The bulk of the microorganisms that form the biofilm are \"Streptococcus mutans\" and other anaerobes, though the precise composition varies by location in the mouth. Examples of such",
      "poor hygiene or injury) areas of the body normally not colonized or sterile (such as the blood, or the lower respiratory tract, or the abdominal cavity), disease can result (causing, respectively, bacteremia/sepsis, pneumonia, and peritonitis). The Human Microbiome Project found that individuals host thousands of bacterial types, different body sites having their own distinctive communities. Skin and vaginal sites showed smaller diversity than the mouth and gut, these showing the greatest richness. The bacterial makeup for a given site on a body varies from person to person, not only in type, but also in abundance. Bacteria of the same species"
    ]
  ],
  [
    "Shampoo should not be used for washing the skin when showering or bathing because it is specifically formulated for cleaning hair and not suitable for the skin, which can lead to dryness, irritation, and potential infection.",
    [
      "than daily because showering, especially with hot water, can dry out and irritate the skin, remove beneficial bacteria, and cause small cracks that can lead to infection. According to some dermatologists, too much cleanliness for young children can lead to allergies or eczema. Used shower water can be employed as greywater. Showering is mostly part of a daily routine primarily to promote cleanliness and prevent odor, disease and infection. Advances in science and medicine in the 19th century began to realize the benefit of regular bathing to an individual's health. As a result, most modern cultures encourage a daily personal",
      "avoid chemicals and ingredients used in many shampoos that make hair greasy over time. Shampoo Shampoo () is a hair care product, originating from the Indian subcontinent, typically in the form of a viscous liquid, that is used for cleaning hair. Less commonly, shampoo is available in bar form, like a bar of soap. Shampoo is used by applying it to wet hair, massaging the product into the hair, and then rinsing it out. Some users may follow a shampooing with the use of hair conditioner. The typical reason of using shampoo is to remove the unwanted build-up of sebum",
      "is washed as part of a shower or bathing with shampoo, a specialized surfactant. Shampoos work by applying water and shampoo to the hair. The shampoo breaks the surface tension of the water, allowing the hair to become soaked. This is known as the wetting action. The wetting action is caused by the head of the shampoo molecule attracting the water to the hair shaft. Conversely, the tail of the shampoo molecule is attracted to the grease, dirt and oil on the hair shaft. The physical action of shampooing makes the grease and dirt become an emulsion that is then"
    ]
  ],
  [
    "Adrenaline 1 in 1000 has the same strength as the explosive material known as TNT (trinitrotoluene).",
    [
      "Explosive An explosive material, also called an explosive, is a reactive substance that contains a great amount of potential energy that can produce an explosion if released suddenly, usually accompanied by the production of light, heat, sound, and pressure. An explosive charge is a measured quantity of explosive material, which may be composed of a single ingredient or a combination of two or more. The potential energy stored in an explosive material may, for example, be Explosive materials may be categorized by the speed at which they expand. Materials that detonate (the front of the chemical reaction moves faster through",
      "that are excess or deficient for 100 grams of a compound. where \"X\" = number of atoms of carbon, \"Y\" = number of atoms of hydrogen, \"Z\" = number of atoms of oxygen, and \"M\" = number of atoms of metal (metallic oxide produced). In the case of TNT (CH(NO)CH), Molecular weight = 227.1 \"X\" = 7 (number of carbon atoms) \"Y\" = 5 (number of hydrogen atoms) \"Z\" = 6 (number of oxygen atoms) Therefore, Because sensitivity, brisance, and strength are properties resulting from a complex explosive chemical reaction, a simple relationship such as oxygen balance cannot be depended",
      "(the amount of ammonium nitrate in the medium) or \"cartridge strength\" (the potential explosive strength generated by an amount of explosive of a certain density and grain size used in comparison to the explosive strength generated by an equivalent density and grain size of a standard explosive). For example, high-explosive \"65% Extra Dynamite\" has a weight strength of 65% ammonium nitrate and 35% \"dope\" (the absorbent medium mixed with the stabilizers and additives). Its \"cartridge strength\" would be its weight in pounds times its strength in relation to an equal amount of ANFO (the civilian baseline standard) or TNT (the"
    ]
  ],
  [
    "A masked allele is referred to as a recessive allele.",
    [
      "a gene are called alleles. The terms Alleles and Modifiers are used interchangeably and describe the same concept. An allele identified with a capital letter is a dominant trait, one identified with a lower-case letter is a recessive trait. Because sex cells (sperm and ova) contain only half the usual number of chromosomes, each parent contributes one allele in each gene set to the ensuing offspring. When an individual's gene set contains two copies of the same allele, it is called homozygous for that gene. When it has two different alleles, it is heterozygous. For a recessive trait to be",
      "copies of the allele that codes for the recessive trait. This allele, often called the \"recessive allele\", is usually represented by the lowercase form of the letter used for the corresponding dominant trait (such as, with reference to the example above, \"p\" for the recessive allele producing white flowers in pea plants). The genotype of an organism that is homozygous-recessive for a particular trait is represented by a doubling of the appropriate letter, such as \"pp\". A diploid organism is heterozygous at a gene locus when its cells contain two different alleles (one wild-type allele and one mutant allele) of",
      "the recent European colonization), include: Haplogroup A haplotype is a group of alleles in an organism that are inherited together from a single parent, and a haplogroup (haploid from the , \"haplo\u00fbs\", \"onefold, simple\" and ) is a group of similar haplotypes that share a common ancestor with a single-nucleotide polymorphism mutation. More specifically, a haplogroup is a combination of alleles at different chromosomes regions that are closely linked and that tend to be inherited together. As a haplogroup consists of similar haplotypes, it is usually possible to predict a haplogroup from haplotypes. Haplogroups pertain to a single line of"
    ]
  ],
  [
    "The symptoms of hypernatraemic dehydration include severe thirst, deterioration, confusion, unexplained tiredness, purple fingernails, seizures, and eventually death at a loss of between fifteen and twenty-five percent of body water.",
    [
      "deterioration, accompanied by severe thirst. Death occurs at a loss of between fifteen and twenty-five percent of the body water. Mild dehydration is characterized by thirst and general discomfort and is usually resolved with oral rehydration. Dehydration can cause hypernatremia (high levels of sodium ions in the blood) and is distinct from hypovolemia (loss of blood volume, particularly plasma). The hallmarks of dehydration include thirst and neurological changes such as headaches, general discomfort, loss of appetite, decreased urine volume (unless polyuria is the cause of dehydration), confusion, unexplained tiredness, purple fingernails and seizures. The symptoms of dehydration become increasingly severe",
      "skin and respiratory tract. In humans, dehydration can be caused by a wide range of diseases and states that impair water homeostasis in the body. These occur primarily through either impaired thirst/water access or sodium excess. Dehydration occurs when water intake is not enough to replace free water lost due to normal physiologic processes, including breathing, urination, and perspiration, or other causes, including diarrhea and vomiting. Dehydration can be life-threatening when severe and lead to seizures or respiratory arrest, and also carries the risk of osmotic cerebral edema if rehydration is overly rapid. The term dehydration has sometimes been used",
      "body water is called dehydration and will eventually lead to death by hypernatremia. Methods used in the management of dehydration include assisted drinking or oral rehydration therapy. An overconsumption of water can lead to water intoxication, which can dangerously dilute the concentration of salts in the body. Overhydration sometimes occurs among athletes and outdoor laborers, but it can also be a sign of disease or damage to the hypothalamus. A persistent desire to drink inordinate quantities of water is a psychological condition termed polydipsia. It is often accompanied by polyuria and may itself be a symptom of Diabetes mellitus or"
    ]
  ],
  [
    "Signs of increased breathing effort include nasal flaring, contraction of sternomastoid, and thoraco-abdominal paradox.",
    [
      "examination. Signs that represent significant severity include hypotension, hypoxemia, tracheal deviation, altered mental status, unstable dysrhythmia, stridor, intercostal indrawing, cyanosis, tripod positioning, pronounced use of accessory muscles (sternocleidomastoid, scalenes) and absent breath sounds. A number of scales may be used to quantify the degree of shortness of breath. It may be subjectively rated on a scale from 1 to 10 with descriptors associated with the number (The Modified Borg Scale). Alternatively a scale such as the MRC breathlessness scale might be used \u2013 it suggests five grades of dyspnea based on the circumstances in which it arises. A number of",
      "increased by gestalt or by examining the patient looking for signs of increased breathing effort. These signs include nasal flaring, the contraction of sternomastoid, and thoraco-abdominal paradox. In the diving industry the performance of breathing apparatus is often referred to as work of breathing. In this context it generally means the work of an average single breath taken through the specified apparatus for given conditions of ambient pressure, underwater environment, flow rate during the breathing cycle, and gas mixture - underwater divers may breathe oxygen-rich Breathing gas to reduce the risk of decompression sickness, or gases containing helium to reduce",
      "but the movements typically become even more pronounced. Monitors for airflow at the nose and mouth demonstrate that efforts to breathe are not only present but that they are often exaggerated. The chest muscles and diaphragm contract and the entire body may thrash and struggle. An \"event\" can be either an apnea, characterised by complete cessation of airflow for at least 10 seconds, or a hypopnea in which airflow decreases by 50 percent for 10 seconds or decreases by 30 percent if there is an associated decrease in the oxygen saturation or an arousal from sleep. To grade the severity"
    ]
  ],
  [
    "Proteins and phosphate act as intracellular buffers that can limit pH changes when the rate of glycolysis is high.",
    [
      "pH. Extracellular buffers include bicarbonate and ammonia, while proteins and phosphate act as intracellular buffers. The bicarbonate buffering system is especially key, as carbon dioxide (CO) can be shifted through carbonic acid (HCO) to hydrogen ions and bicarbonate (HCO) as shown below. Acid\u2013base imbalances that overcome the buffer system can be compensated in the short term by changing the rate of ventilation. This alters the concentration of carbon dioxide in the blood, shifting the above reaction according to Le Chatelier's principle, which in turn alters the pH. For instance, if the blood pH drops too low (\"acidemia\"), the body will",
      "the extracellular fluid, including the blood plasma, is normally tightly regulated between 7.32 and 7.42, by the chemical buffers, the respiratory system, and the renal system. Aqueous buffer solutions will react with strong acids or strong bases by absorbing excess hydrogen ions, or hydroxide ions, replacing the strong acids and bases with weak acids and weak bases. This has the effect of damping the effect of pH changes, or reducing the pH change that would otherwise have occurred. But buffers cannot correct abnormal pH levels in a solution, be that solution in a test tube or in the extracellular fluid.",
      "lies on our original buffer line. However, if the P is now varied without further addition of strong acid or strong base to the solution, a new buffer line can be determined that lies above and approximately parallel to the original buffer line. Similarly, in a physiologic system such as a living body, removal of protons, for example, by vomiting the acidic contents of the stomach, will result in an increase in pH and an increase in bicarbonate concentration, bringing the system to a new, higher buffer line. Such a disturbance is called a metabolic alkalosis (Fig. 12). Alternatively, if"
    ]
  ],
  [
    "The primary causes of Addison's Disease are autoimmune conditions in the Western world and infections, especially tuberculosis, worldwide.",
    [
      "disease and genetic causes) directly affect the adrenal cortex. If a problem that affects the hypothalamic-pituitary-adrenal axis arises outside the gland, it is a \"secondary adrenal insufficiency\". Addison's disease refers to primary hypoadrenalism, which is a deficiency in glucocorticoid and mineralocorticoid production by the adrenal gland. In the Western world, Addison's disease is most commonly an autoimmune condition, in which the body produces antibodies against cells of the adrenal cortex. Worldwide, the disease is more frequently caused by infection, especially from tuberculosis. A distinctive feature of Addison's disease is hyperpigmentation of the skin, which presents with other nonspecific symptoms such",
      "Addison's disease Addison's disease, also known as primary adrenal insufficiency and hypocortisolism, is a long-term endocrine disorder in which the adrenal glands do not produce enough steroid hormones. Symptoms generally come on slowly and may include abdominal pain, weakness, and weight loss. Darkening of the skin in certain areas may also occur. Under certain circumstances, an adrenal crisis may occur with low blood pressure, vomiting, lower back pain, and loss of consciousness. An adrenal crisis can be triggered by stress, such as from an injury, surgery, or infection. Addison's disease arises from problems with the adrenal gland such that not",
      "treatment with intravenous injections to treat the crisis. Individuals with Addison's disease have more than a doubled mortality rate. Furthermore, individuals with Addison's disease and diabetes mellitus have an almost 4 time increase in mortality compared to individuals with only diabetes. Addison\u2019s disease is named after Thomas Addison, the British physician who first described the condition in \"On the Constitutional and Local Effects of Disease of the Suprarenal Capsules\" (1855). All of Addison's six original patients had tuberculosis of the adrenal glands. While Addison's six patients in 1855 all had adrenal tuberculosis, the term \"Addison's disease\" does not imply an"
    ]
  ],
  [
    "Signs and symptoms of circulatory overload include weakness, fast heart rate, fast breathing, sweating, anxiety, increased thirst, confusion, unconsciousness, or cardiac arrest as complications worsen (Shock (circulatory)).",
    [
      "Shock (circulatory) Shock is the state of not enough blood flow to the tissues of the body as a result of problems with the circulatory system. Initial symptoms may include weakness, fast heart rate, fast breathing, sweating, anxiety, and increased thirst. This may be followed by confusion, unconsciousness, or cardiac arrest as complications worsen. Shock is divided into four main types based on the underlying cause: low volume, cardiogenic, obstructive, and distributive shock. Low volume shock may be from bleeding, vomiting, or pancreatitis. Cardiogenic shock may be due to a heart attack or cardiac contusion. Obstructive shock may be due",
      "pressure overload, while a right ventricular heave suggests right ventricular pressure overload. Other signs provide evidence for specific causes of pressure overload. Hypertension is diagnosed by sphygmomanometry. A narrow pulse pressure is a sign of aortic stenosis. The chest x-ray may show pulmonary hyperaemia in the case of pulmonary hypertension, and pulmonary oligemia in pulmonary stenosis. Pulmonary hypertension is also associated with chronic lung disease. Coarctation of the aorta presents with a significant difference in blood pressure between the upper and lower limbs, a systolic murmur or \"radiofemoral delay\". Treatment will depend on the underlying cause. However, in general, symptomatic",
      "caused by e.g. arterial compression, rupture or pathological vasoconstriction). Symptoms may begin quickly or slowly depending on the size of the embolus and how much it blocks the blood flow. Symptoms of embolisation in an organ vary with the organ involved but commonly include: Later symptoms are closely related to infarction of the affected tissue. This may cause permanently decreased organ function. For example, symptoms of myocardial infarction mainly include chest pain, dyspnea, diaphoresis (an excessive form of sweating), weakness, light-headedness, nausea, vomiting, and palpitations. Symptoms of limb infarction include coldness, decreased or no pulse beyond the site of blockage,"
    ]
  ],
  [
    "Common causes of acute urinary retention include blockage of the urethra, nerve problems, certain medications, weak bladder muscles, benign prostatic hyperplasia (BPH), urethral strictures, bladder stones, constipation, cystocele, and tumors.",
    [
      "pass urine. In some people, the disorder starts gradually but in others it may appear suddenly. Acute urinary retention is a medical emergency and requires prompt treatment. The pain can be excruciating when urine is not able to flow out. Moreover, one can develop severe sweating, chest pain, anxiety and high blood pressure. Other patients may develop a shock-like condition and may require admission to a hospital. Serious complications of untreated urinary retention include bladder damage and chronic kidney failure. Urinary retention is a disorder treated in a hospital, and the quicker one seeks treatment, the fewer the complications. In",
      "Urinary retention Urinary retention is an inability to completely empty the bladder. Onset can be sudden or gradual. When of sudden onset, symptoms include an inability to urinate and lower abdominal pain. When of gradual onset, symptoms may include loss of bladder control, mild lower abdominal pain, and a weak urine stream. Those with long term problems are at risk of urinary tract infections. Causes include blockage of the urethra, nerve problems, certain medications, and weak bladder muscles. Blockage can be caused by benign prostatic hyperplasia (BPH), urethral strictures, bladder stones, a cystocele, constipation, or tumors. Nerve problems can occur",
      "80 this increases 30%. Urinary retention is characterised by poor urinary stream with intermittent flow, straining, a sense of incomplete voiding, and hesitancy (a delay between trying to urinate and the flow actually beginning). As the bladder remains full, it may lead to incontinence, nocturia (need to urinate at night), and high frequency. Acute retention, causing complete anuria, is a medical emergency, as the bladder can stretch to an enormous size, and possibly tear if not dealt with quickly. If the bladder distends enough, it becomes painful. In such a case, there may be suprapubic constant, dull, pain. The increase"
    ]
  ],
  [
    "The recommended tool for cleaning the mouth is mouthwash, which should be used as an aid to brushing rather than a replacement.",
    [
      "claiming to kill bacteria that make up plaque or to freshen breath. In their basic form, they are usually recommended for use after brushing but some manufacturers recommend pre-brush rinsing. Dental research has recommended that mouthwash should be used as an aid to brushing rather than a replacement, because the sticky resistant nature of plaque prevents it from being actively removed by chemicals alone, and physical detachment of the sticky proteins is required. Tooth soap cleans gums as well as fissures and pits in teeth using soap. The soap helps remove oils, residue and other contaminants. It is available in",
      "there was a limited evidence base for best practice. Common use involves rinsing the mouth with about 20-50 ml (2/3 fl oz) of mouthwash. The wash is typically swished or gargled for about half a minute and then spat out. Most companies suggest not drinking water immediately after using mouthwash. In some brands, the expectorate is stained, so that one can see the bacteria and debris. Mouthwash should not be used immediately after brushing the teeth so as not to wash away the beneficial fluoride residue left from the toothpaste. Similarly, the mouth should not be rinsed out with water",
      "supplement toothbrushing and interdental cleaning. These include special toothpicks, oral irrigators, and other devices. A 2015 Cochrane review found insufficient evidence to determine whether the interdental brushing decreases the levels of plaque when compared to flossing. Teeth can be cleaned by scrubbing with a twig instead of a toothbrush. Plant sap in the twig takes the place of toothpaste. In many parts of the world teeth cleaning twigs are used. In the Muslim world the miswak or \"siwak\" is made from twigs or roots that are said to have an antiseptic effect when used for cleaning teeth. Teeth cleaning (also"
    ]
  ],
  [
    "Itopride and pirenzepine may be used to enhance gastric motility by increasing acetylcholine concentrations and stimulating the M receptor.",
    [
      "Itopride increases acetylcholine concentrations by inhibiting dopamine D2 receptors and acetylcholinesterase. Higher acetylcholine increases GI peristalsis, increases the lower esophageal sphincter pressure, stimulates gastric motility, accelerates gastric emptying, and improves gastro-duodenal coordination. Itopride given as a single dose study found that it also raises levels of motilin, somatostatin and lowers levels of cholecystokinin, as well as adrenocorticotropic hormone. These effects may also contribute to itopride's pharmacology. Anticholinergic agents reduce the action of itopride. It is worth noting that itopride is a relatively new drug and that it is, therefore, possible that other drugs may interact with itopride, rendering contraindications or",
      "barrier. Pirenzepine has been investigated for use in myopia control. It promotes the homodimerization or oligomerisation of M receptors. Pirenzepine Pirenzepine (Gastrozepin), an M selective antagonist, is used in the treatment of peptic ulcers, as it reduces gastric acid secretion and reduces muscle spasm. It is in a class of drugs known as muscarinic receptor antagonists - acetylcholine being the neurotransmitter of the parasympathetic nervous system which initiates the rest-and-digest state (as opposed to fight-or-flight), resulting in an increase in gastric motility and digestion; whereas pirenzepine would inhibit these actions and cause decreased gastric motility leading to delayed gastric emptying",
      "Pirenzepine Pirenzepine (Gastrozepin), an M selective antagonist, is used in the treatment of peptic ulcers, as it reduces gastric acid secretion and reduces muscle spasm. It is in a class of drugs known as muscarinic receptor antagonists - acetylcholine being the neurotransmitter of the parasympathetic nervous system which initiates the rest-and-digest state (as opposed to fight-or-flight), resulting in an increase in gastric motility and digestion; whereas pirenzepine would inhibit these actions and cause decreased gastric motility leading to delayed gastric emptying and constipation. It has no effects on the brain and spinal cord as it cannot diffuse through the blood\u2013brain"
    ]
  ],
  [
    "The presence of creatine kinase (CK-MB) in blood plasma is significant for diagnostic purposes as it is a cardiac marker used to assist in the diagnosis of an acute myocardial infarction.",
    [
      "heart, in addition to the MM-CK homodimer, also the heterodimer MB-CK consisting of one muscle (M-CK) and one brain-type (B-CK) subunit is expressed. The latter may be an important serum marker for myocardial infarction, if released from damaged myocardial cells into the blood where it can be detected by clinical chemistry. CKM (gene) Creatine kinase, muscle also known as CKM is a creatine kinase that in humans is encoded by the \"CKM\" gene. In the figure to the right, the crystal structure of the muscle-type M-CK monomer is shown. In vivo, two such monomers arrange symmetrically to form the active",
      "cases of CK-MB levels exceeding the blood level of total CK have been reported, especially in newborns with cardiac malformations, especially ventricular septal defects. This reversal of ratios is in favor of pulmonary emboli or vasculitis. An autoimmune reaction creating a complex molecule of CK and IgG should be taken into consideration. CPK-MB test The CPK-MB test is a cardiac marker used to assist diagnoses of an acute myocardial infarction. It measures the blood level of CK-MB (creatine kinase-muscle/brain), the bound combination of two variants (isoenzymes CKM and CKB) of the enzyme phosphocreatine kinase. In some locations, the test has",
      "because they are highly specific for cardiac disease. Testing for the MB form of creatine kinase provides information about the heart's blood supply, but is used less frequently because it is less specific and sensitive. Other blood tests are often taken to help understand a person's general health and risk factors that may contribute to heart disease. These often include a full blood count investigating for anaemia, and basic metabolic panel that may reveal any disturbances in electrolytes. A coagulation screen is often required to ensure that the right level of anticoagulation is given. Fasting lipids and fasting blood glucose"
    ]
  ],
  [
    "An artificial stoma should be sited as low down in the tract as possible to allow for maximal natural digestion before eliminating fecal matter from the body.",
    [
      "Stoma (medicine) In anatomy, a stoma (plural stomata or stomas) is any opening in the body. For example, a mouth, a nose, and an anus are natural stomata. Any hollow organ can be manipulated into an artificial stoma as necessary. This includes the esophagus, stomach, duodenum, ileum, colon, pleural cavity, ureters, urinary bladder, and renal pelvis. Such a stoma may be permanent or temporary. Surgical procedures that involve the creation of an artificial stoma have names that typically end with the suffix \"-ostomy\", and the same names are also often used to refer to the stoma thus created. For example,",
      "as low down in the tract as possible, as this allows the maximal amount of natural digestion to occur before eliminating fecal matter from the body. The stoma is usually covered with a removable pouching system (adhesive or mechanical) that collects and contains the output for later disposal. Modern pouching systems enable most individuals to resume normal activities and lifestyles after surgery, often with no outward physical evidence of the stoma or its pouching system. When planning the position of the stoma, a stoma nurse should bear in mind the height of the person's waist and beltline so that clothes",
      "can fit as before. Also a peri-stomal hernia belt worn from the start can help prevent the stoma from developing a serious hernia problem. The historical practice of trepanation was also a type of stoma. Stoma (medicine) In anatomy, a stoma (plural stomata or stomas) is any opening in the body. For example, a mouth, a nose, and an anus are natural stomata. Any hollow organ can be manipulated into an artificial stoma as necessary. This includes the esophagus, stomach, duodenum, ileum, colon, pleural cavity, ureters, urinary bladder, and renal pelvis. Such a stoma may be permanent or temporary. Surgical"
    ]
  ],
  [
    "The rapid improvement in women's world record performances in recent years can be attributed to advancements in training techniques, technology, and increased participation and support for female athletes.",
    [
      "International Association of Athletics Federations in 1981. As of June 21, 2009, eight women's world records have been ratified by the IAAF in the event. Before the event was recognised by the IAAF as an official world record event the 3000 metres was the most common international women's long-distance track event, although women did sometimes compete over 10,000 m before its addition to the World Championships and Olympic programme in 1987 and 1988, respectively. Auto times to the hundredth of a second were required by the IAAF for events up to and including 10,000 m from 1981. However, Henry Rono's",
      "the most prestigious competition for the distance and it attracts elite level, international competitors. The winner of the race is occasionally referred to as \"the world's fastest\" man or woman, reflecting the high level of the competition and the quality of performances. , the current Olympic records of 9.63 for men and 10.62 seconds for women rank as the second and third fastest times in history, for men and women respectively. The standard of performances at the Olympics has progressed in line with the discipline as a whole and the times in the final often rank highly in the end-of-season",
      "for women, set by Berhane Adere in 2003. The world record has never been broken or equalled at the competition by either men or women, reflecting the lack of pacemaking and athletes' more tactical approach to championship races. Haile Gebrselassie is the most successful athlete of the event with four gold medals and also a silver and a bronze, spanning a period from 1993 to 2003. His Ethiopian compatriot Kenenisa Bekele matched his feat of four consecutive titles in 2009. Tirunesh Dibaba is the most successful woman, with three gold medals to her name (2005, 2007, 2013, plus a silver"
    ]
  ],
  [
    "During fertilization, genetic information is recombined through the mixing of sex cells, resulting in a unique mix of genes for the organism, similar to shuffling different hands of playing cards from each parent.",
    [
      "sex cells and recombining that information during fertilisation. This is like mixing different hands of playing cards, with an organism getting a random mix of half of the cards from one parent, and half of the cards from the other. Mendel called the information \"factors\"; however, they later became known as genes. Genes are the basic units of heredity in living organisms. They contain the information that directs the physical development and behavior of organisms. Genes are made of DNA. DNA is a long molecule made up of individual molecules called nucleotides. Genetic information is encoded in the sequence of",
      "wherein organisms have three or more sets of genetic variation (3n or more). Crossing over (genetic recombination) and random segregation during meiosis can result in the production of new alleles or new combinations of alleles. Furthermore, random fertilization also contributes to variation. Variation and recombination can be facilitated by transposable genetic elements, endogenous retroviruses, LINEs, SINEs, etc. For a given genome of a multicellular organism, genetic variation may be acquired in somatic cells or inherited through the germline. Genetic variation can be divided into different forms according to the size and type of genomic variation underpinning genetic change. Small-scale sequence",
      "invade surrounding tissues. In certain lymphocytes in the human immune system, V(D)J recombination generates different genomic sequences such that each cell produces a unique antibody or T cell receptors. During meiosis, diploid cells divide twice to produce haploid germ cells. During this process, recombination results in a reshuffling of the genetic material from homologous chromosomes so each gamete has a unique genome. Genomes are more than the sum of an organism's genes and have traits that may be measured and studied without reference to the details of any particular genes and their products. Researchers compare traits such as karyotype (chromosome"
    ]
  ],
  [
    " is generally not recoverable in negligence cases unless the plaintiff can prove an assumption of responsibility by the defendant and known reliance on the defendant, or vulnerability in the sense of the plaintiff's inability to protect themselves from the risk of the loss.",
    [
      "the general rule is that damages for economic loss which are not consequential upon damage to person or property are not recoverable in negligence even if the loss is foreseeable. Economic loss may be recoverable in cases where the plaintiff can prove an assumption of responsibility by the defendant and known reliance on the defendant by the plaintiff, or vulnerability in the sense of the inability of the plaintiff to take steps to protect itself from the risk of the loss. Cases in which the High Court has held that economic loss was recoverable include: Justice Cardozo's indeterminacy concerns were",
      "agree with the majority, finding that the loss was both direct and foreseeable consequence of the defendant's negligence and should therefore be recovered. In his view, in most cases, spurious claims could be avoided either on the grounds that no duty was owed or that the damage was too remote. The judgment has outlined in very clear terms that there are two types of economic loss: economic loss consequential on physical damage and \"pure\" economic loss. Only the first is in principle recoverable. This has led to much litigation concerning the precise distinction between economic and physical damage as well",
      "economic loss include the following: The latter case is exemplified by the English case of \"Spartan Steel and Alloys Ltd v Martin & Co Ltd\". Similar losses are also restricted in German law, though not in French law beyond the normal requirements that a claimant's asserted loss must be certain and directly caused. Recovery at law for pure economic loss is restricted under some circumstances in some jurisdictions, in particular in tort in common law jurisdictions, for fear that it is potentially unlimited and could represent a \"crushing liability\" against which parties would find it impossible to insure. In Australia,"
    ]
  ],
  [
    "Signs of Directed Attention Fatigue include feeling unusually distractible, impatient, forgetful, or cranky, leading to bad judgment, apathy, accidents, and increased stress levels, impacting behavior and cognitive function.",
    [
      "individuals are less likely to help someone in need. They are also more aggressive, less tolerant, and less sensitive to socially important cues. Fatigue that is experienced by participants of these kinds of studies is induced by attention-intensive tasks, and the observed effects of such fatigue are correlated with decline in inhibitory control. Signs of Directed Attention Fatigue include temporarily feeling unusually distractible, impatient, forgetful, or cranky when there is no associated illness. In more severe forms, it can lead to bad judgment, apathy, or accidents, and can contribute to increased stress levels. There are 6 major areas of mental",
      "much other research that has been performed regarding directed attention fatigue, the brain's ability to maintain directed attention is heightened after exposure to nature. Directed attention fatigue Directed attention fatigue (DAF) is a neuro-psychological phenomenon that results from overuse of the brain\u2019s inhibitory attention mechanisms, which handle incoming distractions while maintaining focus on a specific task. The greatest threat to a given focus of attention is competition from other stimuli that can cause a shift in focus. This is because one maintains focus on a particular thought by inhibiting all potential distractions and not by strengthening that central mental activity.",
      "Directed attention fatigue Directed attention fatigue (DAF) is a neuro-psychological phenomenon that results from overuse of the brain\u2019s inhibitory attention mechanisms, which handle incoming distractions while maintaining focus on a specific task. The greatest threat to a given focus of attention is competition from other stimuli that can cause a shift in focus. This is because one maintains focus on a particular thought by inhibiting all potential distractions and not by strengthening that central mental activity. Directed attention fatigue occurs when a particular part of the brain\u2019s global inhibitory system is overworked due to the suppression of increasing numbers of"
    ]
  ],
  [
    "The process that allows NADH and FADH2 to deliver their electrons to power the production of ATP in the mitochondrion is oxidative phosphorylation, which occurs in the electron transport chain.",
    [
      "of electrons, is used to reduce NAD+ and FAD to NADH and FADH, respectively. NADH and FADH contain the stored energy harnessed from the initial glucose molecule and is used in the electron transport chain where the bulk of the ATP is produced. The last process in aerobic respiration is oxidative phosphorylation, also known as the electron transport chain. Here NADH and FADH, which contain the energy in the form of electrons, deliver their electrons to the inner membranes of the mitochondrion to power the production of ATP. Oxidative phosphorylation contributes the majority of the ATP produced, compared to glycolysis",
      "powers most cellular reactions. A small amount of ATP is available from substrate-level phosphorylation, for example, in glycolysis. In most organisms the majority of ATP is generated in electron transport chains, while only some obtain ATP by fermentation. Most eukaryotic cells have mitochondria, which produce ATP from products of the citric acid cycle, fatty acid oxidation, and amino acid oxidation. At the mitochondrial inner membrane, electrons from NADH and FADH2 pass through the electron transport chain to oxygen, which is reduced to water. The electron transport chain comprises an enzymatic series of electron donors and acceptors. Each electron donor will",
      "proposed were: NADH is oxidized into NAD, H ions, and electrons by an enzyme. FADH is also oxidized into H ions, electrons, and FAD. As those electrons travel farther through the electron transport chain in the inner membrane, energy is gradually released and used to pump the hydrogen ions from the splitting of NADH and FADH into the space between the inner membrane and the outer membrane (called the intermembrane space), creating an electrochemical gradient. This electrochemical gradient creates potential energy (see \"\") across the inner mitochondrial membrane known as the proton-motive force. As a result, chemiosmosis occurs, and the"
    ]
  ],
  [
    "The tendon reflex operates as a feedback mechanism to control muscle tension by causing muscle relaxation before muscle force becomes so great that tendons might be torn.",
    [
      "relaxation before the tendon tension becomes high enough to cause damage. First, as a load is placed on the muscle, the afferent neuron from the Golgi tendon organ fires into the central nervous system. Second, the motor neuron from the spinal cord is inhibited via an IPSP and muscle relaxes. The stretch reflex operates as a feedback mechanism to control muscle length by causing muscle contraction. In contrast, the tendon reflex operates as a feedback mechanism to control muscle tension by causing muscle relaxation before muscle force becomes so great that tendons might be torn. Although the tendon reflex is",
      "Golgi tendon reflex The Golgi tendon reflex is a normal component of the reflex arc of the peripheral nervous system. In a Golgi tendon reflex, skeletal muscle contraction causes the antagonist muscle to simultaneously lengthen and relax. This reflex is also called the inverse myotatic reflex, because it is the inverse of the stretch reflex. Though muscle tension is increasing during the contraction, alpha motor neurons in the spinal cord supplying the muscle are inhibited. However, antagonistic muscles are activated. The Golgi tendon reflex operates as a protective feedback mechanism to control the tension of an active muscle by causing",
      "over the entire muscle by preventing \u201cmuscle fibers connected with overstimulated tendon organs so that their contraction is more comparable to the contraction of the rest of the muscle.\u201d Tendon reflex Tendon reflex (or T-reflex) may refer to: To test the reflex, tap on the tendon. In a healthy individual the intensity on both sides is equal. This means that the connections between the spinal cord and the muscles are undamaged. Main Spinal Nerve Roots Involved: The Golgi tendon reflex is a response to extensive tension on a tendon. It helps avoid strong muscle contractions which could tear the tendon"
    ]
  ],
  [
    "The Golgi apparatus packages proteins by modifying them and sorting them for transport to their next destinations within the cell.",
    [
      "apparatus is in the formation of proteoglycans. Enzymes in the Golgi append proteins to glycosaminoglycans, thus creating proteoglycans. Glycosaminoglycans are long unbranched polysaccharide molecules present in the extracellular matrix of animals. The vesicles that leave the rough endoplasmic reticulum are transported to the \"cis\" face of the Golgi apparatus, where they fuse with the Golgi membrane and empty their contents into the lumen. Once inside the lumen, the molecules are modified, then sorted for transport to their next destinations. Those proteins destined for areas of the cell other than either the endoplasmic reticulum or the Golgi apparatus are moved through",
      "it interacts with several other proteins to gain its final state. After translation, proteins within the ER make sure that the protein is folded correctly. If after a first attempt the folding is unsuccessful, a second folding is attempted. If this fails too the protein is exported to the cytosol and labelled for destruction. Aside from the folding, there is also a sugar chain added to the protein. After these changes, the protein is transported to the Golgi apparatus by a coated vesicle using coating protein COPII. In the Golgi apparatus, the sugar chains are modified by adding or removing",
      "face cisternae, and enzymes catalyzing later modifications are found in \"trans\" face cisternae of the Golgi stacks. The Golgi apparatus is a major collection and dispatch station of protein products received from the endoplasmic reticulum (ER). Proteins synthesized in the ER are packaged into vesicles, which then fuse with the Golgi apparatus. These cargo proteins are modified and destined for secretion via exocytosis or for use in the cell. In this respect, the Golgi can be thought of as similar to a post office: it packages and labels items which it then sends to different parts of the cell or"
    ]
  ],
  [
    "No, the pulse condition cannot be effectively diagnosed by palpating the pulse in the early morning before any food or drink has been consumed.",
    [
      "circumstances under which they occur are important, as is information about caffeine intake (tea or coffee drinking), and whether continual palpitations can be stopped by deep breathing or changing body positions. It is also very helpful to know how they start and stop (abruptly or not), whether or not they are regular, and approximately how fast the pulse rate is during an attack. If the person has discovered a way of stopping the palpitations, that is also helpful information. The diagnosis is usually not made by a routine medical examination and electrical tracing of the heart's activity (ECG), because most",
      "people cannot arrange to have their symptoms be present while visiting the doctor. Nevertheless, findings such as a heart murmur or an abnormality of the ECG, which could point to the probable diagnosis, may be discovered. In particular, ECG changes that can be associated with specific disturbances of the heart rhythm may be picked up; so routine physical examination and ECG remain important in the assessment of palpitation. Blood tests, particularly tests of thyroid gland function are also important baseline investigations (an overactive thyroid gland is a potential cause for palpitations; the treatment in that case is to treat the",
      "Pulse In medicine, a pulse represents the tactile arterial palpation of the heartbeat by trained fingertips. The pulse may be palpated in any place that allows an artery to be compressed near the surface of the body, such as at the neck (carotid artery), wrist (radial artery), at the groin (femoral artery), behind the knee (popliteal artery), near the ankle joint (posterior tibial artery), and on foot (dorsalis pedis artery). Pulse (or the count of arterial pulse per minute) is equivalent to measuring the heart rate. The heart rate can also be measured by listening to the heart beat by"
    ]
  ],
  [
    "The function of the pyruvate dehydrogenase complex in cellular respiration is to convert pyruvate into acetyl-CoA through a process called pyruvate decarboxylation, which links the glycolysis metabolic pathway to the citric acid cycle and releases energy via NADH.",
    [
      "Pyruvate dehydrogenase Pyruvate dehydrogenase is the first component enzyme of pyruvate dehydrogenase complex (PDC). The pyruvate dehydrogenase complex contributes to transforming pyruvate into acetyl-CoA by a process called pyruvate decarboxylation. Acetyl-CoA may then be used in the citric acid cycle to carry out cellular respiration, so pyruvate dehydrogenase contributes to linking the glycolysis metabolic pathway to the citric acid cycle and releasing energy via NADH. Pyruvate dehydrogenase (E1) performs the first two reactions within the pyruvate dehydrogenase complex (PDC): a decarboxylation of substrate 1 (pyruvate) and a reductive acetylation of substrate 2 (lipoic acid). Lipoic acid is covalently bound to",
      "Pyruvate dehydrogenase complex Pyruvate dehydrogenase complex (PDC) is a complex of three enzymes that converts pyruvate into acetyl-CoA by a process called pyruvate decarboxylation. Acetyl-CoA may then be used in the citric acid cycle to carry out cellular respiration, and this complex links the glycolysis metabolic pathway to the citric acid cycle. Pyruvate decarboxylation is also known as the \"pyruvate dehydrogenase reaction\" because it also involves the oxidation of pyruvate. This multi-enzyme complex is related structurally and functionally to the oxoglutarate dehydrogenase and branched-chain oxo-acid dehydrogenase multi-enzyme complexes. The reaction catalysed by pyruvate dehydrogenase complex is: Initially, pyruvate and thiamine",
      "next enzyme, thus greatly increasing the rate of reaction. The pyruvate dehydrogenase complex is responsible for the oxidative decarboxylation of pyruvate, with the final product being Acetyl CoA. Overall the complex catalyzes five reactions, with the overall reaction being: Pyruvate + CoA + NAD \u2192 acetyl-CoA + CO There are three different coenzymes required throughout the 5 steps that this complex carries out: thiamine pyrophosphate (TPP), lipoamide, and coenzyme A. This step is only one of the central metabolic pathway carried out by eukaryotes, in which glucose is oxidized to form carbon dioxide, water, and ATP. The E1 complex specifically"
    ]
  ],
  [
    "Some common injuries and diseases that can cause knee pain include ligamentous tears, fractures, soft tissue inflammation, osteoarthritis, infections, gout, psoriasis, and overuse injuries such as patellar tendonitis or synovial bursitis.",
    [
      "Knee pain Knee pain is pain in or around the knee. The knee joint consists of an articulation between four bones: the femur, tibia, fibula and patella. There are four compartments to the knee. These are the medial and lateral tibiofemoral compartments, the patellofemoral compartment and the superior tibiofibular joint. The components of each of these compartments can suffer from repetitive strain, injury or disease. Running long distance can cause pain to the knee joint, as it is high-impact exercise. Some common injuries include: Some of the diseases of cause of knee pain include the following: Common deformities of the",
      "sound at the time of the injury (indicates ligamentous tear or fracture), swelling, infections, ability to stand or walk, sensation of instability (suggestive of subluxation), or any previous traumatic injuries to the joint are all important historical features. The most common knee problems are: soft tissue inflammation, injury, or osteoarthritis. The mechanism of the knee injury can give a clue of the possible structures that can be injured. For example, applying valgus stress on the knee can cause medial collateral ligament rupture, meanwhile a varus force can cause lateral collateral ligament rupture. When a person suddenly slows down during running,",
      "after exercise, infections, history of gout or psoriasis, and previous activities that contributes to long-term overuse of the knee joint should be asked. Knee pain due to long-term overuse are reproducible. For example, repetitive jumping can cause inflammation of patellar tendon. Repetitive kneeling can cause prepatellar inflammation of synovial bursa. Physical examination of the knee begins by observing the person's gait to assess for any abnormalities seen while walking. Gait assessment can be used to differentiate genuine knee pain or pain which referred from hip, lower back or the foot. A person can be asked to perform a duckwalk. This"
    ]
  ],
  [
    "The British National Formulary (BNF) is prepared by a Joint Formulary Committee (JFC) comprising representatives of the British Medical Association (BMA), the Royal Pharmaceutical Society, and the Department of Health, with information on drugs sourced from manufacturers' product literature, medical and pharmaceutical literature, regulatory authorities, and professional bodies.",
    [
      "the BNF. Many individuals and organisations contribute towards the preparation of the BNF. It is jointly authored by the British Medical Association (BMA) and the Royal Pharmaceutical Society; and is jointly published by the BMJ Group (which is owned by the BMA), and the Pharmaceutical Press (owned by the Royal Pharmaceutical Society). It is published under the authority of a Joint Formulary Committee (JFC), which comprises representatives of the two professional bodies, and the Department of Health (DoH). Information on drugs is drawn from the manufacturers' product literature, medical and pharmaceutical literature, regulatory authorities and professional bodies. Advice is constructed",
      "British National Formulary The British National Formulary (BNF) is a United Kingdom (UK) pharmaceutical reference book that contains a wide spectrum of information and advice on prescribing and pharmacology, along with specific facts and details about many medicines available on the UK National Health Service (NHS). Information within the BNF includes indication(s), contraindications, side effects, doses, legal classification, names and prices of available proprietary and generic formulations, and any other notable points. Though it is a national formulary, it nevertheless also includes entries for some medicines which are not available under the NHS, and must be prescribed and/or purchased privately.",
      "first published in 1949 as the National Formulary, with updated versions appearing every three years until 1976. The fifth version in 1957 saw its name change to \"The British National Formulary\". A new look version, under the auspices of Owen Wade, was released in 1981. A study in Northern Ireland looking at prescribing in 1965, reported that the BNF was likely able to serve the requirements of prescribers in general practice, while also achieving a cost saving. By 2003, issue 46 of the BNF contained 3000 interactions or groups of interactions, with about 900 of these marked by a bullet."
    ]
  ],
  [
    "The three clinics in the upper arm that are tested for reflexes are the biceps, triceps, and brachialis.",
    [
      "upward. The arm proper (\"brachium\"), sometimes called the upper arm, the region between the shoulder and the elbow, is composed of the humerus with the elbow joint at its distal end. The elbow joint is a complex of three joints \u2014 the humeroradial, humeroulnar, and superior radioulnar joints \u2014 the former two allowing flexion and extension whilst the latter, together with its inferior namesake, allows supination and pronation at the wrist. Triceps is the major extensor and brachialis and biceps the major flexors. Biceps is, however, the major supinator and while performing this action it ceases to be an effective",
      "the upper limb equivalent of the Babinski's sign test Hoffmann's reflex is often erroneously confused with Babinski's. However the two reflexes are quite different, and should not be equated with each other. A positive Babinski sign is considered a pathological sign of upper motor neuron disease \"except\" for infants, in whom it is normal. Whereas, a positive Hoffmann's sign can be present in an entirely normal patient. A positive Hoffmann's sign in the normal patients is more commonly found in those who are naturally hyper-reflexive (e.g. 3+ reflexes). A positive Hoffmann's sign is a worrisome finding of a disease process",
      "Reflex A reflex, or reflex action, is an involuntary and nearly instantaneous movement in response to a stimulus. A reflex is made possible by neural pathways called reflex arcs which can act on an impulse before that impulse reaches the brain. The reflex is then an automatic response to a stimulus that does not receive or need conscious thought. Myotatic reflexes The myotatic reflexes (also known as \"deep tendon reflexes\"), provide information on the integrity of the central nervous system and peripheral nervous system. Generally, decreased reflexes indicate a peripheral problem, and lively or exaggerated reflexes a central one. A"
    ]
  ],
  [
    "is most commonly caused by a pneumothorax.",
    [
      "life-threatening emergency. The normal position of the trachea is straight up and down, running along the center of the front side of the throat. Certain conditions can cause the trachea to shift to one side or the other. This is a medical emergency that requires immediate medical attention to discover the cause of the shift and begin an appropriate course of treatment. There are several causes for a tracheal deviation, and the condition often presents along with difficulty breathing, coughing and abnormal breath sounds. The most common cause of tracheal deviation is a pneumothorax, which is a collection of air",
      "result of the wide range of causes of tracheal deviation, it is absolutely essential to seek medical attention in order to obtain an accurate diagnosis. Since tracheal deviation is a sign as opposed to a condition, treatment is focused on correcting the cause of the finding. In the case of pneumothorax, thoracentesis or chest tube insertion is performed to relieve the pressure within the affected pleural cavity. Tracheal deviation Tracheal deviation is a clinical sign that results from unequal intrathoracic pressure within the chest cavity. It is most commonly associated with traumatic pneumothorax, but can be caused by a number",
      "Tracheal deviation Tracheal deviation is a clinical sign that results from unequal intrathoracic pressure within the chest cavity. It is most commonly associated with traumatic pneumothorax, but can be caused by a number of both acute and chronic health issues, such as pneumonectomy, atelectasis, pleural effusion, fibrothorax (pleural fibrosis), or some cancers (tumors within the bronchi, lung, or pleural cavity) and certain lymphomas associated with the mediastinal lymph nodes. In most adults and children, the trachea can be seen and felt directly in the middle of the anterior (front side) neck behind the jugular notch of the manubrium and superior"
    ]
  ],
  [
    "The main health risk associated with potable use of reclaimed water is the potential presence of pharmaceutical and other household chemicals or their derivatives that may persist in the water.",
    [
      "health protection for both planned and unplanned (or de facto) reuse and increase public confidence in water reuse. Many humans associate a feeling of disgust with reclaimed water and 13% of a survey group said they would not even sip it. Nonetheless, the main health risk for potable use of reclaimed water is the potential for pharmaceutical and other household chemicals or their derivatives (Environmental persistent pharmaceutical pollutants) to persist in this water. This would be less of a concern if human excreta was kept out of sewage by using dry toilets or systems that treat blackwater separately from greywater.",
      "To address these concerns about the source water, reclaimed water providers use multi-barrier treatment processes and constant monitoring to ensure that reclaimed water is safe and treated properly for the intended end use. There is debate about possible health and environmental effects. To address these concerns, A Risk Assessment Study of potential health risks of recycled water and comparisons to conventional Pharmaceuticals and Personal Care Product (PPCP) exposures was conducted by the WateReuse Research Foundation. For each of four scenarios in which people come into contact with recycled water used for irrigation - children on the playground, golfers, and landscape,",
      "incidences of illness or disease from either microbial pathogens or chemicals, and the risks of using reclaimed water for irrigation are not measurably different from irrigation using potable water. A 2012 study conducted by the National Research Council in the United States of America found that the risk of exposure to certain microbial and chemical contaminants from drinking reclaimed water does not appear to be any higher than the risk experienced in at least some current drinking water treatment systems, and may be orders of magnitude lower. This report recommends adjustments to the federal regulatory framework that could enhance public"
    ]
  ],
  [
    "The factors that determine the normal range of motion in specific joints include age, gender, injury, surgery, immobilization, and individual differences in flexibility and muscle strength.",
    [
      "a joint can move between the flexed position and the extended position. The act of attempting to increase this distance through therapeutic exercises (range of motion therapy\u2014stretching from flexion to extension for physiological gain) is also sometimes called range of motion. Each specific joint has a normal range of motion that is expressed in degrees. The reference values for the normal ROM in individuals differ slightly depending on age and gender. For example, as an individual ages, they typically lose a small amount of ROM. Analog and traditional devices to measure range of motion in the joints of the body",
      "the joints involved: Apart from this motions can also be divided into: The study of movement is known as kinesiology. A categoric list of movements of the human body and the muscles involved can be found at list of movements of the human body. The prefix \"hyper-\" is sometimes added to describe movement beyond the normal limits, such as in \"hypermobility\", \"hyperflexion\" or \"hyperextension\". The range of motion describes the total range of motion that a joint is able to do. For example, if a part of the body such as a joint is overstretched or \"bent backwards\" because of",
      "or persons or a passive motion machine. When passive range of motion is applied, the joint of an individual receiving exercise is completely relaxed while the outside force moves the body part, such as a leg or arm, throughout the available range. Injury, surgery, or immobilization of a joint may affect the normal joint range of motion. Active range of motion is movement of a joint provided entirely by the individual performing the exercise. In this case, there is no outside force aiding in the movement. Active assist range of motion is described as a joint receiving partial assistance from"
    ]
  ],
  [
    "The United Kingdom Misuse of Drugs Act 1971 regulates controlled drugs, which are classified into three classes (A, B, and C) with penalties for illegal possession and supply varying within each class.",
    [
      "Drugs controlled by the UK Misuse of Drugs Act Drugs controlled by the United Kingdom (UK) Misuse of Drugs Act 1971 are listed in this article. These drugs are known in the UK as \"controlled drugs\", because this is the term by which the act itself refers to them. In more general terms, however, many of these drugs are also controlled by the Medicines Act 1968, there are many other drugs which are controlled by the Medicines Act but not by the Misuse of Drugs Act, and other substances which may be considered drugs (alcohol, for example) are controlled by",
      "levels of harm previously demonstrated by David Nutt show that alcohol and tobacco were among the most lethal, while some class A drugs, such as MDMA, LSD, and magic mushrooms, were among the least harmful. Misuse of Drugs Act 1971 The Misuse of Drugs Act 1971 is an Act of the Parliament of the United Kingdom. It represents action in line with treaty commitments under the Single Convention on Narcotic Drugs, the Convention on Psychotropic Substances, and the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances. Offences under the Act include: It is often presented as",
      "Therefore, for example, various opiates are available legally as prescription-only medicines, and cannabis (hemp) may be grown under licence for 'industrial purposes'. The Misuse of Drugs Regulations 2001, created under the 1971 Act, are about licensing of production, possession and supply of substances classified under the act. The act creates three classes of controlled substances, A, B, and C, and ranges of penalties for illegal or unlicensed possession and possession with intent to supply are graded differently within each class. The lists of substances within each class can be amended by order, so the Home Secretary can list new drugs"
    ]
  ],
  [
    "VO2 max is the maximum rate of oxygen consumption measured during incremental exercise, typically assessed through a graded exercise test where exercise intensity is progressively increased until maximal oxygen consumption is reached.",
    [
      "began measuring oxygen consumption during exercise. Notable contributions were made by Henry Taylor at the University of Minnesota, Scandinavian scientists Per-Olof \u00c5strand and Bengt Saltin in the 1950s and 60s, the Harvard Fatigue Laboratory, German universities, and the Copenhagen Muscle Research Centre among others. VO2 max VO max (also maximal oxygen consumption, maximal oxygen uptake, peak oxygen uptake or maximal aerobic capacity) is the maximum rate of oxygen consumption measured during incremental exercise; that is, exercise of increasing intensity. The name is derived from three abbreviations: \"V\" for volume, \"O\" for oxygen, and \"max\" for maximum. Maximal oxygen consumption reflects",
      "VO2 max VO max (also maximal oxygen consumption, maximal oxygen uptake, peak oxygen uptake or maximal aerobic capacity) is the maximum rate of oxygen consumption measured during incremental exercise; that is, exercise of increasing intensity. The name is derived from three abbreviations: \"V\" for volume, \"O\" for oxygen, and \"max\" for maximum. Maximal oxygen consumption reflects the cardiorespiratory fitness of an individual, which is a powerful modifiable determinant of life expectancy in United States adults. It is also an important determinant of endurance capacity during prolonged exercise. VO max is widely used as an indicator of cardiorespiratory fitness. In 2016,",
      "the aerobic energy system. In general clinical and athletic testing, this usually involves a graded exercise test (either on a treadmill or on a cycle ergometer) in which exercise intensity is progressively increased while measuring: VO max is reached when oxygen consumption remains at a steady state despite an increase in workload. VO max is properly defined by the Fick equation: The necessity for a subject to exert maximum effort in order to accurately measure VO max can be dangerous in those with compromised respiratory or cardiovascular systems; thus, sub-maximal tests for \"estimating\" VO max have been developed. An estimate"
    ]
  ],
  [
    "Muscle contractions in skeletal muscle are initiated by nerve impulses sent to the muscle from motor neurons, causing the muscle fibers to contract. (1)",
    [
      "a single, short muscle contraction called a muscle twitch. If there is a problem at the neuromuscular junction, a very prolonged contraction may occur, such as the muscle contractions that result from tetanus. Also, a loss of function at the junction can produce paralysis. Skeletal muscles are organized into hundreds of motor units, each of which involves a motor neuron, attached by a series of thin finger-like structures called axon terminals. These attach to and control discrete bundles of muscle fibers. A coordinated and fine tuned response to a specific circumstance will involve controlling the precise number of motor units",
      "Only skeletal and smooth muscles are part of the musculoskeletal system and only the skeletal muscles can move the body. Cardiac muscles are found in the heart and are used only to circulate blood; like the smooth muscles, these muscles are not under conscious control. Skeletal muscles are attached to bones and arranged in opposing groups around joints. Muscles are innervated, to communicate nervous energy to, by nerves, which conduct electrical currents from the central nervous system and cause the muscles to contract. In mammals, when a muscle contracts, a series of reactions occur. Muscle contraction is stimulated by the",
      "individual muscle group. In vitro muscle testing is used for more complete characterization of muscle properties. The electrical activity associated with muscle contraction are measured via electromyography (EMG). EMG is a common technique used in many disciplines within the Exercise and Rehab Sciences. Skeletal muscle has two physiological responses: relaxation and contraction. The mechanisms for which these responses occur generate electrical activity measured by EMG. Specifically, EMG can measure the action potential of a skeletal muscle, which occurs from the hyperpolarization of the motor axons from nerve impulses sent to the muscle (1). EMG is used in research for determining"
    ]
  ],
  [
    "Some signs of phlebitis include a slow onset of a tender red area along the superficial veins on the skin, a long, thin red area, pain, swelling, hemorrhagic bleb formation, ecchymosis, and systemic symptoms such as nausea, dizziness, chills, coagulopathy, and shock.",
    [
      "determine treatment. Phlebitis was first described by the Scottish surgeon John Hunter in 1784. Phlebitis Phlebitis or venitis is the inflammation of a vein, usually in the legs. It most commonly occurs in superficial veins. Phlebitis often occurs in conjunction with thrombosis and is then called thrombophlebitis or superficial thrombophlebitis. Unlike deep vein thrombosis, the probability that superficial thrombophlebitis will cause a clot to break up and be transported in pieces to the lung is very low. There is usually a slow onset of a tender red area along the superficial veins on the skin. A long, thin red area",
      "Thrombophlebitis Thrombophlebitis is a phlebitis (\"inflammation\" of a vein) related to a thrombus (blood clot). When it occurs repeatedly in different locations, it is known as thrombophlebitis migrans, \"(migrating thrombophlebitis)\" The following symptoms or signs are often associated with thrombophlebitis, although thrombophlebitis is not restricted to the veins of the legs. In terms of complications, one of the most serious occurs when the superficial blood clot is associated with a deeper venous thrombosis; this can then dislodge, traveling through the heart and occluding the dense capillary network of the lungs This is a pulmonary embolism which can be life-threatening to",
      "can cause pain, swelling, hemorrhagic bleb formation, and ecchymosis. Any swelling is usually not particularly severe, but it can involve all of the affected limb, as well as the trunk. Systemic symptoms can include nausea, dizziness, chills, coagulopathy, and shock. Klauber (1997) includes an account of a man who had been bitten on the first joint of the index finger of the right hand, with only a single fang penetrating. Although the bite was described as no more painful than a pin prick, a doctor was seen within about 25 minutes, and 10 cc of antivenin were administered. Within 2.5"
    ]
  ],
  [
    "The energy charge of a cell is directly related to the concentrations of ATP, ADP, and AMP.",
    [
      "equilibrium (<chem>ATP + AMP <=> 2 ADP</chem>). The energy charge is related to ATP, ADP and AMP concentrations. It was first defined by Atkinson and Walton who found that it was necessary to take into account the concentration of all three nucleotides, rather than just ATP and ADP, to account for the energy status in metabolism. Since the adenylate kinase maintains two ADP molecules in equilibrium with one ATP (<chem>2 ADP <=> ATP + AMP</chem>), Atkinson defined the adenylate energy charge as : The energy charge of most cells varies between 0.7 and 0.95 - oscillations in this range are",
      "the adenylate energy charge by decreasing the total {ATP+ADP+AMP} concentration. Energy charge The adenylate energy charge is an index used to measure the energy status of biological cells. ATP or Mg-ATP is the principal molecule for storing and transferring energy in the cell : it is used for biosynthetic pathways, maintenance of transmembrane gradients, movement, cell division, etc... More than 90% of the ATP is produced by phosphorylation of ADP by the ATP synthase. ATP can also be produced by \u201csubstrate level phosphorylation\u201d reactions (ADP phosphorylation by (1,3)-bisphosphoglycerate, phosphoenolpyruvate, phosphocreatine), by the succinate-CoA ligase and phosphoenolpyruvate carboxylkinase, and by adenylate",
      "ATP (into adenosine diphosphate (ADP) and inorganic phosphate) by utilizing it in biological processes. In a cell, the ratio of ATP to ADP concentrations is known as the \"energy charge\" of the cell. A cell can use this energy charge to relay information about cellular needs; if there is more ATP than ADP available, the cell can use ATP to do work, but if there is more ADP than ATP available, the cell must synthesize ATP via oxidative phosphorylation. Living organisms produce ATP from energy sources via oxidative phosphorylation. The terminal phosphate bonds of ATP are relatively weak compared with"
    ]
  ],
  [
    "The sarcoplasmic reticulum in muscle cells stores calcium ions and releases them into the sarcoplasm when the muscle fiber is stimulated, allowing for the interaction with contractile proteins and muscle contraction.",
    [
      "contraction of all the sarcomeres results in the contraction of the whole muscle fiber. This contraction of the myocyte is triggered by the action potential over the cell membrane of the myocyte. The action potential uses transverse tubules to get from the surface to the interior of the myocyte, which is continuous within the cell membrane. Sarcoplasmic reticula are membranous bags that transverse tubules touch but remain separate from. These wrap themselves around each sarcomere and are filled with Ca. Excitation of a myocyte causes depolarization at its synapses, the neuromuscular junctions, which triggers action potential. With a singular neuromuscular",
      "bound to their membranes and drifting within the confines of their lumens. This fundamental difference is indicative of their functions: The endoplasmic reticulum synthesizes molecules, while the sarcoplasmic reticulum stores calcium ions and pumps them out into the sarcoplasm when the muscle fiber is stimulated. After their release from the sarcoplasmic reticulum, calcium ions interact with contractile proteins that utilize ATP to shorten the muscle fiber. The sarcoplasmic reticulum plays a major role in excitation-contraction coupling. The endoplasmic reticulum serves many general functions, including the folding of protein molecules in sacs called cisternae and the transport of synthesized proteins in",
      "contract, calcium is released from the sarcoplasmic reticulum of the cells as well as the T tubules. The calcium release triggers sliding of the actin and myosin fibrils leading to contraction. A plentiful supply of mitochondria provide the energy for the contractions. Typically, cardiomyocytes have a single, central nucleus, but can also have two or more. Cardiac muscle cells branch freely and are connected by junctions known as intercalated discs which help the synchronized contraction of the muscle. The sarcolemma (membrane) from adjacent cells bind together at the intercalated discs. They consist of desmosomes, specialized linking proteoglycans, tight junctions, and"
    ]
  ],
  [
    "Pupil size and reactivity are used as fundamental parameters in neurological observation charts to monitor neurological status, predict patient outcomes, and detect changes early for timely treatment interventions.",
    [
      "anisocoria can be an indicator of a pathological process or neurological dysfunction. Investigators have used pupil size and reactivity as fundamental parameters of outcome predictive models in conjunction with other clinical information such as age, mechanism of injury, and Glasgow Coma Scale, and have correlated the models with the presence and location of intracranial mass lesions. The \"National Institutes of Health Stroke Scale\" (NIHSS) uses pupillary response as a systematic assessment tool to provide a quantitative measure of stroke-related neurologic deficit and to evaluate acuity of stroke patients, determine appropriate treatment, and predict patient outcome. Traditionally, pupil measurements have been",
      "Pupillometry Pupillometry, the measurement of pupil size and reactivity, is a key part of the clinical neurological exam for patients with a wide variety of neurological injuries. It is also used in psychology. For more than 100 years, clinicians have evaluated the pupils of patients with suspected or known brain injury or impaired consciousness to monitor neurological status and trends, checking for pupil size and reactivity to light. In fact, before the advent of electricity, doctors checked a patient\u2019s reaction to light using a candle. Today, clinicians routinely evaluate pupils as a component of the neurological examination and monitoring of",
      "pupil response. Automated pupillometry removes subjectivity from the pupillary evaluation, providing more accurate and trendable pupil data, and allowing earlier detection of changes for more timely patient treatment. A recent study published in the \"American Journal of Critical Care\" revealed that critical care and neurosurgical nurses consistently underestimated pupil size, were unable to identify anisocoria, and incorrectly assessed pupil reactivity. It concluded that automated pupillometry is a necessary tool for accuracy and consistency, and that it might facilitate earlier detection of subtle pupil changes, allowing more effective and timely diagnostic and treatment interventions. In addition, a study from The University"
    ]
  ],
  [
    "(Social workers in the community are commonly appointed as 'social supervisors' to patients who are subject to conditional discharge has been discharge under section 41 of Forensic social work)",
    [
      "to the social work role: \u2022 assessment; \u2022 care co-ordination; \u2022 report writing and presentation; \u2022 working with individuals and families; \u2022 working in collaboration with service users and carers; \u2022 undertaking social supervision with conditionally discharged patients and the supervision of those subject to supervision and in the case of those within forensic community teams, community treatment orders; \u2022 working with external agencies and multi-agency public protection arrangements (MAPPAs); \u2022 continuing professional development. Social workers in the community are commonly appointed as 'social supervisors' to patients who are subject to conditional discharge has been discharge under section 41 of",
      "and when necessary, arrange aftercare (under s.117 of the Mental Health Act 1983) and social assessments. Psychiatric Social Worker's are now called Mental Health Professionals, Mental Health Social Worker, of if trained, an Approved Mental Health Professional are often located within Community Mental Health Team, hospital or based in the local authority. The social worker fulfills the role of the Social Supervisor to specify suitable accommodations for discharged patients, and to assess risk. They provide specialist social care reports to the Mental Health First Tier Tribunal. The Department of Health in England currently identifies the following functions as being key",
      "community work and community detention. Supervision can be imposed for a period between 6 months and 1 year and requires the offender to report to a probation officer as and when directed. Reporting usually begins with once a week, but may be relaxed if the offender is compliant, to once a fortnight then once a month. The offender is required to notify the probation officer where they live and work and must obey any directives which prohibit \"association with specified persons\" such as co-offenders. Unlike 'supervision', which is mainly a monitoring process, intensive supervision is a sentence focused on rehabilitation."
    ]
  ],
  [
    "CT scan plays a crucial role in the preoperative evaluation of a symptomatic goitre by providing additional information regarding local extension, enlargement, calcifications, and preoperative surgical planning.",
    [
      "reported. A goiter is an abnormal thyroid gland proliferation that manifests as multi-nodular, uni-nodular, or non-nodular diffuse glandular enlargement. A goiter is formed of solid matrix, colloid cysts, blood products, calcification, and fibrosis, and this heterogeneity may lead to variable appearances on a CT scan (Figs. 13, ,1414 and and15)15). US is more sensitive in evaluating thyroid nodules within a goiter; however, a symptomatic goiter may require surgical treatment with total thyroidectomy, and in this case CT plays an additional role in preoperative evaluation. Specific aspects for examination on a CT scan during the preoperative evaluation for goiter include extension,",
      "enlargement, and calcifications. Management of ITNs depends on several factors including nodule size, patient\u2019s age, overall health status, and the presence or absence of suspicious features such as lymphadenopathy and/or invasion of adjacent structures. A CT scan provides additional important information regarding the local extension of cancer or presence of mass effect, and is useful in evaluating recurrent disease. Furthermore, CT examination plays a crucial role in preoperative evaluation and preoperative surgical planning for patients with symptomatic goiter. Computed tomography of the thyroid In CT scan of the thyroid, focal and diffuse thyroid abnormalities are commonly encountered. These findings can",
      "findings. In addition, the role of imaging in the assessment of thyroid carcinoma (before and after treatment) and preoperative thyroid goiter is explored, as well as localization of ectopic and congenital thyroid tissue. Thyroid ultrasonography is the modality of choice for thyroid evaluation. Yet, focal and diffuse thyroid abnormalities are commonly encountered during the interpretation of computed tomography (CT) exams performed for various clinical purposes. For example, CT often detects incidental thyroid nodules (ITNs). It plays an important role in the evaluation of thyroid cancer. Thyroid disorders are common and include many entities. They can be symptomatic, asymptomatic, diffuse, focal,"
    ]
  ],
  [
    "; the heart can be used for the body of an idol, covets, gives thanks, is susceptible to fear, dies, melts, takes in words, turns in repentance, becomes hot, and is like a stone.",
    [
      "of the previous day (because the previous day would have been less severe than the current day, and therefore they longed for its return). In the heart fears. A Midrash catalogued the wide range of additional capabilities of the heart reported in the Hebrew Bible. The heart speaks, sees, hears, walks, falls, stands, rejoices, cries, is comforted, is troubled, becomes hardened, grows faint, grieves, can be broken, becomes proud, rebels, invents, cavils, overflows, devises, desires, goes astray, lusts, is refreshed, can be stolen, is humbled, is enticed, errs, trembles, is awakened, loves, hates, envies, is searched, is rent, meditates, is",
      "the heart lusts. A Midrash catalogued the wide range of additional capabilities of the heart reported in the Hebrew Bible. The heart speaks, sees, hears, walks, falls, stands, rejoices, cries, is comforted, is troubled, becomes hardened, grows faint, grieves, fears, can be broken, becomes proud, rebels, invents, cavils, overflows, devises, desires, goes astray, is refreshed, can be stolen, is humbled, is enticed, errs, trembles, is awakened, loves, hates, envies, is searched, is rent, meditates, is like a fire, is like a stone, turns in repentance, becomes hot, dies, melts, takes in words, is susceptible to fear, gives thanks, covets, becomes",
      "used for the body of an idol \u2014 items that were merely \"among\" idols \u2014 were permitted to be used. In the heart cavils. A Midrash catalogued the wide range of additional capabilities of the heart reported in the Hebrew Bible. The heart speaks, sees, hears, walks, falls, stands, rejoices, cries, is comforted, is troubled, becomes hardened, grows faint, grieves, fears, can be broken, becomes proud, rebels, invents, overflows, devises, desires, goes astray, lusts, is refreshed, can be stolen, is humbled, is enticed, errs, trembles, is awakened, loves, hates, envies, is searched, is rent, meditates, is like a fire, is"
    ]
  ],
  [
    "Patients who are at risk of in-hospital deterioration and cardiac arrest should be assessed using an Early Warning Score (EWS). (Early warning score)",
    [
      "Early warning score An early warning score (EWS) is a guide used by medical services to quickly determine the degree of illness of a patient. It is based on the vital signs (respiratory rate, oxygen saturation, temperature, blood pressure, pulse/heart rate, AVPU response). Scores were developed in the late 1990s when studies showed that in-hospital deterioration and cardiac arrest was often preceded by a period of increasing abnormalities in the vital signs. The resulting observations are compared to a normal range to generate a single composite score, for instance based on the following diagram (an early modified EWS): A score",
      "The first recorded EWS was developed by a team in James Paget Hospital, Norfolk, United Kingdom, and presented at the May 1997 conference of the Intensive Care Society. Early warning score An early warning score (EWS) is a guide used by medical services to quickly determine the degree of illness of a patient. It is based on the vital signs (respiratory rate, oxygen saturation, temperature, blood pressure, pulse/heart rate, AVPU response). Scores were developed in the late 1990s when studies showed that in-hospital deterioration and cardiac arrest was often preceded by a period of increasing abnormalities in the vital signs.",
      "of five or more is statistically linked to increased likelihood of death or admission to an intensive care unit. Within hospitals, the EWS is used as part of a \"track-and-trigger\" system whereby an increasing score produces an escalated response varying from increasing the frequency of patient's observations (for a low score) up to urgent review by a rapid response or Medical Emergency Team (MET call). Concerns by nursing staff may also be used to trigger such call, as concerns may precede changes in vital signs. Throughout the world the EWS is based on the principle that clinical deterioration can be"
    ]
  ],
  [
    "Cuffs are used on tracheostomy tubes to secure the tube in place and minimize air leakage and the risk of aspiration (Tracheostomy tubes may have cuffs, inflatable balloons at the end of the tube to secure it in place).",
    [
      "given muscle relaxants before the insertion of the laryngeal mask airway, they may gag and aspirate when the drugs are worn off. At that point, the laryngeal mask airway should be removed immediately to eliminate the gag response and buy time to start at new alternative intubation technique. A tracheal tube is inserted into the trachea through the mouth or nose. Endotracheal tubes contain high-volume, low-pressure balloon cuffs to minimize air leakage and the risk of aspiration. Cuffed tubes were made originally for adults and children over 8 years old, but cuffed tubes have been used in infants and younger",
      "cannula remains in place but, because of the buildup of secretions, there is an inner cannula that may be removed for cleaning after use or it may be replaced. Tracheostomy tubes may have cuffs, inflatable balloons at the end of the tube to secure it in place. A tracheostomy tube may be fenestrated with one or several holes to let air through the larynx, allowing speech. Special tracheostomy tube valves (such as the Passy-Muir valve) have been created to assist people in their speech. The patient can inhale through the unidirectional tube. Upon expiration, pressure causes the valve to close,",
      "view the glottis). A tracheal tube is a catheter that is inserted into the trachea for the primary purpose of establishing and maintaining a patent (open and unobstructed) airway. Tracheal tubes are frequently used for airway management in the settings of general anesthesia, critical care, mechanical ventilation and emergency medicine. Many different types of tracheal tubes are available, suited for different specific applications. An endotracheal tube is a specific type of tracheal tube that is nearly always inserted through the mouth (orotracheal) or nose (nasotracheal). It is a breathing conduit designed to be placed into the airway of critically injured,"
    ]
  ],
  [
    "A peptide bond is an amide type of covalent chemical bond linking two consecutive alpha-amino acids along a peptide or protein chain, while an isopeptide bond is a different type of amide bond between two amino acids.",
    [
      "Peptide Peptides (from Gr.: \u03c0\u03b5\u03c0\u03c4\u03cc\u03c2, \"pept\u00f3s\" \"digested\"; derived from \u03c0\u03ad\u03c3\u03c3\u03b5\u03b9\u03bd, \"p\u00e9ssein\" \"to digest\") are short chains of amino acid monomers linked by peptide (amide) bonds. The covalent chemical bonds are formed when the carboxyl group of one amino acid reacts with the amino group of another. The shortest peptides are dipeptides, consisting of 2 amino acids joined by a single peptide bond, followed by tripeptides, tetrapeptides, etc. A polypeptide is a long, continuous, and unbranched peptide chain. Hence, peptides fall under the broad chemical classes of biological oligomers and polymers, alongside nucleic acids, oligosaccharides and polysaccharides, etc. Peptides are distinguished",
      "Peptide bond A peptide bond is an amide type of covalent chemical bond linking two consecutive alpha-amino acids from C1 (carbon number one) of one alpha-amino acid and N2 (nitrogen number two) of another along a peptide or protein chain. It can also be called an eupeptide bond to separate it from an isopeptide bond, a different type of amide bond between two amino acids. When two amino acids form a \"dipeptide\" through a \"peptide bond\" it is type of condensation reaction. In this kind of condensation, two amino acids approach each other, with the non-side chain (C1) carboxylic acid",
      "less than similar compounds such as esters. Nevertheless, peptide bonds can undergo chemical reactions, usually through an attack of an electronegative atom on the carbonyl carbon, breaking the carbonyl double bond and forming a tetrahedral intermediate. This is the pathway followed in proteolysis and, more generally, in N-O acyl exchange reactions such as those of inteins. When the functional group attacking the peptide bond is a thiol, hydroxyl or amine, the resulting molecule may be called a cyclol or, more specifically, a thiacyclol, an oxacyclol or an azacyclol, respectively. Peptide bond A peptide bond is an amide type of covalent"
    ]
  ],
  [
    "The four practical strategies for estimating test reliability are the test-retest reliability method, inter-rater reliability, internal consistency reliability, and item response theory.",
    [
      "high- and low-scoring test-takers. Item response theory extends the concept of reliability from a single index to a function called the \"information function\". The IRT information function is the inverse of the conditional observed score standard error at any given test score. The goal of estimating reliability is to determine how much of the variability in test scores is due to errors in measurement and how much is due to variability in true scores. Four practical strategies have been developed that provide workable methods of estimating test reliability. 1. Test-retest reliability method: directly assesses the degree to which test scores",
      "specific ways. (A) Inter-Rater reliability: Inter-Rater reliability is estimate of agreement between independent raters. This is most useful for subjective responses. Cohen\u2019s Kappa, Krippendorff\u2019s Alpha, Intra-Class correlation coefficients, Correlation coefficients, Kendal\u2019s concordance coefficient, etc. are useful statistical tools. (B) Test-Retest Reliability: Test-Retest Procedure is estimation of temporal consistency of the test. A test is administered twice to the same sample with a time interval. Correlation between two sets of scores is used as an estimate of reliability. Testing conditions are assumed to be identical. (C) Internal Consistency Reliability: Internal consistency reliability estimates consistency of items with each other. Split-half reliability",
      "time. It is generally simple to administer. Its assessment procedure should be particular and time-efficient. The principle of reliability refers to the stability of scores over time and different raters. There are four types of reliability: student-related which can be personal problems, sickness, or fatigue, rater-related which includes bias and subjectivity, test administration-related which is the conditions of test taking process, test-related which is basically related to the nature of a test. Validity refers to the tests that measure what it claims to measure. The assessment instrument is authentic when it is contextualized, contains natural language and meaningful, relevant, and"
    ]
  ],
  [
    "Gene doping is not known to occur among athletes, but there are concerns about the potential use of gene therapy to enhance performance in sporting events (\"Cheating at the Paralympic Games; unusual to the bloodstream\", \"Types of Gene Therapy\", \"Designer baby\").",
    [
      "events. Another form of doping is \"boosting\", used by athletes with a spinal cord injury to induce autonomic dysreflexia and increase blood pressure. This was banned by the IPC in 1994 but is still an ongoing problem in the sport. Another potential concern is the use of gene therapy among Paralympic athletes. All Paralympic athletes are banned from enhancing their abilities through gene doping, but it is extremely difficult to differentiate these concepts. The World Anti-Doping Agency is currently researching both gene doping and gene therapy, in part to discern the boundary between the two closely related concepts. Having sent",
      "unusual to the bloodstream, so athletic officials would be unable to detect chemicals in a blood or urine test. An example of gene doping would be proving athlete with erythropoietin (EPO), a hormone that increases the red blood cell count. Lastly, viral vectors are able to mimic the methods of a normal virus in the human body to introduce favorable genes into a human cell. For instance, scientists are able to positively change the host's genome by removing the genes that cause disease from a virus and replacing it with genes of the desired trait (\u201cTypes of Gene Therapy\u201d). The",
      "the treatment of blindness due to Leber's congenital amaurosis. The price of this treatment was 850,000 US dollars for both eyes. Speculated uses for gene therapy include: Athletes might adopt gene therapy technologies to improve their performance. Gene doping is not known to occur, but multiple gene therapies may have such effects. Kayser et al. argue that gene doping could level the playing field if all athletes receive equal access. Critics claim that any therapeutic intervention for non-therapeutic/enhancement purposes compromises the ethical foundations of medicine and sports. Genetic engineering could be used to cure diseases, but also to change physical"
    ]
  ],
  [
    "Three of the five main uses of the human hand include grasping objects, providing tactile feedback through nerve endings in the fingers, and forming a compact fist for fighting purposes.",
    [
      "the human hand include: There are five digits attached to the hand, notably with a nail fixed to the end in place of the normal claw. The four fingers can be folded over the palm which allows the grasping of objects. Each finger, starting with the one closest to the thumb, has a colloquial name to distinguish it from the others: The thumb (connected to the first metacarpal bone and trapezium) is located on one of the sides, parallel to the arm. A reliable way of identifying human hands is from the presence of opposable thumbs. Opposable thumbs are identified",
      "hand has five metacarpals and eight carpal bones. Fingers contain some of the densest areas of nerve endings in the body, and are the richest source of tactile feedback. They also have the greatest positioning capability of the body; thus, the sense of touch is intimately associated with hands. Like other paired organs (eyes, feet, legs) each hand is dominantly controlled by the opposing brain hemisphere, so that handedness\u2014the preferred hand choice for single-handed activities such as writing with a pencil, reflects individual brain functioning. Among humans, the hands play an important function in body language and sign language. Likewise",
      "formation of a compact fist, presumably for fighting purposes. The fist is compact and thus effective as a weapon. It also provides protection for the fingers. However, this is not widely accepted to be one of the primary selective pressures acting on hand morphology throughout human evolution, with tool use and production being thought to be far more influential. Hand A hand is a prehensile, multi-fingered appendage located at the end of the forearm or forelimb of primates such as humans, chimpanzees, monkeys, and lemurs. A few other vertebrates such as the koala (which has two opposable thumbs on each"
    ]
  ],
  [
    "can be caused by a variety of factors, including diet, irritable bowel syndrome, lactose intolerance, reflux, constipation, Crohn's disease, bowel obstruction, ascites, tumors, intestinal parasites, infections, and certain foods.",
    [
      "with it is the hiccup. Hiccups are harmless and will diminish on their own; they also help to release gas that is in the digestive tract before it moves down to the intestines and causes bloating. Important but uncommon causes of abdominal bloating include ascites and tumors. There are many causes of bloating, including: diet, irritable bowel syndrome, lactose intolerance, reflux, and constipation. Specific medical conditions like Crohn's disease or bowel obstruction can also contribute to the amount of stomach bloating experienced. Common causes of abdominal bloating are: Important but uncommon causes of abdominal bloating include: In animals, causes of",
      "take a careful medical history. Here are the most common causes of abdominal distension classified as an underlying cause and as a secondary disease. As an underlying disease cause: As a secondary disease cause: Bloating is not life-threatening. In most cases, bloating can be handled with simple home remedies and changes in lifestyle. Persistent or recurrent bloating may be caused by intestinal parasites, other infections, or other medical conditions. Certain foods have been known to worsen bloating. Poorly digested components of many foods are excreted into the large intestine where they are degraded by bacteria, producing excess gas. Depending on",
      "causes swelling or bloating in the abdominal area before it is released. A common gastrointestinal problem is constipation\u2014infrequent bowel movements, hard stools, or strain during the movements\u2014which causes serious cases of bloating. Since most cases of constipation are temporary, simple lifestyle changes, such as getting more exercise and increasing one's intake of fiber, can go a long way toward alleviating constipation. Some cases of constipation will continue to worsen and require unconventional methods to release the feces and reduce the amount of stomach bloating. Blood in the stool, intense pain in the abdomen, rectal pain, and unexplained weight loss should"
    ]
  ],
  [
    "The Pediatric Glasgow Coma Scale is used to assess the level of consciousness of child patients.",
    [
      "the scale is now used by first aid, EMS and doctors as being applicable to all acute medical and trauma patients. In hospital it is also used in chronic patient monitoring, in for instance, intensive care. The Pediatric Glasgow Coma Scale (also known as Pediatric Glasgow Coma Score or simply PGCS) is the equivalent of the Glasgow Coma Scale (GCS) used to assess the mental state of adult patients. As many of the assessments for an adult patient would not be appropriate for infants, the scale was modified slightly. As with the GCS, the PGCS comprises three tests: eye, verbal",
      "Coma scale A coma scale is a system to assess the severity of coma. There are several such systems: The Glasgow Coma Scale is neurological scale which aims to give a reliable, objective way of recording the conscious state of a person, for initial as well as continuing assessment. A patient is assessed against the criteria of the scale, and the resulting points give a patient score between 3 (indicating deep unconsciousness) and either 14 (original scale) or 15 (the more widely used modified or revised scale). GCS was initially used to assess level of consciousness after head injury and",
      "coma or death) whilst the highest is 15 (fully awake and aware person). The pediatric GCS is commonly used in emergency medical services. Source: Any combined score of less than eight represents a significant risk of mortality. Paediatric Glasgow Coma Scale The Paediatric Glasgow Coma Scale (BrE) (also known as Pediatric Glasgow Coma Score (AmE) or simply PGCS) is the equivalent of the Glasgow Coma Scale (GCS) used to assess the level of consciousness of child patients. As many of the assessments for an adult patient would not be appropriate for infants, the Glascow Coma Scale was modified slightly to"
    ]
  ],
  [
    "The combination of signs of left ventricular dysfunction with ejection fraction less than 60%, severe pulmonary hypertension with pulmonary artery systolic pressure greater than 50 mmHg at rest or 60 mmHg during activity, and new onset atrial fibrillation is most suggestive of mixed mitral valve disease with a predominance of mitral regurgitation. (Significant mitral valve regurgitation has a prevalence of approximately 2% of the population, affecting males and females equally.)",
    [
      "include signs of left ventricular dysfunction with ejection fraction less than 60%, severe pulmonary hypertension with pulmonary artery systolic pressure greater than 50 mmHg at rest or 60 mmHg during activity, and new onset atrial fibrillation. Significant mitral valve regurgitation has a prevalence of approximately 2% of the population, affecting males and females equally. It is one of the two most common valvular heart diseases in the elderly. Mitral insufficiency Mitral regurgitation (MR), Mitral insufficiency (abbreviation MI best avoided because it is more commonly used in lieu of myocardial infarction), or mitral incompetence is a disorder of the heart in",
      "involve the cardiovascular system: undue fatigue, shortness of breath, heart palpitations, racing heartbeats, or chest pain radiating to the back, shoulder, or arm. Cold arms, hands, and feet can also be linked to MFS because of inadequate circulation. A heart murmur, abnormal reading on an ECG, or symptoms of angina can indicate further investigation. The signs of regurgitation from prolapse of the mitral or aortic valves (which control the flow of blood through the heart) result from cystic medial degeneration of the valves, which is commonly associated with MFS (see mitral valve prolapse, aortic regurgitation). However, the major sign that",
      "patients failed to demonstrate classic vegetations. All had rheumatic heart disease (RHD) and presented with prolonged fever. All had severe eccentric mitral regurgitation (MR). (One had severe aortic regurgitation (AR) also.) One had flail posterior mitral leaflet (PML). The mitral valve, so named because of its resemblance to a bishop's mitre, is the heart valve that prevents the backflow of blood from the left ventricle into the left atrium of the heart. It is composed of two leaflets, one anterior and one posterior, that close when the left ventricle contracts. Each leaflet is composed of three layers of tissue: the"
    ]
  ],
  [
    "safety measures should be taken before a patient undergoes anesthesia for a surgical procedure, including confirming patient identity, site of operation, procedure consent, allergies, blood loss discussion, completion of an anaesthetic safety check, and performing a 'time out' with the entire team before the first incision. (WHO Surgical Safety Checklist; Joint Commission two identifiers, medication allergy and reconciliation, site markings requirements, Anesthesiology fasting, oxygen, and airway status, transfusion and fluid requirements, and equipment availability; Medicare Core Measure SCIP 1a; Joint Commission Universal Protocol)",
    [
      "patients. It is important to increase awareness of this safety initiative. Before the patient undergoes anaesthesia, the patient must confirm his/her identity, the site of operation, what procedure is to be carried out and that he/she has consented for the procedure. The site of operation must be marked if applicable. Then the presence or absence of allergies must be checked, the amount of expected blood loss discussed, and an anaesthetic safety check must be completed. Before the surgical procedure begins (i.e. before the first incision), the entire team must take a 'time out' (stop and pause). At this stage, all",
      "peri-operative time periods for surgical patients: From the moment that the patient checks until they are wheeled into the operative suite, the following safety checks are verified: Joint Commission two identifiers, medication allergy and reconciliation, site markings requirements, Anesthesiology fasting, oxygen, and airway status, transfusion and fluid requirements, and equipment availability before taking the patient into the operative suite. Upon entering the operative suite the following quality and patient safety tasks are performed: Medicare Core Measure SCIP 1a, and Joint Commission Universal Protocol before surgical incision. Before closure of the operative wound, the following tasks should be completed: Yield of",
      "use of fail-safe systems that decrease the odds of catastrophic misuse of the machine. Patients under general anesthesia must undergo continuous physiological monitoring to ensure safety. In the US, the American Society of Anesthesiologists (ASA) has established minimum monitoring guidelines for patients receiving general anesthesia, regional anesthesia, or sedation. These include electrocardiography (ECG), heart rate, blood pressure, inspired and expired gases, oxygen saturation of the blood (pulse oximetry), and temperature. In the UK the Association of Anaesthetists (AAGBI) have set minimum monitoring guidelines for general and regional anesthesia. For minor surgery, this generally includes monitoring of heart rate, oxygen saturation,"
    ]
  ],
  [
    "The net products formed when one molecule of glucose is converted into two molecules of pyruvate are two molecules of pyruvate and two molecules of ATP (origin text referenced: glucose degrading into two molecules of pyruvate, through various steps, with the help of different enzymes).",
    [
      "glucose degrading into two molecules of pyruvate, through various steps, with the help of different enzymes. It occurs in ten steps and proves that phosphorylation is a much required and necessary step to attain the end products. Phosphorylation initiates the reaction in step 1 of the preparatory step (first half of glycolysis), and initiates step 6 of payoff phase (second phase of glycolysis). Glucose, by nature, is a small molecule with the ability to diffuse in and out of the cell. By phosphorylating glucose (adding a negatively charged phosphate group), glucose is converted to glucose-6-phosphate and trapped within the cell",
      "it into the energy rich molecule pyruvate. Inside the overall reaction there lie many steps that need to be followed in order for the original glucose molecule to be transformed into pyruvate. The glucose first gathers a phosphate group from an ATP molecule in order to become glucose-6-phosphate. It is then changed into fructose 6-phosphate, with the assistance of phosphoglucose isomerase, which is then changed into fructose 1,6-biphosphate when the fructose molecule receives a phosphate group from another ATP. The next step in the chain is crucial for cells in order to make more energy than they expend through the",
      "potassium hydrogen sulfate, by the oxidation of propylene glycol by a strong oxidizer (e.g., potassium permanganate or bleach), or by the hydrolysis of acetyl cyanide, formed by reaction of acetyl chloride with potassium cyanide: Pyruvate is an important chemical compound in biochemistry. It is the output of the metabolism of glucose known as glycolysis. One molecule of glucose breaks down into two molecules of pyruvate, which are then used to provide further energy, in one of two ways. Pyruvate is converted into acetyl-coenzyme A, which is the main input for a series of reactions known as the Krebs cycle (also"
    ]
  ],
  [
    "In the pay-off phase of glycolysis, 2 ATP molecules are produced via substrate-level phosphorylation.",
    [
      "substrate-level phosphorylation. The second substrate-level phosphorylation occurs by dephosphorylating phosphoenolpyruvate, catalyzed by pyruvate kinase, producing pyruvate and ATP. During the preparatory phase, each 6-carbon glucose molecule is broken into two 3-carbon molecules. Thus, in glycolysis dephosphorylation results in the production of 4 ATP. However, the prior preparatory phase consumes 2 ATP, so the net yield in glycolysis is 2 ATP. 2 molecules of NADH are also produced and can be used in oxidative phosphorylation to generate more ATP. ATP can be generated by substrate-level phosphorylation in mitochondria in a pathway that is independent from the proton motive force. In the",
      "The amount of energy released by oxidative phosphorylation is high, compared with the amount produced by anaerobic fermentation. Glycolysis produces only 2 ATP molecules, but somewhere between 30 and 36 ATPs are produced by the oxidative phosphorylation of the 10 NADH and 2 succinate molecules made by converting one molecule of glucose to carbon dioxide and water, while each cycle of beta oxidation of a fatty acid yields about 14 ATPs. These ATP yields are theoretical maximum values; in practice, some protons leak across the membrane, lowering the yield of ATP. The electron transport chain carries both protons and electrons,",
      "are: two GTP, six NADH, two QH, and four CO. The above reactions are balanced if P represents the HPO ion, ADP and GDP the ADP and GDP ions, respectively, and ATP and GTP the ATP and GTP ions, respectively. The total number of ATP molecules obtained after complete oxidation of one glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is estimated to be between 30 and 38. The theoretical maximum yield of ATP through oxidation of one molecule of glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is 38 (assuming 3 molar equivalents of ATP per equivalent"
    ]
  ],
  [
    "Some potential causes of rectal bleeding include anal fissures, hemorrhoids, anal sex, ulcerative colitis, enema administration, and sexually transmitted infections such as gonorrhea or chlamydia.",
    [
      "other cases, the formation of a hemorrhoid is attributed to anal sex. If bleeding occurs as a result of anal sex, it may also be because of a tear in the anal or rectal tissues (an anal fissure) or perforation (a hole) in the colon, the latter of which being a serious medical issue that should be remedied by immediate medical attention. Because of the rectum's lack of elasticity, the anal mucous membrane being thin, and small blood vessels being present directly beneath the mucous membrane, tiny tears and bleeding in the rectum usually result from penetrative anal sex, though",
      "and limiting the amount that is absorbed into the bloodstream. Rectal corticosteroid enemas are sometimes used to treat mild or moderate ulcerative colitis. They also may be used along with systemic (oral or injection) corticosteroids or other medicines to treat severe disease or mild to moderate disease that has spread too far to be treated effectively by medicine inserted into the rectum alone. Improper administration of an enema can cause electrolyte imbalance (with repeated enemas) or ruptures to the bowel or rectal tissues resulting in internal bleeding. However, these occurrences are rare in healthy, sober adults. Internal bleeding or rupture",
      "to empty the bowels, diarrhea, rectal bleeding and possible discharge, a feeling of not having adequately emptied the bowels, involuntary spasms and cramping during bowel movements, left-sided abdominal pain, passage of mucus through the rectum, and anorectal pain. Gonorrhea Chlamydia Herpes Simplex Virus 1 and 2 Syphilis Proctitis has many possible causes. It may occur idiopathically (idiopathic proctitis, that is, arising spontaneously or from an unknown cause). Other causes include damage by irradiation (for example in radiation therapy for cervical cancer and prostate cancer) or as a sexually transmitted infection, as in lymphogranuloma venereum and herpes proctitis. Studies suggest a"
    ]
  ],
  [
    "Mutations impact the phenotype of an organism by introducing genetic variation, allowing for adaptation to a changing environment, and driving evolution through natural selection by selecting for advantageous traits that increase reproductive success.",
    [
      "are selected to survive and reproduce more offspring. Natural selection selects for the phenotype or the characteristics of an organism that gives the organism a reproductive advantage in which it becomes the gene pool of a population. In addition, mutations also arise in the genome of an individual organism and offspring(s) can inherit such mutations. This genetic variation allows more organisms to adapt to a changing environment. It is often the case in the study of phylogenies that the vast majority of organisms of interest are long extinct. It is therefore a matter of speculation to reconstruct what ancestral organisms",
      "acting upon heritable variation caused by genetic mutations. However, natural selection acts on phenotypes and mutation does not in itself produce phenotypic variation, thus, there is a conceptual gap regarding the connection between a mutation and the potential change in phenotype. For a mutation to readily alter a phenotype, and hence be visible to natural selection, it has to modify the ontogenetic trajectory, a process referred to as \"developmental reprogramming\". Some kinds of reprogramming are more likely to occur than others given the nature of the genotype-phenotype map, which determines the propensity of a system to vary in a particular",
      "gene. These changes in DNA sequence are called mutations. Mutations produce new alleles of genes. Sometimes these changes stop the functioning of that gene or make it serve another advantageous function, such as the melanin genes discussed above. These mutations and their effects on the traits of organisms are one of the causes of evolution. A population of organisms evolves when an inherited trait becomes more common or less common over time. For instance, all the mice living on an island would be a single population of mice: some with white fur, some gray. If over generations, white mice became"
    ]
  ],
  [
    "The highest of three readings is used as the recorded value of the Peak Expiratory Flow Rate, which is a predictor of mortality and poor prognosis in air quality studies and asthma research.",
    [
      "trended over time in both rural and metropolitan areas both as air quality studies and as studies on asthma due to the Peak Flow measurement's accuracy as a predictor of mortality and poor prognosis. Measurements may be based on 1 second or less but are usually reported as a volume per minute. Electronic devices will sample the flow and multiply the sample volume(Litres) by 60, divided by the sample time(seconds) for a result measured in L/minute : formula_4 The highest of three readings is used as the recorded value of the Peak Expiratory Flow Rate. It may be plotted out",
      "particular individual. More recently, medical calculators have been developed to calculate predicted values for peak expiratory flow. There are a number of non-equivalent scales used in the interpretation of peak expiratory flow. Some examples of Reference Values are given below. There is a wide natural variation in results from healthy test subjects. In 2004 the UK switched from the original Wright scale to the newer, more accurate European scale. Wright values may be converted to the EU scale using the following formula: The reverse calculation is: Where formula_3 is the value in the Wright scale. These formulas have also been",
      "the late 1950s, and the subsequent development of a more portable, lower cost version (the \"Mini-Wright\" peak flow meter), other designs and copies have become available across the world. Peak expiratory flow The peak expiratory flow (PEF), also called peak expiratory flow rate (PEFR) is a person's maximum speed of expiration, as measured with a peak flow meter, a small, hand-held device used to monitor a person's ability to breathe out air. It measures the airflow through the bronchi and thus the degree of obstruction in the airways. Peak expiratory flow is typically measured in units of liters per minute"
    ]
  ],
  [
    "The highest flow recorded at the dam site was on January 22, 1959.",
    [
      "be in the hazy area...It is not known...whether a structure of the dam may give and...people downstream should be advised.\" At this time, the flow sensor in the dam recorded its record discharge of , and the stream height gauge, at the dam's downstream side, registered a record . On January 20, 1996, the gauge recorded its second-highest recorded crest of . A severe ice jam also developed behind the dam on this date. The record minimum recorded discharge was on March 2, 1969, when the flow sensor registered . On September 9, 2011, 44 flood gates were opened due",
      "conduits, in front of the gates and discharges into the middle conduit below the gate. Invert elevation siphon 927.25.<br>Other Structures: None.<br>Maximum flow of record at the dam site: (01-22-1959)<br>Reservoir design flood peak flow: The normal pool level of the lake is , at which a reservoir is formed. During times of excessive rain and snow melt, the corps of engineers can impound more water, up to a maximum possible level of with . The highest pool of record is on March 22, 2008. Levels above normal can cause road closures near the lake. In November of each year the lake",
      "human intervention, but discharge at the mouth is chiefly controlled by New Exchequer Dam. The Lake McClure gauge, located at the former mining town of Bagby, is probably the most accurate gauge for flows overall. The average annual flow recorded there was from 1923 to 1966. A peak of was reported there on December 23, 1955. For the mouth gauge, the highest flow was only in 1950. Finally, for the gauge at Happy Isles, the largest flow ever recorded was in the 1997 Yosemite floods, which destroyed many campgrounds, roads, paths, and bridges in the valley. According to a study"
    ]
  ],
  [
    "Risk factors for toxicity when taking paracetamol include alcoholism, malnutrition, and the taking of certain other medications.",
    [
      "survived, the liver necrosis runs its course, and liver and kidney function typically return to normal in a few weeks. The severity of paracetamol toxicity varies depending on the dose and whether appropriate treatment is received. The toxic dose of paracetamol is highly variable. In general the recommended maximum daily dose for healthy adults is 4 grams. Higher doses lead to increasing risk of toxicity. In adults, single doses above 10 grams or 200 mg/kg of bodyweight, whichever is lower, have a reasonable likelihood of causing toxicity. Toxicity can also occur when multiple smaller doses within 24 hours exceed these",
      "some cases will resolve while others will result in death. Paracetamol poisoning can occur accidentally or as an attempt to end one's life. Risk factors for toxicity include alcoholism, malnutrition, and the taking of certain other medications. Liver damage results not from paracetamol itself, but from one of its metabolites, \"N\"-acetyl-\"p\"-benzoquinone imine (NAPQI). NAPQI decreases the liver's glutathione and directly damages cells in the liver. Diagnosis is based on the blood level of paracetamol at specific times after the medication was taken. These values are often plotted on the Rumack-Matthew nomogram to determine level of concern. Treatment may include activated",
      "factors can potentially increase the risk of developing paracetamol toxicity. Chronic excessive alcohol consumption can induce CYP2E1, thus increasing the potential toxicity of paracetamol. In one study of patients with liver injury, 64% reported alcohol intakes of greater than 80 grams a day, while 35% took 60 grams a day or less. Whether chronic alcoholism should be considered a risk factor has been debated by some clinical toxicologists. For chronic alcohol users, acute alcohol ingestion at the time of a paracetamol overdose may have a protective effect. For non-chronic alcohol users, acute alcohol consumption had no protective effect. Fasting is"
    ]
  ],
  [
    "Circular double-stranded nucleic acid molecules are usually DNA molecules.",
    [
      "Nucleic acid molecules are usually unbranched, and may occur as linear and circular molecules. For example, bacterial chromosomes, plasmids, mitochondrial DNA, and chloroplast DNA are usually circular double-stranded DNA molecules, while chromosomes of the eukaryotic nucleus are usually linear double-stranded DNA molecules. Most RNA molecules are linear, single-stranded molecules, but both circular and branched molecules can result from RNA splicing reactions. The total amount of pyrimidine is equal to the total amount of purines. The diameter of the helix is about 20A. One DNA or RNA molecule differs from another primarily in the sequence of nucleotides. Nucleotide sequences are of",
      "molecules are probably the largest individual molecules known. Well-studied biological nucleic acid molecules range in size from 21 nucleotides (small interfering RNA) to large chromosomes (human chromosome 1 is a single molecule that contains 247 million base pairs). In most cases, naturally occurring DNA molecules are double-stranded and RNA molecules are single-stranded. There are numerous exceptions, however\u2014some viruses have genomes made of double-stranded RNA and other viruses have single-stranded DNA genomes, and, in some circumstances, nucleic acid structures with three or four strands can form. Nucleic acids are linear polymers (chains) of nucleotides. Each nucleotide consists of three components: a",
      "sites on sugar molecules in adjacent nucleotides. In a double helix, the two strands are oriented in opposite directions, which permits base pairing and complementarity between the base-pairs, all which is essential for replicating or transcribing the encoded information found in DNA. Unlike in nucleic acid nucleotides, singular cyclic nucleotides are formed when the phosphate group is bound twice to the same sugar molecule, i.e., at the corners of the sugar hydroxyl groups. These individual nucleotides function in cell metabolism rather than the nucleic acid structures of long-chain molecules. Nucleic acids then are polymeric macromolecules assembled from nucleotides, the monomer-units"
    ]
  ],
  [
    "The main sign of jaundice is a yellowish discoloration of the white area of the eye and the skin, and it is best detected by examining the sclerae.",
    [
      "antibodies most commonly directed this type of reaction. This process is much more controlled when compared with intravascular haemolysis. Liver has adequate capacity in processing the bilirubin. Therefore, jaundice rarely occurs. Early signs are fever, low blood pressure, anxiety, and red-colored urine. Late signs are generalized bleeding, caused by disseminated intravascular coagulation, and low blood pressure. Laboratory assessment is based on a positive Direct Antiglobulin Test (DAT), a decrease in serum haptoglobin, and an increase in blood levels of the enzyme lactate dehydrogenase and indirect bilirubin levels. This may be caused by preformed IgM anti-A, anti-B or both. It may",
      "functional integrity is damaged. This allows the escape of conjugated bilirubin into the circulation as occurs in hepatitis and hepatic cirrhosis). The detection of urinary bilirubin is an early indication of liver disease and its presence or absence can be used to determine the causes of clinical jaundice. The jaundice produced by the accelerated destruction of red blood cells does not produce bilirubinuria, as the high serum bilirubin is found in the unconjugated form and the kidneys are unable to excrete it. The test strips use a diazotization reaction in order to detect bilirubin. The bilirubin combines with a diazonium",
      "management may involve treating infectious causes and stopping medication that could be contributing. Among newborns, depending on age and prematurity, a bilirubin greater than 4\u201321 mg/dL (68-360 \u00b5mol/L) may be treated with phototherapy or exchanged transfusion. The itchiness may be helped by draining the gallbladder or ursodeoxycholic acid. The word \"jaundice\" is from the French \"jaunisse\", meaning \"yellow disease\". The main sign of jaundice is a yellowish discoloration of the white area of the eye and the skin. Urine is dark in colour. Slight increases in serum bilirubin are best detected by examining the sclerae, which have a particular affinity"
    ]
  ],
  [
    "Thymidine is not found in RNA.",
    [
      "present: CpG islands in DNA (are often methylated), all eukaryotic mRNA (capped with a methyl-7-guanosine), and several bases of rRNAs (are methylated). Often, tRNAs are heavily modified postranscriptionally in order to improve their conformation or base pairing, in particular in/near the anticodon: inosine can base pair with C, U, and even with A, whereas thiouridine (with A) is more specific than uracil (with a purine). Other common tRNA base modifications are pseudouridine (which gives its name to the T\u03a8C loop), dihydrouridine (which does not stack as it is not aromatic), queuosine, wyosine, and so forth. Nevertheless, these are all modifications",
      "cytosine, adenine, and uridine. Another commonly modified base in tRNA is the position adjacent to the anticodon. Position 37 is often hypermodified with bulky chemical modifications. These modifications prevent frameshifting and increase anticodon-codon binding stability through stacking interactions. Ribosomal RNA modifications are made throughout the ribosome synthesis. Modifications primarily play a role in the structure of the rRNA in order to protect translational efficiency. There are over 160 RNA modifications identified. Chemical modifications can range from simple methylations (m6A) to hypermodifications (i6A) that require several steps for synthesis. Hypermodified bases are primarily seen at position 34 and 37 of the",
      "defined by containing ribose nucleic acid. In some occasions, DNA and RNA may contain some minor bases. Methylated forms of the major bases are most common in DNA. In viral DNA, some bases may be hydroxymethylated or glucosylated. In RNA, minor or modified bases occur more frequently. Some examples include hypoxanthine, dihydrouracil, methylated forms of uracil, cytosine, and guanine, as well as modified nucleoside pseudouridine. Nucleotides with phosphate groups in positions other than on the 5' carbon have also been observed. Examples include ribonucleoside 2',3'-cyclic monophosphates which are isolatable intermediates, and ribonucleoside 3'-monophosphates which are end products of the hydrolysis"
    ]
  ],
  [
    "Bronchial breathing is characterized by recurrent episodes of wheezing, shortness of breath, chest tightness, and coughing, while cardiac asthma is a condition where constriction of airways occurs due to heart failure rather than bronchoconstriction (Asthma, Bronchiectasis).",
    [
      "Asthma is a common condition and affects over 300 million people around the world. Asthma causes recurring episodes of wheezing, breathlessness, chest tightness, and coughing, particularly at night or in the early morning. A peak flow meter can record variations in the severity of asthma over time. Spirometry, a measurement of lung function, can provide an assessment of the severity, reversibility, and variability of airflow limitation, and help confirm the diagnosis of asthma. Bronchiectasis refers to the abnormal, irreversible dilatation of the bronchi caused by destructive and inflammatory changes in the airway walls. Bronchiectasis has three major anatomical patterns: cylindrical",
      "subjects, the constriction does not return to normal, and recurs after three to four hours, which may last up to a day or more. The first is named the \"early asthmatic response\", and the latter the \"late asthmatic response\". Bronchioconstriction can occur as a result of anaphylaxis, even when the allergen is not inhaled. Bronchoconstriction is defined as the narrowing of the airways in the lungs (bronchi and bronchioles). Air flow in air passages can get restricted in three ways: The bronchial spasm is due to the activation of parasympathetic nervous system. Postganglionic parasympathetic fibers will release acetylcholine causing the",
      "which means \"panting\". Asthma is characterized by recurrent episodes of wheezing, shortness of breath, chest tightness, and coughing. Sputum may be produced from the lung by coughing but is often hard to bring up. During recovery from an attack, it may appear pus-like due to high levels of white blood cells called eosinophils. Symptoms are usually worse at night and in the early morning or in response to exercise or cold air. Some people with asthma rarely experience symptoms, usually in response to triggers, whereas others may have marked and persistent symptoms. A number of other health conditions occur more"
    ]
  ],
  [
    "Dopaminergic drugs are commonly responsible for patients presenting with acute confusion.",
    [
      "can result from chronic organic brain pathologies, such as dementia, as well. The most common causes of drug induced acute confusion are dopaminergic drugs (used for the treatment of Parkinson's disease), diuretics, tricyclic or tetracyclic antidepressants and benzodiazepines. The elderly, and especially those with pre-existing dementia, are most at risk for drug induced acute confusional states. New research is finding a link between Vitamin D deficiency and cognitive impairment (which includes 'foggy brain'). Confusion Confusion (from Latin \"confus\u012do, -\u014dnis\", from \"confundere\": \"to pour together;\" \"to mingle together;\" \"to confuse\") is the state of being bewildered or unclear in one\u2019s mind",
      "Confusion Confusion (from Latin \"confus\u012do, -\u014dnis\", from \"confundere\": \"to pour together;\" \"to mingle together;\" \"to confuse\") is the state of being bewildered or unclear in one\u2019s mind about something. It is used both casually and in medical terminology. The diagnostic use of \"confusion\" can be linked to a wide range of causes including side effects from drugs, mental illness and much more. The term \"acute mental confusion\" is often used interchangeably with delirium in the \"International Statistical Classification of Diseases and Related Health Problems\" and the \"Medical Subject Headings\" publications to describe the pathology. These refer to the loss of",
      "orientation, or the ability to place oneself correctly in the world by time, location and personal identity. Mental confusion is sometimes accompanied by disordered consciousness (the loss of linear thinking) and memory loss (the ability to correctly recall previous events or learn new material). Confusion may result from drug side effects or from a relatively sudden brain dysfunction. Acute confusion is often called delirium (or \"acute confusional state\"), although delirium often includes a much broader array of disorders than simple confusion. These disorders include the inability to focus attention; various impairments in awareness; and temporal or spatial dis-orientation. Mental confusion"
    ]
  ],
  [
    "The functions associated with the frontal lobe of the brain's cortex include controlling attention, abstract thinking, behavior, problem-solving tasks, physical reactions, and personality (Frontal lobe).",
    [
      "input from the sensory areas and lower parts of the brain and are involved in the complex cognitive processes of perception, thought, and decision-making. The main functions of the frontal lobe are to control attention, abstract thinking, behaviour, problem solving tasks, and physical reactions and personality. The occipital lobe is the smallest lobe; its main functions are visual reception, visual-spatial processing, movement, and colour recognition. There is a smaller occipital lobule in the lobe known as the cuneus. The temporal lobe controls auditory and visual memories, language, and some hearing and speech. The cerebrum contains the ventricles where the cerebrospinal",
      "into three parts \u2013 the orbital part, the triangular part, and the opercular part. The frontal lobe contains most of the dopamine neurons in the cerebral cortex. The dopaminergic pathways are associated with reward, attention, short-term memory tasks, planning, and motivation. Dopamine tends to limit and select sensory information arriving from the thalamus to the forebrain. On the lateral surface of the human brain, the central sulcus separates the frontal lobe from the parietal lobe. The lateral sulcus separates the frontal lobe from the temporal lobe. The frontal lobe can be divided into a lateral, polar, orbital (above the orbit;",
      "like walking. The function of the frontal lobe involves the ability to project future consequences resulting from current actions. Frontal lobe functions also include override and suppression of socially unacceptable response as well as differentiation tasks. The frontal lobe also plays an important part in integrating longer non-task based memories stored across the brain. These are often memories associated with emotions derived from input from the brain's limbic system. The frontal lobe modifies those emotions to generally fit socially acceptable norms. Psychological tests that measure frontal lobe function include finger tapping (as the frontal lobe controls voluntary movement), the Wisconsin"
    ]
  ],
  [
    "The eight carpal bones that make up the wrist are the scaphoid, lunate, triquetral, pisiform, trapezium, trapezoid, capitate, and hamate.",
    [
      "Carpal bones The carpal bones are the eight small bones that make up the wrist (or carpus) that connects the hand to the forearm. The term \"carpus\" is derived from the Latin carpus and the Greek \u03ba\u03b1\u03c1\u03c0\u03cc\u03c2 (karp\u00f3s), meaning \"wrist\". In human anatomy, the main role of the wrist is to facilitate effective positioning of the hand and powerful use of the extensors and flexors of the forearm, and the mobility of individual carpal bones increase the freedom of movements at the wrist. In tetrapods, the carpus is the sole cluster of bones in the wrist between the radius and",
      "the hand: In the hand proper a total of 13 bones form part of the wrist: eight carpal bones\u2014scaphoid, lunate, triquetral, pisiform, trapezium, trapezoid, capitate, and hamate\u2014 and five metacarpal bones\u2014the first, second, third, fourth, and fifth metacarpal bones. The midcarpal joint is the S-shaped joint space separating the proximal and distal rows of carpal bones. The intercarpal joints, between the bones of each row, are strengthened by the radiate carpal and pisohamate ligaments and the palmar, interosseous, and dorsal intercarpal ligaments. Some degree of mobility is possible between the bones of the proximal row while the bones of the",
      "Wrist In human anatomy, the wrist is variously defined as 1) the carpus or carpal bones, the complex of eight bones forming the proximal skeletal segment of the hand; (2) the wrist joint or radiocarpal joint, the joint between the radius and the carpus and (3) the anatomical region surrounding the carpus including the distal parts of the bones of the forearm and the proximal parts of the metacarpus or five metacarpal bones and the series of joints between these bones, thus referred to as \"wrist joints\". This region also includes the carpal tunnel, the anatomical snuff box, bracelet lines,"
    ]
  ],
  [
    "Consultation with stakeholders and the public is an integral part of the process of drafting and adopting legal acts in the European Institutions (Legal Acts of the European Union).",
    [
      "legislative acts. Their function is to fill in the detail omitted by legislative acts. Legal Acts of the European Union Legal Acts of the European Union are laws which are adopted by the Institutions of the European Union in order to exercise the powers given to them by the EU Treaties. They come in five forms: regulations, directives, decisions, recommendations and opinions. Regulations and directives can be either legislative or non-legislative acts. Legislative acts are normally adopted by the Council of the European Union and the European Parliament acting together, and have their legal basis in the treaties. Non-legislative acts",
      "Legal Acts of the European Union Legal Acts of the European Union are laws which are adopted by the Institutions of the European Union in order to exercise the powers given to them by the EU Treaties. They come in five forms: regulations, directives, decisions, recommendations and opinions. Regulations and directives can be either legislative or non-legislative acts. Legislative acts are normally adopted by the Council of the European Union and the European Parliament acting together, and have their legal basis in the treaties. Non-legislative acts are adopted by the European Commission in pursuance with powers given to it by",
      "is a carefully managed process which helps the transformation of the countries involved, extending peace, stability, prosperity, democracy, human rights and the rule of law across Europe.The European Union enlargement process took a major step forward on 3 October 2005 when accession negotiations were opened with Turkey and Croatia. After years of preparation the two candidates formally opened the next stage of the accession process. The negotiations relate to the adoption and implementation of the EU body of law, known as the \"acquis. The acquis\" is approximately 130,000 pages of legal documents grouped into 35 chapters and forms the rules"
    ]
  ],
  [
    "Coinsurance in the insurance market refers to the joint assumption of risk between multiple parties, such as insurers and insured individuals, with risk shared based on percentages.",
    [
      "former coinsurance allows for 6 months of coverage, compared to 15 months for 125%. In the international insurance market, coinsurance refers to the joint assumption of risk between various insurers. Coinsurance is generally widely used in the European insurance market. In this context, a common insurance contract is used and the risk is shared based on percentages between the insurance companies. Often, one insurance company will lead. When leading the insurance company will be responsible for administering various aspects of the insurance policy, such as premium, any claims and the insurance documents. In this situation, a charge is levied (termed",
      "Co-insurance Coinsurance in insurance, is the splitting or spreading of risk among multiple parties. In the U.S. insurance market, co-insurance is the joint assumption of risk between the insurer and the insured. In title insurance, it also means the sharing of risks between two or more title insurance companies. In health insurance, coinsurance is sometimes used synonymously with copayment, but copayment is fixed while the coinsurance is a percentage that the insurer pays after the insurance policy's deductible is exceeded, up to the policy's stop loss. It can be expressed as a pair of percentages with the insurer's portion stated",
      "lead office commission). Co-insurance Coinsurance in insurance, is the splitting or spreading of risk among multiple parties. In the U.S. insurance market, co-insurance is the joint assumption of risk between the insurer and the insured. In title insurance, it also means the sharing of risks between two or more title insurance companies. In health insurance, coinsurance is sometimes used synonymously with copayment, but copayment is fixed while the coinsurance is a percentage that the insurer pays after the insurance policy's deductible is exceeded, up to the policy's stop loss. It can be expressed as a pair of percentages with the"
    ]
  ],
  [
    "Before a speaking valve is used, the thread type must be checked to ensure it matches the cylinder, and the valve must be inspected and maintained to ensure it remains fit for service.",
    [
      "and valve must be of matching thread specification, clean and full form, undamaged and free of cracks, burrs and other imperfections. Ultrasonic inspection may be substituted for the pressure test, which is usually a hydrostatic test and may be either a proof test or a volumetric expansion test, depending on the cylinder design specification. Test pressure is specified in the stamp markings of the cylinder. Valves that are to be reused are inspected and maintained to ensure they remain fit for service. Before fitting the valve the thread type must be checked to ensure that a valve with matching thread",
      "be inspected and maintained to ensure they remain fit for service. The recommended practice for valve inspection and maintenance includes inspection, and where applicable correction of threads, cleaning of components, replacement of excessively worn and damaged parts, packing and safety devices, lubrication as applicable with approved lubricants for the gas service, checks for correct operation and sealing at intended operating pressure. Checks may be done with the valve fitted to the cylinder after inspection and testing, or before the valve is fitted. Gauging of threads may be mandatory to ensure the integrity of parallel threads. If the gauge exceeds the",
      "ensure that the cylinder is fit for use. Cylinders that fail the tests or inspection and cannot be fixed should be rendered unserviceable after notifying the owner of the reason for failure. Before starting work the cylinder must be identified from the labelling and permanent stamp markings, and the ownership and contents verified. Before internal inspection the valve must be removed after depressurising and verifying that the valve is open. Cylinders containing breathing gases do not need special precautions for discharge except that high oxygen fraction gases should not be released in an enclosed space because of the fire hazard."
    ]
  ],
  [
    "Intravenous drug administration is the fastest route for delivering medications throughout the body.",
    [
      "correct dose. Some medications can have an unpleasant taste or irritate the mouth. Inhalation by smoking a substance is likely the most rapid way to deliver drugs to the brain, as the substance travels directly to the brain without being diluted in the systemic circulation. The severity of dependence on psychoactive drugs tends to increase with more rapid drug delivery. The term injection encompasses intravenous (IV), intramuscular (IM), subcutaneous (SC) and intradermal (ID) administration. Parenteral administration generally acts more rapidly than topical or enteral administration, with onset of action often occurring in 15\u201330 seconds for IV, 10\u201320 minutes for IM",
      "there is no loss of drug. The fastest route of absorption is inhalation, and not as mistakenly considered the intravenous administration. Absorption is a primary focus in drug development and medicinal chemistry, since the drug must be absorbed before any medicinal effects can take place. Moreover, the drug's pharmacokinetic profile can be easily and significantly changed by adjusting factors that affect absorption. In the most common situation, a tablet is ingested and passes through the esophagus to the stomach. The rate of dissolution is a key target for controlling the duration of a drug's effect, and as such, several dosage",
      "affects the risk for psychological dependence and addiction independently of other risk factors, such as dosage and frequency of use. Intravenous injection is the fastest route of drug administration, causing blood concentrations to rise the most quickly, followed by smoking, use of a suppository (rectal or vaginal insertion), insufflation (snorting a powderized form), and ingestion (swallowing). While the onset of the rush induced by injection can occur in as little as a few seconds, the oral route of administration requires up to half an hour before the initial high sets in. Drug injection via intravenous administration, intramuscular administration, or subcutaneous"
    ]
  ],
  [
    "Enzyme activity can be affected by factors such as inhibitors, activators, temperature, pH, substrate concentration, and enzyme saturation properties.",
    [
      "and are not consumed in chemical reactions, nor do they alter the equilibrium of a reaction. Enzymes differ from most other catalysts by being much more specific. Enzyme activity can be affected by other molecules: inhibitors are molecules that decrease enzyme activity, and activators are molecules that increase activity. Many therapeutic drugs and poisons are enzyme inhibitors. An enzyme's activity decreases markedly outside its optimal temperature and pH, and many enzymes are (permanently) denatured when exposed to excessive heat, losing their structure and catalytic properties. Some enzymes are used commercially, for example, in the synthesis of antibiotics. Some household products",
      "While the first prediction is well established, the second is more elusive. Mathematical analysis of the effect of enzyme-substrate unbinding on enzymatic reactions at the single-molecule level has shown that unbinding of an enzyme from a substrate can reduce the rate of product formation under some conditions, but may also have the opposite effect. As substrate concentrations increase, a tipping point can be reached where an increase in the unbinding rate results in an increase, rather than a decrease, of the reaction rate. The results indicate that enzymatic reactions can behave in ways that violate the classical Michaelis-Menten equation, and",
      "properties of an enzyme are how easily the enzyme becomes saturated with a particular substrate, and the maximum rate it can achieve. Knowing these properties suggests what an enzyme might do in the cell and can show how the enzyme will respond to changes in these conditions. Enzyme assays are laboratory procedures that measure the rate of enzyme reactions. Since enzymes are not consumed by the reactions they catalyse, enzyme assays usually follow changes in the concentration of either substrates or products to measure the rate of reaction. There are many methods of measurement. Spectrophotometric assays observe change in the"
    ]
  ],
  [
    "The initial energy source for very high force contractions lasting 1-2 seconds is the anaerobic muscle fibers, as they contract so forcefully that the aerobic fibers are shut out at maximum load.",
    [
      "the extremes, a muscle will fire fibres of both the aerobic or anaerobic types on any given exercise, in varying ratio depending on the load on the intensity of the contraction. This is known as the energy system continuum. At higher loads, the muscle will recruit all muscle fibres possible, both anaerobic (\"fast-twitch\") and aerobic (\"slow-twitch\"), in order to generate the most force. However, at maximum load, the anaerobic processes contract so forcefully that the aerobic fibers are completely shut out, and all work is done by the anaerobic processes. Because the anaerobic muscle fibre uses its fuel faster than",
      "the stronger the eccentric contraction will be, which in turn produces even greater tension. This tension, which is potential force, is then given back in the return movement when the muscular contractions switch to the concentric or shortening regime. However, for maximum return of energy, minimum time must elapse from when the force is received to when they are returned. The greater the time between receiving the forces and giving them back, the less is the return and the less the height that can be achieved in the jump. Most of the lengthening and shortening occurs in the respective muscle",
      "Pulsed power Pulsed power is the science and technology of accumulating energy over a relatively long period of time and releasing it very quickly, thus increasing the instantaneous power. Energy is typically stored within electrostatic fields (capacitors), magnetic fields (inductor), as mechanical energy (using large flywheels connected to special purpose high current alternators), or as chemical energy (high-current lead-acid batteries, or explosives). By releasing the stored energy over a very short interval (a process that is called energy compression), a huge amount of peak power can be delivered to a load. For example, if one joule of energy is stored"
    ]
  ],
  [
    "Systolic and diastolic arterial blood pressures change throughout the day in a circadian rhythm and in response to stress, nutritional factors, drugs, disease, exercise, and momentarily from standing up, with hypertension referring to abnormally high pressure and hypotension to abnormally low pressure.",
    [
      "Systolic and diastolic arterial blood pressures are not static but undergo natural variations from one heartbeat to another and throughout the day (in a circadian rhythm). They also change in response to stress, nutritional factors, drugs, disease, exercise, and momentarily from standing up. Sometimes the variations are large. Hypertension refers to arterial pressure being abnormally high, as opposed to hypotension, when it is abnormally low. Along with body temperature, respiratory rate, and pulse rate, blood pressure is one of the four main vital signs routinely monitored by medical professionals and healthcare providers. Measuring pressure invasively, by penetrating the arterial wall",
      "ventricles are filled with blood. An example of normal measured values for a resting, healthy adult human is 120 mmHg systolic and 80 mmHg diastolic (written as 120/80 mmHg, and spoken as \"one-twenty over eighty\"). Systolic and diastolic arterial blood pressures are not static but undergo natural variations from one heartbeat to another and throughout the day (in a circadian rhythm). They also change in response to stress, nutritional factors, drugs, disease, exercise, and momentarily from standing up. Sometimes the variations are large. Hypertension refers to arterial pressure being abnormally high, as opposed to hypotension, when it is abnormally low.",
      "the \"capacitance vessels\" of the body because over 70% of the blood volume resides in the venous system. Veins are more compliant than arteries and expand to accommodate changing volume. The blood pressure in the circulation is principally due to the pumping action of the heart. The pumping action of the heart generates pulsatile blood flow, which is conducted into the arteries, across the micro-circulation and eventually, back via the venous system to the heart. During each heartbeat, systemic arterial blood pressure varies between a maximum (systolic) and a minimum (diastolic) pressure. In physiology, these are often simplified into one"
    ]
  ],
  [
    "Sedatives, analgesics, paralytic agents, hypertonic saline, and mannitol are used to decrease elevating intracranial pressure until the cause can be identified.",
    [
      "Treatment of raised ICP may be as simple as tilting the person's bed and straightening the head to promote blood flow through the veins of the neck. Sedatives, analgesics and paralytic agents are often used. Hypertonic saline can improve ICP by reducing the amount of cerebral water (swelling), though it is used with caution to avoid electrolyte imbalances or heart failure. Mannitol, an osmotic diuretic, appears to be equally effective at reducing ICP. Some concerns; however, have been raised regarding some of the studies performed. Diuretics, drugs that increase urine output to reduce excessive fluid in the system, may be",
      "is the result of a lumbar puncture, a \"blood patch\" may be applied to seal the site of CSF leakage. Various medical treatments have been proposed; only the intravenous administration of caffeine and theophylline has shown to be particularly useful. Intracranial pressure Intracranial pressure (ICP) is the pressure inside the skull and thus in the brain tissue and cerebrospinal fluid (CSF). ICP is measured in millimeters of mercury (mmHg) and, at rest, is normally 7\u201315 mmHg for a supine adult. The body has various mechanisms by which it keeps the ICP stable, with CSF pressures varying by about 1 mmHg",
      "be needed if the level of consciousness is very low, or if there is evidence of respiratory failure. If there are signs of raised intracranial pressure, measures to monitor the pressure may be taken; this would allow the optimization of the cerebral perfusion pressure and various treatments to decrease the intracranial pressure with medication (e.g. mannitol). Seizures are treated with anticonvulsants. Hydrocephalus (obstructed flow of CSF) may require insertion of a temporary or long-term drainage device, such as a cerebral shunt. Empiric antibiotics (treatment without exact diagnosis) should be started immediately, even before the results of the lumbar puncture and"
    ]
  ],
  [
    "The pH of pure water at 25 \u00b0C is 7.",
    [
      "temperature increases. Pure water is neutral, pH 7 at (25 \u00b0C), being neither an acid nor a base. Contrary to popular belief, the pH value can be less than 0 or greater than 14 for very strong acids and bases respectively. Measurements of pH are important in agronomy, medicine, chemistry, water treatment, and many other applications. The pH scale is traceable to a set of standard solutions whose pH is established by international agreement. Primary pH standard values are determined using a concentration cell with transference, by measuring the potential difference between a hydrogen electrode and a standard electrode such",
      "the pH of pure water is 7.47. At 25 \u00b0C it's 7.00, and at 100 \u00b0C it's 6.14. This definition was adopted because ion-selective electrodes, which are used to measure pH, respond to activity. Ideally, electrode potential, \"E\", follows the Nernst equation, which, for the hydrogen ion can be written as where \"E\" is a measured potential, \"E\" is the standard electrode potential, \"R\" is the gas constant, \"T\" is the temperature in kelvins, \"F\" is the Faraday constant. For H number of electrons transferred is one. It follows that electrode potential is proportional to pH when pH is defined",
      "seawater, or at elevated temperatures, like those in thermal power plants. We can also define p\"K\" formula_6 \u2212log \"K\" (which is approximately 14 at 25 \u00b0C). This is analogous to the notations pH and p\"K\" for an acid dissociation constant, where the symbol p denotes a cologarithm. The logarithmic form of the equilibrium constant equation is p\"K\" = pH + pOH. The dependence of the water ionization on temperature and pressure has been investigated thoroughly. The value of p\"K\" decreases as temperature increases from the melting point of ice to a minimum at c. 250 \u00b0C, after which it increases"
    ]
  ],
  [
    "The typical timeframe for a PICC line to remain in situ is around 1-2 weeks.",
    [
      "of the column stationary phase to increase resolution and separation while reducing run time. The separation and run time also depends on the film thickness (of the stationary phase), the column diameter and the column length. The column(s) in a GC are contained in an oven, the temperature of which is precisely controlled electronically. (When discussing the \"temperature of the column,\" an analyst is technically referring to the temperature of the column oven. The distinction, however, is not important and will not subsequently be made in this article.) The rate at which a sample passes through the column is directly",
      "As the chemicals exit the end of the column, they are detected and identified electronically. The function of the stationary phase in the column is to separate different components, causing each one to exit the column at a different time (\"retention time\"). Other parameters that can be used to alter the order or time of retention are the carrier gas flow rate, column length and the temperature.In a GC analysis, a known volume of gaseous or liquid analyte is injected into the \"entrance\" (head) of the column, usually using a microsyringe (or, solid phase microextraction fibers, or a gas source",
      "years). While these methods can work on some stuck pixels others cannot be fixed by the above methods. Also, some stuck pixels will reappear after being fixed if the screen is left off for several hours. In LCD manufacture, it is common for a display to be manufactured that has a number of sub-pixel defects (each pixel is composed of three primary-colored sub-pixels). The number of faulty pixels tolerated, before a screen is rejected, is dependent on the class that the manufacturer has given the display (although officially described by the ISO 13406-2 standard, not all manufacturers interpret this standard"
    ]
  ],
  [
    "The clinic sensitivity of the nitrogenase to oxygen in microbial metabolism is determined by the irreversible inhibition of nitrogenase by dioxygen, which degradatively oxidizes the Fe-S cofactors, requiring mechanisms for nitrogen fixers to protect nitrogenase from oxygen \"in vivo.\"",
    [
      "sensitivity of the nitrogenase to oxygen. Microbial metabolism Microbial metabolism is the means by which a microbe obtains the energy and nutrients (e.g. carbon) it needs to live and reproduce. Microbes use many different types of metabolic strategies and species can often be differentiated from each other based on metabolic characteristics. The specific metabolic properties of a microbe are the major factors in determining that microbe's ecological niche, and often allow for that microbe to be useful in industrial processes or responsible for biogeochemical cycles. All microbial metabolisms can be arranged according to three principles: 1. How the organism obtains",
      "are irreversibly inhibited by dioxygen, which degradatively oxidizes the Fe-S cofactors. This requires mechanisms for nitrogen fixers to protect nitrogenase from oxygen \"in vivo\". Despite this problem, many use oxygen as a terminal electron acceptor for respiration. Although the ability of some nitrogen fixers such as Azotobacteraceae to employ an oxygen-labile nitrogenase under aerobic conditions has been attributed to a high metabolic rate, allowing oxygen reduction at the cell membrane, the effectiveness of such a mechanism has been questioned at oxygen concentrations above 70 \u00b5M (ambient concentration is 230 \u00b5M O), as well as during additional nutrient limitations. In addition",
      "nitrogenase (leading to overestimates of nitrogenase by ARA). Bottle or chamber-based assays may produce negative impacts on microbial systems as a result of containment or disruption of the microenvironment through handling, leading to underestimation of nitrogenase. Despite these weaknesses, such assays are very useful in assessing relative rates or temporal patterns in nitrogenase activity. Nitrogenase Nitrogenases are enzymes () that are produced by certain bacteria, such as cyanobacteria (blue-green algae). These enzymes are responsible for the reduction of nitrogen (N) to ammonia (NH). Nitrogenases are the only family of enzymes known to catalyze this reaction, which is a key step"
    ]
  ],
  [
    "Humans have 23 distinct types of chromosomes, including 22 autosomes and 1 pair of sex chromosomes.",
    [
      "= 46) while human sperm and eggs have 23 chromosomes (N = 23). Humans have 23 distinct types of chromosomes, the 22 autosomes and the special category of sex chromosomes. There are two distinct sex chromosomes, the X chromosome and the Y chromosome. A diploid human cell has 23 chromosomes from that person's father and 23 from the mother. That is, your body has two copies of human chromosome number 2, one from each of your parents. Immediately after DNA replication a human cell will have 46 \"double chromosomes\". In each double chromosome there are two copies of that chromosome's",
      "spite of their appearance, chromosomes are structurally highly condensed, which enables these giant DNA structures to be contained within a cell nucleus. Chromosomes in humans can be divided into two types: autosomes (body chromosome(s)) and allosome (sex chromosome(s)). Certain genetic traits are linked to a person's sex and are passed on through the sex chromosomes. The autosomes contain the rest of the genetic hereditary information. All act in the same way during cell division. Human cells have 23 pairs of chromosomes (22 pairs of autosomes and one pair of sex chromosomes), giving a total of 46 per cell. In addition",
      "Among the 23 pairs of chromosomes there are 22 pairs of autosomes and one pair of sex chromosomes. Like other mammals, humans have an XY sex-determination system, so that females have the sex chromosomes XX and males have XY. One human genome was sequenced in full in 2003, and currently efforts are being made to achieve a sample of the genetic diversity of the species (see International HapMap Project). By present estimates, humans have approximately 22,000 genes. The variation in human DNA is very small compared to other species, possibly suggesting a population bottleneck during the Late Pleistocene (around 100,000"
    ]
  ],
  [
    "Antibiotics should be administered prior to catheterization if infections from catheter insertion sites are suspected, especially if the catheter has been in place for more than 48 hours.",
    [
      "catheter insertion sites (in-situ more than 48 hours) can be taken if infections from these sites are suspected. In severe sepsis and septic shock, broad-spectrum antibiotics (usually two, a \u03b2-lactam antibiotic with broad coverage, or broad-spectrum carbapenem combined with fluoroquinolones, macrolides, or aminoglycosides) are recommended. However, combination of antibiotics is not recommended for the treatment of sepsis but without shock and immunocompromised persons unless the combination is used to broaden the anti-bacterial activity. The choice of antibiotics is important in determining the survival of the person. Some recommend they be given within one hour of making the diagnosis, stating that",
      "for every hour of delay in the administration of antibiotics, there is an associated 6% rise in mortality. Others did not find a benefit with early administration. Several factors determine the most appropriate choice for the initial antibiotic regimen. These factors include local patterns of bacterial sensitivity to antibiotics, whether the infection is thought to be a hospital or community-acquired infection, and which organ systems are thought to be infected. Antibiotic regimens should be reassessed daily and narrowed if appropriate. Treatment duration is typically 7\u201310 days with the type of antibiotic used directed by the results of cultures. If the",
      "The presence of bacteria in the blood almost always requires treatment with antibiotics. This is because there are high mortality rates from progression to sepsis if antibiotics are delayed. The treatment of bacteremia should begin with empiric antibiotic coverage. Any patient presenting with signs or symptoms of bacteremia or a positive blood culture should be started on intravenous antibiotics. The choice of antibiotic is determined by the most likely source of infection and by the characteristic organisms that typically cause that infection. Other important considerations include the patient's past history of antibiotic use, the severity of the presenting symptoms, and"
    ]
  ],
  [
    "The presence of a pulsation in the neck that corresponds to the arterial pulsation in the anatomical snuff box suggests that it is arterial.",
    [
      "arterial pulsation in the anatomical snuff box is therefore recommended. The radial artery lies superficially in front of the distal end of the radius, between the tendons of the brachioradialis and flexor carpi radialis; it is here that clinician takes the radial pulse. (where it is commonly used to assess the heart rate and cardiac rhythm). Presence of \"radial pulse\" was thought to indicate a systolic blood pressure of at least 70 mmHg, as estimated from the 50% percentile, although this was found to generally be an overestimation of a patient's true blood pressure. The radial artery can be less",
      "flow waveforms in terms of wave propagation and reflection, although recently attempts have been made to integrate wave propagation and Windkessel approaches. The situation may even be more complex since the arterial wall contains smooth muscle cells that hypothetically contribute to the pressure wave from the heart on a beat-to-beat basis. The theory of arterial acceleration proposes that the onset of systole triggers a short-lasting depolarisation within smooth muscle cells that spreads along the arterial tree as a peristaltic wave. This mechanism is thought to amplify and distribute the pressure wave over the arterial tree allowing it to penetrate in",
      "patient, in the case of collapsed veins in other parts of body (e.g. arms). The positive pulsation of the femoral artery signifies that the heart is beating and also blood is flowing to the lower extremity. It is also necessary to appreciate clinically that this is a case where the nerve is more lateral than the vein. In most other cases the nerve (relative to its associated artery and vein) would be the deepest or more medial followed by the artery and then the vein. But in this case it is the opposite. This must be remembered when venous or"
    ]
  ],
  [
    "The overall process of glycolysis involves breaking down a glucose molecule into two pyruvate molecules through a series of enzymatic steps, with the initial phosphorylation of glucose being a crucial and necessary step for the pathway to proceed (Glycolysis).",
    [
      "in the subsequent decades, to include further details of its regulation and integration with other metabolic pathways. The first five steps are regarded as the preparatory (or investment) phase, since they consume energy to convert the glucose into two three-carbon sugar phosphates (G3P). The first step in glycolysis is phosphorylation of glucose by a family of enzymes called hexokinases to form glucose 6-phosphate (G6P). This reaction consumes ATP, but it acts to keep the glucose concentration low, promoting continuous transport of glucose into the cell through the plasma membrane transporters. In addition, it blocks the glucose from leaking out \u2013",
      "only a few of the more complex carbohydrates. The disaccharide lactose, for instance, requires the enzyme lactase to be broken into its monosaccharide components, glucose and galactose. Glycolysis is the process of breaking down a glucose molecule into two pyruvate molecules, while storing energy released during this process as ATP and NADH. Nearly all organisms that break down glucose utilize glycolysis. Glucose regulation and product use are the primary categories in which these pathways differ between organisms. In some tissues and organisms, glycolysis is the sole method of energy production. This pathway is anaerobic, because it doesn\u2019t require oxygen. Glycolysis",
      "glucose degrading into two molecules of pyruvate, through various steps, with the help of different enzymes. It occurs in ten steps and proves that phosphorylation is a much required and necessary step to attain the end products. Phosphorylation initiates the reaction in step 1 of the preparatory step (first half of glycolysis), and initiates step 6 of payoff phase (second phase of glycolysis). Glucose, by nature, is a small molecule with the ability to diffuse in and out of the cell. By phosphorylating glucose (adding a negatively charged phosphate group), glucose is converted to glucose-6-phosphate and trapped within the cell"
    ]
  ],
  [
    "The signs and symptoms of hemorrhoids include painless, bright red rectal bleeding during defecation for internal hemorrhoids, and pain and swelling in the anal area for external hemorrhoids.",
    [
      "Hemorrhoid Hemorrhoids, also called piles, are vascular structures in the anal canal. In their normal state, they are cushions that help with stool control. They become a disease when swollen or inflamed; the unqualified term \"hemorrhoid\" is often used to refer to the disease. The signs and symptoms of hemorrhoids depend on the type present. Internal hemorrhoids often result in painless, bright red rectal bleeding when defecating. External hemorrhoids often result in pain and swelling in the area of the anus. If bleeding occurs it is usually darker. Symptoms frequently get better after a few days. A skin tag may",
      "helping maintain anal closure. Hemorrhoid symptoms are believed to result when these vascular structures slide downwards or when venous pressure is excessively increased. Increased internal and external anal sphincter pressure may also be involved in hemorrhoid symptoms. Two types of hemorrhoids occur: internals from the superior hemorrhoidal plexus and externals from the inferior hemorrhoidal plexus. The dentate line divides the two regions. Hemorrhoids are typically diagnosed by physical examination. A visual examination of the anus and surrounding area may diagnose external or prolapsed hemorrhoids. A rectal exam may be performed to detect possible rectal tumors, polyps, an enlarged prostate, or",
      "remain after the healing of an external hemorrhoid. While the exact cause of hemorrhoids remains unknown, a number of factors which increase pressure in the abdomen are believed to be involved. This may include constipation, diarrhea and sitting on the toilet for a long time. Hemorrhoids are also more common during pregnancy. Diagnosis is made by looking at the area. Many people incorrectly refer to any symptom occurring around the anal area as \"hemorrhoids\" and serious causes of the symptoms should be ruled out. Colonoscopy or sigmoidoscopy is reasonable to confirm the diagnosis and rule out more serious causes. Often,"
    ]
  ],
  [
    "The potential benefit of Selective Androgen Receptor Modulators (SARMs) compared to traditional anabolic steroids is their ability to be more selective in their action, allowing for more diverse uses with fewer side effects.",
    [
      "Selective androgen receptor modulator Selective androgen receptor modulators or SARMs are a novel class of androgen receptor ligands. (The name follows the terminology currently used for similar molecules targeting the estrogen receptor, \"selective estrogen receptor modulators,\" such as tamoxifen.) They are intended to have the same kind of effects as androgenic drugs but be much more selective in their action, allowing them to be used for more uses than the relatively limited legitimate uses of anabolic steroids. Currently used androgens for male hormone replacement therapy are typically injectable or skin delivery formulations of testosterone or testosterone esters. Injectable forms of",
      "emerged recently, and are in pre-clinical development. Small molecule antiandrogens that are available today have undesirable side effects caused by complete, non-selective inhibition of AR action. To minimize these side effects, a new class of tissue selective androgen receptor modulators (SARMs) has been proposed as a novel approach for the treatment of prostate cancer. These ligands should behave as antagonists in the prostate with either no activity or agonist activity in other target tissues, so as to have little or no effects in the anabolic tissues or central nervous system (CNS). However discovering this new class of ligands might be",
      "testosterone, which may be a problem in human clinical use. Selective androgen receptor modulators may also be used by athletes to assist in training and increase physical stamina and fitness, potentially producing effects similar to anabolic steroids but with significantly fewer side effects. For this reason, SARMs have already been banned by the World Anti-Doping Agency since January 2008 despite no drugs from this class yet being in clinical use, and blood tests for all known SARMs are currently being developed. BMS-564,929 BMS-564,929 is an investigational selective androgen receptor modulator (SARM) which is being developed by Bristol-Myers Squibb for treatment"
    ]
  ],
  [
    "The enzymes involved in the process of \u03b2-oxidation of fatty acids include acyl-CoA dehydrogenase, enoyl-CoA hydratase, 3-hydroxyacyl-CoA dehydrogenase, and acetyl-CoA acyltransferase.",
    [
      "overall reaction for one cycle of beta oxidation is: Fatty acid catabolism consists of: Free fatty acids cannot penetrate any biological membrane due to their negative charge. Free fatty acids must cross the cell membrane through specific transport proteins, such as the SLC27 family fatty acid transport protein. Once in the cytosol, the following processes bring fatty acids into the mitochondrial matrix so that beta-oxidation can take place. Once the fatty acid is inside the mitochondrial matrix, beta-oxidation occurs by cleaving two carbons every cycle to form acetyl-CoA. The process consists of 4 steps. Fatty acids are oxidized by most",
      "Beta oxidation In biochemistry and metabolism, beta-oxidation is the catabolic process by which fatty acid molecules are broken down in the cytosol in prokaryotes and in the mitochondria in eukaryotes to generate acetyl-CoA, which enters the citric acid cycle, and NADH and FADH, which are co-enzymes used in the electron transport chain. It is named as such because the beta carbon of the fatty acid undergoes oxidation to a carbonyl group. Beta-oxidation is primarily facilitated by the mitochondrial trifunctional protein, an enzyme complex associated with the inner mitochondrial membrane, although very long chain fatty acids are oxidized in peroxisomes. The",
      "carboxykinase, and converted to free glucose. \u03b2-Oxidation of unsaturated fatty acids poses a problem since the location of a cis bond can prevent the formation of a trans-\u0394 bond. These situations are handled by an additional two enzymes, Enoyl CoA isomerase or 2,4 Dienoyl CoA reductase. Whatever the conformation of the hydrocarbon chain, \u03b2-oxidation occurs normally until the acyl CoA (because of the presence of a double bond) is not an appropriate substrate for acyl CoA dehydrogenase, or enoyl CoA hydratase: To summarize: Fatty acid oxidation also occurs in peroxisomes when the fatty acid chains are too long to be"
    ]
  ],
  [
    "Increased concentration of troponin, B-type natriuretic peptide (BNP), and ischemia-modified (aka MI) albumin are products of ADP degradation that can be observed in the blood during multiple sprint sports (Exercise physiology).",
    [
      "as troponin, B-type natriuretic peptide (BNP), and ischemia-modified (aka MI) albumin. This can be misinterpreted by medical personnel as signs of myocardial infarction, or cardiac dysfunction. In these clinical conditions, such cardiac biomarkers are produced by irreversible injury of muscles. In contrast, the processes that create them after strenuous exertion in endurance sports are reversible, with their levels returning to normal within 24-hours (further research, however, is still needed). Humans are specifically adapted to engage in prolonged strenuous muscular activity (such as efficient long distance bipedal running). This capacity for endurance running evolved to allow the running down of game",
      "work. Endurance training primarily work the slow twitch (type 1) fibers and develop such fibers in their efficiency and resistance to fatigue. Catabolism also improves increasing the athletes capacity to use fat and glycogen stores as an energy source. These metabolic processes are known as glycogenolysis, glycolysis and lipolysis. There is higher efficiency in oxygen transport and distribution. In recent years it has been recognized that oxidative enzymes such as succinate dehydrogenase (SDH) that enable mitochondria to break down nutrients to form ATP increase by 2.5 times in well trained endurance athletes In addition to SDH, myoglobin increases by 75-80%",
      "to how lactate threshold effects endurance performance. The contribution of one's blood lactate levels accumulating is attributed to potential skeletal muscle hypoxemia but also to the production of more glucose that can be used as energy. The inability to establish a singular set of physiological contributions to blood lactate accumulation's effect on the exercising individual creates a correlative role for lactate threshold in marathon performance as opposed to a causal role. In order to sustain high intensity running, a marathon runner must obtain sufficient glycogen stores. Glycogen can be found in the skeletal muscles or liver. With low levels of"
    ]
  ],
  [
    "The recommended position for performing CPR on individuals less than 20 years of age with non heart-related causes of cardiac arrest is in the supine position.",
    [
      "arrest due to non heart related causes and in people less than 20 years of age, standard CPR is superior to compression-only CPR. Standard CPR is performed with the person in supine position. Prone CPR or reverse CPR is CPR performed on a person lying on their chest, by turning the head to the side and compressing the back. Due to the head being turned, the risk of vomiting and complications caused by aspiration pneumonia may be reduced. The American Heart Association's current guideline recommends to perform CPR in the supine position, and limits prone CPR to situations where the",
      "Bystanders more commonly administer CPR when in public than when at the person's home, although health care professionals are responsible for more than half of out-of-hospital resuscitation attempts. People with no connection to the person are more likely to perform CPR than are a member of their family. There is also a clear relation between cause of arrest and the likelihood of a bystander initiating CPR. Lay persons are most likely to give CPR to younger people in cardiac arrest in a public place when it has a medical cause; those in arrest from trauma, exsanguination or intoxication are less",
      "call to the emergency services. Unfortunately, many persons experiencing symptoms (for example, angina) that may lead to a cardiac arrest ignore these warning symptoms or, recognizing these warning symptoms correctly, fail to activate the EMS system, preferring to contact relatives instead (the elderly often contact their adult offspring rather than contact emergency services). To be most effective, bystanders should provide CPR immediately after a patient collapses. Properly performed CPR can keep the heart in a shockable ventricular fibrillation for 10\u201312 minutes longer. Most adults who can be saved from cardiac arrest are in ventricular fibrillation or pulseless ventricular tachycardia. Early"
    ]
  ],
  [
    "The resynthesis of phosphocreatine after very high intensity exercise occurs through the conversion of ATP into PCr via mitochondrial creatine kinase, which is then transported back into the muscle cells for storage and use in future exercise (Physiology of marathons).",
    [
      "inorganic phosphate for ATP formation. At the onset of exercise phosphocreatine is broken down to provide ATP for muscle contraction. ATP hydrolysis results in products of ADP and inorganic phosphate. The inorganic phosphate will be transported into the mitochondrial matrix, while the free creatine passes through the outer membrane where it will be resynthesised into PCr. The antiporter transports the ADP into the matrix, while transporting ATP out. Due to the high concentration of ATP around the mitrochondrial creatine kinase, it will convert ATP into PCr which will then move back out into the cells cytoplasm to be converted into",
      "maintained. The Phosphogenic (ATP-PC) anaerobic energy pathway restores ATP after its breakdown via Creatine Phosphate stored in skeletal muscle. This pathway is anaerobic because it does not require oxygen to synthesize or utilize ATP. ATP restoration only lasts for approximately the first 30 seconds of exercise. This rapid rate of ATP production is essential at the onset of exercise. The amount of creatine phosphate and ATP stored in the muscle is small, readily available, and is utilized quickly due these two factors. An example of exercise that would primarily use this energy pathway could be weight lifting or performing sprints.",
      "that combines phosphocreatine and adenosine diphosphate (ADP) into ATP and creatine. This resource is short lasting because oxygen is required for the resynthesis of phosphocreatine via mitochondrial creatine kinase. Therefore, under anaerobic conditions, this substrate is finite and only lasts between approximately 10 to 30 seconds of high intensity work. Fast glycolysis, however, can function for approximately 2 minutes prior to fatigue, and predominately uses intracellular glycogen as a substrate. Glycogen is broken down rapidly via glycogen phosphorylase into individual glucose units during intense exercise. Glucose is then oxidized to pyruvate and under anaerobic conditions is reduced to lactic acid."
    ]
  ],
  [
    "The main bony landmarks of the elbow joint are the medial and lateral epicondyles of the humerus, and the olecranon of the ulna.",
    [
      "some elbow-related terms, as in \"cubital nodes\" for example. The elbow joint has three different portions surrounded by a common joint capsule. These are joints between the three bones of the elbow, the humerus of the upper arm, and the radius and the ulna of the forearm. When in anatomical position there are four main bony landmarks of the elbow. At the lower part of the humerus are the medial and lateral epicondyles, on the side closest to the body (medial) and on the side away from the body (lateral) surfaces. The third landmark is the olecranon found at the",
      "Elbow The elbow is the visible joint between the upper and lower parts of the arm. It includes prominent landmarks such as the olecranon, the elbow pit, the lateral and medial epicondyles, and the elbow joint. The elbow joint is the synovial hinge joint between the humerus in the upper arm and the radius and ulna in the forearm which allows the forearm and hand to be moved towards and away from the body. The elbow is specific to humans and other primates. The name for the elbow in Latin is \"cubitus\", and so the word cubital is used in",
      "present, but pathologic when elevated by fluid, and always pathologic when posterior. The function of the elbow joint is to extend and flex the arm grasp and reach for objects. The range of movement in the elbow is from 0 degrees of elbow extension to 150 of elbow flexion. Muscles contributing to function are all flexion (biceps brachii, brachialis, and brachioradialis) and extension muscles (triceps and anconeus). In humans, the main task of the elbow is to properly place the hand in space by shortening and lengthening the upper limb. While the superior radioulnar joint shares joint capsule with the"
    ]
  ],
  [
    "A hypertonic solution is not related to transcendental functions.",
    [
      "\"transcendentally transcendental\". H\u00f6lder's theorem shows that the gamma function is in this category. Hypertranscendental functions usually arise as the solutions to functional equations, for example the gamma function. Hypertranscendental function A hypertranscendental function or transcendentally transcendental function is an analytic function which is not the solution of an algebraic differential equation with coefficients in Z (the integers) and with algebraic initial conditions. All hypertranscendental functions are transcendental functions. The term 'transcendentally transcendental' was introduced by E. H. Moore in 1896; the term 'hypertranscendental' was introduced by D. D. Morduhai-Boltovskoi in 1914. One standard definition (there are slight variants) defines solutions",
      "Hypertranscendental function A hypertranscendental function or transcendentally transcendental function is an analytic function which is not the solution of an algebraic differential equation with coefficients in Z (the integers) and with algebraic initial conditions. All hypertranscendental functions are transcendental functions. The term 'transcendentally transcendental' was introduced by E. H. Moore in 1896; the term 'hypertranscendental' was introduced by D. D. Morduhai-Boltovskoi in 1914. One standard definition (there are slight variants) defines solutions of differential equations of the form where formula_2 is a polynomial with constant coefficients, as \"algebraically transcendental\" or \"differentially algebraic\". Transcendental functions which are not \"algebraically transcendental\" are",
      "find solutions. In some cases, special functions can be used to write the solutions to transcendental equations in closed form. In particular, formula_7 has a solution in terms of the Lambert W function. Transcendental equation A transcendental equation is an equation containing a transcendental function of the variable(s) being solved for. Such equations often do not have closed-form solutions. Examples include: Equations where the variable to be solved for appears only once, as an argument to the transcendental function, are easily solvable with inverse functions; similarly if the equation can be factored or transformed to such a case: Some can"
    ]
  ],
  [
    "The jugular venous pressure can be used to indirectly assess central venous pressure when placing central venous lines (Jugular venous pressure).",
    [
      "a central venous catheter, which is a tube inserted in the neck veins). A 1996 systematic review concluded that a high jugular venous pressure makes a high central venous pressure more likely, but does not significantly help confirm a low central venous pressure. The study also found that agreement between doctors on the jugular venous pressure can be poor. Jugular venous pressure The jugular venous pressure (JVP, sometimes referred to as jugular venous pulse) is the indirectly observed pressure over the venous system via visualization of the internal jugular vein. It can be useful in the differentiation of different forms",
      "and measure central venous pressure. Reasons for the use of central lines include: Central venous catheters usually remain in place for a longer period than other venous access devices, especially when the reason for their use is longstanding (such as total parenteral nutrition in a chronically ill person). For such indications, a Hickman line, a PICC line, or a Port-a-Cath may be considered because of their smaller infection risk. Sterile technique is highly important here, as a line may serve as an entry point for pathogenic organisms. Additionally, the line itself may become infected with bacteria such as \"Staphylococcus aureus\"",
      "Jugular venous pressure The jugular venous pressure (JVP, sometimes referred to as jugular venous pulse) is the indirectly observed pressure over the venous system via visualization of the internal jugular vein. It can be useful in the differentiation of different forms of heart and lung disease. Classically three upward deflections and two downward deflections have been described. The patient is positioned at a 45\u00b0 incline, and the filling level of the external jugular vein determined. Visualize the internal jugular vein when looking for the pulsation. In healthy people, the filling level of the jugular vein should be less than 3"
    ]
  ],
  [
    "The concept of maximum sustainable power in ecological systems refers to the maximum rate of useful energy transformation that can be maintained without exceeding the system's regenerative capacity, ultimately aiming to ensure ecosystem sustainability and long-term survival.",
    [
      "\u2013 to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine. Odum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example,",
      "Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power. The mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)",
      "from within an ecosystem that exceeds the system's regenerative capacity. This is seen in particular in non-renewable resources wherein consumption outstrips production. In a general sense in his work, it refers to the depletion of global resources beyond the Earth's ability to regenerate them. The concept in this sense is based on the bio-physical carrying capacity of an ecosystem; through measuring ecological footprints human society can determine the rate at which it is depleting natural resources. Ultimately, the imperative of sustainability requires human society to live within the means of the ecological system to support life over the long term."
    ]
  ],
  [
    "Fast-twitch fibers rely on a well-developed, anaerobic, short term, glycolytic system for energy transfer and can contract and develop tension at 2\u20133 times the rate of slow-twitch fibers.",
    [
      "The fast twitch fibers rely on a well-developed, anaerobic, short term, glycolytic system for energy transfer and can contract and develop tension at 2\u20133 times the rate of slow twitch fibers. Fast twitch muscles are much better at generating short bursts of strength or speed than slow muscles, and so fatigue more quickly. The slow twitch fibers generate energy for ATP re-synthesis by means of a long term system of aerobic energy transfer. These mainly include the ATPase type I and MHC type I fibers. They tend to have a low activity level of ATPase, a slower speed of contraction",
      "are white due to relatively low myoglobin and a reliance on glycolytic enzymes. Fibers can also be classified on their twitch capabilities, into fast and slow twitch. These traits largely, but not completely, overlap the classifications based on color, ATPase, or MHC. Some authors define a fast twitch fiber as one in which the myosin can split ATP very quickly. These mainly include the ATPase type II and MHC type II fibers. However, fast twitch fibers also demonstrate a higher capability for electrochemical transmission of action potentials and a rapid level of calcium release and uptake by the sarcoplasmic reticulum.",
      "factor of determining the performance is within 0.2 seconds, how many available muscle motor units can be recruited for contraction, yet how much fast-twitch fibers in the body. as the result, an athlete lacking fast twitch fibers has better control of nervous system that recruited all the fast-twitch fibers in the body, the athlete tends to have superior performance in comparison to the athlete with less control of nervous system while having greater number of fast twitch fibers. From above, people can considerably increase their strength without increasing the size of their muscle, because the body becomes more efficient at"
    ]
  ],
  [
    "The rate of blood lactate accumulation during normal metabolism and exercise is determined by factors such as monocarboxylate transporters, concentration and isoform of lactate dehydrogenase, oxidative capacity of tissues, and the efficiency of lactate clearance rates.",
    [
      "acid have a higher melting point. In animals, -lactate is constantly produced from pyruvate via the enzyme lactate dehydrogenase (LDH) in a process of fermentation during normal metabolism and exercise. It does not increase in concentration until the rate of lactate production exceeds the rate of lactate removal, which is governed by a number of factors, including monocarboxylate transporters, concentration and isoform of LDH, and oxidative capacity of tissues. The concentration of blood lactate is usually at rest, but can rise to over 20 mM during intense exertion and as high as 25 mM afterward. In addition to other biological",
      "to lactate . As found by Brooks, et al., while lactate is disposed of mainly through oxidation and only a minor fraction supports gluconeogenesis, lactate is the main gluconeogenic precursor during sustained exercise. Brooks demonstrated in his earlier studies that little difference in lactate production rates were seen in trained and untrained subjects at equivalent power outputs. What was seen, however, was more efficient clearance rates of lactate in the trained subjects suggesting an upregulation of MCT protein. Local lactate use depends on exercise exertion. During rest, approximately 50% of lactate disposal take place through lactate oxidation whereas in time",
      "to how lactate threshold effects endurance performance. The contribution of one's blood lactate levels accumulating is attributed to potential skeletal muscle hypoxemia but also to the production of more glucose that can be used as energy. The inability to establish a singular set of physiological contributions to blood lactate accumulation's effect on the exercising individual creates a correlative role for lactate threshold in marathon performance as opposed to a causal role. In order to sustain high intensity running, a marathon runner must obtain sufficient glycogen stores. Glycogen can be found in the skeletal muscles or liver. With low levels of"
    ]
  ],
  [
    "The enzymes of glycolysis are regulated by reciprocal control mechanisms that inhibit glycolysis and activate gluconeogenesis, preventing both pathways from running simultaneously in a futile cycle, while the rate of gluconeogenesis is ultimately controlled by the key enzyme fructose-1,6-bisphosphatase.",
    [
      "is called gluconeogenesis. Gluconeogenesis converts pyruvate to glucose-6-phosphate through a series of intermediates, many of which are shared with glycolysis. However, this pathway is not simply glycolysis run in reverse, as several steps are catalyzed by non-glycolytic enzymes. This is important as it allows the formation and breakdown of glucose to be regulated separately, and prevents both pathways from running simultaneously in a futile cycle. Although fat is a common way of storing energy, in vertebrates such as humans the fatty acids in these stores cannot be converted to glucose through gluconeogenesis as these organisms cannot convert acetyl-CoA into pyruvate;",
      "citrate activate gluconeogenesis enzymes (pyruvate carboxylase and fructose-1,6-bisphosphatase, respectively), while at the same time inhibiting the glycolytic enzyme pyruvate kinase. This system of reciprocal control allow glycolysis and gluconeogenesis to inhibit each other and prevents a futile cycle of synthesizing glucose to only break it down. The majority of the enzymes responsible for gluconeogenesis are found in the cytosol; the exceptions are mitochondrial pyruvate carboxylase and, in animals, phosphoenolpyruvate carboxykinase. The latter exists as an isozyme located in both the mitochondrion and the cytosol. The rate of gluconeogenesis is ultimately controlled by the action of a key enzyme, fructose-1,6-bisphosphatase, which",
      "muscles and in hibernating animals during periodical arousal from torpor. It has been reported that the glucose metabolism substrate cycle is not a futile cycle but a regulatory process. For example, when energy is suddenly needed, ATP is replaced by AMP, a much more reactive adenine. The simultaneous carrying out of glycolysis and gluconeogenesis is an example of a futile cycle, represented by the following equation: formula_1 For example, during glycolysis, fructose-6-phosphate is converted to fructose-1,6-bisphosphate in a reaction catalysed by the enzyme phosphofructokinase 1 (PFK-1). ATP + fructose-6-phosphate \u2192 Fructose-1,6-bisphosphate + ADP But during gluconeogenesis (i.e. synthesis of glucose"
    ]
  ],
  [
    "The energy released from the breakdown of high-energy phosphates can sustain maximal exertion exercise for approximately the first 30 seconds. (Phosphogenic (ATP-PC) anaerobic energy pathway)",
    [
      "High-energy phosphate High-energy phosphate can mean one of two things: High-energy phosphate bonds are pyrophosphate bonds, acid anhydride linkages formed by taking phosphoric acid derivatives and dehydrating them. As a consequence, the hydrolysis of these bonds is exergonic under physiological conditions, releasing energy. Except for PP \u2192 2 P, these reactions are, in general, not allowed to go uncontrolled in the human cell but are instead coupled to other processes needing energy to drive them to completion. Thus, high-energy phosphate reactions can: The one exception is of value because it allows a single hydrolysis, ATP + HO \u2192 AMP +",
      "system. High energy phosphates are stored in limited quantities within muscle cells. Anaerobic glycolysis exclusively uses glucose (and glycogen) as a fuel in the absence of oxygen, or more specifically when ATP is needed at rates that exceed those provided by aerobic metabolism. The consequence of such rapid glucose breakdown is the formation of lactic acid (or more appropriately, its conjugate base lactate at biological pH levels). Physical activities that last up to about thirty seconds rely primarily on the former, . Beyond this time both aerobic and anaerobic glycolysis-based metabolic systems begin to predominate. The by-product of anaerobic glycolysis,",
      "maintained. The Phosphogenic (ATP-PC) anaerobic energy pathway restores ATP after its breakdown via Creatine Phosphate stored in skeletal muscle. This pathway is anaerobic because it does not require oxygen to synthesize or utilize ATP. ATP restoration only lasts for approximately the first 30 seconds of exercise. This rapid rate of ATP production is essential at the onset of exercise. The amount of creatine phosphate and ATP stored in the muscle is small, readily available, and is utilized quickly due these two factors. An example of exercise that would primarily use this energy pathway could be weight lifting or performing sprints."
    ]
  ],
  [
    "Coenzymes are organic molecules needed for enzyme activity, while cofactors can be either organic or inorganic substances required for enzyme function, and prosthetic groups are tightly bound coenzymes or cofactors that are essential for enzyme activity.",
    [
      "slightly different definitions of coenzymes, cofactors, and prosthetic groups. Some consider tightly bound organic molecules as prosthetic groups and not as coenzymes, while others define all non-protein organic molecules needed for enzyme activity as coenzymes, and classify those that are tightly bound as coenzyme prosthetic groups. These terms are often used loosely. A 1980 letter in \"Trends in Biochemistry Sciences\" noted the confusion in the literature and the essentially arbitrary distinction made between prosthetic groups and coenzymes group and proposed the following scheme. Here, cofactors were defined as an additional substance apart from protein and substrate that is required for",
      "enzyme can \"grasp\" the coenzyme to switch it between different catalytic centers. Cofactors can be divided into two major groups: organic Cofactors, such as flavin or heme, and inorganic cofactors, such as the metal ions Mg, Cu, Mn, or iron-sulfur clusters. Organic cofactors are sometimes further divided into \"coenzymes\" and \"prosthetic groups\". The term coenzyme refers specifically to enzymes and, as such, to the functional properties of a protein. On the other hand, \"prosthetic group\" emphasizes the nature of the binding of a cofactor to a protein (tight or covalent) and, thus, refers to a structural property. Different sources give",
      "Cofactor (biochemistry) A cofactor is a non-protein chemical compound or metallic ion that is required for an enzyme's activity. Cofactors can be considered \"helper molecules\" that assist in biochemical transformations. The rates at which these happen are characterized by enzyme kinetics. Cofactors can be subclassified as either inorganic ions or complex organic molecules called coenzymes, the latter of which is mostly derived from vitamins and other organic essential nutrients in small amounts. A coenzyme that is tightly or even covalently bound is termed a prosthetic group. Cosubstrates are transiently bound to the protein and will be released at some point,"
    ]
  ],
  [
    "The different types of peripheral neuropathy include mononeuropathy, symmetrical polyneuropathy, mononeuritis multiplex, and polyneuropathy, which can be further classified by the type of nerve predominantly involved or by the underlying cause.",
    [
      "usage, the word \"neuropathy\" (neuro-, \"nervous system\" and -pathy, \"disease of\") without modifier usually means \"peripheral neuropathy\". Neuropathy affecting just one nerve is called \"mononeuropathy\" and neuropathy involving nerves in roughly the same areas on both sides of the body is called \"symmetrical polyneuropathy\" or simply \"polyneuropathy\". When two or more (typically just a few, but sometimes many) separate nerves in disparate areas of the body are affected it is called \"mononeuritis multiplex\", \"multifocal mononeuropathy\", or \"multiple mononeuropathy\". Peripheral neuropathy may be chronic (a long-term condition where symptoms begin subtly and progress slowly) or acute (sudden onset, rapid progress, and",
      "slow resolution). Acute neuropathies demand urgent diagnosis. Motor nerves (that control muscles), sensory nerves, or autonomic nerves (that control automatic functions such as heart rate, body temperature, and breathing) may be affected. More than one type of nerve may be affected at the same time. Peripheral neuropathies may be classified according to the type of nerve predominantly involved, or by the underlying cause. Neuropathy may cause painful cramps, fasciculations (fine muscle twitching), muscle loss, bone degeneration, and changes in the skin, hair, and nails. Additionally, motor neuropathy may cause impaired balance and coordination or, most commonly, muscle weakness; sensory neuropathy",
      "Polyneuropathy Polyneuropathy (poly- + neuro- + -pathy) is damage or disease affecting peripheral nerves (peripheral neuropathy) in roughly the same areas on both sides of the body, featuring weakness, numbness, and burning pain. It usually begins in the hands and feet and may progress to the arms and legs and sometimes to other parts of the body where it may affect the autonomic nervous system. It may be acute or chronic. A number of different disorders may cause polyneuropathy, including diabetes and some types of Guillain\u2013Barr\u00e9 syndrome. Polyneuropathies may be classified in different ways, such as by \"cause\", by \"presentation\","
    ]
  ],
  [
    "is characterized by involuntary flinging motions of the extremities, often involving proximal muscles and sometimes even the facial muscles, with wide amplitudes of motion and continuous random movements on one side of the body (\"Hemiballismus\").",
    [
      "involuntary movements of the proximal parts of the limbs. This activity is almost ceaseless and movements are often complex and combined\". Hemiballismus is usually characterized by involuntary flinging motions of the extremities. The movements are often violent and have wide amplitudes of motion. They are continuous and random and can involve proximal or distal muscles on one side of the body. Some cases even include the facial muscles. It is common for arms and legs to move together. The more a patient is active, the more the movements increase. With relaxation comes a decrease in movements. Physicians can measure the",
      "damage that causes hemiballismus occurs within the basal ganglia, there are still cases that have been documented on which damage to cortical structures has caused hemiballistic movements. Diagnosis of hemiballismus is a clinical one, made with observation during clinical examination. It should be noted that hemiballismus is a clinical sign with a variety of differing etiologies. Therefore a diagnosis underlying this clinical sign should be sought. The observer should note sudden, flinging movements of a limb(s) and occasionally the face. This is commonly unilateral (\"Hemiballismus\"). The movements must be distinguished from other hyperkinetic movement disorders such as tremor (generally more",
      "severity of the disorder by having the patient perform a series of basic, predetermined tasks and counting the hemiballistic movements during a set time session. The physicians then rate the patient on a severity scale. This scale gives scientists and clinicians a way to compare patients and determine the range of the disorder. The name \"hemiballismus\" literally means \"half ballistic\", referring to the violent, flailing movements observed on one side of the body. In examining the causes of hemiballismus, it is important to remember that this disorder is extremely rare. While hemiballismus can result from the following list, just because"
    ]
  ],
  [
    "can be caused by smoking cigarettes, exposure to second-hand smoke, environmental factors such as high levels of PM10 or nitrogen dioxide, and certain medications.",
    [
      "problems such as affecting a person's ability to ensure a consistent sleep, daytime fatigue, difficulty concentrating at work or school, headache, and dizziness. Other more severe but rare complications include fainting, urinary incontinence, and broken ribs, caused by excessive coughing. Possible causes alone or in conjunction can cause the chronic cough which include: Developing a chronic cough can occur from different life style choices. These include smoking cigarettes that the individual smokes themselves or breathes from second-hand exposure. Long-term exposure to smoke can irritate airways and lead to chronic cough and in severe cases lung damage. Other risk-factors include exposure",
      "Europe and USA is 9\u201333% of the population. Chronic cough is more common in those who smoke by threefold compared to people who never smoke. Data analysis shows that exposure to tobacco smoke in a home environment is a risk factor for children due to second hand smoke inhalation. Other causes of chronic cough include higher PM10 concentrations have been related to increase cough and sore throat in children. An increase in nitrogen dioxide has also show a rising association with chronic cough syndrome. A cough that is 4 weeks or longer in duration is considered chronic for children. Most",
      "such as smoking, environmental exposure or medication. From this doctors can opt to use chest radiography if the patient does not smoke, take any angiotensin-converting enzyme inhibitor, or have a persistent cough after the period of medication. A prolonged cough such as one that falls under the chronic cough syndrome can become a medical emergency. Concerning symptoms are: a high fever, coughing of blood, chest pain, difficulty of breathing, appetite loss, excess mucus being coughed, fatigue, night sweats, and unexplained weight loss. By diagnosing which type of cough is present during the chronic cough, individuals can further identify the cause"
    ]
  ],
  [
    "In addition to brushing teeth, it is recommended to use mouthwash as an aid to brushing rather than a replacement, and to clean between the teeth using tools such as floss, flossettes, or interdental brushes.",
    [
      "claiming to kill bacteria that make up plaque or to freshen breath. In their basic form, they are usually recommended for use after brushing but some manufacturers recommend pre-brush rinsing. Dental research has recommended that mouthwash should be used as an aid to brushing rather than a replacement, because the sticky resistant nature of plaque prevents it from being actively removed by chemicals alone, and physical detachment of the sticky proteins is required. Tooth soap cleans gums as well as fissures and pits in teeth using soap. The soap helps remove oils, residue and other contaminants. It is available in",
      "meal. Cleaning between the teeth is called interdental cleaning and is as important as tooth brushing. This is because a toothbrush cannot reach between the teeth and therefore only removes about 50% of plaque off the surface. There are many tools to clean between the teeth, including floss, flossettes, and interdental brushes; it is up to each individual to choose which tool he or she prefers to use. Sometimes white or straight teeth are associated with oral hygiene, but a hygienic mouth may have stained teeth and/or crooked teeth. For appearance reasons, people may seek out teeth whitening and orthodontics.",
      "Oral hygiene Oral hygiene is the practice of keeping one's mouth clean and free of disease and other problems (e.g. bad breath) by regular brushing of the teeth (dental hygiene) and cleaning between the teeth. It is important that oral hygiene be carried out on a regular basis to enable prevention of dental disease and bad breath. The most common types of dental disease are tooth decay (\"cavities\", \"dental caries\") and gum diseases, including gingivitis, and periodontitis. General guidelines suggest brushing twice a day: after breakfast and before going to bed, but ideally the mouth would be cleaned after every"
    ]
  ],
  [
    "The best approach to post-operative analgesic drug use to minimize the risk of prolonged opioid use is to utilize preventive analgesia by reducing activity in the body's pain-signalling system before, during, and immediately after surgery, and to consider alternative pain management options such as non-opioid medications and non-pharmacological interventions.",
    [
      "Preventive analgesia Preventive analgesia is a practice aimed at reducing short- and long-term post-surgery pain. Activity in the body's pain signalling system during surgery produces \"sensitization\"; that is, it increases the intensity of post-operative pain. Reducing activity in the body's pain-signalling system by the use of analgesics before, during and immediately after surgery is thought to reduce subsequent sensitization, and consequently the intensity of post-surgery pain. The types of nerve activity targeted in preventive analgesia include pre-surgery pain, all pain-system activity caused during surgery, and pain produced post-surgery by damage and inflammation. A person's assessment of pain intensity from standard",
      "as pain following surgery). For immediate relief of moderate to severe acute pain opioids are frequently the treatment of choice due to their rapid onset, efficacy and reduced risk of dependence. However a new report showed a clear risk of prolonged opioid use when opioid analgesics are initiated for an acute pain management following surgery or trauma. They have also been found to be important in palliative care to help with the severe, chronic, disabling pain that may occur in some terminal conditions such as cancer, and degenerative conditions such as rheumatoid arthritis. In many cases opioids are a successful",
      "of adult patients develop significant bleeding at this time which may sometimes require surgical intervention. Post-operative pain relief is subject to change. Traditionally, pain relief has been provided by relatively mild narcotic analgesics such as Acetaminophen with codeine, for milder pain, and stronger narcotic analgesics for more severe pain. Recently (January 2011), the FDA reduced the recommended total 24-hour dose because of concern about liver toxicity from the Acetominophen component. An alternative is the use of non-steroidal anti-inflammatory agents, themselves giving rise to concerns that their effect on platelets might increase the risk of post-operative bleeding. In turn, this has"
    ]
  ],
  [
    "Mechanical, chemical, or thermal stimuli can cause nociceptive pain. (Noxious stimulus)",
    [
      "Noxious stimulus A noxious stimulus is \"an actually or potentially tissue damaging event.\" It is a prerequisite for nociception, which itself is a prerequisite for nociceptive pain. Noxious stimuli can either be mechanical (e.g. pinching or other tissue deformation), chemical (e.g. exposure to acid or irritant), or thermal (e.g. high or low temperatures). There are some types of tissue damage that are not detected by any sensory receptors, and thus cannot cause pain. Therefore, not all noxious stimuli are adequate stimuli of nociceptors. The adequate stimuli of nociceptors are termed \"nociceptive stimuli\". A \" nociceptive stimulus\" is defined as \"an",
      "actually or potentially tissue damaging event transduced and encoded by nociceptors.\" Noxious stimulus A noxious stimulus is \"an actually or potentially tissue damaging event.\" It is a prerequisite for nociception, which itself is a prerequisite for nociceptive pain. Noxious stimuli can either be mechanical (e.g. pinching or other tissue deformation), chemical (e.g. exposure to acid or irritant), or thermal (e.g. high or low temperatures). There are some types of tissue damage that are not detected by any sensory receptors, and thus cannot cause pain. Therefore, not all noxious stimuli are adequate stimuli of nociceptors. The adequate stimuli of nociceptors are",
      "the stimulus to be used to map out the intensity of the pain through sensory discrimination. There are two main types of pain that we experience in our bodies: pain caused by damage of body tissue and pain caused by nerve damage. Nociceptive pain serves as a warning or signal for tissue damage and works to preserve the body\u2019s equilibrium and functionality. This pain is signaled by the interworkings of both the peripheral and central nervous systems. Another type of pain, known as neuropathic pain, is caused by a direct problem or disease that affects the nerves in the central"
    ]
  ],
  [
    "Serial peak flow readings over a two week period are considered diagnostic in predicting VAD device failures by trended measurements over time in both rural and metropolitan areas due to the Peak Flow measurement's accuracy as a predictor of mortality and poor prognosis.",
    [
      "faults. VA is a key component of a Condition Monitoring (CM) program, and is often referred to as Predictive Maintenance (PdM). Most commonly VA is used to detect faults in rotating equipment (Fans, Motors, Pumps, and Gearboxes etc.) such as Unbalance, Misalignment, rolling element bearing faults and resonance conditions. VA can use the units of Displacement, Velocity and Acceleration displayed as a time waveform (TWF), but most commonly the spectrum is used, derived from a fast Fourier transform of the TWF. The vibration spectrum provides important frequency information that can pinpoint the faulty component. The fundamentals of vibration analysis can",
      "have major transmission problems in a new vehicle. In practice, the mean time between failures (MTBF, 1/\u03bb) is often reported instead of the failure rate. This is valid and useful if the failure rate may be assumed constant \u2013 often used for complex units / systems, electronics \u2013 and is a general agreement in some reliability standards (Military and Aerospace). It does in this case \"only\" relate to the flat region of the bathtub curve, which is also called the \"useful life period\". Because of this, it is incorrect to extrapolate MTBF to give an estimate of the service lifetime",
      "trended over time in both rural and metropolitan areas both as air quality studies and as studies on asthma due to the Peak Flow measurement's accuracy as a predictor of mortality and poor prognosis. Measurements may be based on 1 second or less but are usually reported as a volume per minute. Electronic devices will sample the flow and multiply the sample volume(Litres) by 60, divided by the sample time(seconds) for a result measured in L/minute : formula_4 The highest of three readings is used as the recorded value of the Peak Expiratory Flow Rate. It may be plotted out"
    ]
  ],
  [
    "Indwelling catheters should be avoided when there are alternatives available and when patients and caregivers discuss potential alternatives with their healthcare providers.",
    [
      "tract infection is the most common type of hospital-acquired infection. Indwelling catheters should be avoided when there are alternatives, and when patients and caregivers discuss alternatives to indwelling urinary catheters with their physicians and nurses then sometimes an alternative may be found. Physicians can reduce their use of indwelling urinary catheters when they follow evidence-based guidelines for usage, such as those published by the Centers for Disease Control and Prevention. All catheterised bladders become colonised with bacteria within 24 hours. This is not infection and is very poorly understood by clinicians. Whilst the presence of a catheter does increase the",
      "bladder to stop it from slipping out. Manufacturers usually produce Foley catheters using silicone or coated natural latex. Coatings include polytetrafluoroethylene, hydrogel, or a silicon elastomer \u2013 the different properties of these surface coatings determine whether the catheter is suitable for 28-day or 3-month indwelling duration. Foley catheters should be used only when indicated, as use increases the risk of catheter-associated urinary tract infection and other adverse effects. Indwelling urinary catheters are most commonly used to assist people who cannot urinate on their own. Indications for using a catheter include providing relief when there is urinary retention, monitoring urine output",
      "body. To prevent overdoses, there is a limit on the number of times a patient can push the button. If a patient pushes the button too much at once, the PCA will reject the request. For the patient's bladder control, a catheter will be inserted so that a patient can urinate without having to move. A catheter is inserted because the patient will not have much free movement to be able to get up and walk to the bathroom. The most common type of catheter used after major surgeries is an indwelling Foley catheter. The indwelling Foley catheter is most"
    ]
  ],
  [
    "Yes, Paget's Disease is associated with elevated alkaline phosphatase levels in the blood and potential hereditary factors.",
    [
      "clinical manifestation of Paget's disease is usually an elevated alkaline phosphatase in the blood. Paget's disease may be diagnosed using one or more of the following tests: Although initially diagnosed by a primary care physician, endocrinologists (internal medicine physicians who specialize in hormonal and metabolic disorders), rheumatologists (internal medicine physicians who specialize in joint and muscle disorders), orthopedic surgeons, neurosurgeons, neurologists, oral and maxillofacial surgeons, and otolaryngologists are generally knowledgeable about treating Paget's disease, and may be called upon to evaluate specialized symptoms. It can sometimes difficult to predict whether a person with Paget's disease, who otherwise has no signs",
      "is an alkaline phosphatase that is located in the placenta and associated with the gonadal and urologic cancers. An alkaline phosphatase isoenzyme test can be done to check for elevated ALP levels. Tissues that contain high levels of ALP include the liver, bile ducts, and bones. Normal levels of ALP range from (44 to 147) U/L (units per liter) and significantly elevated levels may be an indication of conditions such as various types of cancer, bone disease such as Paget disease, liver disease such as hepatitis, blood disorders, or other conditions. Elevated alkaline phosphatase is most commonly caused by liver",
      "Elevated alkaline phosphatase Elevated alkaline phosphatase occurs when levels of alkaline phosphatase (ALP) exceed the reference range. This group of enzymes has a low substrate specificity and catalyzes the hydrolysis of phosphate esters in a basic environment. The major function of alkaline phosphatase is transporting across cell membranes. Alkaline phosphatases are present in many human tissues, including bone, intestine, kidney, liver, placenta and white blood cells. Damage to these tissues causes the release of ALP into the bloodstream. Elevated levels can be detected through a blood test. Elevated alkaline phosphate is associated with certain medical conditions or syndromes (e.g., hyperphosphatasia"
    ]
  ],
  [
    "The first action taken when confirming cardiac arrest is to check for responsiveness and breathing, and if the victim is unresponsive and not breathing, to immediately begin CPR with chest compressions at a rate of 120 beats per minute. (Cardiology; Basic life support)",
    [
      "the risk including long QT syndrome. The initial heart rhythm is most often ventricular fibrillation. The diagnosis is confirmed by finding no pulse. While a cardiac arrest may be caused by heart attack or heart failure these are not the same. Prevention includes not smoking, physical activity, and maintaining a healthy weight. Treatment for cardiac arrest is immediate cardiopulmonary resuscitation (CPR) and, if a shockable rhythm is present, defibrillation. Among those who survive targeted temperature management may improve outcomes. An implantable cardiac defibrillator may be placed to reduce the chance of death from recurrence. In the United States, cardiac arrest",
      "can perform the first three of the four steps. The AHA-recommended steps for resuscitation are known as DRS CABCDE: If the patient is unresponsive and not breathing, the responder begins CPR with chest compressions at a rate of 120 beats per minute in cycles of 30 chest compressions to 2 breaths. If responders are unwilling or unable to perform rescue breathing, they are to perform compression-only CPR, because any attempt at resuscitation is better than no attempt. For children, for whom the main cause of cardiac arrest is from breathing related issues, 5 initial rescue breaths is highly advised followed",
      "informative stickers on the defibrillator column, provides instructions to follow in the case of sudden cardiac arrest: Firstly, check to see if the person who has collapsed is conscious or not. If the victim is unconscious, call 112 immediately. Meanwhile, find the nearest defibrillator, remove it from its box, take it to the victim and open the lid. Then, simply follow the instructions provided by the device and, finally, wait for the police or ambulance to arrive. Time is of the essence, as the chances of surviving sudden cardiac arrest decrease dramatically after the first five minutes. It is therefore"
    ]
  ],
  [
    "Performing cardiac catheterization with the patient awake is safer as the patient can immediately report any discomfort or problems, facilitating rapid correction of any undesirable events.",
    [
      "local anaesthesia such as lidocaine and minimal general sedation, throughout the procedure. Performing the procedure with the patient awake is safer as the patient can immediately report any discomfort or problems and thereby facilitate rapid correction of any undesirable events. Medical monitors fail to give a comprehensive view of the patient's immediate well-being; how the patient feels is often a most reliable indicator of procedural safety. Death, myocardial infarction, stroke, serious ventricular arrhythmia, and major vascular complications each occur in less than 1% of patients undergoing catheterization. However, though the imaging portion of the examination is often brief, because of",
      "invasive physical treatment for angina and some of the complications of severe atherosclerosis, (b) treating heart attacks before complete damage has occurred and (c) research for better understanding of the pathology of coronary artery disease and atherosclerosis. In the early 1960s, cardiac catheterization frequently took several hours and involved significant complications for as many as 2\u20133% of patients. With multiple incremental improvements over time, simple coronary catheterization examinations are now commonly done more rapidly and with significantly improved outcomes. Indications for cardiac catheterization include the following: The patient being examined or treated is usually awake during catheterization, ideally with only",
      "during procedures. Ideal table positioning between the x-ray source and receiver, and radiation monitoring via thermoluminescent dosimetry, are two main ways of reducing a person's exposure to radiation. People with certain comorbidities (people who have more than one condition at the same time) have a higher risk of adverse events during the cardiac catheterization procedure. These comorbidity conditions include aortic aneurysm, aortic stenosis, extensive three-vessel coronary artery disease, diabetes, uncontrolled hypertension, obesity, renal insufficiency, and unstable angina. There are two major categories of cardiac catheterization: Patients without cardiac symptoms or high-risk markers for a heart problem should not have a"
    ]
  ],
  [
    "The redundancy theory of truth states that asserting a statement is true is the same as asserting the statement itself, suggesting that the concept of truth is redundant and merely a conventional word used in discourse.",
    [
      "Redundancy theory of truth According to the redundancy theory of truth (or the disquotational theory of truth), asserting that a statement is true is completely equivalent to asserting the statement itself. For example, asserting the sentence \" 'Snow is white' is true\" is equivalent to asserting the sentence \"Snow is white\". Redundancy theorists infer from this premise that truth is a redundant concept, in other words, that \"truth\" is a mere word that is conventional to use in certain contexts of discourse but not a word that points to anything in reality. The theory is commonly attributed to Frank P.",
      "true that...' is to agree with, accept, or endorse the statement that 'it's raining.'\" According to the redundancy theory of truth, asserting that a statement is true is completely equivalent to asserting the statement itself. For example, making the assertion that \" 'Snow is white' is true\" is equivalent to asserting \"Snow is white\". Redundancy theorists infer from this premise that truth is a redundant concept; that is, it is merely a word that is traditionally used in conversation or writing, generally for emphasis, but not a word that actually equates to anything in reality. This theory is commonly attributed",
      "redundancy or prosentence involved in the statement such as \"that's true.\" Proponents of pragmatic, constructivist and consensus theories would differ with all of these conclusions, however, and instead assert that the second person making the statement \"that's true\" is actually participating in further verifying, constructing and/or achieving consensus on the proposed truth of the matter \u2014 e.g., the proposition that \"it's raining\". Redundancy theory does not apply to representations that are not analogous to sentences and they do not apply to many other things that are commonly judged to be true or otherwise. Consider the analogy between the sentence \"Snow"
    ]
  ],
  [
    "The Responsible Clinician (RC), representative of the nursing staff at the hospital, and the Approved Mental Health Professional (AMHP) are responsible for ensuring the safety and care of patients admitted to a hospital under mental health law in the UK.",
    [
      "member of the multi-disciplinary team responsible for the detained person's care in hospital, known as the Responsible Clinician or RC (usually a consultant psychiatrist), a representative of the nursing staff at the hospital and the Approved Mental Health Professional (AMHP). Additionally, the RC and AMHP (or more frequently the patient's care coordinator) are required to submit written reports on the person's state of health to the Tribunal in advance of the hearing. Sometimes the primary inpatient nurse for the patient may also submit a written report. Each member of the Tribunal is entitled to an equal voice on questions of",
      "that the offender should stay in hospital for the treatment. However, there are some instance where the protection of the public is a key element in issuing a sentence. Under s 41 of the Mental Health Act 1983 offenders who have severe mental health problems, who are considered a danger to the public, can be sent to a secure hospital such as Broadmoor Hospital. These issues can only be done through the Crown Court. The order can be if necessary for an indefinite period of time. If the offender has been issued with an indefinite sentence they can only be",
      "hospital with responsibility for a detained patient. These are usually non-executive members of the board of the relevant National Health Service Trust and appointed lay 'Associate Managers'. Hospital Managers will hear appeals from patients against their detention and review renewals of lengthy detentions. Cases are heard in similar settings to those heard by the Mental Health Review Tribunal outlined below. Mental Health Review Tribunals (MHRTs) hear appeals against detention under the Act. Their members are appointed by the Lord Chancellor and include a doctor, a lawyer and a lay person (i.e. neither a doctor nor a lawyer). Detained persons have"
    ]
  ],
  [
    "increases muscle capacity by enhancing muscle metabolism, improving cardiovascular fitness, increasing oxygen delivery to working muscles, and indirectly affecting strength and flexibility. (Endurance training)",
    [
      "Endurance training Endurance training is the act of exercising to increase endurance. The term endurance training generally refers to training the aerobic system as opposed to the anaerobic system. The need for endurance in sports is often predicated as the need of cardiovascular and simple muscular endurance, but the issue of endurance is far more complex. Endurance can be divided into two categories including: general endurance and specific endurance. It can be shown that endurance in sport is closely tied to the execution of skill and technique. A well conditioned athlete can be defined as, the athlete who executes his",
      "with increased aerobic, or anaerobic capacity. As aerobic/anaerobic capacity increases, general metabolism rises, muscle metabolism is enhanced, haemoglobin rises, buffers in the bloodstream increase, venous return is improved, stroke volume is improved, and the blood bed becomes more able to adapt readily to varying demands. Each of these results of cardiovascular fitness/cardiorespiratory conditioning will have a direct positive effect on muscular endurance, and an indirect effect on strength and flexibility. To facilitate optimal delivery of oxygen to the working muscles, the person needs to train or participate in activities that will build up the energy stores needed for sport. This",
      "coaches to fine tune his training program so that he could recover between swim events that were sometimes several minutes apart. Much similar to blood glucose for diabetes, lower priced lactate measurement devices are now available but in general the lactate measurement approach is still the domain of the professional coach and elite athlete. Endurance training Endurance training is the act of exercising to increase endurance. The term endurance training generally refers to training the aerobic system as opposed to the anaerobic system. The need for endurance in sports is often predicated as the need of cardiovascular and simple muscular"
    ]
  ],
  [
    "The release of acetylcholine at the motor endplate is caused by the influx of calcium ions into the presynaptic neuron terminal, leading to the exocytosis of synaptic vesicles containing acetylcholine.",
    [
      "occupies all of the binding sites on the enzyme in its path. The acetylcholine that reaches the endplate activates ~2,000 acetylcholine receptors, opening their ion channels which permits sodium ions to move into the endplate producing a depolarization of ~0.5 mV known as a miniature endplate potential (MEPP). By the time the acetylcholine is released from the receptors the acetylcholinesterase has destroyed its bound ACh, which takes about ~0.16 ms, and hence is available to destroy the ACh released from the receptors. When the motor nerve is stimulated there is a delay of only 0.5 to 0.8 msec between the",
      "arrival of the nerve impulse in the motor nerve terminals and the first response of the endplate The arrival of the motor nerve action potential at the presynaptic neuron terminal opens voltage-dependent calcium channels and Ca ions flow from the extracellular fluid into the presynaptic neuron's cytosol. This influx of Ca causes several hundred neurotransmitter-containing vesicles to fuse with the presynaptic neuron's cell membrane through SNARE proteins to release their acetylcholine quanta by exocytosis. The endplate depolarization by the released acetylcholine is called an endplate potential (EPP). The EPP is accomplished when ACh binds the nicotinic acetylcholine receptors (nAChR) at",
      "by blocking transmission at the end plate of the neuromuscular junction. Normally, a nerve impulse arrives at the motor nerve terminal, initiating an influx of calcium ions, which causes the exocytosis of synaptic vesicles containing acetylcholine. Acetylcholine then diffuses across the synaptic cleft. It may be hydrolysed by acetylcholine esterase (AchE) or bind to the nicotinic receptors located on the motor end plate. The binding of two acetylcholine molecules results in a conformational change in the receptor that opens the sodium-potassium channel of the nicotinic receptor. This allows and ions to enter the cell and ions to leave the cell,"
    ]
  ],
  [
    "It is imperative for diabetic patients to strictly monitor their glucose levels regularly to maintain glycemic control and prevent long-term complications of the disease.",
    [
      "the monitoring and control of the disease. Failure to maintain a strict regimen of testing can accelerate symptoms of the condition, and it is therefore imperative that any diabetic patient strictly monitor their glucose levels regularly. \"Glycemic control\" is a medical term referring to the typical levels of blood sugar (glucose) in a person with diabetes mellitus. Much evidence suggests that many of the long-term complications of diabetes, especially the microvascular complications, result from many years of hyperglycemia (elevated levels of glucose in the blood). Good glycemic control, in the sense of a \"target\" for treatment, has become an important",
      "development are continuous glucose monitoring methods and offer the advantage of providing additional information to the subject between the conventional finger stick, blood glucose measurements and over time periods where no finger stick measurements are available (i.e. while the subject is sleeping). For patients with diabetes mellitus type 2, the importance of monitoring and the optimal frequency of monitoring are not clear. A 2011 study found no evidence that blood glucose monitoring leads to better patient outcomes in actual practice. One randomized controlled trial found that self-monitoring of blood glucose did not improve glycosylated hemoglobin (HbA1c) among \"reasonably well controlled",
      "and the compliance of diabetic people to their testing regimens is improved. Although the cost of using blood glucose meters seems high, it is believed to be a cost benefit relative to the avoided medical costs of the complications of diabetes. Recent advances include: A continuous glucose monitor determines glucose levels on a continuous basis (every few minutes). A typical system consists of: Continuous glucose monitors measure the concentration of glucose in a sample of interstitial fluid. Shortcomings of CGM systems due to this fact are: Patients therefore require traditional fingerstick measurements for calibration (typically twice per day) and are"
    ]
  ],
  [
    "According to specific clinical needs, modifications of formulas in autogenic training may include different postures, variations in exercise duration, and customized exercises tailored to individual conditions ((clinical effectiveness of autogenic training)).",
    [
      "clinical effectiveness of autogenic training. The principle of passive concentration in autogenic training makes this technique different from other relaxation techniques such as progressive muscle relaxation and biofeedback, in which trainees try to control physiological functions. As in biofeedback, bidirectional change in physiological activity is possible. Autogenic training is classified as a self-hypnotic technique. It is different from hetero-hypnosis, where trance is induced by another individual. Autogenic training emphasizes a trainee's independence and gives control from therapist to the trainee. By this, the need for physiological feedback devices or a hypnotherapist is eliminated. Autogenic training Autogenic training is a desensitization-relaxation",
      "than environmental stimuli. Passiveness refers to allowing sensations to happen and being an observer rather than a manipulator. The training can be performed in different postures: The technique consists of six standard exercises according to Schultz: When a new exercise step is added in autogenic training, the trainee should always concentrate initially on the already learned exercises and then add a new exercise. In the beginning, a new exercise is added for only brief periods. According to the specific clinical needs, different modifications of formulas are used. These modifications can be classified into 3 main types: A study by Spencer",
      "of practice are not similar. Another study from 1958 hypothesizes that autogenic state is between the normal waking state and sleep. It suggests that EEG patterns occurring during autogenic training are similar to electrophysiological changes occurring during initial stages of sleep. Autogenic training has been said to be contraindicated for people with heart conditions (e.g., individuals who have recently experienced myocardial infarction) or psychotic disorders, for children below the age of 5 and the individuals whose symptoms cannot be controlled. Autogenic training has different applications and is used in a variety of pathophysiological conditions, such as bronchial asthma or hypertension,"
    ]
  ],
  [
    "A tracheostomy is a surgical procedure performed to create an opening in the trachea for emergency airway access, prolonged mechanical ventilation, upper airway obstruction, or decreased clearance of secretions.",
    [
      "and several other metrics. This is often one of the responsibilities of an anesthetist during surgery. The epiglottic vallecula is an important anatomical landmark for carrying out this procedure. In an emergency, or when tracheal intubation is deemed impossible, a tracheotomy is often performed to insert a tube for ventilation, usually when needed for particular types of surgery to be carried out so that the airway can be kept open. The provision of the opening via a tracheotomy is called a \"tracheostomy\". Another less invasive method is used when a procedure can be carried out more quickly, or in an",
      "Gabrielle Giffords, and many others. Across movies and TV shows, there are many situations where an emergency procedure is done on an individual's neck to re-establish an airway. The most common procedure is a cricothyrotomy (or \"crike\"), which is an incision through the skin and cricothyroid membrane. This is often confused or misnamed as a tracheotomy (or \"trach\") and vice versa. However, they are quite different based on location of the opening and length of time the alternate airway is needed. Tracheotomy Tracheotomy (, ), or tracheostomy, is a surgical procedure which consists of making an incision on the anterior",
      "Greek \u03c4\u03c1\u03b1\u03c7\u03b5\u03af\u03b1 \"trache\u00eda\"). The word \"tracheostomy\", including the root \"stom-\" (from Greek \u03c3\u03c4\u03cc\u03bc\u03b1 \"st\u00f3ma\") meaning \"mouth,\" refers to the making of a semi-permanent or permanent opening, and to the opening itself. Some sources offer different definitions of the above terms. Part of the ambiguity is due to the uncertainty of the intended permanence of the stoma at the time it is created. There are four indications for a tracheostomy: 1. Emergency airway access. 2. Airway access for prolonged mechanical ventilation. 3. Functional or mechanical upper airway obstruction. 4. Decreased/incompetent clearance of tracheobronchial secretions. In the chronic (long term) setting, indications"
    ]
  ],
  [
    "The source of hematemesis is generally the upper gastrointestinal tract, typically above the suspensory muscle of duodenum.",
    [
      "potential specific causes; and (4) eliciting the presence of serious associated diseases that might adversely affect the outcome.The information obtained is especially helpful in identifying situations that require aggressive management. https://www.sciencedirect.com/topics/veterinary-science-and-veterinary-medicine/hematemesis Hematemesis Hematemesis or haematemesis is the vomiting of blood. The source is generally the upper gastrointestinal tract, typically above the suspensory muscle of duodenum. Patients can easily confuse it with hemoptysis (coughing up blood), although the latter is more common. Hematemesis \"is always an important sign\". Causes can be: Hematemesis is treated as a medical emergency. The most vital distinction is whether there is blood loss sufficient to cause",
      "Hematemesis Hematemesis or haematemesis is the vomiting of blood. The source is generally the upper gastrointestinal tract, typically above the suspensory muscle of duodenum. Patients can easily confuse it with hemoptysis (coughing up blood), although the latter is more common. Hematemesis \"is always an important sign\". Causes can be: Hematemesis is treated as a medical emergency. The most vital distinction is whether there is blood loss sufficient to cause shock. Correct management is required in such conditions. It is required to perform all tests such as endoscopy before medication. A platelet test is also an important test in such conditions.",
      "diagnosis is easier when the patient has hematemesis. In the absence of hematemesis, 40% to 50% of patients in the emergency department with GI bleeding have an upper source Upper gastrointestinal bleeding Upper gastrointestinal bleeding is gastrointestinal bleeding in the upper gastrointestinal tract, commonly defined as bleeding arising from the esophagus, stomach, or duodenum. Blood may be observed in vomit (hematemesis) or in altered form in the stool (melena). Depending on the severity of the blood loss, there may be symptoms of insufficient circulating blood volume and shock. As a result, upper gastrointestinal bleeding is considered a medical emergency and"
    ]
  ],
  [
    "The criteria used to distinguish rheumatoid arthritis from other medical conditions include clinical signs and symptoms, imaging studies such as X-rays, laboratory testing, and assessment of synovial inflammation through ultrasound imaging.",
    [
      "damage that treatment is meant to avoid. In clinical practice, the following criteria apply: Several other medical conditions can resemble RA, and need to be distinguished from it at the time of diagnosis: Rarer causes which usually behave differently but may cause joint pains: Sometimes arthritis is in an undifferentiated stage (i.e. none of the above criteria is positive), even if synovitis is witnessed and assessed with ultrasound imaging. Many tools can be used to monitor remission in rheumatoid arthritis. formula_1 From this, the disease activity of the affected person can be classified as follows: It is not always a",
      "rheumatoid arthritis is not clear, it is believed to involve a combination of genetic and environmental factors. The underlying mechanism involves the body's immune system attacking the joints. This results in inflammation and thickening of the joint capsule. It also affects the underlying bone and cartilage. The diagnosis is made mostly on the basis of a person's signs and symptoms. X-rays and laboratory testing may support a diagnosis or exclude other diseases with similar symptoms. Other diseases that may present similarly include systemic lupus erythematosus, psoriatic arthritis, and fibromyalgia among others. The goals of treatment are to reduce pain, decrease",
      "in assessing the degree of synovial inflammation as they can show vascular signals of active synovitis. This is important, since in the early stages of RA, the synovium is primarily affected, and synovitis seems to be the best predictive marker of future joint damage. When RA is clinically suspected, a physician may test for rheumatoid factor (RF) and anti-citrullinated protein antibodies (ACPAs measured as anti-CCP antibodies). It is positive in 75-85%, but a negative RF or CCP antibody does not rule out RA, rather, the arthritis is called \"seronegative\", which is in about 15-25% of people with RA. During the"
    ]
  ],
  [
    "It is important to consider providing internet-based information sources for individuals aged over 65 because they are more likely to seek health information, make purchases, and obtain religious information online, and may participate in online communities more than younger people.",
    [
      "from context\u2014 that is, from individuals' educational and professional priorities \u2014 rather than from their age. The findings also indicate that people still rely on other people to get information, especially those within their personal networks. Individuals make decisions based on convenience within the context of their information needs and the situation within which the need arises. Web-based functionalities are the expected norm for services by many people, as the sources that people chose are overwhelmingly digital. Interviewees mentioned search engines and social media sites far more often than physical places, when looking for information. This reliance on digital spaces",
      "being undervalued. Although studies justify the notion that people often do not contribute to online communities, some research shows that older adults are more likely to participate in online communities than younger people because different generations tend to use the internet differently. For example, \"older adults are more likely to seek health information, make purchases, and obtain religious information, but less likely to watch videos, download music, play games, and read blogs online\". This is perhaps due in part to the fact that some online communities cater to older generations. The content of the website often determines what age group",
      "free. People with portable devices, like tablets or smartphones, were significantly more likely to subscribe to digital news content. Additionally, younger people\u201425- to 34-year-olds\u2014are more willing to pay for digital news than older people across all countries. This is in line with the Pew Research Center\u2019s finding in a survey of U.S. Americans that the Internet is a leading source of news for people less than 50. Online newspaper An online newspaper is the online version of a newspaper, either as a stand-alone publication or as the online version of a printed periodical. Going online created more opportunities for newspapers,"
    ]
  ],
  [
    "The main symptoms of carpal tunnel syndrome include pain, numbness, and tingling in the thumb, index finger, middle finger, and the thumb side of the ring fingers, which typically start gradually and may extend up the arm, with weak grip strength and possible muscle wasting at the base of the thumb over time.",
    [
      "Carpal tunnel syndrome Carpal tunnel syndrome (CTS) is a medical condition due to compression of the median nerve as it travels through the wrist at the carpal tunnel. The main symptoms are pain, numbness and tingling in the thumb, index finger, middle finger and the thumb side of the ring fingers. Symptoms typically start gradually and during the night. Pain may extend up the arm. Weak grip strength may occur, and after a long period of time the muscles at the base of the thumb may waste away. In more than half of cases, both sides are affected. Risk factors",
      "Carpal tunnel syndrome is often associated with repetitive motions of the wrist and fingers; jobs like typists, pianists, and meat cutters are at particularly high risk. The tough flexor retinaculum along with the rest of the carpal tunnel cannot expand, putting pressure on the median nerve running through the carpal tunnel with the flexor tendons of the wrist. This results in the symptoms of carpal tunnel syndrome. Symptoms of carpal tunnel syndrome include tingling sensations and muscle weakness in the palm and lateral side of the hand and palm. It is possible that the syndrome may extend and radiate up",
      "splinting or steroid injections. Carpal tunnel syndrome is a common disorder of the hand. This disorder results from compression of an important nerve in the wrist. Disorders like diabetes mellitus, thyroid or rheumatoid arthritis can narrow the tunnel and cause impingement of the nerve. Carpal tunnel syndrome also occurs in people who overuse their hand or perform repetitive actions like using a computer key board, a cashiers machine or a musical instrument. When the nerve is compressed, it can result in disabling symptoms like numbness, tingling, or pain in the middle three fingers. As the condition progresses, it can lead"
    ]
  ],
  [
    "The Krebs cycle generates 1 GTP, which is further converted to ATP, along with 3 NADH and 1 FADH2 per cycle, resulting in a total of 10 NADH, 2 FADH2, and 2 ATP molecules for each glucose molecule.",
    [
      "and the Krebs cycle. While the ATP count is glycolysis and the Krebs cycle is two ATP molecules, the electron transport chain contributes, at most, twenty-eight ATP molecules. A contributing factor is due to the energy potentials of NADH and FADH. As they are brought from the initial process, glycolysis, to the electron transport chain, the energy stored in them are now utilized. A second contributing factor is that cristae, the inner membranes of mitochondria, increase the surface area and therefore the amount of proteins in the membrane that assist in the synthesis of ATP. Along the electron transport chain,",
      "the cytoplasm, the viscous fluid that fills living cells, where the glycolytic reactions take place. The citric acid cycle, also known as the Krebs cycle or the TCA (tricarboxylic acid) cycle is an 8-step process that takes the pyruvate generated by glycolysis and generates 4 NADH, FADH2, and GTP, which is further converted to ATP. It is only in step 5, where GTP is generated, by succinyl-CoA synthetase, and then converted to ATP, that ADP is used (GTP + ADP \u2192 GDP + ATP). Oxidative phosphorylation produces 26 of the 30 equivalents of ATP generated in cellular respiration by transferring",
      "cycle. The citric acid cycle is an 8-step process involving 18 different enzymes and co-enzymes. During the cycle, acetyl-CoA (2 carbons) + oxaloacetate (4 carbons) yields citrate (6 carbons), which is rearranged to a more reactive form called isocitrate (6 carbons). Isocitrate is modified to become \u03b1-ketoglutarate (5 carbons), succinyl-CoA, succinate, fumarate, malate, and, finally, oxaloacetate. The net gain of high-energy compounds from one cycle is 3 NADH, 1 FADH, and 1 GTP; the GTP may subsequently be used to produce ATP. Thus, the total yield from 1 glucose molecule (2 pyruvate molecules) is 6 NADH, 2 FADH, and 2"
    ]
  ],
  [
    "One potential complication of IV therapy involves leakage of vesicant medications into surrounding tissues, causing tissue damage known as extravasation.",
    [
      "Extravasation (intravenous) Extravasation is the leakage of intravenously (IV) infused potentially damaging medications into the extravascular tissue around the site of infusion. The leakage can occur through brittle veins in the elderly, through previous venipuncture access, or through direct leakage from wrongly positioned venous access devices. When the leakage is not of harmful consequence it is known as infiltration. Extravasation of medication during intravenous therapy is an adverse event related to therapy that, depending on the medication, amount of exposure, and location, can potentially cause serious injury and permanent harm, such as tissue necrosis. Milder consequences of extravasation include irritation,",
      "In cases of tissue necrosis, surgical debridement and reconstruction may be necessary. The following steps are typically involved in managing extravasation: List of vesicant and irritant medications: Extravasation (intravenous) Extravasation is the leakage of intravenously (IV) infused potentially damaging medications into the extravascular tissue around the site of infusion. The leakage can occur through brittle veins in the elderly, through previous venipuncture access, or through direct leakage from wrongly positioned venous access devices. When the leakage is not of harmful consequence it is known as infiltration. Extravasation of medication during intravenous therapy is an adverse event related to therapy that,",
      "the skin as well as localized swelling or edema. It is treated by removing the intravenous access device and elevating the affected limb so that the collected fluids can drain away. Sometimes injections of hyaluronidase can be used to speed the dispersal of the fluid/drug. Infiltration is one of the most common adverse effects of IV therapy and is usually not serious unless the infiltrated fluid is a medication damaging to the surrounding tissue, most commonly a vesicant or chemotherapeutic agent, in which case it is called extravasation and extensive necrosis can occur. This occurs when fluids are given at"
    ]
  ],
  [
    "Saliva contains antibacterial compounds such as lysozyme, peroxidase, defensins, cystatins, and IgA.",
    [
      "aid wound healing. Saliva contains cell-derived tissue factor, and many compounds that are antibacterial or promote healing. Salivary tissue factor, associated with microvesicles shed from cells in the mouth, promotes wound healing through the extrinsic blood coagulation cascade. The enzymes lysozyme and peroxidase, defensins, cystatins and an antibody, IgA, are all antibacterial. Thrombospondin and some other components are antiviral. A protease inhibitor, secretory leukocyte protease inhibitor, is present in saliva and is both antibacterial and antiviral, and a promoter of wound healing. Nitrates that are naturally found in saliva break down into nitric oxide on contact with skin, which will",
      "really lose all of it because it is constantly being replenished by our own saliva. Bacteria can then reattach and start to grow and the cycle continues. Bacteria and its growth are two of the principal components of oral ecology. Though bacteria play a major role in oral ecology, another key part is saliva. Saliva keeps the ecosystem of the mouth in balance. It contains its own bacterial enzymes that are beneficial to our health. An example of these are lysozomes. These antibacterial agents in saliva kill bacteria in our mouths and protect from potentially dangerous diseases. In addition, saliva",
      "to promote the remineralisation of the tooth by strengthening the enamel with calcium and phosphate minerals. Saliva can prevent microbial growth based on the elements it contains. For example, lactoferrin in saliva binds naturally with iron. Since iron is a major component of bacterial cell walls, removal of iron breaks down the cell wall, which in turn breaks down the bacteria. Antimicrobial peptides such as histatins inhibit the growth of Candida albicans and Streptococcus mutans. Salivary Immunoglobulin A serves to aggregate oral bacteria such as S. mutans and prevent the formation of dental plaque. Saliva can encourage soft tissue repair"
    ]
  ],
  [
    "The genetic material of an organism is the genome, which consists of DNA (or RNA in RNA viruses), including genes, noncoding DNA, mitochondrial DNA, and chloroplast DNA.",
    [
      "Genome In the fields of molecular biology and genetics, a genome is the genetic material of an organism. It consists of DNA (or RNA in RNA viruses). The genome includes both the genes (the coding regions) and the noncoding DNA, as well as mitochondrial DNA and chloroplast DNA. The study of the genome is called genomics. The term \"genome\" was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name is a blend of the words \"gene\" and \"chromosome\". However, see omics for a more thorough discussion. A few",
      "against a future where genomic information fuels prejudice and extreme class differences between those who can and can't afford genetically engineered children. Genome In the fields of molecular biology and genetics, a genome is the genetic material of an organism. It consists of DNA (or RNA in RNA viruses). The genome includes both the genes (the coding regions) and the noncoding DNA, as well as mitochondrial DNA and chloroplast DNA. The study of the genome is called genomics. The term \"genome\" was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary",
      "the environment and the genome. Human genome <section begin=lead />The human genome is the complete set of nucleic acid sequences for humans, encoded as DNA within the 23 chromosome pairs in cell nuclei and in a small DNA molecule found within individual mitochondria. These are usually treated separately as the nuclear genome, and the mitochondrial genome. Human genomes include both protein-coding DNA genes and noncoding DNA. Haploid human genomes, which are contained in germ cells (the egg and sperm gamete cells created in the meiosis phase of sexual reproduction before fertilization creates a zygote) consist of three billion DNA base"
    ]
  ],
  [
    "Pharmacological agents that alter neurotransmitters in the brain, such as increasing dopamine and decreasing norepinephrine, can influence fatigue development in endurance athletes by either enhancing or impairing exercise performance depending on the specific neurotransmitter manipulated.",
    [
      "in endurance athletes. Existing experimental methods have provided enough evidence to suggest that variations in synaptic serotonin, noradrenaline, and dopamine are significant drivers of central nervous system fatigue. An increased synaptic dopamine concentration in the CNS is strongly ergogenic (promotes exercise performance). An increased synaptic serotonin or noradrenaline concentration in the CNS impairs exercise performance. Manipulation of norepinephrine suggests it may actually play a role in creating a feeling of fatigue. Reboxetine, an NRI inhibitor, decreased time to fatigue and increased subjective feelings of fatigue. This may be explained by a paradoxical decrease in adrenergic activity lead by feedback mechanisms.",
      "5-HT. Controlling central nervous system fatigue can help scientists develop a deeper understanding of fatigue as a whole. Numerous approaches have been taken to manipulate neurochemical levels and behavior. In sports, nutrition plays a large role in athletic performance. In addition to fuel, many athletes consume performance-enhancing drugs including stimulants in order to boost their abilities. Amphetamine is a stimulant that has been found to improve both physical and cognitive performance. Amphetamine blocks the reuptake of dopamine and norepinephrine, which delays the onset of fatigue by increasing the amount of dopamine, despite the concurrent increase in norepinephrine, in the central",
      "Central nervous system fatigue Central nervous system fatigue, or central fatigue, is a form of fatigue that is associated with changes in the synaptic concentration of neurotransmitters within the central nervous system (CNS; including the brain and spinal cord) which affects exercise performance and muscle function and cannot be explained by peripheral factors that affect muscle function. In healthy individuals, central fatigue can occur from prolonged exercise and is associated with neurochemical changes in the brain, primarily involving serotonin (5-HT), noradrenaline, and dopamine. Central fatigue plays an important role in endurance sports and also highlights the importance of proper nutrition"
    ]
  ],
  [
    "Type IIX fibers are typically the first to fatigue in all-out exercises such as sprinting.",
    [
      "II and 55% type I fibers. People at the higher end of any sport tend to demonstrate patterns of fiber distribution e.g. endurance athletes show a higher level of type I fibers. Sprint athletes, on the other hand, require large numbers of type IIX fibers. Middle distance event athletes show approximately equal distribution of the two types. This is also often the case for power athletes such as throwers and jumpers. It has been suggested that various types of exercise can induce changes in the fibers of a skeletal muscle. It is thought that if you perform endurance type events",
      "of properties. These types of properties\u2014while they are partly dependent on the properties of individual fibers\u2014tend to be relevant and measured at the level of the motor unit, rather than individual fiber. Traditionally, fibers were categorized depending on their varying color, which is a reflection of myoglobin content. Type I fibers appear red due to the high levels of myoglobin. Red muscle fibers tend to have more mitochondria and greater local capillary density. These fibers are more suited for endurance and are slow to fatigue because they use oxidative metabolism to generate ATP (adenosine triphosphate). Less oxidative type II fibers",
      "expressed quickly, which means greater power generation. In regard to the apparent conversion of type I to type II fibres through training, leading experts attest that this is in actual fact the alteration of the muscle fibres size and ability to produce force; the percentage of fibres that are type I or type II does not change: In the context of rock climbing, and an isometric exercise preceding a plyometric one, Steve Maisch considers that one of the training goals of complex training is to operate at a level of fatigue thereby encouraging the body to learn to recruit from"
    ]
  ],
  [
    "The main fate of lactate that leaves muscle and enters the circulation is to be transported in the bloodstream and eventually excreted through perspiration, breath, or urine.",
    [
      "intracellular lactate is released into the blood, maintenance of electroneutrality of the blood requires that a cation be released into the blood, as well. This can reduce blood pH. Glycolysis coupled with lactate production is neutral in the sense that it does not produce excess hydrogen cations; however, pyruvate production does produce them. Lactate production is buffered intracellularly, e.g. the lactate-producing enzyme, lactate dehydrogenase, binds one hydrogen cation per pyruvate molecule converted. When such buffer systems become saturated, cells will transport lactate into the bloodstream. Hypoxia certainly causes both a buildup of lactate and acidification, and lactate is therefore a",
      "the cell via diffusion into the blood stream, where it is transported in three ways: HO also diffuses out of the cell into the blood stream, from where it is excreted in the form of perspiration, water vapor in breath, or urine from the kidneys. Water, along with some dissolved solutes, are removed from blood circulation in the nephrons of the kidney and eventually excreted as urine. The products of fermentation can be processed different ways, depending on the cellular conditions. Lactic acid tends to accumulate in the muscles, which causes pain of the muscle and joint as well as",
      "pyruvate and ultimately to glucose, which can travel back to the peripheral tissues, completing the Cori cycle. Thus, lactate has traditionally considered a toxic metabolic byproduct that could give rise to fatigue and muscle pain during anaerobic respiration. Lactate can be thought of essentially as payment for \"oxygen debt\", defined by Hill and Lupton as the \"total amount of oxygen used, after cessation of exercise in recovery there from\". Highly malignant tumors rely heavily on aerobic glycolysis (metabolism of glucose to lactic acid even under presence of oxygen; Warburg effect) and thus need to efflux lactic acid via MCTs to"
    ]
  ],
  [
    "The most rapid method to resynthesize ATP during exercise is through the phosphogenic (ATP-PC) anaerobic energy pathway, which restores ATP via Creatine Phosphate stored in skeletal muscle within the first 30 seconds of exercise.",
    [
      "Anaerobic glycolysis Anaerobic glycolysis is the transformation of glucose to lactate when limited amounts of oxygen (O) are available. Anaerobic glycolysis is only an effective means of energy production during short, intense exercise, providing energy for a period ranging from 10 seconds to 2 minutes. The anaerobic glycolysis (lactic acid) system is dominant from about 10\u201330 seconds during a maximal effort. It replenishes very quickly over this period and produces 2 ATP molecules per glucose molecule or about 5% of glucose's energy potential (38 ATP molecules). The speed at which ATP is produced is about 100 times that of oxidative",
      "maintained. The Phosphogenic (ATP-PC) anaerobic energy pathway restores ATP after its breakdown via Creatine Phosphate stored in skeletal muscle. This pathway is anaerobic because it does not require oxygen to synthesize or utilize ATP. ATP restoration only lasts for approximately the first 30 seconds of exercise. This rapid rate of ATP production is essential at the onset of exercise. The amount of creatine phosphate and ATP stored in the muscle is small, readily available, and is utilized quickly due these two factors. An example of exercise that would primarily use this energy pathway could be weight lifting or performing sprints.",
      "and intensity and by the individual's cardiorespiratory fitness level. Three exercise energy systems can be selectively recruited, depending on the amount of oxygen available, as part of the cellular respiration process to generate the ATP for the muscles. They are ATP, the anaerobic system and the aerobic system. ATP is the usable form of chemical energy for muscular activity. It is stored in most cells, particularly in muscle cells. Other forms of chemical energy, such as those available from food, must be transformed into ATP before they can be utilized by the muscle cells. Since energy is released when ATP"
    ]
  ],
  [
    "Auditory sensory substitution systems can be utilized in arm assessments by transforming information from visual or tactile sensors into auditory signals to compensate for the lack of another sensory modality, allowing patients to perceive holding and manipulating objects with their robotic arm.",
    [
      "development, the information from these arms can be used by patients to perceive that they are holding and manipulating objects while their robotic arm actually accomplishes the task. Auditory sensory substitution systems like the tactile sensory substitution systems aim to use one sensory modality to compensate for the lack of another in order to gain a perception of one that is lacking. With auditory sensory substitution, visual or tactile sensors detect and store information about the external environment. This information is then transformed by brain-machine interfaces into auditory signals that are relayed via the auditory receptors to the brain. Auditory",
      "Sensory substitution Sensory substitution is a change of the characteristics of one sensory modality into stimuli of another sensory modality. A sensory substitution system consists of three parts: a sensor, a coupling system, and a stimulator. The sensor records stimuli and gives them to a coupling system which interprets these signals and transmits them to a stimulator. In case the sensor obtains signals of a kind not originally available to the bearer it is a case of sensory augmentation. Sensory substitution concerns human perception and the plasticity of the human brain; and therefore, allows us to study these aspects of",
      "neuroscience more through neuroimaging. It is hoped that sensory substitution systems can help people by restoring their ability to perceive certain defective sensory modality by using sensory information from a functioning sensory modality. The idea of sensory substitution was introduced in the '60s by Paul Bach-y-Rita as a means of using one sensory modality, mainly taction, to gain environmental information to be used by another sensory modality, mainly vision. Thereafter, the entire field was discussed by Chaim-Meyer Scheff in \"Experimental model for the study of changes in the organization of human sensory information processing through the design and testing of"
    ]
  ],
  [
    "Exercise can impact blood glucose concentration during intermittent high intensity exercise by increasing glucose disposal, which can lead to lower plasma glucose levels and fatigue, particularly in situations of hyperglycemia, and can enhance insulin sensitivity for up to 12-24 hours post-exercise.",
    [
      "stimulates various other intracellular enzymes geared towards increasing energy supply and decreasing all anabolic, or energy requiring, cell functions. Plasma glucose is said to be maintained when there is an equal rate of glucose appearance (entry into the blood) and glucose disposal (removal from the blood). In the healthy individual, the rates of appearance and disposal are essentially equal during exercise of moderate intensity and duration; however, prolonged exercise or sufficiently intense exercise can result in an imbalance leaning towards a higher rate of disposal than appearance, at which point glucose levels fall producing the onset of fatigue. Rate of",
      "Exercise for diabetes: Exercise is a particularly potent tool for glucose control in those who have diabetes mellitus. In a situation of elevated blood glucose (hyperglycemia), moderate exercise can induce greater glucose disposal than appearance, thereby decreasing total plasma glucose concentrations. As stated above, the mechanism for this glucose disposal is independent of insulin, which makes it particularly well-suited for people with diabetes. In addition, there appears to be an increase in sensitivity to insulin for approximately 12\u201324 hours post-exercise. This is particularly useful for those who have type II diabetes and are producing sufficient insulin but demonstrate peripheral resistance",
      "exercise\" The autoregulation of the brain\u2019s blood supply is impaired particularly in warm environments In adults, exercise depletes the plasma glucose available to the brain: short intense exercise (35 min ergometer cycling) can reduce brain glucose uptake by 32%. At rest, energy for the adult brain is normally provided by glucose but the brain has a compensatory capacity to replace some of this with lactate. Research suggests that this can be raised, when a person rests in a brain scanner, to about 17%, with a higher percentage of 25% occurring during hypoglycemia. During intense exercise, lactate has been estimated to"
    ]
  ],
  [
    "One of the most common reasons for stoma formation is deficiencies, which can cause stomatitis.",
    [
      "and participatory plant breeding to find the best suited species such as heat and drought resistant crop varieties that could naturally evolve to the change in the face of food security challenges. Stoma In botany, a stoma (plural \"stomata\"), also called a stomata (plural \"stomates\") (from Greek \u03c3\u03c4\u03cc\u03bc\u03b1, \"mouth\"), is a pore, found in the epidermis of leaves, stems, and other organs, that facilitates gas exchange. The pore is bordered by a pair of specialized parenchyma cells known as guard cells that are responsible for regulating the size of the stomatal opening. The term is usually used collectively to refer",
      "cause deficiencies, which in turn causes stomatitis. Examples include tropical sprue. Aphthous stomatitis (canker sores) is the recurrent appearance of mouth ulcers in otherwise healthy individuals. The cause is not completely understood, but it is thought that the condition represents a T cell mediated immune response which is triggered by a variety of factors. The individual ulcers (aphthae) recur periodically and heal completely, although in the more severe forms new ulcers may appear in other parts of the mouth before the old ones have finished healing. Aphthous stomatitis is one of the most common diseases of the oral mucosa, and",
      "the inhibition of the SPCH, resulting in increased number of stomata. Environmental and hormonal factors can affect stomatal development. Light increases stomatal development in plants; while, plants grown in the dark have a lower amount of stomata. Auxin represses stomatal development by affecting their development at the receptor level like the ERL and TMM receptors. However, a low concentration of auxin allows for equal division of a guard mother cell and increases the chance of producing guard cells. Different classifications of stoma types exist. One that is widely used is based on the types that Julien Joseph Vesque introduced in"
    ]
  ],
  [
    "A surgical support plays a role in assisting with operations by helping to account for swabs and instruments, providing materials to the surgical team, positioning the patient on the operating table, setting up equipment, and acting as a link between the surgical team and the rest of the hospital.",
    [
      "can also be the first assistant to the surgeon. Swabs and instruments are all accounted for by the ODP to check that nothing has been left inside the patient. ODPs may sometimes work in a circulating role during the surgical stage of a patient's care. In this role, they give extra materials to the sterilised person, help position the patient on the operating table, and plan ahead to supply what the surgical team may need. They may also set up extra equipment and act as a link between the surgical team and the rest of the hospital. When the operation",
      "care in the OR. Models created by these different approaches may have a large impact in future surgical innovations, whether for planning, intra-operative or post-operative purposes. This idea of describing the surgical procedure as a sequence of tasks was first introduced by MacKenzie et al. (2001). and formalised in Jannin et al., 2001. The term Surgical Process (SP) has been defined as a set of one or more linked procedures or activities that collectively realise a surgical objective within the context of an organisational structure defining functional roles and relationships. This term is generally used to describe the steps involved",
      "Do-support Do\"-support (or do\"-insertion), in English grammar, is the use of the auxiliary verb \"do\", including its inflected forms \"does\" and \"did\", to form negated clauses and questions as well as other constructions in which subject\u2013auxiliary inversion is required. The verb \"do\" can be used as an auxiliary even in simple declarative sentences, and it usually serves to add emphasis, as in \"I \"did\" shut the fridge.\" However, in the negated and inverted clauses referred to above, it is used because the rules of English syntax permit these constructions only when an auxiliary is present. It is not idiomatic in"
    ]
  ],
  [
    "A larger collection bag may be used at night for patients with a urostomy to allow them to sleep through the night.",
    [
      "(to drain non-specific post-surgical abdominal fluid). In the hospital, the SP tube and external staples will be removed, after several days. The remaining two tubes will each be connected to collection bags worn on each leg and the patient is usually sent home like this. After sufficient healing, and another doctor's visit, the tube will be removed from the stoma. The patient will now begin to catheterize the pouch every two hours. Since one other tube will still be in place, patients can still sleep through the night, since a larger collection bag is attached to that tube at night",
      "pouch adhered to the abdomen to store urine as it is created inside of the body. The urine is drained through a small stoma that is barely visible. This can result in a better body image and broader clothing options. Also, there will not be the worry of an external urostomy appliance coming loose and leaking. The Indiana pouch will require sterile catheters to insert into the stoma to drain the urine every 3-4 hours. To avoid a possible fatal infection, a new sterile intermittent catheter should be used each time and not reused. As with the urostomy appliances, the",
      "modality; must rule out urinary obstruction prior to use.) If an incontinence is due to overflow incontinence, in which the bladder never empties completely, or if the bladder cannot empty because of poor muscle tone, past surgery, or spinal cord injury, a catheter may be used to empty the bladder. A catheter is a tube that can be inserted through the urethra into the bladder to drain urine. Catheters may be used once in a while or on a constant basis, in which case the tube connects to a bag that is attached to the leg. If a long-term (or"
    ]
  ],
  [
    "Aerobic and anaerobic metabolism systems work concurrently in the generation of ATP, with glycolysis serving as the initial process that can continue along the aerobic respiration pathway in the presence of oxygen or be restricted to anaerobic respiration when oxygen is not available.",
    [
      "in such a way that the energy released by the one is always used by the other. Three methods can synthesize ATP: Aerobic and anaerobic systems usually work concurrently. When describing activity, it is not a question of which energy system is working, but which predominates. The term metabolism refers to the various series of chemical reactions that take place within the body. Aerobic refers to the presence of oxygen, whereas anaerobic means with series of chemical reactions that does not require the presence of oxygen. The ATP-CP series and the lactic acid series are anaerobic, whereas the oxygen series",
      "molecules of ATP is therefore one of the most important biochemical pathways found in living organisms. Glycolysis, which means \u201csugar splitting,\u201d is the initial process in the cellular respiration pathway. Glycolysis can be either an aerobic or anaerobic process. When oxygen is present, glycolysis continues along the aerobic respiration pathway. If oxygen is not present, then ATP production is restricted to anaerobic respiration. The location where glycolysis, aerobic or anaerobic, occurs is in the cytosol of the cell. In glycolysis, a six-carbon glucose molecule is split into two three-carbon molecules called pyruvate. These carbon molecules are oxidized into NADH and",
      "bonds\". The hydrolysis of ATP into ADP and inorganic phosphate releases 30.5 kJ/mol of enthalpy, with a change in free energy of 3.4 kJ/mol. The energy released by cleaving either a phosphate (P) or pyrophosphate (PP) unit from ATP at standard state of 1 M are: These abbreviated equations can be written more explicitly (R = adenosyl): With a typical intracellular concentration of 1\u201310 mM, ATP is abundant. The dephosphorylation of ATP and rephosphorylation of ADP and AMP occur repeatedly in the course of aerobic metabolism. ATP can be produced by a number of distinct cellular processes; the three main"
    ]
  ],
  [
    "The athlete would expend 900 kJ of energy during 5 minutes of exercise with an average steady-rate oxygen uptake of 3.0 l/min.",
    [
      "in kcal/day or in J/s = W (1000 kcal/d ~ 48.5 W). This input power can be determined by measuring oxygen uptake, or in the long term food consumption, assuming no change of weight. This includes the power needed just for living, called the basal metabolic rate BMR or roughly the resting metabolic rate. The required food can also be calculated by dividing the output power by the muscle efficiency. This is 18-26%. From the example above, if a 70 kg person is cycling at 15 km/h by expending 60 W and a muscular efficiency of 20% is assumed, roughly",
      "of oxygen consumed while sitting at rest, and is equal to 3.5 ml oxygen per kilogram body weight per minute. In other words, a means of expressing energy cost of physical activity as a multiple of the resting rate. For instance; walking on level ground at about 6 km/h or carrying groceries up a flight of stairs expends about 4 METs of activity. Generally, >7 METs of activity tolerance is considered excellent while <4 is considered poor for surgical candidates. Determining one's functional capacity can elucidate the degree of surgical risk one might undertake for procedures that risk blood loss,",
      "of oxygen consumption is representative of an increase in energy expenditure. VO2 is often measured in absolute terms (ex. Liters/min), but in weight bearing activities, such as running, body mass can have a profound influence on energy expenditure. As a result, it is common to express energy expenditure as the rate of oxygen consumption in relation to body mass (ex. ml/kg/min). Though some recent data may suggest otherwise, it is traditionally well accepted that a strong linear relationship exists between the rate of oxygen consumption and running speed (see figure 1), with energy expenditure increasing with increasing running speed. It"
    ]
  ],
  [
    "Irreversible damage to tissues can occur in as little as 3-4 minutes after the onset of tissue ischemia.",
    [
      "the blood, insufficient blood supply causes tissue to become starved of oxygen. In the highly metabolically active tissues of the heart and brain, irreversible damage to tissues can occur in as little as 3\u20134 minutes at body temperature. The kidneys are also quickly damaged by loss of blood flow (renal ischemia). Tissues with slower metabolic rates may undergo irreversible damage after 20 minutes. Clinical manifestations of acute limb ischemia (which can be summarized as the \"six P's\") include pain, pallor, pulseless, paresthesia, paralysis, and poikilothermia. Without immediate intervention, ischemia may progress quickly to tissue necrosis and gangrene within a few",
      "blood supply. The dead tissue is surrounded by a zone of potentially reversible ischemia that progresses to become a full-thickness \"transmural\" infarct. The initial \"wave\" of infarction can take place over 3\u20134 hours. These changes are seen on gross pathology and cannot be predicted by the presence or absence of Q waves on an ECG. The position, size and extent of an infarct depends on the affected artery, totality of the blockage, duration of the blockage, the presence of collateral blood vessels, oxygen demand, and success of interventional procedures. Tissue death and myocardial scarring alter the normal conduction pathways of",
      "chiefly through necrosis, and do not grow back. A collagen scar forms in their place. When an artery is blocked, cells lack oxygen, needed to produce ATP in mitochondria. ATP is required for the maintenance of electrolyte balance, particularly through the Na/K ATPase. This leads to an ischemic cascade of intracellular changes, necrosis and apoptosis of affected cells. Cells in the area with the worst blood supply, just below the inner surface of the heart (endocardium), are most susceptible to damage. Ischemia first affects this region, the \"subendocardial\" region, and tissue begins to die within 15\u201330 minutes of loss of"
    ]
  ],
  [
    "Exocrine glands are classified based on how their products are secreted as apocrine glands, holocrine glands, or merocrine glands.",
    [
      "glands contain a glandular portion and a duct portion, the structures of which can be used to classify the gland. Exocrine glands are named apocrine glands, holocrine glands, or merocrine glands based on how their products are secreted. Exocrine gland Exocrine glands are glands that produce and secrete substances onto an epithelial surface by way of a duct. Examples of exocrine glands include sweat, salivary, mammary, ceruminous, lacrimal, sebaceous, and mucous. Exocrine glands are one of two types of glands in the human body, the other being endocrine glands, which secrete their products directly into the bloodstream. The liver and",
      "directly onto the apical surface. The glands in this group can be divided into three groups: The type of secretory product of exocrine glands may also be one of three categories: Adenosis is any disease of a gland. The diseased gland has abnormal formation or development of glandular tissue which is sometimes tumorous. Gland A gland is a group of cells in an animal's body that synthesizes substances (such as hormones) for release into the bloodstream (endocrine gland) or into cavities inside the body or its outer surface (exocrine gland). Every gland is formed by an ingrowth from an epithelial",
      "Exocrine gland Exocrine glands are glands that produce and secrete substances onto an epithelial surface by way of a duct. Examples of exocrine glands include sweat, salivary, mammary, ceruminous, lacrimal, sebaceous, and mucous. Exocrine glands are one of two types of glands in the human body, the other being endocrine glands, which secrete their products directly into the bloodstream. The liver and pancreas are both exocrine and endocrine glands; they are exocrine glands because they secrete products\u2014bile and pancreatic juice\u2014into the gastrointestinal tract through a series of ducts, and endocrine because they secrete other substances directly into the bloodstream. Exocrine"
    ]
  ],
  [
    "In many heraldic traditions, only a king of arms has the authority to grant armorial bearings and sometimes certify genealogies and noble titles.",
    [
      "British orders of chivalry have their own kings of arms: King of Arms King of Arms is the senior rank of an officer of arms. In many heraldic traditions, only a king of arms has the authority to grant armorial bearings and sometimes certify genealogies and noble titles. In other traditions, the power has been delegated to other officers of similar rank. In England, the authority to grant a coat of arms is subject to the formal approval of the Earl Marshal in the form of a warrant. In jurisdictions such as the Republic of Ireland the authority to grant",
      "King of Arms King of Arms is the senior rank of an officer of arms. In many heraldic traditions, only a king of arms has the authority to grant armorial bearings and sometimes certify genealogies and noble titles. In other traditions, the power has been delegated to other officers of similar rank. In England, the authority to grant a coat of arms is subject to the formal approval of the Earl Marshal in the form of a warrant. In jurisdictions such as the Republic of Ireland the authority to grant armorial bearings has been delegated to a chief herald that",
      "as the rules and protocols governing the design and description, or \"blazoning\" of arms, and the precedence of their bearers. As early as the late thirteenth century, certain heralds in the employ of monarchs were given the title \"King of Heralds\", which eventually became \"King of Arms.\" In the earliest period, arms were assumed by their bearers without any need for heraldic authority. However, by the middle of the fourteenth century, the principle that only a single individual was entitled to bear a particular coat of arms was generally accepted, and disputes over the ownership of arms seems to have"
    ]
  ],
  [
    "The symptoms and signs of phlebitis include pain, swelling, tenderness, redness, and possibly systemic symptoms such as nausea, dizziness, chills, coagulopathy, and shock; if an IV site shows signs of infection, it should be assessed by a healthcare professional for appropriate treatment.",
    [
      "pain). Rarely, blood is coughed up. Painful or inflamed joints can occur when the joints are involved. Septic shock can also arise. This presents with low blood pressure, increased heart rate, decreased urine output and an increased rate of breathing. Some cases will also present with meningitis, which will typically manifest as neck stiffness, headache and sensitivity of the eyes to light. Liver enlargement and spleen enlargement can be found, but are not always associated with liver or spleen abscesses. Other signs and symptoms that may occur: The bacteria causing the thrombophlebitis are anaerobic bacteria that are typically normal components",
      "determine treatment. Phlebitis was first described by the Scottish surgeon John Hunter in 1784. Phlebitis Phlebitis or venitis is the inflammation of a vein, usually in the legs. It most commonly occurs in superficial veins. Phlebitis often occurs in conjunction with thrombosis and is then called thrombophlebitis or superficial thrombophlebitis. Unlike deep vein thrombosis, the probability that superficial thrombophlebitis will cause a clot to break up and be transported in pieces to the lung is very low. There is usually a slow onset of a tender red area along the superficial veins on the skin. A long, thin red area",
      "can cause pain, swelling, hemorrhagic bleb formation, and ecchymosis. Any swelling is usually not particularly severe, but it can involve all of the affected limb, as well as the trunk. Systemic symptoms can include nausea, dizziness, chills, coagulopathy, and shock. Klauber (1997) includes an account of a man who had been bitten on the first joint of the index finger of the right hand, with only a single fang penetrating. Although the bite was described as no more painful than a pin prick, a doctor was seen within about 25 minutes, and 10 cc of antivenin were administered. Within 2.5"
    ]
  ],
  [
    "The hormone primarily responsible for fluid regulation in the body is antidiuretic hormone (ADH).",
    [
      "hormone (ADH), is synthesized and regulates fluid loss by manipulating the urinary tract. This pathway allows water reabsorption within the body and decreases the amount of water lost through perspiration. ADH has the greatest effect on blood pressure within the body. Under normal circumstances, ADH will regulate the blood pressure and increase or decrease the blood volume when needed. However, when stress becomes chronic, homeostatic regulation of blood pressure is lost. Vasopressin is released and causes a static increase in blood pressure. This increase in blood pressure under stressful conditions ensures that muscles receive the oxygen that they need to",
      "time to adjust to the new water intake level. This can cause someone who drinks a lot of water to become dehydrated more easily than someone who routinely drinks less. These outputs are in balance with the input of ~2500 ml/day. The body's homeostatic control mechanisms, which maintain a constant internal environment, ensure that a balance between fluid gain and fluid loss is maintained. The anti-diuretic hormones vasopressin (ADH) and aldosterone play a major role in this. If the body is becoming fluid-\"deficient\", this will be sensed by osmoreceptors in the vascular organ of lamina terminalis and subfornical organ. These",
      "a narrow range (homeostasis of body fluid osmolality); conditions can cause that feedback system to malfunction (pathophysiology); and the consequences of the malfunction of that system on the size and solute concentration of the fluid compartments. There is a hypothalamic-renal feedback system which normally maintains the concentration of the serum sodium within a narrow range. This system operates as follows: in some of the cells of the hypothalamus, there are osmoreceptors which respond to an elevated serum sodium in body fluids by signalling the posterior pituitary gland to secrete antidiuretic hormone (ADH), (also called vasopressin). ADH then enters the bloodstream"
    ]
  ],
  [
    "The National Academy of Medicine recommends a minimum intake of 130 g of carbohydrate per day. (Source: National Academy of Medicine)",
    [
      "carbohydrate diet less than 45%, and a high carbohydrate diet more than 45%. The National Academy of Medicine recommends a minimum intake of 130 g of carbohydrate per day. The FAO and WHO similarly recommend that the majority of dietary energy come from carbohydrates. A popular misconception driving adoption of the diet for weight loss, is that by reducing carbohydrate intake dieters can in some way avoid weight gain from the calories in other macronutrients. However any weight loss resulting from a low-carbohydrate diet probably comes merely from reduced overall calorie intake. A category of diets is known as low-glycemic-index",
      "into the subject. The American Academy of Family Physicians defines low-carbohydrate diets as diets that restrict carbohydrate intake to 20 to 60 grams per day, typically less than 20% of caloric intake. A 2016 review of low-carbohydrate diets classified diets with 50g of carbohydrate per day (less than 10% of total calories) as \"very low\" and diets with 40% of calories from carbohydrates as \"mild\" low-carbohydrate diets. In a 2015 review Richard D. Feinman and colleagues proposed that a very low carbohydrate diet had less that 10% caloric intake from carbohydrate, a low carbohydrate diet less than 26%, a medium",
      "g) \u2013 each day. The recommended maximum daily intake of sodium \u2013 the amount above which health problems appear \u2013 is 2,300 milligrams per day for adults, about 1 teaspoon of salt (5.9 g). The recommended adequate intake of sodium is 1,500 milligrams (3.9 g salt) per day, and people over 50 need even less.\" Reference Daily Intake The Reference Daily Intake (RDI) is the daily intake level of a nutrient that is considered to be sufficient to meet the requirements of 97\u201398% of healthy individuals in every demographic in the United States. While developed for the US population, it"
    ]
  ],
  [
    "The inactivation of the catalytic machinery during recovery from exercise affects phosphocreatine resynthesis by limiting the availability of ATP and creatine phosphate for the synthesis of ATP via the phosphogenic anaerobic energy pathway.",
    [
      "maintained. The Phosphogenic (ATP-PC) anaerobic energy pathway restores ATP after its breakdown via Creatine Phosphate stored in skeletal muscle. This pathway is anaerobic because it does not require oxygen to synthesize or utilize ATP. ATP restoration only lasts for approximately the first 30 seconds of exercise. This rapid rate of ATP production is essential at the onset of exercise. The amount of creatine phosphate and ATP stored in the muscle is small, readily available, and is utilized quickly due these two factors. An example of exercise that would primarily use this energy pathway could be weight lifting or performing sprints.",
      "that combines phosphocreatine and adenosine diphosphate (ADP) into ATP and creatine. This resource is short lasting because oxygen is required for the resynthesis of phosphocreatine via mitochondrial creatine kinase. Therefore, under anaerobic conditions, this substrate is finite and only lasts between approximately 10 to 30 seconds of high intensity work. Fast glycolysis, however, can function for approximately 2 minutes prior to fatigue, and predominately uses intracellular glycogen as a substrate. Glycogen is broken down rapidly via glycogen phosphorylase into individual glucose units during intense exercise. Glucose is then oxidized to pyruvate and under anaerobic conditions is reduced to lactic acid.",
      "a breakdown of the second chemical bond and regeneration of the enzyme. The proposed chemical mechanism does not depend on the concentration of the substrates or products in the medium. However, a shift in their concentration mainly causes free energy changes in the first and final steps of the reactions () and () due to the changes in the free energy content of every molecule, whether S or P, in water solution. This approach is in accordance with the following mechanism of muscle contraction. The final step of ATP hydrolysis in skeletal muscle is the product release caused by the"
    ]
  ],
  [
    "During transcription, RNA polymerase makes a copy of a gene from the DNA to produce messenger RNA (mRNA).",
    [
      "begins with transcription, and ultimately ends in degradation. During its life, an mRNA molecule may also be processed, edited, and transported prior to translation. Eukaryotic mRNA molecules often require extensive processing and transport, while prokaryotic mRNA molecules do not. A molecule of eukaryotic mRNA and the proteins surrounding it are together called a messenger RNP. Transcription is when RNA is made from DNA. During transcription, RNA polymerase makes a copy of a gene from the DNA to mRNA as needed. This process is similar in eukaryotes and prokaryotes. One notable difference, however, is that eukaryotic RNA polymerase associates with mRNA-processing",
      "the transcription produces messenger RNA (mRNA); the mRNA, in turn, serves as a template for the protein's synthesis through translation. Alternatively, the transcribed gene may encode for non-coding RNA such as microRNA, ribosomal RNA (rRNA), transfer RNA (tRNA), or enzymatic RNA molecules called ribozymes. Overall, RNA helps synthesize, regulate, and process proteins; it therefore plays a fundamental role in performing functions within a cell. In virology, the term may also be used when referring to mRNA synthesis from an RNA molecule (i.e., RNA replication). For instance, the genome of a negative-sense single-stranded RNA (ssRNA -) virus may be template for",
      "cell membrane. Once within the cell, they must then leave the cell's transport mechanism to take action within the cytoplasm, which houses the ribosomes that direct manufacture of proteins. Messenger RNA Messenger RNA (mRNA) is a large family of RNA molecules that convey genetic information from DNA to the ribosome, where they specify the amino acid sequence of the protein products of gene expression. RNA polymerase transcribes primary transcript mRNA (known as pre-mRNA) into processed, mature mRNA. This mature mRNA is then translated into a polymer of amino acids: a protein, as summarized in the central dogma of molecular biology."
    ]
  ],
  [
    "Oerel in Lower Saxony, Germany has a history of belonging to the Prince-Archbishopric of Bremen, which later became part of the Duchy of Bremen before being annexed by various entities including the Kingdom of Westphalia and France, ultimately becoming part of the Kingdom of Hanover and later the Stade Region in 1823.",
    [
      "Oerel Oerel is a municipality in the district of Rotenburg, in Lower Saxony, Germany. Oerel belonged to the Prince-Archbishopric of Bremen, established in 1180. In 1648 the Prince-Archbishopric was transformed into the Duchy of Bremen, which was first ruled in personal union by the Swedish Crown - interrupted by a Danish occupation (1712\u20131715) - and from 1715 on by the Hanoverian Crown. In 1807 the ephemeral Kingdom of Westphalia annexed the Duchy, before France annexed it in 1810. In 1813 the Duchy was restored to the Electorate of Hanover, which - after its upgrade to the Kingdom of Hanover in",
      "1814 - incorporated the Duchy in a real union and the Ducal territory, including Oerel, became part of the new Stade Region, established in 1823. Oerel Oerel is a municipality in the district of Rotenburg, in Lower Saxony, Germany. Oerel belonged to the Prince-Archbishopric of Bremen, established in 1180. In 1648 the Prince-Archbishopric was transformed into the Duchy of Bremen, which was first ruled in personal union by the Swedish Crown - interrupted by a Danish occupation (1712\u20131715) - and from 1715 on by the Hanoverian Crown. In 1807 the ephemeral Kingdom of Westphalia annexed the Duchy, before France annexed",
      "whole or in part, belong today to the state of Lower Saxony: the Bishopric of Osnabr\u00fcck, the Bishopric of M\u00fcnster, the County of Bentheim, the County of Hoya, the Principality of East Frisia, the Principality of Verden, the County of Diepholz, the County of Oldenburg, the County of Schaumburg and the County of Spiegelberg. At the same time a distinction was made with the eastern part of the old Saxon lands from the central German principalities later called Upper Saxony for dynastic reasons. (see also \u2192 Electorate of Saxony, History of Saxony). The close historical links between the domains of"
    ]
  ],
  [
    "Taking the short-acting bronchodilator first before other treatments during clinic night is important because it provides quick or \"rescue\" relief from acute bronchoconstriction, helping to control and prevent symptoms effectively (\"Bronchodilator; Bronchodilator\").",
    [
      "night. Bronchodilators can be used 15 minutes before PD is done to maximise its benefits. The most affected area is drained first to prevent infected secretions spilling into healthy lung. Drainage time varies but each position requires 10 minutes. If an entire hemithorax is involved, each lobe has to drained individual but a maximum of three position per session is considered sufficient. The procedure is discontinued if the patient complains of headache, discomfort, dizziness, palpitations, fatigue and dyspnea. Patients may be dyspnic after the various manuovers as the head-down position increases the work of breathing, reduces tidal volume and decreases",
      "quick or \"rescue\" relief from acute bronchoconstriction. Long-acting bronchodilators help to control and prevent symptoms. The three types of prescription bronchodilating drugs are \u03b2(\"beta two\")-adrenergic agonists (short- and long-acting), anticholinergics (short-acting), and theophylline (long-acting). These are quick-relief or \"rescue\" medications that provide quick, temporary relief from asthma symptoms or flare-ups. These medications usually take effect within 20 minutes or less, and can last from four to six hours. These inhaled medications are best for treating sudden and severe or new asthma symptoms. Taken 15 to 20 minutes ahead of time, these medications can also prevent asthma symptoms triggered by exercise",
      "Bronchodilator A bronchodilator is a substance that dilates the bronchi and bronchioles, decreasing resistance in the respiratory airway and increasing airflow to the lungs. Bronchodilators may be endogenous (originating naturally within the body), or they may be medications administered for the treatment of breathing difficulties. They are most useful in obstructive lung diseases, of which asthma and chronic obstructive pulmonary disease are the most common conditions. Although this remains somewhat controversial, they might be useful in bronchiolitis and bronchiectasis. They are often prescribed but of unproven significance in restrictive lung diseases. Bronchodilators are either short-acting or long-acting. Short-acting medications provide"
    ]
  ],
  [
    "Intention tremor typically occurs at a low frequency below 5 Hz.",
    [
      "resulting in the development of a tremor. A working diagnosis is made from a neurological examination and evaluation. Parts of a complete examination include a physical examination, MRI, patient history, and electrophysiological and accelerometric studies. A diagnosis of solely intention tremor can only be made if the tremor is of low frequency (below 5 Hz) and without the presence of any resting tremors. Electrophysiological studies can be useful in determining frequency of the tremor, and accelerometric studies quantify tremor amplitude. MRI is used to locate damage to and degradation of the cerebellum that may be causing the intention tremor. Focal",
      "Intention tremor Intention tremor, also known as cerebellar tremor, is a dyskinetic disorder characterized by a broad, coarse, and low frequency (below 5 Hz) tremor. The amplitude of an intention tremor increases as an extremity approaches the endpoint of deliberate and visually guided movement (hence the name intention tremor). An intention tremor is usually perpendicular to the direction of movement. When experiencing an intention tremor, one often overshoots or undershoots their target, a condition known as dysmetria. Intention tremor is the result of dysfunction of the cerebellum, particularly on the same side as the tremor in the lateral zone, which",
      "includes intention tremors, among other symptoms. This disease affects the proximal muscles of the head, shoulders, and neck. Tremors of this disease occur at frequencies of 2\u20134 Hz or more. Intention tremor is also known to be associated with infections, West Nile virus, rubella, H. influenza, rabies, and varicella. A variety of poisons have been shown to cause intention tremor, including mercury, methyl bromide, and phosphine. In addition, vitamin deficiencies have been linked to intention tremor, especially deficiency in vitamin E. Pharmacological agents such as anti-arrhythmic drugs, anti-epileptic agents, benzodiazepine, cyclosporine, lithium, neuroleptics, and stimulants have been known to cause"
    ]
  ],
  [
    "During exercise or muscle contraction, glucose is transported into muscle cells primarily through the translocation of GLUT4 transporters to the plasma membrane, allowing for facilitated diffusion of glucose into the cell (GLUT4).",
    [
      "of either exercise or muscle contraction. During exercise, the body needs to convert glucose to ATP to be used as energy. As G-6-P concentrations decrease, hexokinase becomes less inhibited, and the glycolytic and oxidative pathways that make ATP are able to proceed. This also means that muscle cells are able to take in more glucose as its intracellular concentrations decrease. In order to increase glucose levels in the cell, GLUT4 is the primary transporter used in this facilitated diffusion. Although muscle contractions function in a similar way and also induce the translocation of GLUT4 into the plasma membrane, the two",
      "show that GLUT1 plays a larger role in cardiac muscles than it does in skeletal muscles. GLUT4, however, is still believed to be the primary transporter for glucose. Much like in other tissues, GLUT4 also responds to insulin signaling, and is transported into the plasma membrane to facilitate the diffusion of glucose into the cell. Adipose tissue, commonly known as fat, is a depository for energy in order to conserve metabolic homeostasis. As the body takes in energy in the form of glucose, some is expended, and the rest is stored as glycogen primarily in the liver, muscle cells, or",
      "They include molecules such as adenosine triphosphate (ATP), glycogen and creatine phosphate. ATP binds to the myosin head and causes the \u2018ratchetting\u2019 that results in contraction according to the sliding filament model. Creatine phosphate stores energy so ATP can be rapidly regenerated within the muscle cells from adenosine diphosphate (ADP) and inorganic phosphate ions, allowing for sustained powerful contractions that last between 5\u20137 seconds. Glycogen is the intramuscular storage form of glucose, used to generate energy quickly once intramuscular creatine stores are exhausted, producing lactic acid as a metabolic byproduct. Substrate shortage is one of the causes of metabolic fatigue."
    ]
  ],
  [
    "Oxygen sensors are used in medical applications such as anesthesia monitors, respirators, oxygen concentrators, and pulse oximeters to measure oxygen levels in the blood and ensure proper respiratory function.",
    [
      "measure respiration or production of oxygen and use a different approach. Oxygen sensors are used in oxygen analyzers, which find extensive use in medical applications such as anesthesia monitors, respirators and oxygen concentrator so. Oxygen sensors are also used in hypoxic air fire prevention systems to continuously monitor the oxygen concentration inside the protected volumes. There are many different ways of measuring oxygen. These include technologies such as zirconia, electrochemical (also known as galvanic), infrared, ultrasonic, paramagnetic, and very recently, laser methods. Automotive oxygen sensors, colloquially known as O (\"\u014d two\") sensors, make modern electronic fuel injection and emission control",
      "to the patient's finger or ear. The sensor uses light to estimate how much oxygen is in the blood. A pulse oximeter works by beaming red and infrared light through capillaries. The amount of red and infrared light transmitted provides an approximate measure of oxygen in the blood. The oximeter reading is based on the color of the blood: oxygenated blood is a brighter red than deoxygenated blood, which appears as bluish purple. This test measures the precise levels of oxygen and carbon dioxide in the blood. A blood sample is drawn from an artery, typically in the wrist. A",
      "mellitus type 2, obesity and cancer. Other fields of application are e.g. sports science and the connection between mitochondrial function and aging. The usual equipment includes a seal-able metabolic chamber, an oxygen sensor, and devices for data recording, stirring, thermostatisation and a way to introduce chemicals into the chamber. As described above for whole-animal respirometry the choice of materials is very important. Plastic materials are not suitable for the chamber because of their oxygen storage capacity. When plastic materials are unavoidable (e.g. for o-rings, coatings of stirrers, or stoppers) polymers with a very low oxygen permeability (like PVDF as opposed"
    ]
  ],
  [
    "It is important to measure blood pressure in an arm that is above the level of the heart to ensure accurate readings and to account for variations in blood pressure throughout the day.",
    [
      "on the floor and with limbs uncrossed. The blood pressure cuff should always be against bare skin, as readings taken over a shirt sleeve are less accurate. The same arm should be used for all measurements. During the reading, the arm that is used should be relaxed and kept at heart level, for example by resting it on a table. Since blood pressure varies throughout the day, home measurements should be taken at the same time of day. A Joint Scientific Statement From the American Heart Association, American Society of Hypertension, and Preventive Cardiovascular Nurses Association on home monitoring in",
      "and some can detect irregular heartbeats. In humans, the cuff is normally placed smoothly and snugly around an upper arm, at roughly the same vertical height as the heart while the subject is seated with the arm supported. Other sites of placement depend on species, it may include the flipper or tail. It is essential that the correct size of cuff is selected for the patient. Too small a cuff results in too high a pressure, while too large a cuff results in too low a pressure. For clinical measurements it is usual to measure and record both arms in",
      "Blood pressure Blood pressure (BP) is the pressure of circulating blood on the walls of blood vessels. Used without further specification, \"blood pressure\" usually refers to the pressure in large arteries of the systemic circulation. Blood pressure is usually expressed in terms of the systolic pressure (maximum during one heartbeat) over diastolic pressure (minimum in between two heartbeats) and is measured in millimeters of mercury (mmHg), above the surrounding atmospheric pressure. Blood pressure is one of the vital signs, along with respiratory rate, heart rate, oxygen saturation, and body temperature. Normal resting blood pressure in an adult is approximately systolic,"
    ]
  ],
  [
    "Cyriax's Rule helps diagnose musculoskeletal lesions by determining the type of tissue dysfunction based on the presence of pain with active and passive range of motion in specific directions.",
    [
      "of simple objective clinical exams that would effectively diagnose soft tissue musculoskeletal lesions. His collected results, after many years of trial and error, coalesced into a set of systematic simple clinical exams for each joint and a treatment system for the soft tissue lesions around each joint. Cyriax's Rule states that pain with both active range of motion and passive range of motion in the same direction points to inert tissue dysfunction(ligament,capsular, cartilage). Pain with active range of motion in one direction and pain with passive range of motion in the opposite direction signal contractile tissue dysfunction. Cyriax's papers are",
      "cysts can be identified with nuclear imaging combined with specific tracers, such as sestamibi. Identification of muscular degeneration or lack of reflex can occur through clinical testing of deep tendon reflexes, or via photomotogram (an achilles tendon reflex test). Fine needle aspiration (FNA) can be used to biopsy bone lesions, once found on an X-ray or other scan. Such tests can be vital in diagnosis and can also prevent unnecessary treatment and invasive surgery. Conversely, FNA biopsy of tumors of the parathyroid gland is not recommended for diagnosing parathyroid carcinoma and may in fact be harmful, as the needle can",
      "aspiration is used to aid diagnosis of a cystic lesion, e.g. fluid aspirate from a radicular cyst may appear straw colored and display shimmering due to cholesterol content. Almost always, the cyst lining is sent to a pathologist for histopathologic examination after it has been surgically removed. This means that the exact diagnosis of the type of cyst is often made in retrospect. When treatment is required, this is usually by surgical removal of the cyst. There are four ways in which cysts are managed: The prognosis depends upon the type, size and location of a cyst. Most cysts are"
    ]
  ],
  [
    "Male patients are advised to take their own electric razor to the hospital to avoid irritation to the skin and difficulty in caring for remaining hair removal marks after shaving.",
    [
      "charging device. The design of some electric shavers has been criticized for their protruding cutting design as being conducive to hair loss. Electric razor The electric razor (also known as the dry razor, electric shaver, or simply shaver) is a razor with a rotating or oscillating blade. The electric razor usually does not require the use of shaving cream, soap, or water. The razor may be powered by a small DC motor, which is either powered by batteries or mains electricity. Many modern ones are powered using rechargeable batteries. Alternatively, an electro-mechanical oscillator driven by an AC-energized solenoid may be",
      "been a technology that imitates the shape of an eyebrow to match a trend that pursues \"naturalness\". The male contouring injection surgery, which considers the skeleton of a man, is characterized by the effect of losing weight in the face as well as providing clear facial features. For men who are often annoyed with their shaves, their beards are worn through laser procedures. The use of a razor can be irritating to the skin, and the remaining hair removal marks after the shave are difficult to care so they often seek laser procedures. When asked, 'What are the criteria for",
      "using running water or an included cleaning machine, reducing cleaning effort. Some patience is necessary when starting to use a razor of this type, as the skin usually takes some time to adjust to the way that the electric razor lifts and cuts the hairs. Moisturizers designed specifically for electric shaving are available. Since at least the mid-1960s, battery-operated electric razors have been available using rechargeable batteries sealed inside the razor's case, previously nickel cadmium or, more recently, nickel metal hydride. Some modern shavers use Lithium-ion batteries that do not suffer from memory effect. Sealed battery shavers either have built-in"
    ]
  ],
  [
    "The pancreas secretes insulin in the digestive system.",
    [
      "gland in the digestive system. It is both an endocrine gland and an exocrine gland. The endocrine part secretes insulin when the blood sugar becomes high; insulin moves glucose from the blood into the muscles and other tissues for use as energy. The endocrine part releases glucagon when the blood sugar is low; glucagon allows stored sugar to be broken down into glucose by the liver in order to re-balance the sugar levels. The pancreas produces and releases important digestive enzymes in the pancreatic juice that it delivers to the duodenum. The pancreas lies below and at the back of",
      "Pancreas The pancreas is an organ of the digestive system and endocrine system of vertebrates. In humans, it is located in the abdominal cavity behind the stomach. The pancreas is a mixed gland, having both an endocrine and an exocrine function. As an endocrine gland, it secretes into the blood several important hormones, including insulin, glucagon, somatostatin, and pancreatic polypeptide. As an exocrine gland, it secretes pancreatic juice into the duodenum through the pancreatic duct. This juice contains bicarbonate, which neutralizes acid entering the duodenum from the stomach; and digestive enzymes, which break down carbohydrates, proteins, and lipids in ingested",
      "the culinary name of sweetbread. Pancreas The pancreas is an organ of the digestive system and endocrine system of vertebrates. In humans, it is located in the abdominal cavity behind the stomach. The pancreas is a mixed gland, having both an endocrine and an exocrine function. As an endocrine gland, it secretes into the blood several important hormones, including insulin, glucagon, somatostatin, and pancreatic polypeptide. As an exocrine gland, it secretes pancreatic juice into the duodenum through the pancreatic duct. This juice contains bicarbonate, which neutralizes acid entering the duodenum from the stomach; and digestive enzymes, which break down carbohydrates,"
    ]
  ],
  [
    "Healthcare providers can ensure a large proportion of the drug reaches the lower airways when teaching a patient inhaler technique by emphasizing correct inhaler technique, including proper hand positioning and actuation of the device, to optimize medication delivery (Metered-dose inhaler).",
    [
      "are sold as a complete unit or the individual canister as a refill prescription. While MDIs are commonly used in the treatment of lung-based disorders, their use requires dexterity to complete the required sequential steps to achieve application of these devices. Incorrect completion of one or more steps in the use of an MDI can substantially reduce the delivery of the administrated medication and consequently its effectiveness and safety. Numerous studies have demonstrated that between 50-100% of patients do not use their inhaler devices correctly, with patients often unaware that they are using their inhaled medication incorrectly. Incorrect inhaler technique",
      "prevent contamination. To use the inhaler the patient presses down on the top of the canister, with their thumb supporting the lower portion of the actuator. Actuation of the device releases a single metered dose of the formulation which contains the medication either dissolved or suspended in the propellant. Breakup of the volatile propellant into droplets, followed by rapid evaporation of these droplets, results in the generation of an aerosol consisting of micrometer-sized medication particles that are then inhaled. Metered-dose inhalers are only one type of inhaler, but they are the most commonly used type. The replacement of chlorofluorocarbons propellants",
      "of inhalations to be taken daily may vary between individuals based on the severity of the disease. Once started, patients seldom follow treatment regimens as directed, and seldom complete the course of treatment. In respect of hypertension, 50% of patients completely drop out of care within a year of diagnosis. Persistence with first-line single antihypertensive drugs is extremely low during the first year of treatment. As far as lipid-lowering treatment is concerned, only one third of patients are compliant with at least 90% of their treatment. Intensification of patient care interventions (e.g. electronic reminders, pharmacist-led interventions, healthcare professional education of"
    ]
  ],
  [
    "A patient should maintain an upright posture during a tilt table test in order to induce symptoms associated with a drop in blood pressure or cardiac arrhythmia, which are indicative of a positive test result.",
    [
      "or isoproterenol, to create further susceptibility to the test. In all cases, the patient is instructed not to move. Symptoms, blood pressure, pulse, electrocardiogram, and sometimes blood oxygen saturation are recorded. The test either ends when the patient faints or develops other significant symptoms, or after a set period (usually from 20 to 45 minutes, depending on the facility or individualized protocol). A tilt table test is considered positive if the patient experiences symptoms associated with a drop in blood pressure or cardiac arrhythmia. A normal person's blood pressure will not drop dramatically while standing, because the body will compensate",
      "non-invasively. The table then creates a change in posture from lying to standing. Before taking the test, the patient may be instructed to fast for a period before the test will take place and to stop taking any medications. On the day of the tilt table test, an intravenous line may be placed in case the patient needs to be given medications quickly; however, this may influence the results of the test and may only be indicated in particular circumstances. More recently, most investigators monitor cerebral perfusion pressure using mean flow velocity recording with transcranial Doppler ultrasound in supine horizontal",
      "position, during and after head-up tilt. An 18 MHz ultrasound transducer is placed on the temporal bone above the cheekbone, using headgear to hold the probe in place. A tilt table test can be done in different ways and be modified for individual circumstances. In some cases, the patient will be strapped to a tilt table lying flat and then tilted or suspended completely or almost completely upright (as if standing). Most of the time, the patient is suspended at an angle of 60 to 80 degrees. Sometimes, the patient will be given a drug, such as Glyceryl trinitrate (nitroglycerin)"
    ]
  ],
  [
    "Indications for a tracheostomy include the need for long-term mechanical ventilation and removal of tracheal secretions in patients such as comatose patients or those undergoing extensive head and neck surgery.",
    [
      "is performed as high in the trachea as possible. If only one of these nerves is damaged, the patient's voice may be impaired (dysphonia); if both of the nerves are damaged, the patient will be unable to speak (aphonia). In the acute setting, indications for tracheotomy are similar to those for cricothyrotomy. In the chronic setting, indications for tracheotomy include the need for long-term mechanical ventilation and removal of tracheal secretions (e.g., comatose patients, or extensive surgery involving the head and neck). There are significant differences in airway anatomy and respiratory physiology between children and adults, and these are taken",
      "pneumomediastinum, and pneumothorax. Cricothyrotomy is used as emergency surgical access due to being fast and simple. Another surgical airway method is called tracheostomy. Tracheostomy is done in the operating room by a surgeon. This is the preferred method for patients requiring long-term ventilation. Tracheostomy uses skin puncture and dilators to insert the tracheostomy tube. Patients with respiratory arrest can be intubated without drugs. However, patients can be given sedating and paralytic drugs to minimize discomfort and help out with intubation. Pretreatment includes 100% oxygen, lidocaine, and atropine. 100% oxygen should be administered for 3 to 5 minutes. The time depends",
      "stridor is whether or not tracheal intubation or tracheostomy is immediately necessary. A reduction in oxygen saturation is considered a late sign of airway obstruction, particularly in a child with healthy lungs and normal gas exchange. Some patients will need immediate tracheal intubation. If intubation can be delayed for a period, a number of other potential options can be considered, depending on the severity of the situation and other clinical details. These include: In obese patients elevation of the panniculus has shown to relieve symptoms by 80%. Stridor Stridor (Latin for \"creaking or grating noise\") is a high-pitched breath sound"
    ]
  ],
  [
    "As exercise intensity increases, the body prioritizes different energy pathways based on the duration and intensity of the exercise being performed, with the rate of oxygen consumption and energy expenditure also increasing with higher intensity activities (Physiology of marathons).",
    [
      "and chemical impulses. In addition, the body is able to efficiently use the three energy systems which include the phosphagen system, the glycolytic system, and the oxidative system. In most cases, as the body is exposed to physical activity, the core temperature of the body tends to rise as heat gain becomes larger than the amount of heat lost. \u201cThe factors that contribute to heat gain during exercise include anything that stimulate metabolic rate, anything from the external environment that causes heat gain, and the ability of the body to dissipate heat under any given set of circumstances\u201d. In response",
      "to supply its self with enough energy to support all the corresponding changes in the body at work. The 3 energy systems involved in exercise are the Phosphogenic, Anaerobic and Aerobic energy pathways. The simultaneous action of these three energy pathways prioritizes one specific pathway over the others depending on the type of exercise an individual is partaking in. This differential prioritization is based on the duration and intensity of the particular exercise being performed. The variable usage of these energy pathways is central to the mechanisms that allow for long, sustained exercise, such as a marathon running, to be",
      "of oxygen consumption is representative of an increase in energy expenditure. VO2 is often measured in absolute terms (ex. Liters/min), but in weight bearing activities, such as running, body mass can have a profound influence on energy expenditure. As a result, it is common to express energy expenditure as the rate of oxygen consumption in relation to body mass (ex. ml/kg/min). Though some recent data may suggest otherwise, it is traditionally well accepted that a strong linear relationship exists between the rate of oxygen consumption and running speed (see figure 1), with energy expenditure increasing with increasing running speed. It"
    ]
  ],
  [
    "Hospital-acquired pneumonia is the leading cause of death among hospital-acquired infections.",
    [
      "in-hospital infections had risen from 5,972 in 2008 to 48,815 in 2017. The Centers for Disease Control and Prevention (CDC) estimated roughly 1.7 million hospital-associated infections, from all types of bacteria combined, cause or contribute to 99,000 deaths each year. Other estimates indicate 10%, or 2 million, patients a year become infected, with the annual cost ranging from $4.5 billion to $11 billion. In the USA, the most frequent type of infection hospitalwide is urinary tract infection (36%), followed by surgical site infection (20%), and bloodstream infection and pneumonia (both 11%). In 1841, Ignaz Semmelweis, a Hungarian obstetrician was working",
      "inhibitors (piperacillin/tazobactam) Hospital-acquired pneumonia Hospital-acquired pneumonia (HAP) or nosocomial pneumonia refers to any pneumonia contracted by a patient in a hospital at least 48\u201372 hours after being admitted. It is thus distinguished from community-acquired pneumonia. It is usually caused by a bacterial infection, rather than a virus. HAP is the second most common nosocomial infection (after urinary tract infections) and accounts for 15\u201320% of the total. It is the most common cause of death among nosocomial infections and is the primary cause of death in intensive care units. HAP typically lengthens a hospital stay by 1\u20132 weeks. New or progressive",
      "Hospital-acquired pneumonia Hospital-acquired pneumonia (HAP) or nosocomial pneumonia refers to any pneumonia contracted by a patient in a hospital at least 48\u201372 hours after being admitted. It is thus distinguished from community-acquired pneumonia. It is usually caused by a bacterial infection, rather than a virus. HAP is the second most common nosocomial infection (after urinary tract infections) and accounts for 15\u201320% of the total. It is the most common cause of death among nosocomial infections and is the primary cause of death in intensive care units. HAP typically lengthens a hospital stay by 1\u20132 weeks. New or progressive infiltrate on"
    ]
  ],
  [
    ", phosphocreatine acts as a rapidly mobilizable reserve of high-energy phosphates in skeletal muscle cells to recycle adenosine triphosphate, the energy currency of the cell.",
    [
      "Phosphocreatine Phosphocreatine, also known as creatine phosphate (CP) or PCr (Pcr), is a phosphorylated creatine molecule that serves as a rapidly mobilizable reserve of high-energy phosphates in skeletal muscle and the brain to recycle adenosine triphosphate, the energy currency of the cell. In the kidneys, the enzyme catalyzes the conversion of two amino acids \u2014 arginine and glycine \u2014 into guanidinoacetate (also called glycocyamine or GAA), which is then transported in the blood to the liver. A methyl group is added to GAA from the amino acid methionine by the enzyme GAMT, forming non-phosphorylated creatine. This is then released into",
      "use of phosphocreatine (PCr) through a reversible reaction with the enzyme creatine kinase (CK). In skeletal muscle, PCr concentrations may reach 20\u201335 mM or more. Additionally, in most muscles, the ATP regeneration capacity of CK is very high and is therefore not a limiting factor. Although the cellular concentrations of ATP are small, changes are difficult to detect because ATP is continuously and efficiently replenished from the large pools of PCr and CK. Creatine has the ability to increase muscle stores of PCr, potentially increasing the muscle\u2019s ability to resynthesize ATP from ADP to meet increased energy demands. Genetic deficiencies",
      "ATP (by cytoplasmic creatine kinase) to be used as energy for muscle contraction. In some vertebrates, arginine phosphate plays a similar role. Creatine phosphate shuttle The creatine phosphate shuttle is an intracellular energy shuttle which facilitates transport of high energy phosphate from muscle cell mitochondria to myofibrils. This is part of phosphocreatine metabolism. In mitochondria, ATP levels are very high as a result of glycolysis, TCA cycle, oxidative phosphorylation processes, whereas creatine phosphate levels are low. This makes conversion of creatine to phosphocreatine a highly favored reaction. Phosphocreatine is a very-high-energy compound. It then diffuses from mitochondria to myofibrils. In"
    ]
  ],
  [
    "A person's self-report of pain is the most reliable measure of pain in the healthcare setting.",
    [
      "or relationship problems may need to meet with a crisis counselor. During every shift that a nurse is on duty, they must do an assessment of the patient. If they suspect the patient is becoming addicted, they must notify the physician. Pain assessment Pain is often regarded as the fifth vital sign in regard to healthcare because it is accepted now in healthcare that pain, like other vital signs, is an objective sensation rather than subjective. As a result nurses are trained and expected to assess pain. Pain assessment and re-assessment after administration of analgesics or pain management is regulated",
      "women tolerate a lesser level of intense electric shock than Jewish or Native American women. Some individuals in all cultures have significantly higher than normal pain perception and tolerance thresholds. For instance, patients who experience painless heart attacks have higher pain thresholds for electric shock, muscle cramp and heat. A person's self-report is the most reliable measure of pain. Some health care professionals may underestimate pain severity. A definition of pain widely employed in nursing, emphasizing its subjective nature and the importance of believing patient reports, was introduced by Margo McCaffery in 1968: \"Pain is whatever the experiencing person says",
      "Documentation of the patient\u2019s pain experience is critical and may range from the use of a patient diary, (which should be patient driven), to recording pain entirely by the healthcare professional or caregiver. Effective communication between the patient and the healthcare team is fundamental to this holistic approach. The more frequently healthcare professionals measure pain, the greater the likelihood of introducing or changing pain management practices. At present there are few local options for the treatment of persistent pain, whilst managing the exudate levels present in many chronic wounds. Important properties of such local options are that they provide an"
    ]
  ]
]